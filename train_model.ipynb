{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133d5180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\envs\\tf26\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from unidecode import unidecode\n",
    "import re      #  để làm việc với biểu thức chính quy (regular expressions), để xử lý,thao tác chuỗi theo các pattern cụ thể\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5bba074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3afb4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"./dataset.txt\", on_bad_lines='skip', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3caebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||smoothie d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||smoothie d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||cà phê sữa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||cà phê sữa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||nước ép th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||nước ép th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||khay dưa h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||khay dưa h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||cơm gà xối...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||cơm gà xối...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||bánh xu nh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||bánh xu nh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||nước ép xư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||nước ép xư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||ép thơm||0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||ép thơm||0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||nước ép dư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||nước ép dư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||lục trà ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||lục trà ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||nước ép lự...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||nước ép lự...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||trà đào hồ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||trà đào hồ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||nước ép tá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||nước ép tá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||cơm thịt r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||cơm thịt r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||dưa hấu lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||dưa hấu lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||gà ủ muối ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||gà ủ muối ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||nước ép dư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||nước ép dư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||kombucha l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||kombucha l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||dưa hấu mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||dưa hấu mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||set bơ chà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||set bơ chà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||trà tắc kh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||trà tắc kh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||combo cuốn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||combo cuốn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||trà vải dư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||trà vải dư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||phô mai qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||phô mai qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ||cam vắt ly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo||cam vắt ly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0   nước ép dưa hấu siêu ngọt khổng lồ||smoothie d...\n",
       "1   nuoc ep dua hau sieu ngot khong lo||smoothie d...\n",
       "2   nước ép dưa hấu siêu ngọt khổng lồ||cà phê sữa...\n",
       "3   nuoc ep dua hau sieu ngot khong lo||cà phê sữa...\n",
       "4   nước ép dưa hấu siêu ngọt khổng lồ||nước ép th...\n",
       "5   nuoc ep dua hau sieu ngot khong lo||nước ép th...\n",
       "6   nước ép dưa hấu siêu ngọt khổng lồ||khay dưa h...\n",
       "7   nuoc ep dua hau sieu ngot khong lo||khay dưa h...\n",
       "8   nước ép dưa hấu siêu ngọt khổng lồ||cơm gà xối...\n",
       "9   nuoc ep dua hau sieu ngot khong lo||cơm gà xối...\n",
       "10  nước ép dưa hấu siêu ngọt khổng lồ||bánh xu nh...\n",
       "11  nuoc ep dua hau sieu ngot khong lo||bánh xu nh...\n",
       "12  nước ép dưa hấu siêu ngọt khổng lồ||nước ép xư...\n",
       "13  nuoc ep dua hau sieu ngot khong lo||nước ép xư...\n",
       "14     nước ép dưa hấu siêu ngọt khổng lồ||ép thơm||0\n",
       "15     nuoc ep dua hau sieu ngot khong lo||ép thơm||0\n",
       "16  nước ép dưa hấu siêu ngọt khổng lồ||nước ép dư...\n",
       "17  nuoc ep dua hau sieu ngot khong lo||nước ép dư...\n",
       "18  nước ép dưa hấu siêu ngọt khổng lồ||lục trà ch...\n",
       "19  nuoc ep dua hau sieu ngot khong lo||lục trà ch...\n",
       "20  nước ép dưa hấu siêu ngọt khổng lồ||nước ép lự...\n",
       "21  nuoc ep dua hau sieu ngot khong lo||nước ép lự...\n",
       "22  nước ép dưa hấu siêu ngọt khổng lồ||trà đào hồ...\n",
       "23  nuoc ep dua hau sieu ngot khong lo||trà đào hồ...\n",
       "24  nước ép dưa hấu siêu ngọt khổng lồ||nước ép tá...\n",
       "25  nuoc ep dua hau sieu ngot khong lo||nước ép tá...\n",
       "26  nước ép dưa hấu siêu ngọt khổng lồ||cơm thịt r...\n",
       "27  nuoc ep dua hau sieu ngot khong lo||cơm thịt r...\n",
       "28  nước ép dưa hấu siêu ngọt khổng lồ||dưa hấu lo...\n",
       "29  nuoc ep dua hau sieu ngot khong lo||dưa hấu lo...\n",
       "30  nước ép dưa hấu siêu ngọt khổng lồ||gà ủ muối ...\n",
       "31  nuoc ep dua hau sieu ngot khong lo||gà ủ muối ...\n",
       "32  nước ép dưa hấu siêu ngọt khổng lồ||nước ép dư...\n",
       "33  nuoc ep dua hau sieu ngot khong lo||nước ép dư...\n",
       "34  nước ép dưa hấu siêu ngọt khổng lồ||kombucha l...\n",
       "35  nuoc ep dua hau sieu ngot khong lo||kombucha l...\n",
       "36  nước ép dưa hấu siêu ngọt khổng lồ||dưa hấu mi...\n",
       "37  nuoc ep dua hau sieu ngot khong lo||dưa hấu mi...\n",
       "38  nước ép dưa hấu siêu ngọt khổng lồ||set bơ chà...\n",
       "39  nuoc ep dua hau sieu ngot khong lo||set bơ chà...\n",
       "40  nước ép dưa hấu siêu ngọt khổng lồ||trà tắc kh...\n",
       "41  nuoc ep dua hau sieu ngot khong lo||trà tắc kh...\n",
       "42  nước ép dưa hấu siêu ngọt khổng lồ||combo cuốn...\n",
       "43  nuoc ep dua hau sieu ngot khong lo||combo cuốn...\n",
       "44  nước ép dưa hấu siêu ngọt khổng lồ||trà vải dư...\n",
       "45  nuoc ep dua hau sieu ngot khong lo||trà vải dư...\n",
       "46  nước ép dưa hấu siêu ngọt khổng lồ||phô mai qu...\n",
       "47  nuoc ep dua hau sieu ngot khong lo||phô mai qu...\n",
       "48  nước ép dưa hấu siêu ngọt khổng lồ||cam vắt ly...\n",
       "49  nuoc ep dua hau sieu ngot khong lo||cam vắt ly..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44be2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset.txt\", sep='|', on_bad_lines='skip', header=None)\n",
    "df = df[[0,2,4]]\n",
    "df.columns = ['description','name','labels']\n",
    "df['labels'] = df['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ea754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbf767c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ</td>\n",
       "      <td>smoothie dưa hấu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo</td>\n",
       "      <td>smoothie dưa hấu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ</td>\n",
       "      <td>cà phê sữa gấu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nuoc ep dua hau sieu ngot khong lo</td>\n",
       "      <td>cà phê sữa gấu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nước ép dưa hấu siêu ngọt khổng lồ</td>\n",
       "      <td>nước ép thơm nguyên chất chai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356855</th>\n",
       "      <td>rtc canh ga sot ro ti khay</td>\n",
       "      <td>bánh tráng cuốn sốt me bơ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356856</th>\n",
       "      <td>súp rau củ trứng cút</td>\n",
       "      <td>trứng cút rim bơ tỏi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356857</th>\n",
       "      <td>sup rau cu trung cut</td>\n",
       "      <td>trứng cút rim bơ tỏi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356858</th>\n",
       "      <td>súp rau củ trứng cút</td>\n",
       "      <td>chả cả thêm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356859</th>\n",
       "      <td>sup rau cu trung cut</td>\n",
       "      <td>chả cả thêm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1356860 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                description                           name  \\\n",
       "0        nước ép dưa hấu siêu ngọt khổng lồ               smoothie dưa hấu   \n",
       "1        nuoc ep dua hau sieu ngot khong lo               smoothie dưa hấu   \n",
       "2        nước ép dưa hấu siêu ngọt khổng lồ                 cà phê sữa gấu   \n",
       "3        nuoc ep dua hau sieu ngot khong lo                 cà phê sữa gấu   \n",
       "4        nước ép dưa hấu siêu ngọt khổng lồ  nước ép thơm nguyên chất chai   \n",
       "...                                     ...                            ...   \n",
       "1356855          rtc canh ga sot ro ti khay      bánh tráng cuốn sốt me bơ   \n",
       "1356856                súp rau củ trứng cút           trứng cút rim bơ tỏi   \n",
       "1356857                sup rau cu trung cut           trứng cút rim bơ tỏi   \n",
       "1356858                súp rau củ trứng cút                    chả cả thêm   \n",
       "1356859                sup rau cu trung cut                    chả cả thêm   \n",
       "\n",
       "         labels  \n",
       "0             1  \n",
       "1             1  \n",
       "2             0  \n",
       "3             0  \n",
       "4             1  \n",
       "...         ...  \n",
       "1356855       0  \n",
       "1356856       1  \n",
       "1356857       1  \n",
       "1356858       0  \n",
       "1356859       0  \n",
       "\n",
       "[1356860 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ecc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df, remove = train_test_split(df, test_size=0.95, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2439c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba335d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92bc89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27b86dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class VietnameseDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_function(descriptions, names):\n",
    "    combined_texts = descriptions + \" [SEP] \" + names\n",
    "    return tokenizer(combined_texts.tolist(), padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[[\"description\", \"name\"]], df[\"labels\"], test_size=0.2, random_state=42, stratify=df[\"labels\"].values.tolist()\n",
    ")\n",
    "\n",
    "# Tokenize training and validation data\n",
    "train_encodings = preprocess_function(train_texts[\"description\"], train_texts[\"name\"])\n",
    "val_encodings = preprocess_function(val_texts[\"description\"], val_texts[\"name\"])\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = VietnameseDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = VietnameseDataset(val_encodings, val_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccf95602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, AutoTokenizer, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class PhoBERTWithClassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PhoBERTWithClassification, self).__init__()\n",
    "        self.phobert = RobertaModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        self.linear = torch.nn.Linear(768, 768)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # PhoBERT does not use `token_type_ids`\n",
    "        output_with_pooling = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_with_pooling[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.linear(pooler)\n",
    "        pooler = self.activation(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "class PhoBERTTrainer:\n",
    "    def __init__(self, model, tokenizer, train_dataset, val_dataset, batch_size=16, lr=5e-5, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        self.optimizer = AdamW(model.parameters(), lr=lr)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.device = device\n",
    "\n",
    "    def train_epoch(self, epoch, total_epochs):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Initialize tqdm progress bar (dynamic update frequency)\n",
    "        progress_bar = tqdm(\n",
    "            self.train_dataloader,\n",
    "            desc=f\"Epoch {epoch+1}/{total_epochs} - Training\",\n",
    "            unit=\"batch\",\n",
    "            leave=False,\n",
    "            miniters=50,  # Update bar every 10 iterations (reduces CPU load)\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        total_batches = len(self.train_dataloader)\n",
    "\n",
    "        for batch_idx, batch in enumerate(self.train_dataloader):\n",
    "            input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "            labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar only every `miniters`\n",
    "            if batch_idx % progress_bar.miniters == 0 or batch_idx == total_batches - 1:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                batches_done = batch_idx + 1\n",
    "                batches_left = total_batches - batches_done\n",
    "                time_left = elapsed_time / batches_done * batches_left\n",
    "\n",
    "                progress_bar.set_postfix(\n",
    "                    {\n",
    "                        \"Batch Loss\": f\"{loss.item():.4f}\",\n",
    "                        \"Avg Loss\": f\"{total_loss / batches_done:.4f}\",\n",
    "                        \"Time Left\": f\"{time_left / 60:.2f} min\",\n",
    "                    }\n",
    "                )\n",
    "                progress_bar.update(progress_bar.miniters)  # Advance the progress bar by `miniters`\n",
    "        return total_loss / len(self.train_dataloader)\n",
    "\n",
    "    def evaluate(self, epoch, total_epochs):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        preds, true_labels = [], []\n",
    "\n",
    "        # Initialize tqdm progress bar for evaluation\n",
    "        progress_bar = tqdm(\n",
    "            self.val_dataloader,\n",
    "            desc=f\"Epoch {epoch+1}/{total_epochs} - Evaluating\",\n",
    "            unit=\"batch\",\n",
    "            leave=False,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "                labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(true_labels, preds)\n",
    "        return total_loss / len(self.val_dataloader), accuracy\n",
    "\n",
    "    def fine_tune(self, epochs=3):\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Starting Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            train_loss = self.train_epoch(epoch, epochs)\n",
    "            val_loss, val_accuracy = self.evaluate(epoch, epochs)\n",
    "\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs} Results:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f837dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhoBERTWithClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9521546e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\envs\\tf26\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 - Training:   0%|                                                                | 0/3393 [00:00<?, ?batch/s]\u001b[AC:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14248\\3382488418.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "\n",
      "Epoch 1/3 - Training:   0%|       | 0/3393 [00:00<?, ?batch/s, Batch Loss=0.7031, Avg Loss=0.7031, Time Left=30.21 min]\u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 100/3393 [00:00<00:17, 187.15batch/s, Batch Loss=0.7031, Avg Loss=0.7031, Time Left=30.21\u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 100/3393 [00:11<00:17, 187.15batch/s, Batch Loss=0.7031, Avg Loss=0.7031, Time Left=30.21\u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 100/3393 [00:11<00:17, 187.15batch/s, Batch Loss=0.6931, Avg Loss=0.6948, Time Left=25.18\u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 101/3393 [00:11<08:32,  6.42batch/s, Batch Loss=0.6931, Avg Loss=0.6948, Time Left=25.18 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 101/3393 [00:11<08:32,  6.42batch/s, Batch Loss=0.6816, Avg Loss=0.6943, Time Left=25.24 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 102/3393 [00:11<08:53,  6.17batch/s, Batch Loss=0.6816, Avg Loss=0.6943, Time Left=25.24 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 102/3393 [00:12<08:53,  6.17batch/s, Batch Loss=0.6716, Avg Loss=0.6935, Time Left=25.24 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 103/3393 [00:12<09:19,  5.88batch/s, Batch Loss=0.6716, Avg Loss=0.6935, Time Left=25.24 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 103/3393 [00:12<09:19,  5.88batch/s, Batch Loss=0.6868, Avg Loss=0.6933, Time Left=25.26 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 104/3393 [00:12<09:57,  5.50batch/s, Batch Loss=0.6868, Avg Loss=0.6933, Time Left=25.26 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 104/3393 [00:13<09:57,  5.50batch/s, Batch Loss=0.6715, Avg Loss=0.6925, Time Left=25.29 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 105/3393 [00:13<10:42,  5.12batch/s, Batch Loss=0.6715, Avg Loss=0.6925, Time Left=25.29 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 105/3393 [00:13<10:42,  5.12batch/s, Batch Loss=0.7011, Avg Loss=0.6928, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 106/3393 [00:13<12:01,  4.55batch/s, Batch Loss=0.7011, Avg Loss=0.6928, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 106/3393 [00:14<12:01,  4.55batch/s, Batch Loss=0.6733, Avg Loss=0.6922, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 107/3393 [00:14<13:11,  4.15batch/s, Batch Loss=0.6733, Avg Loss=0.6922, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 107/3393 [00:14<13:11,  4.15batch/s, Batch Loss=0.6945, Avg Loss=0.6922, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 108/3393 [00:14<14:42,  3.72batch/s, Batch Loss=0.6945, Avg Loss=0.6922, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 108/3393 [00:15<14:42,  3.72batch/s, Batch Loss=0.7211, Avg Loss=0.6931, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 109/3393 [00:15<16:04,  3.41batch/s, Batch Loss=0.7211, Avg Loss=0.6931, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 109/3393 [00:15<16:04,  3.41batch/s, Batch Loss=0.7391, Avg Loss=0.6945, Time Left=25.47 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 110/3393 [00:15<17:23,  3.15batch/s, Batch Loss=0.7391, Avg Loss=0.6945, Time Left=25.47 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 110/3393 [00:15<17:23,  3.15batch/s, Batch Loss=0.6567, Avg Loss=0.6934, Time Left=25.55 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 111/3393 [00:15<19:26,  2.81batch/s, Batch Loss=0.6567, Avg Loss=0.6934, Time Left=25.55 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 111/3393 [00:16<19:26,  2.81batch/s, Batch Loss=0.6600, Avg Loss=0.6925, Time Left=25.55 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 112/3393 [00:16<20:38,  2.65batch/s, Batch Loss=0.6600, Avg Loss=0.6925, Time Left=25.55 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 112/3393 [00:16<20:38,  2.65batch/s, Batch Loss=0.6979, Avg Loss=0.6926, Time Left=25.57 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 113/3393 [00:16<21:53,  2.50batch/s, Batch Loss=0.6979, Avg Loss=0.6926, Time Left=25.57 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 113/3393 [00:17<21:53,  2.50batch/s, Batch Loss=0.7086, Avg Loss=0.6930, Time Left=25.58 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 114/3393 [00:17<22:47,  2.40batch/s, Batch Loss=0.7086, Avg Loss=0.6930, Time Left=25.58 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 114/3393 [00:17<22:47,  2.40batch/s, Batch Loss=0.6963, Avg Loss=0.6931, Time Left=25.67 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 115/3393 [00:17<24:22,  2.24batch/s, Batch Loss=0.6963, Avg Loss=0.6931, Time Left=25.67 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 115/3393 [00:18<24:22,  2.24batch/s, Batch Loss=0.6786, Avg Loss=0.6927, Time Left=25.64 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 116/3393 [00:18<24:35,  2.22batch/s, Batch Loss=0.6786, Avg Loss=0.6927, Time Left=25.64 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 116/3393 [00:18<24:35,  2.22batch/s, Batch Loss=0.7051, Avg Loss=0.6930, Time Left=25.64 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 117/3393 [00:18<24:32,  2.22batch/s, Batch Loss=0.7051, Avg Loss=0.6930, Time Left=25.64 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 117/3393 [00:19<24:32,  2.22batch/s, Batch Loss=0.6862, Avg Loss=0.6929, Time Left=25.66 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 118/3393 [00:19<25:01,  2.18batch/s, Batch Loss=0.6862, Avg Loss=0.6929, Time Left=25.66 \u001b[A\n",
      "Epoch 1/3 - Training:   3%| | 118/3393 [00:19<25:01,  2.18batch/s, Batch Loss=0.6758, Avg Loss=0.6925, Time Left=25.65 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 119/3393 [00:19<24:55,  2.19batch/s, Batch Loss=0.6758, Avg Loss=0.6925, Time Left=25.65 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 119/3393 [00:20<24:55,  2.19batch/s, Batch Loss=0.7089, Avg Loss=0.6929, Time Left=25.65 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 120/3393 [00:20<25:04,  2.18batch/s, Batch Loss=0.7089, Avg Loss=0.6929, Time Left=25.65 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 120/3393 [00:20<25:04,  2.18batch/s, Batch Loss=0.6984, Avg Loss=0.6930, Time Left=25.74 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 121/3393 [00:20<26:21,  2.07batch/s, Batch Loss=0.6984, Avg Loss=0.6930, Time Left=25.74 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 121/3393 [00:21<26:21,  2.07batch/s, Batch Loss=0.6554, Avg Loss=0.6922, Time Left=25.76 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 122/3393 [00:21<26:19,  2.07batch/s, Batch Loss=0.6554, Avg Loss=0.6922, Time Left=25.76 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 122/3393 [00:21<26:19,  2.07batch/s, Batch Loss=0.6908, Avg Loss=0.6921, Time Left=25.74 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 123/3393 [00:21<25:54,  2.10batch/s, Batch Loss=0.6908, Avg Loss=0.6921, Time Left=25.74 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 123/3393 [00:22<25:54,  2.10batch/s, Batch Loss=0.6976, Avg Loss=0.6923, Time Left=25.80 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 124/3393 [00:22<26:37,  2.05batch/s, Batch Loss=0.6976, Avg Loss=0.6923, Time Left=25.80 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 124/3393 [00:22<26:37,  2.05batch/s, Batch Loss=0.6935, Avg Loss=0.6923, Time Left=25.81 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 125/3393 [00:22<26:21,  2.07batch/s, Batch Loss=0.6935, Avg Loss=0.6923, Time Left=25.81 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 125/3393 [00:23<26:21,  2.07batch/s, Batch Loss=0.6625, Avg Loss=0.6917, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 126/3393 [00:23<26:44,  2.04batch/s, Batch Loss=0.6625, Avg Loss=0.6917, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 126/3393 [00:23<26:44,  2.04batch/s, Batch Loss=0.6967, Avg Loss=0.6918, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 127/3393 [00:23<26:23,  2.06batch/s, Batch Loss=0.6967, Avg Loss=0.6918, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 127/3393 [00:24<26:23,  2.06batch/s, Batch Loss=0.7278, Avg Loss=0.6925, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 128/3393 [00:24<26:13,  2.07batch/s, Batch Loss=0.7278, Avg Loss=0.6925, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 128/3393 [00:24<26:13,  2.07batch/s, Batch Loss=0.6671, Avg Loss=0.6920, Time Left=25.88 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 129/3393 [00:24<26:25,  2.06batch/s, Batch Loss=0.6671, Avg Loss=0.6920, Time Left=25.88 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 129/3393 [00:25<26:25,  2.06batch/s, Batch Loss=0.6965, Avg Loss=0.6921, Time Left=25.90 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   4%| | 130/3393 [00:25<26:28,  2.05batch/s, Batch Loss=0.6965, Avg Loss=0.6921, Time Left=25.90 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 130/3393 [00:25<26:28,  2.05batch/s, Batch Loss=0.6528, Avg Loss=0.6914, Time Left=25.89 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 131/3393 [00:25<26:11,  2.08batch/s, Batch Loss=0.6528, Avg Loss=0.6914, Time Left=25.89 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 131/3393 [00:26<26:11,  2.08batch/s, Batch Loss=0.6710, Avg Loss=0.6910, Time Left=25.93 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 132/3393 [00:26<26:40,  2.04batch/s, Batch Loss=0.6710, Avg Loss=0.6910, Time Left=25.93 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 132/3393 [00:26<26:40,  2.04batch/s, Batch Loss=0.6814, Avg Loss=0.6908, Time Left=25.93 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 133/3393 [00:26<26:21,  2.06batch/s, Batch Loss=0.6814, Avg Loss=0.6908, Time Left=25.93 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 133/3393 [00:27<26:21,  2.06batch/s, Batch Loss=0.6709, Avg Loss=0.6905, Time Left=25.96 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 134/3393 [00:27<26:43,  2.03batch/s, Batch Loss=0.6709, Avg Loss=0.6905, Time Left=25.96 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 134/3393 [00:27<26:43,  2.03batch/s, Batch Loss=0.6845, Avg Loss=0.6904, Time Left=25.95 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 135/3393 [00:27<26:11,  2.07batch/s, Batch Loss=0.6845, Avg Loss=0.6904, Time Left=25.95 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 135/3393 [00:28<26:11,  2.07batch/s, Batch Loss=0.6913, Avg Loss=0.6904, Time Left=26.03 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 136/3393 [00:28<27:29,  1.97batch/s, Batch Loss=0.6913, Avg Loss=0.6904, Time Left=26.03 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 136/3393 [00:28<27:29,  1.97batch/s, Batch Loss=0.6385, Avg Loss=0.6895, Time Left=26.07 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 137/3393 [00:28<27:42,  1.96batch/s, Batch Loss=0.6385, Avg Loss=0.6895, Time Left=26.07 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 137/3393 [00:29<27:42,  1.96batch/s, Batch Loss=0.6693, Avg Loss=0.6892, Time Left=26.09 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 138/3393 [00:29<27:44,  1.96batch/s, Batch Loss=0.6693, Avg Loss=0.6892, Time Left=26.09 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 138/3393 [00:29<27:44,  1.96batch/s, Batch Loss=0.6769, Avg Loss=0.6890, Time Left=26.15 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 139/3393 [00:29<28:15,  1.92batch/s, Batch Loss=0.6769, Avg Loss=0.6890, Time Left=26.15 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 139/3393 [00:30<28:15,  1.92batch/s, Batch Loss=0.7162, Avg Loss=0.6894, Time Left=26.17 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 140/3393 [00:30<27:50,  1.95batch/s, Batch Loss=0.7162, Avg Loss=0.6894, Time Left=26.17 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 140/3393 [00:30<27:50,  1.95batch/s, Batch Loss=0.6714, Avg Loss=0.6892, Time Left=26.23 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 141/3393 [00:30<28:40,  1.89batch/s, Batch Loss=0.6714, Avg Loss=0.6892, Time Left=26.23 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 141/3393 [00:31<28:40,  1.89batch/s, Batch Loss=0.6803, Avg Loss=0.6890, Time Left=26.27 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 142/3393 [00:31<28:36,  1.89batch/s, Batch Loss=0.6803, Avg Loss=0.6890, Time Left=26.27 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 142/3393 [00:31<28:36,  1.89batch/s, Batch Loss=0.6641, Avg Loss=0.6887, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 143/3393 [00:31<27:27,  1.97batch/s, Batch Loss=0.6641, Avg Loss=0.6887, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 143/3393 [00:32<27:27,  1.97batch/s, Batch Loss=0.6913, Avg Loss=0.6887, Time Left=26.26 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 144/3393 [00:32<27:11,  1.99batch/s, Batch Loss=0.6913, Avg Loss=0.6887, Time Left=26.26 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 144/3393 [00:32<27:11,  1.99batch/s, Batch Loss=0.6684, Avg Loss=0.6884, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 145/3393 [00:32<26:55,  2.01batch/s, Batch Loss=0.6684, Avg Loss=0.6884, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 145/3393 [00:33<26:55,  2.01batch/s, Batch Loss=0.7116, Avg Loss=0.6887, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 146/3393 [00:33<26:09,  2.07batch/s, Batch Loss=0.7116, Avg Loss=0.6887, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 146/3393 [00:33<26:09,  2.07batch/s, Batch Loss=0.6074, Avg Loss=0.6876, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 147/3393 [00:33<26:24,  2.05batch/s, Batch Loss=0.6074, Avg Loss=0.6876, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 147/3393 [00:34<26:24,  2.05batch/s, Batch Loss=0.6303, Avg Loss=0.6868, Time Left=26.28 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 148/3393 [00:34<27:15,  1.98batch/s, Batch Loss=0.6303, Avg Loss=0.6868, Time Left=26.28 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 148/3393 [00:34<27:15,  1.98batch/s, Batch Loss=0.7057, Avg Loss=0.6871, Time Left=26.34 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 149/3393 [00:34<27:51,  1.94batch/s, Batch Loss=0.7057, Avg Loss=0.6871, Time Left=26.34 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 149/3393 [00:35<27:51,  1.94batch/s, Batch Loss=0.6104, Avg Loss=0.6860, Time Left=26.32 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 150/3393 [00:35<26:58,  2.00batch/s, Batch Loss=0.6104, Avg Loss=0.6860, Time Left=26.32 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 150/3393 [00:35<26:58,  2.00batch/s, Batch Loss=0.6886, Avg Loss=0.6861, Time Left=26.38 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 151/3393 [00:35<28:05,  1.92batch/s, Batch Loss=0.6886, Avg Loss=0.6861, Time Left=26.38 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 151/3393 [00:36<28:05,  1.92batch/s, Batch Loss=0.7473, Avg Loss=0.6869, Time Left=26.36 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 152/3393 [00:36<27:14,  1.98batch/s, Batch Loss=0.7473, Avg Loss=0.6869, Time Left=26.36 \u001b[A\n",
      "Epoch 1/3 - Training:   4%| | 152/3393 [00:36<27:14,  1.98batch/s, Batch Loss=0.7241, Avg Loss=0.6873, Time Left=26.38 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 153/3393 [00:36<27:24,  1.97batch/s, Batch Loss=0.7241, Avg Loss=0.6873, Time Left=26.38 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 153/3393 [00:37<27:24,  1.97batch/s, Batch Loss=0.6553, Avg Loss=0.6869, Time Left=26.39 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 154/3393 [00:37<27:17,  1.98batch/s, Batch Loss=0.6553, Avg Loss=0.6869, Time Left=26.39 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 154/3393 [00:37<27:17,  1.98batch/s, Batch Loss=0.6337, Avg Loss=0.6863, Time Left=26.44 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 155/3393 [00:37<28:15,  1.91batch/s, Batch Loss=0.6337, Avg Loss=0.6863, Time Left=26.44 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 155/3393 [00:38<28:15,  1.91batch/s, Batch Loss=0.6328, Avg Loss=0.6856, Time Left=26.49 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 156/3393 [00:38<28:40,  1.88batch/s, Batch Loss=0.6328, Avg Loss=0.6856, Time Left=26.49 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 156/3393 [00:38<28:40,  1.88batch/s, Batch Loss=0.6317, Avg Loss=0.6849, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 157/3393 [00:38<27:45,  1.94batch/s, Batch Loss=0.6317, Avg Loss=0.6849, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 157/3393 [00:39<27:45,  1.94batch/s, Batch Loss=0.7090, Avg Loss=0.6852, Time Left=26.49 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 158/3393 [00:39<27:46,  1.94batch/s, Batch Loss=0.7090, Avg Loss=0.6852, Time Left=26.49 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 158/3393 [00:39<27:46,  1.94batch/s, Batch Loss=0.6911, Avg Loss=0.6853, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 159/3393 [00:39<27:00,  2.00batch/s, Batch Loss=0.6911, Avg Loss=0.6853, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 159/3393 [00:40<27:00,  2.00batch/s, Batch Loss=0.8044, Avg Loss=0.6867, Time Left=26.46 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 160/3393 [00:40<26:25,  2.04batch/s, Batch Loss=0.8044, Avg Loss=0.6867, Time Left=26.46 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 160/3393 [00:40<26:25,  2.04batch/s, Batch Loss=0.6058, Avg Loss=0.6858, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 161/3393 [00:40<26:12,  2.06batch/s, Batch Loss=0.6058, Avg Loss=0.6858, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 161/3393 [00:41<26:12,  2.06batch/s, Batch Loss=0.6550, Avg Loss=0.6854, Time Left=26.44 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 162/3393 [00:41<26:08,  2.06batch/s, Batch Loss=0.6550, Avg Loss=0.6854, Time Left=26.44 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 162/3393 [00:41<26:08,  2.06batch/s, Batch Loss=0.6688, Avg Loss=0.6852, Time Left=26.43 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   5%| | 163/3393 [00:41<26:01,  2.07batch/s, Batch Loss=0.6688, Avg Loss=0.6852, Time Left=26.43 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 163/3393 [00:42<26:01,  2.07batch/s, Batch Loss=0.6218, Avg Loss=0.6845, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 164/3393 [00:42<26:33,  2.03batch/s, Batch Loss=0.6218, Avg Loss=0.6845, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 164/3393 [00:42<26:33,  2.03batch/s, Batch Loss=0.6264, Avg Loss=0.6838, Time Left=26.43 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 165/3393 [00:42<26:19,  2.04batch/s, Batch Loss=0.6264, Avg Loss=0.6838, Time Left=26.43 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 165/3393 [00:43<26:19,  2.04batch/s, Batch Loss=0.6349, Avg Loss=0.6833, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 166/3393 [00:43<25:43,  2.09batch/s, Batch Loss=0.6349, Avg Loss=0.6833, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 166/3393 [00:43<25:43,  2.09batch/s, Batch Loss=0.6096, Avg Loss=0.6825, Time Left=26.43 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 167/3393 [00:43<26:32,  2.03batch/s, Batch Loss=0.6096, Avg Loss=0.6825, Time Left=26.43 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 167/3393 [00:44<26:32,  2.03batch/s, Batch Loss=0.6314, Avg Loss=0.6819, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 168/3393 [00:44<26:09,  2.05batch/s, Batch Loss=0.6314, Avg Loss=0.6819, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 168/3393 [00:44<26:09,  2.05batch/s, Batch Loss=0.6960, Avg Loss=0.6821, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 169/3393 [00:44<27:04,  1.98batch/s, Batch Loss=0.6960, Avg Loss=0.6821, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 169/3393 [00:45<27:04,  1.98batch/s, Batch Loss=0.6604, Avg Loss=0.6818, Time Left=26.43 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 170/3393 [00:45<26:16,  2.04batch/s, Batch Loss=0.6604, Avg Loss=0.6818, Time Left=26.43 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 170/3393 [00:45<26:16,  2.04batch/s, Batch Loss=0.6928, Avg Loss=0.6820, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 171/3393 [00:45<25:57,  2.07batch/s, Batch Loss=0.6928, Avg Loss=0.6820, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 171/3393 [00:46<25:57,  2.07batch/s, Batch Loss=0.5445, Avg Loss=0.6805, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 172/3393 [00:46<26:17,  2.04batch/s, Batch Loss=0.5445, Avg Loss=0.6805, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 172/3393 [00:46<26:17,  2.04batch/s, Batch Loss=0.5803, Avg Loss=0.6795, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 173/3393 [00:46<26:17,  2.04batch/s, Batch Loss=0.5803, Avg Loss=0.6795, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 173/3393 [00:47<26:17,  2.04batch/s, Batch Loss=0.5760, Avg Loss=0.6784, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 174/3393 [00:47<27:09,  1.98batch/s, Batch Loss=0.5760, Avg Loss=0.6784, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 174/3393 [00:47<27:09,  1.98batch/s, Batch Loss=0.6361, Avg Loss=0.6780, Time Left=26.48 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 175/3393 [00:47<28:00,  1.91batch/s, Batch Loss=0.6361, Avg Loss=0.6780, Time Left=26.48 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 175/3393 [00:48<28:00,  1.91batch/s, Batch Loss=0.7261, Avg Loss=0.6785, Time Left=26.46 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 176/3393 [00:48<27:06,  1.98batch/s, Batch Loss=0.7261, Avg Loss=0.6785, Time Left=26.46 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 176/3393 [00:48<27:06,  1.98batch/s, Batch Loss=0.6368, Avg Loss=0.6781, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 177/3393 [00:48<27:25,  1.95batch/s, Batch Loss=0.6368, Avg Loss=0.6781, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 177/3393 [00:49<27:25,  1.95batch/s, Batch Loss=0.6511, Avg Loss=0.6778, Time Left=26.50 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 178/3393 [00:49<27:53,  1.92batch/s, Batch Loss=0.6511, Avg Loss=0.6778, Time Left=26.50 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 178/3393 [00:49<27:53,  1.92batch/s, Batch Loss=0.6590, Avg Loss=0.6776, Time Left=26.49 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 179/3393 [00:49<27:19,  1.96batch/s, Batch Loss=0.6590, Avg Loss=0.6776, Time Left=26.49 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 179/3393 [00:50<27:19,  1.96batch/s, Batch Loss=0.5510, Avg Loss=0.6764, Time Left=26.49 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 180/3393 [00:50<26:51,  1.99batch/s, Batch Loss=0.5510, Avg Loss=0.6764, Time Left=26.49 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 180/3393 [00:50<26:51,  1.99batch/s, Batch Loss=0.6227, Avg Loss=0.6759, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 181/3393 [00:50<26:20,  2.03batch/s, Batch Loss=0.6227, Avg Loss=0.6759, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 181/3393 [00:51<26:20,  2.03batch/s, Batch Loss=0.6913, Avg Loss=0.6761, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 182/3393 [00:51<26:10,  2.04batch/s, Batch Loss=0.6913, Avg Loss=0.6761, Time Left=26.47 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 182/3393 [00:51<26:10,  2.04batch/s, Batch Loss=0.6732, Avg Loss=0.6760, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 183/3393 [00:51<25:49,  2.07batch/s, Batch Loss=0.6732, Avg Loss=0.6760, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 183/3393 [00:52<25:49,  2.07batch/s, Batch Loss=0.4749, Avg Loss=0.6742, Time Left=26.46 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 184/3393 [00:52<26:25,  2.02batch/s, Batch Loss=0.4749, Avg Loss=0.6742, Time Left=26.46 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 184/3393 [00:52<26:25,  2.02batch/s, Batch Loss=0.5677, Avg Loss=0.6732, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 185/3393 [00:52<26:13,  2.04batch/s, Batch Loss=0.5677, Avg Loss=0.6732, Time Left=26.45 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 185/3393 [00:53<26:13,  2.04batch/s, Batch Loss=0.5039, Avg Loss=0.6716, Time Left=26.44 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 186/3393 [00:53<25:48,  2.07batch/s, Batch Loss=0.5039, Avg Loss=0.6716, Time Left=26.44 \u001b[A\n",
      "Epoch 1/3 - Training:   5%| | 186/3393 [00:53<25:48,  2.07batch/s, Batch Loss=0.5620, Avg Loss=0.6707, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 187/3393 [00:53<25:33,  2.09batch/s, Batch Loss=0.5620, Avg Loss=0.6707, Time Left=26.42 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 187/3393 [00:54<25:33,  2.09batch/s, Batch Loss=0.5402, Avg Loss=0.6695, Time Left=26.43 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 188/3393 [00:54<26:06,  2.05batch/s, Batch Loss=0.5402, Avg Loss=0.6695, Time Left=26.43 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 188/3393 [00:54<26:06,  2.05batch/s, Batch Loss=0.4806, Avg Loss=0.6678, Time Left=26.40 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 189/3393 [00:54<25:32,  2.09batch/s, Batch Loss=0.4806, Avg Loss=0.6678, Time Left=26.40 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 189/3393 [00:55<25:32,  2.09batch/s, Batch Loss=0.5294, Avg Loss=0.6666, Time Left=26.39 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 190/3393 [00:55<25:31,  2.09batch/s, Batch Loss=0.5294, Avg Loss=0.6666, Time Left=26.39 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 190/3393 [00:55<25:31,  2.09batch/s, Batch Loss=0.4210, Avg Loss=0.6645, Time Left=26.39 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 191/3393 [00:55<25:56,  2.06batch/s, Batch Loss=0.4210, Avg Loss=0.6645, Time Left=26.39 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 191/3393 [00:56<25:56,  2.06batch/s, Batch Loss=0.7364, Avg Loss=0.6651, Time Left=26.37 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 192/3393 [00:56<25:23,  2.10batch/s, Batch Loss=0.7364, Avg Loss=0.6651, Time Left=26.37 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 192/3393 [00:56<25:23,  2.10batch/s, Batch Loss=0.5337, Avg Loss=0.6640, Time Left=26.36 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 193/3393 [00:56<25:13,  2.11batch/s, Batch Loss=0.5337, Avg Loss=0.6640, Time Left=26.36 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 193/3393 [00:56<25:13,  2.11batch/s, Batch Loss=0.5731, Avg Loss=0.6632, Time Left=26.36 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 194/3393 [00:56<25:39,  2.08batch/s, Batch Loss=0.5731, Avg Loss=0.6632, Time Left=26.36 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 194/3393 [00:57<25:39,  2.08batch/s, Batch Loss=0.5067, Avg Loss=0.6619, Time Left=26.34 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 195/3393 [00:57<25:26,  2.10batch/s, Batch Loss=0.5067, Avg Loss=0.6619, Time Left=26.34 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 195/3393 [00:57<25:26,  2.10batch/s, Batch Loss=0.2866, Avg Loss=0.6588, Time Left=26.33 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   6%| | 196/3393 [00:57<25:30,  2.09batch/s, Batch Loss=0.2866, Avg Loss=0.6588, Time Left=26.33 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 196/3393 [00:58<25:30,  2.09batch/s, Batch Loss=0.4623, Avg Loss=0.6571, Time Left=26.33 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 197/3393 [00:58<25:51,  2.06batch/s, Batch Loss=0.4623, Avg Loss=0.6571, Time Left=26.33 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 197/3393 [00:58<25:51,  2.06batch/s, Batch Loss=0.6178, Avg Loss=0.6568, Time Left=26.33 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 198/3393 [00:58<25:48,  2.06batch/s, Batch Loss=0.6178, Avg Loss=0.6568, Time Left=26.33 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 198/3393 [00:59<25:48,  2.06batch/s, Batch Loss=0.3994, Avg Loss=0.6547, Time Left=26.31 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 199/3393 [00:59<25:31,  2.09batch/s, Batch Loss=0.3994, Avg Loss=0.6547, Time Left=26.31 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 199/3393 [00:59<25:31,  2.09batch/s, Batch Loss=0.4241, Avg Loss=0.6529, Time Left=26.32 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 200/3393 [00:59<26:05,  2.04batch/s, Batch Loss=0.4241, Avg Loss=0.6529, Time Left=26.32 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 200/3393 [01:00<26:05,  2.04batch/s, Batch Loss=0.4170, Avg Loss=0.6510, Time Left=26.30 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 201/3393 [01:00<26:00,  2.05batch/s, Batch Loss=0.4170, Avg Loss=0.6510, Time Left=26.30 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 201/3393 [01:00<26:00,  2.05batch/s, Batch Loss=0.4955, Avg Loss=0.6497, Time Left=26.30 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 202/3393 [01:00<26:08,  2.03batch/s, Batch Loss=0.4955, Avg Loss=0.6497, Time Left=26.30 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 202/3393 [01:01<26:08,  2.03batch/s, Batch Loss=0.4252, Avg Loss=0.6480, Time Left=26.29 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 203/3393 [01:01<25:31,  2.08batch/s, Batch Loss=0.4252, Avg Loss=0.6480, Time Left=26.29 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 203/3393 [01:01<25:31,  2.08batch/s, Batch Loss=0.4845, Avg Loss=0.6467, Time Left=26.27 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 204/3393 [01:01<25:17,  2.10batch/s, Batch Loss=0.4845, Avg Loss=0.6467, Time Left=26.27 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 204/3393 [01:02<25:17,  2.10batch/s, Batch Loss=0.5152, Avg Loss=0.6457, Time Left=26.27 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 205/3393 [01:02<25:38,  2.07batch/s, Batch Loss=0.5152, Avg Loss=0.6457, Time Left=26.27 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 205/3393 [01:02<25:38,  2.07batch/s, Batch Loss=0.6362, Avg Loss=0.6456, Time Left=26.26 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 206/3393 [01:02<25:26,  2.09batch/s, Batch Loss=0.6362, Avg Loss=0.6456, Time Left=26.26 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 206/3393 [01:03<25:26,  2.09batch/s, Batch Loss=0.7386, Avg Loss=0.6463, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 207/3393 [01:03<25:14,  2.10batch/s, Batch Loss=0.7386, Avg Loss=0.6463, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 207/3393 [01:03<25:14,  2.10batch/s, Batch Loss=0.4173, Avg Loss=0.6446, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 208/3393 [01:03<25:36,  2.07batch/s, Batch Loss=0.4173, Avg Loss=0.6446, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 208/3393 [01:04<25:36,  2.07batch/s, Batch Loss=0.7344, Avg Loss=0.6453, Time Left=26.22 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 209/3393 [01:04<25:08,  2.11batch/s, Batch Loss=0.7344, Avg Loss=0.6453, Time Left=26.22 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 209/3393 [01:04<25:08,  2.11batch/s, Batch Loss=0.4499, Avg Loss=0.6438, Time Left=26.21 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 210/3393 [01:04<25:14,  2.10batch/s, Batch Loss=0.4499, Avg Loss=0.6438, Time Left=26.21 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 210/3393 [01:05<25:14,  2.10batch/s, Batch Loss=0.4311, Avg Loss=0.6422, Time Left=26.21 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 211/3393 [01:05<25:36,  2.07batch/s, Batch Loss=0.4311, Avg Loss=0.6422, Time Left=26.21 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 211/3393 [01:05<25:36,  2.07batch/s, Batch Loss=0.4084, Avg Loss=0.6405, Time Left=26.20 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 212/3393 [01:05<25:24,  2.09batch/s, Batch Loss=0.4084, Avg Loss=0.6405, Time Left=26.20 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 212/3393 [01:06<25:24,  2.09batch/s, Batch Loss=0.4343, Avg Loss=0.6390, Time Left=26.20 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 213/3393 [01:06<25:56,  2.04batch/s, Batch Loss=0.4343, Avg Loss=0.6390, Time Left=26.20 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 213/3393 [01:06<25:56,  2.04batch/s, Batch Loss=0.4619, Avg Loss=0.6377, Time Left=26.23 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 214/3393 [01:06<27:11,  1.95batch/s, Batch Loss=0.4619, Avg Loss=0.6377, Time Left=26.23 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 214/3393 [01:07<27:11,  1.95batch/s, Batch Loss=0.3778, Avg Loss=0.6358, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 215/3393 [01:07<27:56,  1.90batch/s, Batch Loss=0.3778, Avg Loss=0.6358, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 215/3393 [01:07<27:56,  1.90batch/s, Batch Loss=0.4359, Avg Loss=0.6344, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 216/3393 [01:07<27:22,  1.93batch/s, Batch Loss=0.4359, Avg Loss=0.6344, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 216/3393 [01:08<27:22,  1.93batch/s, Batch Loss=0.4532, Avg Loss=0.6331, Time Left=26.27 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 217/3393 [01:08<28:01,  1.89batch/s, Batch Loss=0.4532, Avg Loss=0.6331, Time Left=26.27 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 217/3393 [01:08<28:01,  1.89batch/s, Batch Loss=0.4047, Avg Loss=0.6315, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 218/3393 [01:08<26:57,  1.96batch/s, Batch Loss=0.4047, Avg Loss=0.6315, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 218/3393 [01:09<26:57,  1.96batch/s, Batch Loss=0.3281, Avg Loss=0.6294, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 219/3393 [01:09<27:10,  1.95batch/s, Batch Loss=0.3281, Avg Loss=0.6294, Time Left=26.25 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 219/3393 [01:09<27:10,  1.95batch/s, Batch Loss=0.5781, Avg Loss=0.6290, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 220/3393 [01:09<26:13,  2.02batch/s, Batch Loss=0.5781, Avg Loss=0.6290, Time Left=26.24 \u001b[A\n",
      "Epoch 1/3 - Training:   6%| | 220/3393 [01:10<26:13,  2.02batch/s, Batch Loss=0.5361, Avg Loss=0.6284, Time Left=26.23 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 221/3393 [01:10<26:15,  2.01batch/s, Batch Loss=0.5361, Avg Loss=0.6284, Time Left=26.23 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 221/3393 [01:10<26:15,  2.01batch/s, Batch Loss=0.5462, Avg Loss=0.6278, Time Left=26.22 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 222/3393 [01:10<25:48,  2.05batch/s, Batch Loss=0.5462, Avg Loss=0.6278, Time Left=26.22 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 222/3393 [01:11<25:48,  2.05batch/s, Batch Loss=0.4852, Avg Loss=0.6269, Time Left=26.21 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 223/3393 [01:11<25:44,  2.05batch/s, Batch Loss=0.4852, Avg Loss=0.6269, Time Left=26.21 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 223/3393 [01:11<25:44,  2.05batch/s, Batch Loss=0.2553, Avg Loss=0.6244, Time Left=26.21 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 224/3393 [01:11<26:05,  2.02batch/s, Batch Loss=0.2553, Avg Loss=0.6244, Time Left=26.21 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 224/3393 [01:12<26:05,  2.02batch/s, Batch Loss=0.5069, Avg Loss=0.6236, Time Left=26.19 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 225/3393 [01:12<25:15,  2.09batch/s, Batch Loss=0.5069, Avg Loss=0.6236, Time Left=26.19 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 225/3393 [01:12<25:15,  2.09batch/s, Batch Loss=0.3964, Avg Loss=0.6221, Time Left=26.18 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 226/3393 [01:12<25:17,  2.09batch/s, Batch Loss=0.3964, Avg Loss=0.6221, Time Left=26.18 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 226/3393 [01:13<25:17,  2.09batch/s, Batch Loss=0.3262, Avg Loss=0.6201, Time Left=26.18 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 227/3393 [01:13<25:35,  2.06batch/s, Batch Loss=0.3262, Avg Loss=0.6201, Time Left=26.18 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 227/3393 [01:13<25:35,  2.06batch/s, Batch Loss=0.4443, Avg Loss=0.6189, Time Left=26.16 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 228/3393 [01:13<25:19,  2.08batch/s, Batch Loss=0.4443, Avg Loss=0.6189, Time Left=26.16 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 228/3393 [01:14<25:19,  2.08batch/s, Batch Loss=0.3709, Avg Loss=0.6173, Time Left=26.15 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   7%| | 229/3393 [01:14<25:20,  2.08batch/s, Batch Loss=0.3709, Avg Loss=0.6173, Time Left=26.15 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 229/3393 [01:14<25:20,  2.08batch/s, Batch Loss=0.4393, Avg Loss=0.6162, Time Left=26.15 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 230/3393 [01:14<25:24,  2.08batch/s, Batch Loss=0.4393, Avg Loss=0.6162, Time Left=26.15 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 230/3393 [01:15<25:24,  2.08batch/s, Batch Loss=0.3010, Avg Loss=0.6141, Time Left=26.13 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 231/3393 [01:15<25:11,  2.09batch/s, Batch Loss=0.3010, Avg Loss=0.6141, Time Left=26.13 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 231/3393 [01:15<25:11,  2.09batch/s, Batch Loss=0.3672, Avg Loss=0.6125, Time Left=26.13 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 232/3393 [01:15<25:15,  2.09batch/s, Batch Loss=0.3672, Avg Loss=0.6125, Time Left=26.13 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 232/3393 [01:16<25:15,  2.09batch/s, Batch Loss=0.3015, Avg Loss=0.6106, Time Left=26.13 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 233/3393 [01:16<25:57,  2.03batch/s, Batch Loss=0.3015, Avg Loss=0.6106, Time Left=26.13 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 233/3393 [01:16<25:57,  2.03batch/s, Batch Loss=0.4569, Avg Loss=0.6096, Time Left=26.12 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 234/3393 [01:16<25:42,  2.05batch/s, Batch Loss=0.4569, Avg Loss=0.6096, Time Left=26.12 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 234/3393 [01:17<25:42,  2.05batch/s, Batch Loss=0.3336, Avg Loss=0.6079, Time Left=26.11 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 235/3393 [01:17<25:25,  2.07batch/s, Batch Loss=0.3336, Avg Loss=0.6079, Time Left=26.11 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 235/3393 [01:17<25:25,  2.07batch/s, Batch Loss=0.7528, Avg Loss=0.6088, Time Left=26.09 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 236/3393 [01:17<25:06,  2.10batch/s, Batch Loss=0.7528, Avg Loss=0.6088, Time Left=26.09 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 236/3393 [01:17<25:06,  2.10batch/s, Batch Loss=0.2833, Avg Loss=0.6067, Time Left=26.08 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 237/3393 [01:17<25:06,  2.10batch/s, Batch Loss=0.2833, Avg Loss=0.6067, Time Left=26.08 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 237/3393 [01:18<25:06,  2.10batch/s, Batch Loss=0.5000, Avg Loss=0.6061, Time Left=26.07 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 238/3393 [01:18<25:00,  2.10batch/s, Batch Loss=0.5000, Avg Loss=0.6061, Time Left=26.07 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 238/3393 [01:18<25:00,  2.10batch/s, Batch Loss=0.7119, Avg Loss=0.6067, Time Left=26.07 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 239/3393 [01:18<25:37,  2.05batch/s, Batch Loss=0.7119, Avg Loss=0.6067, Time Left=26.07 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 239/3393 [01:19<25:37,  2.05batch/s, Batch Loss=0.2930, Avg Loss=0.6048, Time Left=26.06 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 240/3393 [01:19<25:26,  2.07batch/s, Batch Loss=0.2930, Avg Loss=0.6048, Time Left=26.06 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 240/3393 [01:19<25:26,  2.07batch/s, Batch Loss=0.2593, Avg Loss=0.6027, Time Left=26.06 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 241/3393 [01:19<25:33,  2.06batch/s, Batch Loss=0.2593, Avg Loss=0.6027, Time Left=26.06 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 241/3393 [01:20<25:33,  2.06batch/s, Batch Loss=0.5204, Avg Loss=0.6022, Time Left=26.04 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 242/3393 [01:20<25:17,  2.08batch/s, Batch Loss=0.5204, Avg Loss=0.6022, Time Left=26.04 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 242/3393 [01:20<25:17,  2.08batch/s, Batch Loss=0.4316, Avg Loss=0.6012, Time Left=26.03 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 243/3393 [01:20<25:02,  2.10batch/s, Batch Loss=0.4316, Avg Loss=0.6012, Time Left=26.03 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 243/3393 [01:21<25:02,  2.10batch/s, Batch Loss=0.6463, Avg Loss=0.6015, Time Left=26.03 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 244/3393 [01:21<25:36,  2.05batch/s, Batch Loss=0.6463, Avg Loss=0.6015, Time Left=26.03 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 244/3393 [01:21<25:36,  2.05batch/s, Batch Loss=0.4060, Avg Loss=0.6003, Time Left=26.01 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 245/3393 [01:21<25:04,  2.09batch/s, Batch Loss=0.4060, Avg Loss=0.6003, Time Left=26.01 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 245/3393 [01:22<25:04,  2.09batch/s, Batch Loss=0.6131, Avg Loss=0.6004, Time Left=26.00 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 246/3393 [01:22<24:53,  2.11batch/s, Batch Loss=0.6131, Avg Loss=0.6004, Time Left=26.00 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 246/3393 [01:22<24:53,  2.11batch/s, Batch Loss=0.2275, Avg Loss=0.5982, Time Left=26.00 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 247/3393 [01:22<25:29,  2.06batch/s, Batch Loss=0.2275, Avg Loss=0.5982, Time Left=26.00 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 247/3393 [01:23<25:29,  2.06batch/s, Batch Loss=0.3373, Avg Loss=0.5967, Time Left=25.98 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 248/3393 [01:23<24:59,  2.10batch/s, Batch Loss=0.3373, Avg Loss=0.5967, Time Left=25.98 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 248/3393 [01:23<24:59,  2.10batch/s, Batch Loss=0.3597, Avg Loss=0.5953, Time Left=25.97 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 249/3393 [01:23<24:48,  2.11batch/s, Batch Loss=0.3597, Avg Loss=0.5953, Time Left=25.97 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 249/3393 [01:24<24:48,  2.11batch/s, Batch Loss=0.5026, Avg Loss=0.5948, Time Left=25.97 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 250/3393 [01:24<25:11,  2.08batch/s, Batch Loss=0.5026, Avg Loss=0.5948, Time Left=25.97 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 250/3393 [01:24<25:11,  2.08batch/s, Batch Loss=0.4134, Avg Loss=0.5938, Time Left=25.96 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 251/3393 [01:24<25:17,  2.07batch/s, Batch Loss=0.4134, Avg Loss=0.5938, Time Left=25.96 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 251/3393 [01:25<25:17,  2.07batch/s, Batch Loss=0.1724, Avg Loss=0.5914, Time Left=25.95 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 252/3393 [01:25<25:00,  2.09batch/s, Batch Loss=0.1724, Avg Loss=0.5914, Time Left=25.95 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 252/3393 [01:25<25:00,  2.09batch/s, Batch Loss=0.1699, Avg Loss=0.5890, Time Left=25.95 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 253/3393 [01:25<25:48,  2.03batch/s, Batch Loss=0.1699, Avg Loss=0.5890, Time Left=25.95 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 253/3393 [01:26<25:48,  2.03batch/s, Batch Loss=0.1803, Avg Loss=0.5867, Time Left=25.94 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 254/3393 [01:26<25:26,  2.06batch/s, Batch Loss=0.1803, Avg Loss=0.5867, Time Left=25.94 \u001b[A\n",
      "Epoch 1/3 - Training:   7%| | 254/3393 [01:26<25:26,  2.06batch/s, Batch Loss=0.2131, Avg Loss=0.5846, Time Left=25.94 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 255/3393 [01:26<25:42,  2.03batch/s, Batch Loss=0.2131, Avg Loss=0.5846, Time Left=25.94 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 255/3393 [01:27<25:42,  2.03batch/s, Batch Loss=0.4476, Avg Loss=0.5838, Time Left=25.92 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 256/3393 [01:27<25:17,  2.07batch/s, Batch Loss=0.4476, Avg Loss=0.5838, Time Left=25.92 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 256/3393 [01:27<25:17,  2.07batch/s, Batch Loss=0.3296, Avg Loss=0.5824, Time Left=25.91 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 257/3393 [01:27<24:59,  2.09batch/s, Batch Loss=0.3296, Avg Loss=0.5824, Time Left=25.91 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 257/3393 [01:28<24:59,  2.09batch/s, Batch Loss=0.5383, Avg Loss=0.5822, Time Left=25.91 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 258/3393 [01:28<25:18,  2.06batch/s, Batch Loss=0.5383, Avg Loss=0.5822, Time Left=25.91 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 258/3393 [01:28<25:18,  2.06batch/s, Batch Loss=0.6304, Avg Loss=0.5825, Time Left=25.89 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 259/3393 [01:28<25:03,  2.08batch/s, Batch Loss=0.6304, Avg Loss=0.5825, Time Left=25.89 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 259/3393 [01:29<25:03,  2.08batch/s, Batch Loss=0.3075, Avg Loss=0.5810, Time Left=25.88 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 260/3393 [01:29<24:52,  2.10batch/s, Batch Loss=0.3075, Avg Loss=0.5810, Time Left=25.88 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 260/3393 [01:29<24:52,  2.10batch/s, Batch Loss=0.4446, Avg Loss=0.5802, Time Left=25.88 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 261/3393 [01:29<25:32,  2.04batch/s, Batch Loss=0.4446, Avg Loss=0.5802, Time Left=25.88 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 261/3393 [01:30<25:32,  2.04batch/s, Batch Loss=0.4053, Avg Loss=0.5793, Time Left=25.87 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   8%| | 262/3393 [01:30<25:06,  2.08batch/s, Batch Loss=0.4053, Avg Loss=0.5793, Time Left=25.87 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 262/3393 [01:30<25:06,  2.08batch/s, Batch Loss=0.3227, Avg Loss=0.5779, Time Left=25.86 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 263/3393 [01:30<25:10,  2.07batch/s, Batch Loss=0.3227, Avg Loss=0.5779, Time Left=25.86 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 263/3393 [01:31<25:10,  2.07batch/s, Batch Loss=0.6084, Avg Loss=0.5781, Time Left=25.86 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 264/3393 [01:31<25:22,  2.05batch/s, Batch Loss=0.6084, Avg Loss=0.5781, Time Left=25.86 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 264/3393 [01:31<25:22,  2.05batch/s, Batch Loss=0.4612, Avg Loss=0.5775, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 265/3393 [01:31<25:22,  2.05batch/s, Batch Loss=0.4612, Avg Loss=0.5775, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 265/3393 [01:31<25:22,  2.05batch/s, Batch Loss=0.5824, Avg Loss=0.5775, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 266/3393 [01:31<25:33,  2.04batch/s, Batch Loss=0.5824, Avg Loss=0.5775, Time Left=25.85 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 266/3393 [01:32<25:33,  2.04batch/s, Batch Loss=0.6117, Avg Loss=0.5777, Time Left=25.83 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 267/3393 [01:32<24:57,  2.09batch/s, Batch Loss=0.6117, Avg Loss=0.5777, Time Left=25.83 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 267/3393 [01:32<24:57,  2.09batch/s, Batch Loss=0.2346, Avg Loss=0.5759, Time Left=25.82 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 268/3393 [01:32<24:59,  2.08batch/s, Batch Loss=0.2346, Avg Loss=0.5759, Time Left=25.82 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 268/3393 [01:33<24:59,  2.08batch/s, Batch Loss=0.3465, Avg Loss=0.5747, Time Left=25.81 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 269/3393 [01:33<24:47,  2.10batch/s, Batch Loss=0.3465, Avg Loss=0.5747, Time Left=25.81 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 269/3393 [01:33<24:47,  2.10batch/s, Batch Loss=0.3161, Avg Loss=0.5733, Time Left=25.80 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 270/3393 [01:33<24:53,  2.09batch/s, Batch Loss=0.3161, Avg Loss=0.5733, Time Left=25.80 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 270/3393 [01:34<24:53,  2.09batch/s, Batch Loss=0.2163, Avg Loss=0.5715, Time Left=25.79 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 271/3393 [01:34<24:58,  2.08batch/s, Batch Loss=0.2163, Avg Loss=0.5715, Time Left=25.79 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 271/3393 [01:34<24:58,  2.08batch/s, Batch Loss=0.4251, Avg Loss=0.5708, Time Left=25.79 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 272/3393 [01:34<25:16,  2.06batch/s, Batch Loss=0.4251, Avg Loss=0.5708, Time Left=25.79 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 272/3393 [01:35<25:16,  2.06batch/s, Batch Loss=0.4596, Avg Loss=0.5702, Time Left=25.77 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 273/3393 [01:35<24:59,  2.08batch/s, Batch Loss=0.4596, Avg Loss=0.5702, Time Left=25.77 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 273/3393 [01:35<24:59,  2.08batch/s, Batch Loss=0.4629, Avg Loss=0.5697, Time Left=25.76 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 274/3393 [01:35<24:45,  2.10batch/s, Batch Loss=0.4629, Avg Loss=0.5697, Time Left=25.76 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 274/3393 [01:36<24:45,  2.10batch/s, Batch Loss=0.5074, Avg Loss=0.5694, Time Left=25.76 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 275/3393 [01:36<25:06,  2.07batch/s, Batch Loss=0.5074, Avg Loss=0.5694, Time Left=25.76 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 275/3393 [01:36<25:06,  2.07batch/s, Batch Loss=0.4544, Avg Loss=0.5688, Time Left=25.75 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 276/3393 [01:36<25:01,  2.08batch/s, Batch Loss=0.4544, Avg Loss=0.5688, Time Left=25.75 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 276/3393 [01:37<25:01,  2.08batch/s, Batch Loss=0.2857, Avg Loss=0.5674, Time Left=25.73 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 277/3393 [01:37<24:39,  2.11batch/s, Batch Loss=0.2857, Avg Loss=0.5674, Time Left=25.73 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 277/3393 [01:37<24:39,  2.11batch/s, Batch Loss=0.3476, Avg Loss=0.5663, Time Left=25.73 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 278/3393 [01:37<25:23,  2.04batch/s, Batch Loss=0.3476, Avg Loss=0.5663, Time Left=25.73 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 278/3393 [01:38<25:23,  2.04batch/s, Batch Loss=0.2330, Avg Loss=0.5646, Time Left=25.72 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 279/3393 [01:38<24:57,  2.08batch/s, Batch Loss=0.2330, Avg Loss=0.5646, Time Left=25.72 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 279/3393 [01:38<24:57,  2.08batch/s, Batch Loss=0.2794, Avg Loss=0.5632, Time Left=25.72 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 280/3393 [01:38<25:12,  2.06batch/s, Batch Loss=0.2794, Avg Loss=0.5632, Time Left=25.72 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 280/3393 [01:39<25:12,  2.06batch/s, Batch Loss=0.3272, Avg Loss=0.5621, Time Left=25.71 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 281/3393 [01:39<25:11,  2.06batch/s, Batch Loss=0.3272, Avg Loss=0.5621, Time Left=25.71 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 281/3393 [01:39<25:11,  2.06batch/s, Batch Loss=0.3858, Avg Loss=0.5612, Time Left=25.70 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 282/3393 [01:39<25:08,  2.06batch/s, Batch Loss=0.3858, Avg Loss=0.5612, Time Left=25.70 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 282/3393 [01:40<25:08,  2.06batch/s, Batch Loss=0.2742, Avg Loss=0.5598, Time Left=25.70 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 283/3393 [01:40<25:21,  2.04batch/s, Batch Loss=0.2742, Avg Loss=0.5598, Time Left=25.70 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 283/3393 [01:40<25:21,  2.04batch/s, Batch Loss=0.4708, Avg Loss=0.5594, Time Left=25.69 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 284/3393 [01:40<25:05,  2.07batch/s, Batch Loss=0.4708, Avg Loss=0.5594, Time Left=25.69 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 284/3393 [01:41<25:05,  2.07batch/s, Batch Loss=0.1970, Avg Loss=0.5577, Time Left=25.67 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 285/3393 [01:41<24:44,  2.09batch/s, Batch Loss=0.1970, Avg Loss=0.5577, Time Left=25.67 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 285/3393 [01:41<24:44,  2.09batch/s, Batch Loss=0.4359, Avg Loss=0.5571, Time Left=25.67 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 286/3393 [01:41<25:17,  2.05batch/s, Batch Loss=0.4359, Avg Loss=0.5571, Time Left=25.67 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 286/3393 [01:42<25:17,  2.05batch/s, Batch Loss=0.5286, Avg Loss=0.5570, Time Left=25.66 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 287/3393 [01:42<24:46,  2.09batch/s, Batch Loss=0.5286, Avg Loss=0.5570, Time Left=25.66 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 287/3393 [01:42<24:46,  2.09batch/s, Batch Loss=0.2311, Avg Loss=0.5554, Time Left=25.65 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 288/3393 [01:42<24:50,  2.08batch/s, Batch Loss=0.2311, Avg Loss=0.5554, Time Left=25.65 \u001b[A\n",
      "Epoch 1/3 - Training:   8%| | 288/3393 [01:43<24:50,  2.08batch/s, Batch Loss=0.4704, Avg Loss=0.5550, Time Left=25.64 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 289/3393 [01:43<25:05,  2.06batch/s, Batch Loss=0.4704, Avg Loss=0.5550, Time Left=25.64 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 289/3393 [01:43<25:05,  2.06batch/s, Batch Loss=0.1815, Avg Loss=0.5533, Time Left=25.63 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 290/3393 [01:43<24:35,  2.10batch/s, Batch Loss=0.1815, Avg Loss=0.5533, Time Left=25.63 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 290/3393 [01:43<24:35,  2.10batch/s, Batch Loss=0.2782, Avg Loss=0.5520, Time Left=25.62 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 291/3393 [01:44<24:43,  2.09batch/s, Batch Loss=0.2782, Avg Loss=0.5520, Time Left=25.62 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 291/3393 [01:44<24:43,  2.09batch/s, Batch Loss=0.3698, Avg Loss=0.5512, Time Left=25.62 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 292/3393 [01:44<25:05,  2.06batch/s, Batch Loss=0.3698, Avg Loss=0.5512, Time Left=25.62 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 292/3393 [01:44<25:05,  2.06batch/s, Batch Loss=0.5829, Avg Loss=0.5513, Time Left=25.60 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 293/3393 [01:44<24:44,  2.09batch/s, Batch Loss=0.5829, Avg Loss=0.5513, Time Left=25.60 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 293/3393 [01:45<24:44,  2.09batch/s, Batch Loss=0.5179, Avg Loss=0.5512, Time Left=25.59 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 294/3393 [01:45<24:46,  2.08batch/s, Batch Loss=0.5179, Avg Loss=0.5512, Time Left=25.59 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 294/3393 [01:45<24:46,  2.08batch/s, Batch Loss=0.5873, Avg Loss=0.5513, Time Left=25.59 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   9%| | 295/3393 [01:45<24:51,  2.08batch/s, Batch Loss=0.5873, Avg Loss=0.5513, Time Left=25.59 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 295/3393 [01:46<24:51,  2.08batch/s, Batch Loss=0.3735, Avg Loss=0.5505, Time Left=25.58 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 296/3393 [01:46<24:53,  2.07batch/s, Batch Loss=0.3735, Avg Loss=0.5505, Time Left=25.58 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 296/3393 [01:46<24:53,  2.07batch/s, Batch Loss=0.2693, Avg Loss=0.5492, Time Left=25.57 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 297/3393 [01:46<24:40,  2.09batch/s, Batch Loss=0.2693, Avg Loss=0.5492, Time Left=25.57 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 297/3393 [01:47<24:40,  2.09batch/s, Batch Loss=0.4224, Avg Loss=0.5487, Time Left=25.57 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 298/3393 [01:47<25:12,  2.05batch/s, Batch Loss=0.4224, Avg Loss=0.5487, Time Left=25.57 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 298/3393 [01:47<25:12,  2.05batch/s, Batch Loss=0.1604, Avg Loss=0.5469, Time Left=25.56 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 299/3393 [01:47<24:54,  2.07batch/s, Batch Loss=0.1604, Avg Loss=0.5469, Time Left=25.56 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 299/3393 [01:48<24:54,  2.07batch/s, Batch Loss=0.2723, Avg Loss=0.5457, Time Left=25.55 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 300/3393 [01:48<25:08,  2.05batch/s, Batch Loss=0.2723, Avg Loss=0.5457, Time Left=25.55 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 300/3393 [01:48<25:08,  2.05batch/s, Batch Loss=0.4046, Avg Loss=0.5451, Time Left=25.54 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 301/3393 [01:48<24:51,  2.07batch/s, Batch Loss=0.4046, Avg Loss=0.5451, Time Left=25.54 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 301/3393 [01:49<24:51,  2.07batch/s, Batch Loss=0.4141, Avg Loss=0.5445, Time Left=25.53 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 302/3393 [01:49<24:35,  2.09batch/s, Batch Loss=0.4141, Avg Loss=0.5445, Time Left=25.53 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 302/3393 [01:49<24:35,  2.09batch/s, Batch Loss=0.2851, Avg Loss=0.5434, Time Left=25.52 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 303/3393 [01:49<25:09,  2.05batch/s, Batch Loss=0.2851, Avg Loss=0.5434, Time Left=25.52 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 303/3393 [01:50<25:09,  2.05batch/s, Batch Loss=0.5342, Avg Loss=0.5433, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 304/3393 [01:50<24:36,  2.09batch/s, Batch Loss=0.5342, Avg Loss=0.5433, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 304/3393 [01:50<24:36,  2.09batch/s, Batch Loss=0.1700, Avg Loss=0.5417, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 305/3393 [01:50<24:40,  2.09batch/s, Batch Loss=0.1700, Avg Loss=0.5417, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 305/3393 [01:51<24:40,  2.09batch/s, Batch Loss=0.5841, Avg Loss=0.5419, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 306/3393 [01:51<25:13,  2.04batch/s, Batch Loss=0.5841, Avg Loss=0.5419, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 306/3393 [01:51<25:13,  2.04batch/s, Batch Loss=0.2482, Avg Loss=0.5406, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 307/3393 [01:51<25:07,  2.05batch/s, Batch Loss=0.2482, Avg Loss=0.5406, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 307/3393 [01:52<25:07,  2.05batch/s, Batch Loss=0.5417, Avg Loss=0.5406, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 308/3393 [01:52<24:33,  2.09batch/s, Batch Loss=0.5417, Avg Loss=0.5406, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 308/3393 [01:52<24:33,  2.09batch/s, Batch Loss=0.4658, Avg Loss=0.5403, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 309/3393 [01:52<25:19,  2.03batch/s, Batch Loss=0.4658, Avg Loss=0.5403, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 309/3393 [01:53<25:19,  2.03batch/s, Batch Loss=0.4696, Avg Loss=0.5400, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 310/3393 [01:53<25:42,  2.00batch/s, Batch Loss=0.4696, Avg Loss=0.5400, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 310/3393 [01:53<25:42,  2.00batch/s, Batch Loss=0.3620, Avg Loss=0.5392, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 311/3393 [01:53<25:55,  1.98batch/s, Batch Loss=0.3620, Avg Loss=0.5392, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 311/3393 [01:54<25:55,  1.98batch/s, Batch Loss=0.2428, Avg Loss=0.5380, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 312/3393 [01:54<25:58,  1.98batch/s, Batch Loss=0.2428, Avg Loss=0.5380, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 312/3393 [01:54<25:58,  1.98batch/s, Batch Loss=0.3905, Avg Loss=0.5373, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 313/3393 [01:54<27:17,  1.88batch/s, Batch Loss=0.3905, Avg Loss=0.5373, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 313/3393 [01:55<27:17,  1.88batch/s, Batch Loss=0.5542, Avg Loss=0.5374, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 314/3393 [01:55<27:24,  1.87batch/s, Batch Loss=0.5542, Avg Loss=0.5374, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 314/3393 [01:55<27:24,  1.87batch/s, Batch Loss=0.3043, Avg Loss=0.5364, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 315/3393 [01:55<27:49,  1.84batch/s, Batch Loss=0.3043, Avg Loss=0.5364, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 315/3393 [01:56<27:49,  1.84batch/s, Batch Loss=0.3069, Avg Loss=0.5355, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 316/3393 [01:56<26:39,  1.92batch/s, Batch Loss=0.3069, Avg Loss=0.5355, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 316/3393 [01:56<26:39,  1.92batch/s, Batch Loss=0.5041, Avg Loss=0.5354, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 317/3393 [01:56<26:51,  1.91batch/s, Batch Loss=0.5041, Avg Loss=0.5354, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 317/3393 [01:57<26:51,  1.91batch/s, Batch Loss=0.3533, Avg Loss=0.5346, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 318/3393 [01:57<25:52,  1.98batch/s, Batch Loss=0.3533, Avg Loss=0.5346, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 318/3393 [01:57<25:52,  1.98batch/s, Batch Loss=0.2581, Avg Loss=0.5335, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 319/3393 [01:57<26:12,  1.95batch/s, Batch Loss=0.2581, Avg Loss=0.5335, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 319/3393 [01:58<26:12,  1.95batch/s, Batch Loss=0.3987, Avg Loss=0.5329, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 320/3393 [01:58<26:30,  1.93batch/s, Batch Loss=0.3987, Avg Loss=0.5329, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 320/3393 [01:59<26:30,  1.93batch/s, Batch Loss=0.4827, Avg Loss=0.5327, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 321/3393 [01:59<26:40,  1.92batch/s, Batch Loss=0.4827, Avg Loss=0.5327, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 321/3393 [01:59<26:40,  1.92batch/s, Batch Loss=0.2873, Avg Loss=0.5317, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 322/3393 [01:59<26:32,  1.93batch/s, Batch Loss=0.2873, Avg Loss=0.5317, Time Left=25.49 \u001b[A\n",
      "Epoch 1/3 - Training:   9%| | 322/3393 [02:00<26:32,  1.93batch/s, Batch Loss=0.4585, Avg Loss=0.5314, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 323/3393 [02:00<28:07,  1.82batch/s, Batch Loss=0.4585, Avg Loss=0.5314, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 323/3393 [02:00<28:07,  1.82batch/s, Batch Loss=0.5312, Avg Loss=0.5314, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 324/3393 [02:00<28:09,  1.82batch/s, Batch Loss=0.5312, Avg Loss=0.5314, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 324/3393 [02:01<28:09,  1.82batch/s, Batch Loss=0.2355, Avg Loss=0.5302, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 325/3393 [02:01<27:54,  1.83batch/s, Batch Loss=0.2355, Avg Loss=0.5302, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 325/3393 [02:01<27:54,  1.83batch/s, Batch Loss=0.5266, Avg Loss=0.5302, Time Left=25.52 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 326/3393 [02:01<27:41,  1.85batch/s, Batch Loss=0.5266, Avg Loss=0.5302, Time Left=25.52 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 326/3393 [02:02<27:41,  1.85batch/s, Batch Loss=0.2880, Avg Loss=0.5292, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 327/3393 [02:02<26:44,  1.91batch/s, Batch Loss=0.2880, Avg Loss=0.5292, Time Left=25.51 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 327/3393 [02:02<26:44,  1.91batch/s, Batch Loss=0.2582, Avg Loss=0.5282, Time Left=25.52 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  10%| | 328/3393 [02:02<27:27,  1.86batch/s, Batch Loss=0.2582, Avg Loss=0.5282, Time Left=25.52 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 328/3393 [02:03<27:27,  1.86batch/s, Batch Loss=0.3931, Avg Loss=0.5276, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 329/3393 [02:03<26:21,  1.94batch/s, Batch Loss=0.3931, Avg Loss=0.5276, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 329/3393 [02:03<26:21,  1.94batch/s, Batch Loss=0.4167, Avg Loss=0.5272, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 330/3393 [02:03<26:01,  1.96batch/s, Batch Loss=0.4167, Avg Loss=0.5272, Time Left=25.50 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 330/3393 [02:04<26:01,  1.96batch/s, Batch Loss=0.5252, Avg Loss=0.5272, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 331/3393 [02:04<25:22,  2.01batch/s, Batch Loss=0.5252, Avg Loss=0.5272, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 331/3393 [02:04<25:22,  2.01batch/s, Batch Loss=0.1433, Avg Loss=0.5257, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 332/3393 [02:04<25:50,  1.97batch/s, Batch Loss=0.1433, Avg Loss=0.5257, Time Left=25.48 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 332/3393 [02:05<25:50,  1.97batch/s, Batch Loss=0.2258, Avg Loss=0.5245, Time Left=25.47 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 333/3393 [02:05<24:56,  2.04batch/s, Batch Loss=0.2258, Avg Loss=0.5245, Time Left=25.47 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 333/3393 [02:05<24:56,  2.04batch/s, Batch Loss=0.2767, Avg Loss=0.5236, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 334/3393 [02:05<24:39,  2.07batch/s, Batch Loss=0.2767, Avg Loss=0.5236, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 334/3393 [02:06<24:39,  2.07batch/s, Batch Loss=0.4887, Avg Loss=0.5234, Time Left=25.45 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 335/3393 [02:06<25:05,  2.03batch/s, Batch Loss=0.4887, Avg Loss=0.5234, Time Left=25.45 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 335/3393 [02:06<25:05,  2.03batch/s, Batch Loss=0.3704, Avg Loss=0.5228, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 336/3393 [02:06<26:02,  1.96batch/s, Batch Loss=0.3704, Avg Loss=0.5228, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 336/3393 [02:07<26:02,  1.96batch/s, Batch Loss=0.2314, Avg Loss=0.5217, Time Left=25.47 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 337/3393 [02:07<26:56,  1.89batch/s, Batch Loss=0.2314, Avg Loss=0.5217, Time Left=25.47 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 337/3393 [02:07<26:56,  1.89batch/s, Batch Loss=0.7580, Avg Loss=0.5226, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 338/3393 [02:07<26:15,  1.94batch/s, Batch Loss=0.7580, Avg Loss=0.5226, Time Left=25.46 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 338/3393 [02:08<26:15,  1.94batch/s, Batch Loss=0.4888, Avg Loss=0.5225, Time Left=25.45 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 339/3393 [02:08<25:32,  1.99batch/s, Batch Loss=0.4888, Avg Loss=0.5225, Time Left=25.45 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 339/3393 [02:08<25:32,  1.99batch/s, Batch Loss=0.3510, Avg Loss=0.5218, Time Left=25.44 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 340/3393 [02:08<25:07,  2.03batch/s, Batch Loss=0.3510, Avg Loss=0.5218, Time Left=25.44 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 340/3393 [02:09<25:07,  2.03batch/s, Batch Loss=0.4806, Avg Loss=0.5217, Time Left=25.43 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 341/3393 [02:09<25:27,  2.00batch/s, Batch Loss=0.4806, Avg Loss=0.5217, Time Left=25.43 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 341/3393 [02:09<25:27,  2.00batch/s, Batch Loss=0.3964, Avg Loss=0.5212, Time Left=25.42 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 342/3393 [02:09<24:29,  2.08batch/s, Batch Loss=0.3964, Avg Loss=0.5212, Time Left=25.42 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 342/3393 [02:10<24:29,  2.08batch/s, Batch Loss=0.2586, Avg Loss=0.5202, Time Left=25.42 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 343/3393 [02:10<25:08,  2.02batch/s, Batch Loss=0.2586, Avg Loss=0.5202, Time Left=25.42 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 343/3393 [02:10<25:08,  2.02batch/s, Batch Loss=0.2577, Avg Loss=0.5193, Time Left=25.41 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 344/3393 [02:10<25:13,  2.01batch/s, Batch Loss=0.2577, Avg Loss=0.5193, Time Left=25.41 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 344/3393 [02:11<25:13,  2.01batch/s, Batch Loss=0.3440, Avg Loss=0.5186, Time Left=25.42 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 345/3393 [02:11<26:12,  1.94batch/s, Batch Loss=0.3440, Avg Loss=0.5186, Time Left=25.42 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 345/3393 [02:11<26:12,  1.94batch/s, Batch Loss=0.6679, Avg Loss=0.5192, Time Left=25.40 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 346/3393 [02:11<25:23,  2.00batch/s, Batch Loss=0.6679, Avg Loss=0.5192, Time Left=25.40 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 346/3393 [02:12<25:23,  2.00batch/s, Batch Loss=0.3674, Avg Loss=0.5186, Time Left=25.39 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 347/3393 [02:12<25:07,  2.02batch/s, Batch Loss=0.3674, Avg Loss=0.5186, Time Left=25.39 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 347/3393 [02:12<25:07,  2.02batch/s, Batch Loss=0.2511, Avg Loss=0.5176, Time Left=25.39 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 348/3393 [02:12<25:26,  2.00batch/s, Batch Loss=0.2511, Avg Loss=0.5176, Time Left=25.39 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 348/3393 [02:13<25:26,  2.00batch/s, Batch Loss=0.1870, Avg Loss=0.5164, Time Left=25.38 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 349/3393 [02:13<24:42,  2.05batch/s, Batch Loss=0.1870, Avg Loss=0.5164, Time Left=25.38 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 349/3393 [02:13<24:42,  2.05batch/s, Batch Loss=0.4348, Avg Loss=0.5161, Time Left=25.37 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 350/3393 [02:13<24:37,  2.06batch/s, Batch Loss=0.4348, Avg Loss=0.5161, Time Left=25.37 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 350/3393 [02:14<24:37,  2.06batch/s, Batch Loss=0.4077, Avg Loss=0.5157, Time Left=25.36 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 351/3393 [02:14<24:48,  2.04batch/s, Batch Loss=0.4077, Avg Loss=0.5157, Time Left=25.36 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 351/3393 [02:14<24:48,  2.04batch/s, Batch Loss=0.1630, Avg Loss=0.5144, Time Left=25.35 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 352/3393 [02:14<24:44,  2.05batch/s, Batch Loss=0.1630, Avg Loss=0.5144, Time Left=25.35 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 352/3393 [02:15<24:44,  2.05batch/s, Batch Loss=0.3764, Avg Loss=0.5139, Time Left=25.34 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 353/3393 [02:15<24:25,  2.08batch/s, Batch Loss=0.3764, Avg Loss=0.5139, Time Left=25.34 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 353/3393 [02:15<24:25,  2.08batch/s, Batch Loss=0.3509, Avg Loss=0.5133, Time Left=25.33 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 354/3393 [02:15<24:25,  2.07batch/s, Batch Loss=0.3509, Avg Loss=0.5133, Time Left=25.33 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 354/3393 [02:16<24:25,  2.07batch/s, Batch Loss=0.2165, Avg Loss=0.5123, Time Left=25.32 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 355/3393 [02:16<24:26,  2.07batch/s, Batch Loss=0.2165, Avg Loss=0.5123, Time Left=25.32 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 355/3393 [02:16<24:26,  2.07batch/s, Batch Loss=0.4365, Avg Loss=0.5120, Time Left=25.32 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 356/3393 [02:16<24:55,  2.03batch/s, Batch Loss=0.4365, Avg Loss=0.5120, Time Left=25.32 \u001b[A\n",
      "Epoch 1/3 - Training:  10%| | 356/3393 [02:17<24:55,  2.03batch/s, Batch Loss=0.3967, Avg Loss=0.5116, Time Left=25.31 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 357/3393 [02:17<24:50,  2.04batch/s, Batch Loss=0.3967, Avg Loss=0.5116, Time Left=25.31 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 357/3393 [02:17<24:50,  2.04batch/s, Batch Loss=0.3567, Avg Loss=0.5111, Time Left=25.30 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 358/3393 [02:17<24:36,  2.06batch/s, Batch Loss=0.3567, Avg Loss=0.5111, Time Left=25.30 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 358/3393 [02:18<24:36,  2.06batch/s, Batch Loss=0.2454, Avg Loss=0.5101, Time Left=25.30 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 359/3393 [02:18<25:06,  2.01batch/s, Batch Loss=0.2454, Avg Loss=0.5101, Time Left=25.30 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 359/3393 [02:18<25:06,  2.01batch/s, Batch Loss=0.4901, Avg Loss=0.5100, Time Left=25.29 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 360/3393 [02:18<24:41,  2.05batch/s, Batch Loss=0.4901, Avg Loss=0.5100, Time Left=25.29 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 360/3393 [02:19<24:41,  2.05batch/s, Batch Loss=0.2679, Avg Loss=0.5092, Time Left=25.28 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  11%| | 361/3393 [02:19<25:05,  2.01batch/s, Batch Loss=0.2679, Avg Loss=0.5092, Time Left=25.28 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 361/3393 [02:19<25:05,  2.01batch/s, Batch Loss=0.1822, Avg Loss=0.5081, Time Left=25.27 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 362/3393 [02:19<24:39,  2.05batch/s, Batch Loss=0.1822, Avg Loss=0.5081, Time Left=25.27 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 362/3393 [02:20<24:39,  2.05batch/s, Batch Loss=0.5021, Avg Loss=0.5080, Time Left=25.26 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 363/3393 [02:20<24:22,  2.07batch/s, Batch Loss=0.5021, Avg Loss=0.5080, Time Left=25.26 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 363/3393 [02:20<24:22,  2.07batch/s, Batch Loss=0.3476, Avg Loss=0.5075, Time Left=25.26 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 364/3393 [02:20<24:49,  2.03batch/s, Batch Loss=0.3476, Avg Loss=0.5075, Time Left=25.26 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 364/3393 [02:21<24:49,  2.03batch/s, Batch Loss=0.2606, Avg Loss=0.5066, Time Left=25.25 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 365/3393 [02:21<24:57,  2.02batch/s, Batch Loss=0.2606, Avg Loss=0.5066, Time Left=25.25 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 365/3393 [02:21<24:57,  2.02batch/s, Batch Loss=0.2649, Avg Loss=0.5058, Time Left=25.25 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 366/3393 [02:21<25:01,  2.02batch/s, Batch Loss=0.2649, Avg Loss=0.5058, Time Left=25.25 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 366/3393 [02:22<25:01,  2.02batch/s, Batch Loss=0.0718, Avg Loss=0.5043, Time Left=25.23 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 367/3393 [02:22<24:23,  2.07batch/s, Batch Loss=0.0718, Avg Loss=0.5043, Time Left=25.23 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 367/3393 [02:22<24:23,  2.07batch/s, Batch Loss=0.3083, Avg Loss=0.5036, Time Left=25.22 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 368/3393 [02:22<24:35,  2.05batch/s, Batch Loss=0.3083, Avg Loss=0.5036, Time Left=25.22 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 368/3393 [02:23<24:35,  2.05batch/s, Batch Loss=0.4229, Avg Loss=0.5033, Time Left=25.22 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 369/3393 [02:23<24:46,  2.03batch/s, Batch Loss=0.4229, Avg Loss=0.5033, Time Left=25.22 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 369/3393 [02:23<24:46,  2.03batch/s, Batch Loss=0.3410, Avg Loss=0.5028, Time Left=25.21 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 370/3393 [02:23<24:26,  2.06batch/s, Batch Loss=0.3410, Avg Loss=0.5028, Time Left=25.21 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 370/3393 [02:23<24:26,  2.06batch/s, Batch Loss=0.4701, Avg Loss=0.5027, Time Left=25.20 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 371/3393 [02:23<24:20,  2.07batch/s, Batch Loss=0.4701, Avg Loss=0.5027, Time Left=25.20 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 371/3393 [02:24<24:20,  2.07batch/s, Batch Loss=0.3729, Avg Loss=0.5022, Time Left=25.19 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 372/3393 [02:24<24:43,  2.04batch/s, Batch Loss=0.3729, Avg Loss=0.5022, Time Left=25.19 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 372/3393 [02:24<24:43,  2.04batch/s, Batch Loss=0.3418, Avg Loss=0.5017, Time Left=25.18 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 373/3393 [02:24<24:30,  2.05batch/s, Batch Loss=0.3418, Avg Loss=0.5017, Time Left=25.18 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 373/3393 [02:25<24:30,  2.05batch/s, Batch Loss=0.2890, Avg Loss=0.5010, Time Left=25.18 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 374/3393 [02:25<24:40,  2.04batch/s, Batch Loss=0.2890, Avg Loss=0.5010, Time Left=25.18 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 374/3393 [02:25<24:40,  2.04batch/s, Batch Loss=0.2458, Avg Loss=0.5001, Time Left=25.17 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 375/3393 [02:25<24:21,  2.06batch/s, Batch Loss=0.2458, Avg Loss=0.5001, Time Left=25.17 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 375/3393 [02:26<24:21,  2.06batch/s, Batch Loss=0.3604, Avg Loss=0.4997, Time Left=25.16 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 376/3393 [02:26<24:19,  2.07batch/s, Batch Loss=0.3604, Avg Loss=0.4997, Time Left=25.16 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 376/3393 [02:26<24:19,  2.07batch/s, Batch Loss=0.4217, Avg Loss=0.4994, Time Left=25.15 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 377/3393 [02:26<24:33,  2.05batch/s, Batch Loss=0.4217, Avg Loss=0.4994, Time Left=25.15 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 377/3393 [02:27<24:33,  2.05batch/s, Batch Loss=0.4663, Avg Loss=0.4993, Time Left=25.14 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 378/3393 [02:27<24:45,  2.03batch/s, Batch Loss=0.4663, Avg Loss=0.4993, Time Left=25.14 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 378/3393 [02:27<24:45,  2.03batch/s, Batch Loss=0.6004, Avg Loss=0.4996, Time Left=25.14 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 379/3393 [02:27<24:34,  2.04batch/s, Batch Loss=0.6004, Avg Loss=0.4996, Time Left=25.14 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 379/3393 [02:28<24:34,  2.04batch/s, Batch Loss=0.3217, Avg Loss=0.4991, Time Left=25.12 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 380/3393 [02:28<24:16,  2.07batch/s, Batch Loss=0.3217, Avg Loss=0.4991, Time Left=25.12 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 380/3393 [02:28<24:16,  2.07batch/s, Batch Loss=0.5548, Avg Loss=0.4992, Time Left=25.11 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 381/3393 [02:28<24:00,  2.09batch/s, Batch Loss=0.5548, Avg Loss=0.4992, Time Left=25.11 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 381/3393 [02:29<24:00,  2.09batch/s, Batch Loss=0.5726, Avg Loss=0.4995, Time Left=25.10 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 382/3393 [02:29<23:49,  2.11batch/s, Batch Loss=0.5726, Avg Loss=0.4995, Time Left=25.10 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 382/3393 [02:29<23:49,  2.11batch/s, Batch Loss=0.4777, Avg Loss=0.4994, Time Left=25.10 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 383/3393 [02:29<24:26,  2.05batch/s, Batch Loss=0.4777, Avg Loss=0.4994, Time Left=25.10 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 383/3393 [02:30<24:26,  2.05batch/s, Batch Loss=0.4577, Avg Loss=0.4993, Time Left=25.09 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 384/3393 [02:30<24:23,  2.06batch/s, Batch Loss=0.4577, Avg Loss=0.4993, Time Left=25.09 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 384/3393 [02:30<24:23,  2.06batch/s, Batch Loss=0.4291, Avg Loss=0.4990, Time Left=25.08 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 385/3393 [02:30<24:33,  2.04batch/s, Batch Loss=0.4291, Avg Loss=0.4990, Time Left=25.08 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 385/3393 [02:31<24:33,  2.04batch/s, Batch Loss=0.5598, Avg Loss=0.4992, Time Left=25.07 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 386/3393 [02:31<24:30,  2.04batch/s, Batch Loss=0.5598, Avg Loss=0.4992, Time Left=25.07 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 386/3393 [02:31<24:30,  2.04batch/s, Batch Loss=0.6266, Avg Loss=0.4996, Time Left=25.06 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 387/3393 [02:31<24:09,  2.07batch/s, Batch Loss=0.6266, Avg Loss=0.4996, Time Left=25.06 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 387/3393 [02:32<24:09,  2.07batch/s, Batch Loss=0.2852, Avg Loss=0.4990, Time Left=25.06 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 388/3393 [02:32<24:38,  2.03batch/s, Batch Loss=0.2852, Avg Loss=0.4990, Time Left=25.06 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 388/3393 [02:32<24:38,  2.03batch/s, Batch Loss=0.4132, Avg Loss=0.4987, Time Left=25.05 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 389/3393 [02:32<24:18,  2.06batch/s, Batch Loss=0.4132, Avg Loss=0.4987, Time Left=25.05 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 389/3393 [02:33<24:18,  2.06batch/s, Batch Loss=0.5763, Avg Loss=0.4989, Time Left=25.04 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 390/3393 [02:33<24:43,  2.02batch/s, Batch Loss=0.5763, Avg Loss=0.4989, Time Left=25.04 \u001b[A\n",
      "Epoch 1/3 - Training:  11%| | 390/3393 [02:33<24:43,  2.02batch/s, Batch Loss=0.5728, Avg Loss=0.4992, Time Left=25.03 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 391/3393 [02:33<24:06,  2.08batch/s, Batch Loss=0.5728, Avg Loss=0.4992, Time Left=25.03 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 391/3393 [02:34<24:06,  2.08batch/s, Batch Loss=0.1955, Avg Loss=0.4982, Time Left=25.02 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 392/3393 [02:34<24:06,  2.07batch/s, Batch Loss=0.1955, Avg Loss=0.4982, Time Left=25.02 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 392/3393 [02:34<24:06,  2.07batch/s, Batch Loss=0.4164, Avg Loss=0.4979, Time Left=25.01 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 393/3393 [02:34<24:21,  2.05batch/s, Batch Loss=0.4164, Avg Loss=0.4979, Time Left=25.01 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 393/3393 [02:35<24:21,  2.05batch/s, Batch Loss=0.7908, Avg Loss=0.4989, Time Left=25.01 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  12%| | 394/3393 [02:35<24:19,  2.06batch/s, Batch Loss=0.7908, Avg Loss=0.4989, Time Left=25.01 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 394/3393 [02:35<24:19,  2.06batch/s, Batch Loss=0.4700, Avg Loss=0.4988, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 395/3393 [02:35<24:01,  2.08batch/s, Batch Loss=0.4700, Avg Loss=0.4988, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 395/3393 [02:36<24:01,  2.08batch/s, Batch Loss=0.3841, Avg Loss=0.4984, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 396/3393 [02:36<24:46,  2.02batch/s, Batch Loss=0.3841, Avg Loss=0.4984, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 396/3393 [02:36<24:46,  2.02batch/s, Batch Loss=0.5131, Avg Loss=0.4985, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 397/3393 [02:36<25:01,  2.00batch/s, Batch Loss=0.5131, Avg Loss=0.4985, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 397/3393 [02:37<25:01,  2.00batch/s, Batch Loss=0.2324, Avg Loss=0.4976, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 398/3393 [02:37<25:42,  1.94batch/s, Batch Loss=0.2324, Avg Loss=0.4976, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 398/3393 [02:37<25:42,  1.94batch/s, Batch Loss=0.7725, Avg Loss=0.4985, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 399/3393 [02:37<25:42,  1.94batch/s, Batch Loss=0.7725, Avg Loss=0.4985, Time Left=24.99 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 399/3393 [02:38<25:42,  1.94batch/s, Batch Loss=0.4806, Avg Loss=0.4984, Time Left=24.98 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 400/3393 [02:38<25:02,  1.99batch/s, Batch Loss=0.4806, Avg Loss=0.4984, Time Left=24.98 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 400/3393 [02:38<25:02,  1.99batch/s, Batch Loss=0.3844, Avg Loss=0.4981, Time Left=24.97 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 401/3393 [02:38<25:26,  1.96batch/s, Batch Loss=0.3844, Avg Loss=0.4981, Time Left=24.97 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 401/3393 [02:39<25:26,  1.96batch/s, Batch Loss=0.4085, Avg Loss=0.4978, Time Left=24.97 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 402/3393 [02:39<25:42,  1.94batch/s, Batch Loss=0.4085, Avg Loss=0.4978, Time Left=24.97 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 402/3393 [02:39<25:42,  1.94batch/s, Batch Loss=0.4105, Avg Loss=0.4975, Time Left=24.97 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 403/3393 [02:39<26:05,  1.91batch/s, Batch Loss=0.4105, Avg Loss=0.4975, Time Left=24.97 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 403/3393 [02:40<26:05,  1.91batch/s, Batch Loss=0.4240, Avg Loss=0.4973, Time Left=24.96 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 404/3393 [02:40<25:07,  1.98batch/s, Batch Loss=0.4240, Avg Loss=0.4973, Time Left=24.96 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 404/3393 [02:40<25:07,  1.98batch/s, Batch Loss=0.4006, Avg Loss=0.4970, Time Left=24.96 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 405/3393 [02:40<25:47,  1.93batch/s, Batch Loss=0.4006, Avg Loss=0.4970, Time Left=24.96 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 405/3393 [02:41<25:47,  1.93batch/s, Batch Loss=0.5177, Avg Loss=0.4971, Time Left=24.95 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 406/3393 [02:41<25:02,  1.99batch/s, Batch Loss=0.5177, Avg Loss=0.4971, Time Left=24.95 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 406/3393 [02:41<25:02,  1.99batch/s, Batch Loss=0.3707, Avg Loss=0.4967, Time Left=24.95 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 407/3393 [02:41<25:39,  1.94batch/s, Batch Loss=0.3707, Avg Loss=0.4967, Time Left=24.95 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 407/3393 [02:42<25:39,  1.94batch/s, Batch Loss=0.4310, Avg Loss=0.4965, Time Left=24.94 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 408/3393 [02:42<24:52,  2.00batch/s, Batch Loss=0.4310, Avg Loss=0.4965, Time Left=24.94 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 408/3393 [02:42<24:52,  2.00batch/s, Batch Loss=0.3742, Avg Loss=0.4961, Time Left=24.94 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 409/3393 [02:42<26:00,  1.91batch/s, Batch Loss=0.3742, Avg Loss=0.4961, Time Left=24.94 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 409/3393 [02:43<26:00,  1.91batch/s, Batch Loss=0.2831, Avg Loss=0.4955, Time Left=24.94 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 410/3393 [02:43<25:45,  1.93batch/s, Batch Loss=0.2831, Avg Loss=0.4955, Time Left=24.94 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 410/3393 [02:43<25:45,  1.93batch/s, Batch Loss=0.5332, Avg Loss=0.4956, Time Left=24.93 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 411/3393 [02:43<24:59,  1.99batch/s, Batch Loss=0.5332, Avg Loss=0.4956, Time Left=24.93 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 411/3393 [02:44<24:59,  1.99batch/s, Batch Loss=0.5377, Avg Loss=0.4957, Time Left=24.93 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 412/3393 [02:44<26:13,  1.89batch/s, Batch Loss=0.5377, Avg Loss=0.4957, Time Left=24.93 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 412/3393 [02:44<26:13,  1.89batch/s, Batch Loss=0.5247, Avg Loss=0.4958, Time Left=24.94 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 413/3393 [02:44<26:51,  1.85batch/s, Batch Loss=0.5247, Avg Loss=0.4958, Time Left=24.94 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 413/3393 [02:45<26:51,  1.85batch/s, Batch Loss=0.3206, Avg Loss=0.4953, Time Left=24.92 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 414/3393 [02:45<25:36,  1.94batch/s, Batch Loss=0.3206, Avg Loss=0.4953, Time Left=24.92 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 414/3393 [02:45<25:36,  1.94batch/s, Batch Loss=0.1104, Avg Loss=0.4942, Time Left=24.92 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 415/3393 [02:45<25:37,  1.94batch/s, Batch Loss=0.1104, Avg Loss=0.4942, Time Left=24.92 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 415/3393 [02:46<25:37,  1.94batch/s, Batch Loss=0.1034, Avg Loss=0.4930, Time Left=24.91 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 416/3393 [02:46<25:10,  1.97batch/s, Batch Loss=0.1034, Avg Loss=0.4930, Time Left=24.91 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 416/3393 [02:47<25:10,  1.97batch/s, Batch Loss=0.1702, Avg Loss=0.4921, Time Left=24.91 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 417/3393 [02:47<25:50,  1.92batch/s, Batch Loss=0.1702, Avg Loss=0.4921, Time Left=24.91 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 417/3393 [02:47<25:50,  1.92batch/s, Batch Loss=0.3107, Avg Loss=0.4915, Time Left=24.92 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 418/3393 [02:47<26:27,  1.87batch/s, Batch Loss=0.3107, Avg Loss=0.4915, Time Left=24.92 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 418/3393 [02:48<26:27,  1.87batch/s, Batch Loss=0.2941, Avg Loss=0.4910, Time Left=24.90 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 419/3393 [02:48<25:31,  1.94batch/s, Batch Loss=0.2941, Avg Loss=0.4910, Time Left=24.90 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 419/3393 [02:48<25:31,  1.94batch/s, Batch Loss=0.4353, Avg Loss=0.4908, Time Left=24.91 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 420/3393 [02:48<26:16,  1.89batch/s, Batch Loss=0.4353, Avg Loss=0.4908, Time Left=24.91 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 420/3393 [02:49<26:16,  1.89batch/s, Batch Loss=0.3594, Avg Loss=0.4904, Time Left=24.89 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 421/3393 [02:49<25:03,  1.98batch/s, Batch Loss=0.3594, Avg Loss=0.4904, Time Left=24.89 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 421/3393 [02:49<25:03,  1.98batch/s, Batch Loss=0.3414, Avg Loss=0.4900, Time Left=24.88 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 422/3393 [02:49<24:34,  2.02batch/s, Batch Loss=0.3414, Avg Loss=0.4900, Time Left=24.88 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 422/3393 [02:50<24:34,  2.02batch/s, Batch Loss=0.2481, Avg Loss=0.4893, Time Left=24.88 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 423/3393 [02:50<24:36,  2.01batch/s, Batch Loss=0.2481, Avg Loss=0.4893, Time Left=24.88 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 423/3393 [02:50<24:36,  2.01batch/s, Batch Loss=0.2180, Avg Loss=0.4885, Time Left=24.87 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 424/3393 [02:50<24:24,  2.03batch/s, Batch Loss=0.2180, Avg Loss=0.4885, Time Left=24.87 \u001b[A\n",
      "Epoch 1/3 - Training:  12%| | 424/3393 [02:51<24:24,  2.03batch/s, Batch Loss=0.1260, Avg Loss=0.4875, Time Left=24.86 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 425/3393 [02:51<24:15,  2.04batch/s, Batch Loss=0.1260, Avg Loss=0.4875, Time Left=24.86 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 425/3393 [02:51<24:15,  2.04batch/s, Batch Loss=0.3264, Avg Loss=0.4870, Time Left=24.85 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 426/3393 [02:51<24:09,  2.05batch/s, Batch Loss=0.3264, Avg Loss=0.4870, Time Left=24.85 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 426/3393 [02:51<24:09,  2.05batch/s, Batch Loss=0.5562, Avg Loss=0.4872, Time Left=24.84 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  13%|▏| 427/3393 [02:51<24:29,  2.02batch/s, Batch Loss=0.5562, Avg Loss=0.4872, Time Left=24.84 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 427/3393 [02:52<24:29,  2.02batch/s, Batch Loss=0.4205, Avg Loss=0.4870, Time Left=24.85 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 428/3393 [02:52<25:42,  1.92batch/s, Batch Loss=0.4205, Avg Loss=0.4870, Time Left=24.85 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 428/3393 [02:53<25:42,  1.92batch/s, Batch Loss=0.3808, Avg Loss=0.4867, Time Left=24.84 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 429/3393 [02:53<25:35,  1.93batch/s, Batch Loss=0.3808, Avg Loss=0.4867, Time Left=24.84 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 429/3393 [02:53<25:35,  1.93batch/s, Batch Loss=0.2365, Avg Loss=0.4860, Time Left=24.84 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 430/3393 [02:53<26:00,  1.90batch/s, Batch Loss=0.2365, Avg Loss=0.4860, Time Left=24.84 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 430/3393 [02:54<26:00,  1.90batch/s, Batch Loss=0.4237, Avg Loss=0.4858, Time Left=24.84 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 431/3393 [02:54<26:15,  1.88batch/s, Batch Loss=0.4237, Avg Loss=0.4858, Time Left=24.84 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 431/3393 [02:54<26:15,  1.88batch/s, Batch Loss=0.5091, Avg Loss=0.4859, Time Left=24.83 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 432/3393 [02:54<25:02,  1.97batch/s, Batch Loss=0.5091, Avg Loss=0.4859, Time Left=24.83 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 432/3393 [02:55<25:02,  1.97batch/s, Batch Loss=0.6325, Avg Loss=0.4863, Time Left=24.82 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 433/3393 [02:55<24:12,  2.04batch/s, Batch Loss=0.6325, Avg Loss=0.4863, Time Left=24.82 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 433/3393 [02:55<24:12,  2.04batch/s, Batch Loss=0.4288, Avg Loss=0.4862, Time Left=24.81 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 434/3393 [02:55<24:25,  2.02batch/s, Batch Loss=0.4288, Avg Loss=0.4862, Time Left=24.81 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 434/3393 [02:56<24:25,  2.02batch/s, Batch Loss=0.7284, Avg Loss=0.4868, Time Left=24.79 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 435/3393 [02:56<23:27,  2.10batch/s, Batch Loss=0.7284, Avg Loss=0.4868, Time Left=24.79 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 435/3393 [02:56<23:27,  2.10batch/s, Batch Loss=0.5145, Avg Loss=0.4869, Time Left=24.78 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 436/3393 [02:56<23:34,  2.09batch/s, Batch Loss=0.5145, Avg Loss=0.4869, Time Left=24.78 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 436/3393 [02:56<23:34,  2.09batch/s, Batch Loss=0.5031, Avg Loss=0.4870, Time Left=24.78 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 437/3393 [02:56<23:39,  2.08batch/s, Batch Loss=0.5031, Avg Loss=0.4870, Time Left=24.78 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 437/3393 [02:57<23:39,  2.08batch/s, Batch Loss=0.4539, Avg Loss=0.4869, Time Left=24.76 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 438/3393 [02:57<23:28,  2.10batch/s, Batch Loss=0.4539, Avg Loss=0.4869, Time Left=24.76 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 438/3393 [02:57<23:28,  2.10batch/s, Batch Loss=0.5231, Avg Loss=0.4870, Time Left=24.75 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 439/3393 [02:57<23:21,  2.11batch/s, Batch Loss=0.5231, Avg Loss=0.4870, Time Left=24.75 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 439/3393 [02:58<23:21,  2.11batch/s, Batch Loss=0.5216, Avg Loss=0.4871, Time Left=24.74 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 440/3393 [02:58<23:15,  2.12batch/s, Batch Loss=0.5216, Avg Loss=0.4871, Time Left=24.74 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 440/3393 [02:58<23:15,  2.12batch/s, Batch Loss=0.6259, Avg Loss=0.4874, Time Left=24.73 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 441/3393 [02:58<23:11,  2.12batch/s, Batch Loss=0.6259, Avg Loss=0.4874, Time Left=24.73 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 441/3393 [02:59<23:11,  2.12batch/s, Batch Loss=0.2310, Avg Loss=0.4867, Time Left=24.72 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 442/3393 [02:59<23:08,  2.12batch/s, Batch Loss=0.2310, Avg Loss=0.4867, Time Left=24.72 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 442/3393 [02:59<23:08,  2.12batch/s, Batch Loss=0.3448, Avg Loss=0.4864, Time Left=24.71 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 443/3393 [02:59<23:06,  2.13batch/s, Batch Loss=0.3448, Avg Loss=0.4864, Time Left=24.71 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 443/3393 [03:00<23:06,  2.13batch/s, Batch Loss=0.3335, Avg Loss=0.4859, Time Left=24.70 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 444/3393 [03:00<23:46,  2.07batch/s, Batch Loss=0.3335, Avg Loss=0.4859, Time Left=24.70 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 444/3393 [03:00<23:46,  2.07batch/s, Batch Loss=0.3489, Avg Loss=0.4856, Time Left=24.69 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 445/3393 [03:00<23:32,  2.09batch/s, Batch Loss=0.3489, Avg Loss=0.4856, Time Left=24.69 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 445/3393 [03:01<23:32,  2.09batch/s, Batch Loss=0.5408, Avg Loss=0.4857, Time Left=24.68 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 446/3393 [03:01<23:23,  2.10batch/s, Batch Loss=0.5408, Avg Loss=0.4857, Time Left=24.68 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 446/3393 [03:01<23:23,  2.10batch/s, Batch Loss=0.4601, Avg Loss=0.4856, Time Left=24.67 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 447/3393 [03:01<23:32,  2.09batch/s, Batch Loss=0.4601, Avg Loss=0.4856, Time Left=24.67 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 447/3393 [03:02<23:32,  2.09batch/s, Batch Loss=0.5303, Avg Loss=0.4858, Time Left=24.68 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 448/3393 [03:02<25:30,  1.92batch/s, Batch Loss=0.5303, Avg Loss=0.4858, Time Left=24.68 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 448/3393 [03:02<25:30,  1.92batch/s, Batch Loss=0.3488, Avg Loss=0.4854, Time Left=24.68 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 449/3393 [03:02<25:41,  1.91batch/s, Batch Loss=0.3488, Avg Loss=0.4854, Time Left=24.68 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 449/3393 [03:03<25:41,  1.91batch/s, Batch Loss=0.6935, Avg Loss=0.4860, Time Left=24.67 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 450/3393 [03:03<25:31,  1.92batch/s, Batch Loss=0.6935, Avg Loss=0.4860, Time Left=24.67 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 450/3393 [03:03<25:31,  1.92batch/s, Batch Loss=0.3296, Avg Loss=0.4855, Time Left=24.67 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 451/3393 [03:03<25:40,  1.91batch/s, Batch Loss=0.3296, Avg Loss=0.4855, Time Left=24.67 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 451/3393 [03:04<25:40,  1.91batch/s, Batch Loss=0.4021, Avg Loss=0.4853, Time Left=24.67 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 452/3393 [03:04<25:42,  1.91batch/s, Batch Loss=0.4021, Avg Loss=0.4853, Time Left=24.67 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 452/3393 [03:04<25:42,  1.91batch/s, Batch Loss=0.6814, Avg Loss=0.4858, Time Left=24.66 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 453/3393 [03:04<25:30,  1.92batch/s, Batch Loss=0.6814, Avg Loss=0.4858, Time Left=24.66 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 453/3393 [03:05<25:30,  1.92batch/s, Batch Loss=0.6045, Avg Loss=0.4862, Time Left=24.66 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 454/3393 [03:05<25:26,  1.92batch/s, Batch Loss=0.6045, Avg Loss=0.4862, Time Left=24.66 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 454/3393 [03:05<25:26,  1.92batch/s, Batch Loss=0.4510, Avg Loss=0.4861, Time Left=24.65 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 455/3393 [03:05<25:21,  1.93batch/s, Batch Loss=0.4510, Avg Loss=0.4861, Time Left=24.65 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 455/3393 [03:06<25:21,  1.93batch/s, Batch Loss=0.1251, Avg Loss=0.4851, Time Left=24.65 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 456/3393 [03:06<26:02,  1.88batch/s, Batch Loss=0.1251, Avg Loss=0.4851, Time Left=24.65 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 456/3393 [03:07<26:02,  1.88batch/s, Batch Loss=0.2100, Avg Loss=0.4844, Time Left=24.65 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 457/3393 [03:07<26:21,  1.86batch/s, Batch Loss=0.2100, Avg Loss=0.4844, Time Left=24.65 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 457/3393 [03:07<26:21,  1.86batch/s, Batch Loss=0.2634, Avg Loss=0.4838, Time Left=24.65 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 458/3393 [03:07<25:58,  1.88batch/s, Batch Loss=0.2634, Avg Loss=0.4838, Time Left=24.65 \u001b[A\n",
      "Epoch 1/3 - Training:  13%|▏| 458/3393 [03:08<25:58,  1.88batch/s, Batch Loss=0.4841, Avg Loss=0.4838, Time Left=24.64 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 459/3393 [03:08<25:11,  1.94batch/s, Batch Loss=0.4841, Avg Loss=0.4838, Time Left=24.64 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 459/3393 [03:08<25:11,  1.94batch/s, Batch Loss=0.3017, Avg Loss=0.4833, Time Left=24.64 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  14%|▏| 460/3393 [03:08<25:18,  1.93batch/s, Batch Loss=0.3017, Avg Loss=0.4833, Time Left=24.64 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 460/3393 [03:09<25:18,  1.93batch/s, Batch Loss=0.3784, Avg Loss=0.4831, Time Left=24.62 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 461/3393 [03:09<24:32,  1.99batch/s, Batch Loss=0.3784, Avg Loss=0.4831, Time Left=24.62 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 461/3393 [03:09<24:32,  1.99batch/s, Batch Loss=0.2477, Avg Loss=0.4825, Time Left=24.62 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 462/3393 [03:09<25:11,  1.94batch/s, Batch Loss=0.2477, Avg Loss=0.4825, Time Left=24.62 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 462/3393 [03:10<25:11,  1.94batch/s, Batch Loss=0.3143, Avg Loss=0.4820, Time Left=24.62 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 463/3393 [03:10<25:10,  1.94batch/s, Batch Loss=0.3143, Avg Loss=0.4820, Time Left=24.62 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 463/3393 [03:10<25:10,  1.94batch/s, Batch Loss=0.3142, Avg Loss=0.4816, Time Left=24.61 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 464/3393 [03:10<24:31,  1.99batch/s, Batch Loss=0.3142, Avg Loss=0.4816, Time Left=24.61 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 464/3393 [03:11<24:31,  1.99batch/s, Batch Loss=0.4152, Avg Loss=0.4814, Time Left=24.60 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 465/3393 [03:11<24:58,  1.95batch/s, Batch Loss=0.4152, Avg Loss=0.4814, Time Left=24.60 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 465/3393 [03:11<24:58,  1.95batch/s, Batch Loss=0.4703, Avg Loss=0.4814, Time Left=24.60 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 466/3393 [03:11<24:38,  1.98batch/s, Batch Loss=0.4703, Avg Loss=0.4814, Time Left=24.60 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 466/3393 [03:12<24:38,  1.98batch/s, Batch Loss=0.4235, Avg Loss=0.4812, Time Left=24.59 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 467/3393 [03:12<24:47,  1.97batch/s, Batch Loss=0.4235, Avg Loss=0.4812, Time Left=24.59 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 467/3393 [03:12<24:47,  1.97batch/s, Batch Loss=0.4626, Avg Loss=0.4812, Time Left=24.58 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 468/3393 [03:12<24:40,  1.98batch/s, Batch Loss=0.4626, Avg Loss=0.4812, Time Left=24.58 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 468/3393 [03:13<24:40,  1.98batch/s, Batch Loss=0.2147, Avg Loss=0.4805, Time Left=24.58 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 469/3393 [03:13<24:52,  1.96batch/s, Batch Loss=0.2147, Avg Loss=0.4805, Time Left=24.58 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 469/3393 [03:13<24:52,  1.96batch/s, Batch Loss=0.7384, Avg Loss=0.4812, Time Left=24.57 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 470/3393 [03:13<24:24,  2.00batch/s, Batch Loss=0.7384, Avg Loss=0.4812, Time Left=24.57 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 470/3393 [03:14<24:24,  2.00batch/s, Batch Loss=0.3369, Avg Loss=0.4808, Time Left=24.56 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 471/3393 [03:14<24:14,  2.01batch/s, Batch Loss=0.3369, Avg Loss=0.4808, Time Left=24.56 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 471/3393 [03:14<24:14,  2.01batch/s, Batch Loss=0.1561, Avg Loss=0.4800, Time Left=24.55 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 472/3393 [03:14<23:58,  2.03batch/s, Batch Loss=0.1561, Avg Loss=0.4800, Time Left=24.55 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 472/3393 [03:15<23:58,  2.03batch/s, Batch Loss=0.1402, Avg Loss=0.4791, Time Left=24.54 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 473/3393 [03:15<23:56,  2.03batch/s, Batch Loss=0.1402, Avg Loss=0.4791, Time Left=24.54 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 473/3393 [03:15<23:56,  2.03batch/s, Batch Loss=0.1478, Avg Loss=0.4783, Time Left=24.54 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 474/3393 [03:15<24:15,  2.00batch/s, Batch Loss=0.1478, Avg Loss=0.4783, Time Left=24.54 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 474/3393 [03:16<24:15,  2.00batch/s, Batch Loss=0.1637, Avg Loss=0.4775, Time Left=24.53 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 475/3393 [03:16<24:03,  2.02batch/s, Batch Loss=0.1637, Avg Loss=0.4775, Time Left=24.53 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 475/3393 [03:16<24:03,  2.02batch/s, Batch Loss=0.3652, Avg Loss=0.4772, Time Left=24.52 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 476/3393 [03:16<23:51,  2.04batch/s, Batch Loss=0.3652, Avg Loss=0.4772, Time Left=24.52 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 476/3393 [03:17<23:51,  2.04batch/s, Batch Loss=0.2747, Avg Loss=0.4767, Time Left=24.51 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 477/3393 [03:17<23:22,  2.08batch/s, Batch Loss=0.2747, Avg Loss=0.4767, Time Left=24.51 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 477/3393 [03:17<23:22,  2.08batch/s, Batch Loss=0.2313, Avg Loss=0.4761, Time Left=24.50 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 478/3393 [03:17<23:23,  2.08batch/s, Batch Loss=0.2313, Avg Loss=0.4761, Time Left=24.50 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 478/3393 [03:18<23:23,  2.08batch/s, Batch Loss=0.3723, Avg Loss=0.4758, Time Left=24.49 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 479/3393 [03:18<23:38,  2.05batch/s, Batch Loss=0.3723, Avg Loss=0.4758, Time Left=24.49 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 479/3393 [03:18<23:38,  2.05batch/s, Batch Loss=0.5302, Avg Loss=0.4760, Time Left=24.48 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 480/3393 [03:18<23:21,  2.08batch/s, Batch Loss=0.5302, Avg Loss=0.4760, Time Left=24.48 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 480/3393 [03:19<23:21,  2.08batch/s, Batch Loss=0.3152, Avg Loss=0.4756, Time Left=24.47 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 481/3393 [03:19<23:28,  2.07batch/s, Batch Loss=0.3152, Avg Loss=0.4756, Time Left=24.47 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 481/3393 [03:19<23:28,  2.07batch/s, Batch Loss=0.1805, Avg Loss=0.4749, Time Left=24.47 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 482/3393 [03:19<23:51,  2.03batch/s, Batch Loss=0.1805, Avg Loss=0.4749, Time Left=24.47 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 482/3393 [03:20<23:51,  2.03batch/s, Batch Loss=0.0675, Avg Loss=0.4739, Time Left=24.46 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 483/3393 [03:20<23:59,  2.02batch/s, Batch Loss=0.0675, Avg Loss=0.4739, Time Left=24.46 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 483/3393 [03:20<23:59,  2.02batch/s, Batch Loss=0.2207, Avg Loss=0.4732, Time Left=24.45 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 484/3393 [03:20<23:34,  2.06batch/s, Batch Loss=0.2207, Avg Loss=0.4732, Time Left=24.45 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 484/3393 [03:20<23:34,  2.06batch/s, Batch Loss=0.3089, Avg Loss=0.4728, Time Left=24.44 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 485/3393 [03:20<23:45,  2.04batch/s, Batch Loss=0.3089, Avg Loss=0.4728, Time Left=24.44 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 485/3393 [03:21<23:45,  2.04batch/s, Batch Loss=0.3526, Avg Loss=0.4725, Time Left=24.43 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 486/3393 [03:21<23:38,  2.05batch/s, Batch Loss=0.3526, Avg Loss=0.4725, Time Left=24.43 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 486/3393 [03:21<23:38,  2.05batch/s, Batch Loss=0.5570, Avg Loss=0.4727, Time Left=24.42 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 487/3393 [03:21<23:57,  2.02batch/s, Batch Loss=0.5570, Avg Loss=0.4727, Time Left=24.42 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 487/3393 [03:22<23:57,  2.02batch/s, Batch Loss=0.2688, Avg Loss=0.4723, Time Left=24.42 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 488/3393 [03:22<24:02,  2.01batch/s, Batch Loss=0.2688, Avg Loss=0.4723, Time Left=24.42 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 488/3393 [03:23<24:02,  2.01batch/s, Batch Loss=0.2147, Avg Loss=0.4716, Time Left=24.42 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 489/3393 [03:23<25:08,  1.93batch/s, Batch Loss=0.2147, Avg Loss=0.4716, Time Left=24.42 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 489/3393 [03:23<25:08,  1.93batch/s, Batch Loss=0.1790, Avg Loss=0.4709, Time Left=24.41 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 490/3393 [03:23<24:17,  1.99batch/s, Batch Loss=0.1790, Avg Loss=0.4709, Time Left=24.41 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 490/3393 [03:24<24:17,  1.99batch/s, Batch Loss=0.1791, Avg Loss=0.4702, Time Left=24.40 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 491/3393 [03:24<24:16,  1.99batch/s, Batch Loss=0.1791, Avg Loss=0.4702, Time Left=24.40 \u001b[A\n",
      "Epoch 1/3 - Training:  14%|▏| 491/3393 [03:24<24:16,  1.99batch/s, Batch Loss=0.1166, Avg Loss=0.4694, Time Left=24.39 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 492/3393 [03:24<23:53,  2.02batch/s, Batch Loss=0.1166, Avg Loss=0.4694, Time Left=24.39 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 492/3393 [03:24<23:53,  2.02batch/s, Batch Loss=0.2841, Avg Loss=0.4689, Time Left=24.38 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  15%|▏| 493/3393 [03:24<23:21,  2.07batch/s, Batch Loss=0.2841, Avg Loss=0.4689, Time Left=24.38 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 493/3393 [03:25<23:21,  2.07batch/s, Batch Loss=0.1255, Avg Loss=0.4681, Time Left=24.37 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 494/3393 [03:25<23:49,  2.03batch/s, Batch Loss=0.1255, Avg Loss=0.4681, Time Left=24.37 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 494/3393 [03:25<23:49,  2.03batch/s, Batch Loss=0.5250, Avg Loss=0.4682, Time Left=24.36 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 495/3393 [03:25<23:43,  2.04batch/s, Batch Loss=0.5250, Avg Loss=0.4682, Time Left=24.36 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 495/3393 [03:26<23:43,  2.04batch/s, Batch Loss=0.1500, Avg Loss=0.4675, Time Left=24.36 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 496/3393 [03:26<24:34,  1.97batch/s, Batch Loss=0.1500, Avg Loss=0.4675, Time Left=24.36 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 496/3393 [03:27<24:34,  1.97batch/s, Batch Loss=0.1773, Avg Loss=0.4668, Time Left=24.37 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 497/3393 [03:27<25:47,  1.87batch/s, Batch Loss=0.1773, Avg Loss=0.4668, Time Left=24.37 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 497/3393 [03:27<25:47,  1.87batch/s, Batch Loss=0.4970, Avg Loss=0.4669, Time Left=24.36 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 498/3393 [03:27<25:40,  1.88batch/s, Batch Loss=0.4970, Avg Loss=0.4669, Time Left=24.36 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 498/3393 [03:28<25:40,  1.88batch/s, Batch Loss=0.5356, Avg Loss=0.4670, Time Left=24.37 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 499/3393 [03:28<26:22,  1.83batch/s, Batch Loss=0.5356, Avg Loss=0.4670, Time Left=24.37 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 499/3393 [03:28<26:22,  1.83batch/s, Batch Loss=0.2698, Avg Loss=0.4666, Time Left=24.37 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 500/3393 [03:28<26:47,  1.80batch/s, Batch Loss=0.2698, Avg Loss=0.4666, Time Left=24.37 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 500/3393 [03:29<26:47,  1.80batch/s, Batch Loss=0.1796, Avg Loss=0.4659, Time Left=24.36 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 501/3393 [03:29<25:46,  1.87batch/s, Batch Loss=0.1796, Avg Loss=0.4659, Time Left=24.36 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 501/3393 [03:29<25:46,  1.87batch/s, Batch Loss=0.2849, Avg Loss=0.4655, Time Left=24.35 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 502/3393 [03:29<25:35,  1.88batch/s, Batch Loss=0.2849, Avg Loss=0.4655, Time Left=24.35 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 502/3393 [03:30<25:35,  1.88batch/s, Batch Loss=0.4952, Avg Loss=0.4655, Time Left=24.35 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 503/3393 [03:30<25:06,  1.92batch/s, Batch Loss=0.4952, Avg Loss=0.4655, Time Left=24.35 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 503/3393 [03:30<25:06,  1.92batch/s, Batch Loss=0.2616, Avg Loss=0.4651, Time Left=24.35 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 504/3393 [03:30<25:45,  1.87batch/s, Batch Loss=0.2616, Avg Loss=0.4651, Time Left=24.35 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 504/3393 [03:31<25:45,  1.87batch/s, Batch Loss=0.1944, Avg Loss=0.4644, Time Left=24.34 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 505/3393 [03:31<25:23,  1.90batch/s, Batch Loss=0.1944, Avg Loss=0.4644, Time Left=24.34 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 505/3393 [03:31<25:23,  1.90batch/s, Batch Loss=0.4230, Avg Loss=0.4643, Time Left=24.33 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 506/3393 [03:31<24:37,  1.95batch/s, Batch Loss=0.4230, Avg Loss=0.4643, Time Left=24.33 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 506/3393 [03:32<24:37,  1.95batch/s, Batch Loss=0.1937, Avg Loss=0.4637, Time Left=24.32 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 507/3393 [03:32<24:42,  1.95batch/s, Batch Loss=0.1937, Avg Loss=0.4637, Time Left=24.32 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 507/3393 [03:32<24:42,  1.95batch/s, Batch Loss=0.4718, Avg Loss=0.4637, Time Left=24.32 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 508/3393 [03:32<24:58,  1.93batch/s, Batch Loss=0.4718, Avg Loss=0.4637, Time Left=24.32 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 508/3393 [03:33<24:58,  1.93batch/s, Batch Loss=0.3883, Avg Loss=0.4635, Time Left=24.31 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 509/3393 [03:33<24:42,  1.95batch/s, Batch Loss=0.3883, Avg Loss=0.4635, Time Left=24.31 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 509/3393 [03:33<24:42,  1.95batch/s, Batch Loss=0.2409, Avg Loss=0.4630, Time Left=24.31 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 510/3393 [03:33<24:30,  1.96batch/s, Batch Loss=0.2409, Avg Loss=0.4630, Time Left=24.31 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 510/3393 [03:34<24:30,  1.96batch/s, Batch Loss=0.4353, Avg Loss=0.4630, Time Left=24.30 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 511/3393 [03:34<24:33,  1.96batch/s, Batch Loss=0.4353, Avg Loss=0.4630, Time Left=24.30 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 511/3393 [03:34<24:33,  1.96batch/s, Batch Loss=0.3341, Avg Loss=0.4627, Time Left=24.30 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 512/3393 [03:34<24:52,  1.93batch/s, Batch Loss=0.3341, Avg Loss=0.4627, Time Left=24.30 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 512/3393 [03:35<24:52,  1.93batch/s, Batch Loss=0.3898, Avg Loss=0.4625, Time Left=24.29 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 513/3393 [03:35<24:33,  1.96batch/s, Batch Loss=0.3898, Avg Loss=0.4625, Time Left=24.29 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 513/3393 [03:35<24:33,  1.96batch/s, Batch Loss=0.3240, Avg Loss=0.4622, Time Left=24.28 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 514/3393 [03:35<24:37,  1.95batch/s, Batch Loss=0.3240, Avg Loss=0.4622, Time Left=24.28 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 514/3393 [03:36<24:37,  1.95batch/s, Batch Loss=0.3812, Avg Loss=0.4620, Time Left=24.27 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 515/3393 [03:36<24:29,  1.96batch/s, Batch Loss=0.3812, Avg Loss=0.4620, Time Left=24.27 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 515/3393 [03:37<24:29,  1.96batch/s, Batch Loss=0.1522, Avg Loss=0.4613, Time Left=24.27 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 516/3393 [03:37<24:50,  1.93batch/s, Batch Loss=0.1522, Avg Loss=0.4613, Time Left=24.27 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 516/3393 [03:37<24:50,  1.93batch/s, Batch Loss=0.2348, Avg Loss=0.4608, Time Left=24.26 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 517/3393 [03:37<24:34,  1.95batch/s, Batch Loss=0.2348, Avg Loss=0.4608, Time Left=24.26 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 517/3393 [03:38<24:34,  1.95batch/s, Batch Loss=0.2793, Avg Loss=0.4604, Time Left=24.26 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 518/3393 [03:38<24:29,  1.96batch/s, Batch Loss=0.2793, Avg Loss=0.4604, Time Left=24.26 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 518/3393 [03:38<24:29,  1.96batch/s, Batch Loss=0.3068, Avg Loss=0.4600, Time Left=24.25 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 519/3393 [03:38<24:27,  1.96batch/s, Batch Loss=0.3068, Avg Loss=0.4600, Time Left=24.25 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 519/3393 [03:39<24:27,  1.96batch/s, Batch Loss=0.3679, Avg Loss=0.4598, Time Left=24.25 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 520/3393 [03:39<24:32,  1.95batch/s, Batch Loss=0.3679, Avg Loss=0.4598, Time Left=24.25 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 520/3393 [03:39<24:32,  1.95batch/s, Batch Loss=0.3612, Avg Loss=0.4596, Time Left=24.24 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 521/3393 [03:39<24:38,  1.94batch/s, Batch Loss=0.3612, Avg Loss=0.4596, Time Left=24.24 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 521/3393 [03:40<24:38,  1.94batch/s, Batch Loss=0.1951, Avg Loss=0.4590, Time Left=24.23 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 522/3393 [03:40<24:07,  1.98batch/s, Batch Loss=0.1951, Avg Loss=0.4590, Time Left=24.23 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 522/3393 [03:40<24:07,  1.98batch/s, Batch Loss=0.3222, Avg Loss=0.4587, Time Left=24.22 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 523/3393 [03:40<24:06,  1.98batch/s, Batch Loss=0.3222, Avg Loss=0.4587, Time Left=24.22 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 523/3393 [03:41<24:06,  1.98batch/s, Batch Loss=0.1401, Avg Loss=0.4580, Time Left=24.22 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 524/3393 [03:41<24:23,  1.96batch/s, Batch Loss=0.1401, Avg Loss=0.4580, Time Left=24.22 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 524/3393 [03:41<24:23,  1.96batch/s, Batch Loss=0.1704, Avg Loss=0.4574, Time Left=24.21 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 525/3393 [03:41<24:07,  1.98batch/s, Batch Loss=0.1704, Avg Loss=0.4574, Time Left=24.21 \u001b[A\n",
      "Epoch 1/3 - Training:  15%|▏| 525/3393 [03:42<24:07,  1.98batch/s, Batch Loss=0.1700, Avg Loss=0.4567, Time Left=24.21 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  16%|▏| 526/3393 [03:42<24:18,  1.97batch/s, Batch Loss=0.1700, Avg Loss=0.4567, Time Left=24.21 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 526/3393 [03:42<24:18,  1.97batch/s, Batch Loss=0.1671, Avg Loss=0.4561, Time Left=24.20 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 527/3393 [03:42<23:57,  1.99batch/s, Batch Loss=0.1671, Avg Loss=0.4561, Time Left=24.20 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 527/3393 [03:43<23:57,  1.99batch/s, Batch Loss=0.2233, Avg Loss=0.4556, Time Left=24.19 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 528/3393 [03:43<24:07,  1.98batch/s, Batch Loss=0.2233, Avg Loss=0.4556, Time Left=24.19 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 528/3393 [03:43<24:07,  1.98batch/s, Batch Loss=0.3432, Avg Loss=0.4553, Time Left=24.18 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 529/3393 [03:43<23:40,  2.02batch/s, Batch Loss=0.3432, Avg Loss=0.4553, Time Left=24.18 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 529/3393 [03:44<23:40,  2.02batch/s, Batch Loss=0.2980, Avg Loss=0.4550, Time Left=24.17 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 530/3393 [03:44<23:30,  2.03batch/s, Batch Loss=0.2980, Avg Loss=0.4550, Time Left=24.17 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 530/3393 [03:44<23:30,  2.03batch/s, Batch Loss=0.1517, Avg Loss=0.4543, Time Left=24.17 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 531/3393 [03:44<23:50,  2.00batch/s, Batch Loss=0.1517, Avg Loss=0.4543, Time Left=24.17 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 531/3393 [03:45<23:50,  2.00batch/s, Batch Loss=0.0711, Avg Loss=0.4535, Time Left=24.16 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 532/3393 [03:45<23:34,  2.02batch/s, Batch Loss=0.0711, Avg Loss=0.4535, Time Left=24.16 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 532/3393 [03:45<23:34,  2.02batch/s, Batch Loss=0.2447, Avg Loss=0.4530, Time Left=24.15 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 533/3393 [03:45<24:13,  1.97batch/s, Batch Loss=0.2447, Avg Loss=0.4530, Time Left=24.15 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 533/3393 [03:46<24:13,  1.97batch/s, Batch Loss=0.3886, Avg Loss=0.4529, Time Left=24.15 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 534/3393 [03:46<24:12,  1.97batch/s, Batch Loss=0.3886, Avg Loss=0.4529, Time Left=24.15 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 534/3393 [03:46<24:12,  1.97batch/s, Batch Loss=0.2424, Avg Loss=0.4524, Time Left=24.14 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 535/3393 [03:46<24:29,  1.94batch/s, Batch Loss=0.2424, Avg Loss=0.4524, Time Left=24.14 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 535/3393 [03:47<24:29,  1.94batch/s, Batch Loss=0.4595, Avg Loss=0.4524, Time Left=24.13 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 536/3393 [03:47<24:18,  1.96batch/s, Batch Loss=0.4595, Avg Loss=0.4524, Time Left=24.13 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 536/3393 [03:47<24:18,  1.96batch/s, Batch Loss=0.2212, Avg Loss=0.4519, Time Left=24.13 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 537/3393 [03:47<24:28,  1.95batch/s, Batch Loss=0.2212, Avg Loss=0.4519, Time Left=24.13 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 537/3393 [03:48<24:28,  1.95batch/s, Batch Loss=0.5464, Avg Loss=0.4521, Time Left=24.12 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 538/3393 [03:48<23:56,  1.99batch/s, Batch Loss=0.5464, Avg Loss=0.4521, Time Left=24.12 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 538/3393 [03:48<23:56,  1.99batch/s, Batch Loss=0.2845, Avg Loss=0.4518, Time Left=24.11 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 539/3393 [03:48<24:15,  1.96batch/s, Batch Loss=0.2845, Avg Loss=0.4518, Time Left=24.11 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 539/3393 [03:49<24:15,  1.96batch/s, Batch Loss=0.3741, Avg Loss=0.4516, Time Left=24.10 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 540/3393 [03:49<23:38,  2.01batch/s, Batch Loss=0.3741, Avg Loss=0.4516, Time Left=24.10 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 540/3393 [03:49<23:38,  2.01batch/s, Batch Loss=0.2755, Avg Loss=0.4512, Time Left=24.10 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 541/3393 [03:49<23:39,  2.01batch/s, Batch Loss=0.2755, Avg Loss=0.4512, Time Left=24.10 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 541/3393 [03:50<23:39,  2.01batch/s, Batch Loss=0.3171, Avg Loss=0.4509, Time Left=24.08 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 542/3393 [03:50<23:07,  2.05batch/s, Batch Loss=0.3171, Avg Loss=0.4509, Time Left=24.08 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 542/3393 [03:50<23:07,  2.05batch/s, Batch Loss=0.2842, Avg Loss=0.4506, Time Left=24.08 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 543/3393 [03:50<23:13,  2.05batch/s, Batch Loss=0.2842, Avg Loss=0.4506, Time Left=24.08 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 543/3393 [03:51<23:13,  2.05batch/s, Batch Loss=0.6432, Avg Loss=0.4510, Time Left=24.07 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 544/3393 [03:51<23:45,  2.00batch/s, Batch Loss=0.6432, Avg Loss=0.4510, Time Left=24.07 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 544/3393 [03:51<23:45,  2.00batch/s, Batch Loss=0.2397, Avg Loss=0.4505, Time Left=24.06 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 545/3393 [03:51<23:46,  2.00batch/s, Batch Loss=0.2397, Avg Loss=0.4505, Time Left=24.06 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 545/3393 [03:52<23:46,  2.00batch/s, Batch Loss=0.1727, Avg Loss=0.4499, Time Left=24.06 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 546/3393 [03:52<23:55,  1.98batch/s, Batch Loss=0.1727, Avg Loss=0.4499, Time Left=24.06 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 546/3393 [03:52<23:55,  1.98batch/s, Batch Loss=0.2096, Avg Loss=0.4494, Time Left=24.05 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 547/3393 [03:52<23:36,  2.01batch/s, Batch Loss=0.2096, Avg Loss=0.4494, Time Left=24.05 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 547/3393 [03:53<23:36,  2.01batch/s, Batch Loss=0.1097, Avg Loss=0.4487, Time Left=24.04 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 548/3393 [03:53<23:23,  2.03batch/s, Batch Loss=0.1097, Avg Loss=0.4487, Time Left=24.04 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 548/3393 [03:53<23:23,  2.03batch/s, Batch Loss=0.3876, Avg Loss=0.4486, Time Left=24.03 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 549/3393 [03:53<23:30,  2.02batch/s, Batch Loss=0.3876, Avg Loss=0.4486, Time Left=24.03 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 549/3393 [03:54<23:30,  2.02batch/s, Batch Loss=0.1681, Avg Loss=0.4480, Time Left=24.02 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 550/3393 [03:54<23:09,  2.05batch/s, Batch Loss=0.1681, Avg Loss=0.4480, Time Left=24.02 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 550/3393 [03:54<23:09,  2.05batch/s, Batch Loss=0.0903, Avg Loss=0.4472, Time Left=24.02 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 551/3393 [03:54<23:34,  2.01batch/s, Batch Loss=0.0903, Avg Loss=0.4472, Time Left=24.02 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 551/3393 [03:55<23:34,  2.01batch/s, Batch Loss=0.3157, Avg Loss=0.4470, Time Left=24.01 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 552/3393 [03:55<23:24,  2.02batch/s, Batch Loss=0.3157, Avg Loss=0.4470, Time Left=24.01 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 552/3393 [03:55<23:24,  2.02batch/s, Batch Loss=0.2022, Avg Loss=0.4464, Time Left=24.00 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 553/3393 [03:55<23:43,  1.99batch/s, Batch Loss=0.2022, Avg Loss=0.4464, Time Left=24.00 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 553/3393 [03:56<23:43,  1.99batch/s, Batch Loss=0.1553, Avg Loss=0.4458, Time Left=23.99 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 554/3393 [03:56<23:15,  2.03batch/s, Batch Loss=0.1553, Avg Loss=0.4458, Time Left=23.99 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 554/3393 [03:56<23:15,  2.03batch/s, Batch Loss=0.1110, Avg Loss=0.4451, Time Left=23.98 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 555/3393 [03:56<22:58,  2.06batch/s, Batch Loss=0.1110, Avg Loss=0.4451, Time Left=23.98 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 555/3393 [03:57<22:58,  2.06batch/s, Batch Loss=0.4290, Avg Loss=0.4451, Time Left=23.97 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 556/3393 [03:57<23:25,  2.02batch/s, Batch Loss=0.4290, Avg Loss=0.4451, Time Left=23.97 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 556/3393 [03:57<23:25,  2.02batch/s, Batch Loss=0.0650, Avg Loss=0.4443, Time Left=23.96 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 557/3393 [03:57<22:51,  2.07batch/s, Batch Loss=0.0650, Avg Loss=0.4443, Time Left=23.96 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 557/3393 [03:57<22:51,  2.07batch/s, Batch Loss=0.5817, Avg Loss=0.4446, Time Left=23.95 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 558/3393 [03:57<22:52,  2.07batch/s, Batch Loss=0.5817, Avg Loss=0.4446, Time Left=23.95 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 558/3393 [03:58<22:52,  2.07batch/s, Batch Loss=0.3562, Avg Loss=0.4444, Time Left=23.94 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  16%|▏| 559/3393 [03:58<23:08,  2.04batch/s, Batch Loss=0.3562, Avg Loss=0.4444, Time Left=23.94 \u001b[A\n",
      "Epoch 1/3 - Training:  16%|▏| 559/3393 [03:58<23:08,  2.04batch/s, Batch Loss=0.4437, Avg Loss=0.4444, Time Left=23.94 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 560/3393 [03:58<23:04,  2.05batch/s, Batch Loss=0.4437, Avg Loss=0.4444, Time Left=23.94 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 560/3393 [03:59<23:04,  2.05batch/s, Batch Loss=0.4488, Avg Loss=0.4444, Time Left=23.93 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 561/3393 [03:59<23:27,  2.01batch/s, Batch Loss=0.4488, Avg Loss=0.4444, Time Left=23.93 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 561/3393 [03:59<23:27,  2.01batch/s, Batch Loss=0.2921, Avg Loss=0.4441, Time Left=23.92 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 562/3393 [03:59<23:18,  2.02batch/s, Batch Loss=0.2921, Avg Loss=0.4441, Time Left=23.92 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 562/3393 [04:00<23:18,  2.02batch/s, Batch Loss=0.0808, Avg Loss=0.4434, Time Left=23.92 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 563/3393 [04:00<23:50,  1.98batch/s, Batch Loss=0.0808, Avg Loss=0.4434, Time Left=23.92 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 563/3393 [04:00<23:50,  1.98batch/s, Batch Loss=0.5834, Avg Loss=0.4436, Time Left=23.91 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 564/3393 [04:00<23:34,  2.00batch/s, Batch Loss=0.5834, Avg Loss=0.4436, Time Left=23.91 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 564/3393 [04:01<23:34,  2.00batch/s, Batch Loss=0.7737, Avg Loss=0.4443, Time Left=23.90 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 565/3393 [04:01<23:36,  2.00batch/s, Batch Loss=0.7737, Avg Loss=0.4443, Time Left=23.90 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 565/3393 [04:01<23:36,  2.00batch/s, Batch Loss=0.4926, Avg Loss=0.4444, Time Left=23.89 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 566/3393 [04:01<23:10,  2.03batch/s, Batch Loss=0.4926, Avg Loss=0.4444, Time Left=23.89 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 566/3393 [04:02<23:10,  2.03batch/s, Batch Loss=0.6327, Avg Loss=0.4448, Time Left=23.88 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 567/3393 [04:02<23:06,  2.04batch/s, Batch Loss=0.6327, Avg Loss=0.4448, Time Left=23.88 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 567/3393 [04:02<23:06,  2.04batch/s, Batch Loss=0.8472, Avg Loss=0.4456, Time Left=23.87 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 568/3393 [04:02<23:26,  2.01batch/s, Batch Loss=0.8472, Avg Loss=0.4456, Time Left=23.87 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 568/3393 [04:03<23:26,  2.01batch/s, Batch Loss=0.7037, Avg Loss=0.4461, Time Left=23.86 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 569/3393 [04:03<22:38,  2.08batch/s, Batch Loss=0.7037, Avg Loss=0.4461, Time Left=23.86 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 569/3393 [04:03<22:38,  2.08batch/s, Batch Loss=0.5716, Avg Loss=0.4464, Time Left=23.85 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 570/3393 [04:03<22:30,  2.09batch/s, Batch Loss=0.5716, Avg Loss=0.4464, Time Left=23.85 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 570/3393 [04:04<22:30,  2.09batch/s, Batch Loss=0.6288, Avg Loss=0.4468, Time Left=23.85 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 571/3393 [04:04<23:13,  2.02batch/s, Batch Loss=0.6288, Avg Loss=0.4468, Time Left=23.85 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 571/3393 [04:04<23:13,  2.02batch/s, Batch Loss=0.5418, Avg Loss=0.4470, Time Left=23.84 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 572/3393 [04:04<22:54,  2.05batch/s, Batch Loss=0.5418, Avg Loss=0.4470, Time Left=23.84 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 572/3393 [04:05<22:54,  2.05batch/s, Batch Loss=0.3136, Avg Loss=0.4467, Time Left=23.83 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 573/3393 [04:05<23:19,  2.02batch/s, Batch Loss=0.3136, Avg Loss=0.4467, Time Left=23.83 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 573/3393 [04:05<23:19,  2.02batch/s, Batch Loss=0.3180, Avg Loss=0.4464, Time Left=23.82 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 574/3393 [04:05<22:44,  2.07batch/s, Batch Loss=0.3180, Avg Loss=0.4464, Time Left=23.82 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 574/3393 [04:06<22:44,  2.07batch/s, Batch Loss=0.4502, Avg Loss=0.4464, Time Left=23.81 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 575/3393 [04:06<22:59,  2.04batch/s, Batch Loss=0.4502, Avg Loss=0.4464, Time Left=23.81 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 575/3393 [04:06<22:59,  2.04batch/s, Batch Loss=0.3785, Avg Loss=0.4463, Time Left=23.80 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 576/3393 [04:06<23:23,  2.01batch/s, Batch Loss=0.3785, Avg Loss=0.4463, Time Left=23.80 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 576/3393 [04:07<23:23,  2.01batch/s, Batch Loss=0.7159, Avg Loss=0.4468, Time Left=23.79 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 577/3393 [04:07<22:46,  2.06batch/s, Batch Loss=0.7159, Avg Loss=0.4468, Time Left=23.79 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 577/3393 [04:07<22:46,  2.06batch/s, Batch Loss=0.4450, Avg Loss=0.4468, Time Left=23.78 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 578/3393 [04:07<22:46,  2.06batch/s, Batch Loss=0.4450, Avg Loss=0.4468, Time Left=23.78 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 578/3393 [04:08<22:46,  2.06batch/s, Batch Loss=0.6687, Avg Loss=0.4473, Time Left=23.78 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 579/3393 [04:08<23:00,  2.04batch/s, Batch Loss=0.6687, Avg Loss=0.4473, Time Left=23.78 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 579/3393 [04:08<23:00,  2.04batch/s, Batch Loss=0.5350, Avg Loss=0.4475, Time Left=23.77 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 580/3393 [04:08<22:58,  2.04batch/s, Batch Loss=0.5350, Avg Loss=0.4475, Time Left=23.77 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 580/3393 [04:09<22:58,  2.04batch/s, Batch Loss=0.2833, Avg Loss=0.4471, Time Left=23.76 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 581/3393 [04:09<23:19,  2.01batch/s, Batch Loss=0.2833, Avg Loss=0.4471, Time Left=23.76 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 581/3393 [04:09<23:19,  2.01batch/s, Batch Loss=0.4544, Avg Loss=0.4471, Time Left=23.75 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 582/3393 [04:09<23:09,  2.02batch/s, Batch Loss=0.4544, Avg Loss=0.4471, Time Left=23.75 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 582/3393 [04:10<23:09,  2.02batch/s, Batch Loss=0.2847, Avg Loss=0.4468, Time Left=23.75 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 583/3393 [04:10<23:28,  1.99batch/s, Batch Loss=0.2847, Avg Loss=0.4468, Time Left=23.75 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 583/3393 [04:10<23:28,  1.99batch/s, Batch Loss=0.3286, Avg Loss=0.4466, Time Left=23.74 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 584/3393 [04:10<23:06,  2.03batch/s, Batch Loss=0.3286, Avg Loss=0.4466, Time Left=23.74 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 584/3393 [04:11<23:06,  2.03batch/s, Batch Loss=0.2069, Avg Loss=0.4461, Time Left=23.73 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 585/3393 [04:11<22:43,  2.06batch/s, Batch Loss=0.2069, Avg Loss=0.4461, Time Left=23.73 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 585/3393 [04:11<22:43,  2.06batch/s, Batch Loss=0.3026, Avg Loss=0.4458, Time Left=23.72 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 586/3393 [04:11<23:25,  2.00batch/s, Batch Loss=0.3026, Avg Loss=0.4458, Time Left=23.72 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 586/3393 [04:12<23:25,  2.00batch/s, Batch Loss=0.3694, Avg Loss=0.4457, Time Left=23.71 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 587/3393 [04:12<23:08,  2.02batch/s, Batch Loss=0.3694, Avg Loss=0.4457, Time Left=23.71 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 587/3393 [04:12<23:08,  2.02batch/s, Batch Loss=0.7619, Avg Loss=0.4463, Time Left=23.71 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 588/3393 [04:12<23:14,  2.01batch/s, Batch Loss=0.7619, Avg Loss=0.4463, Time Left=23.71 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 588/3393 [04:13<23:14,  2.01batch/s, Batch Loss=0.2323, Avg Loss=0.4459, Time Left=23.70 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 589/3393 [04:13<23:18,  2.00batch/s, Batch Loss=0.2323, Avg Loss=0.4459, Time Left=23.70 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 589/3393 [04:13<23:18,  2.00batch/s, Batch Loss=0.5322, Avg Loss=0.4461, Time Left=23.69 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 590/3393 [04:13<23:20,  2.00batch/s, Batch Loss=0.5322, Avg Loss=0.4461, Time Left=23.69 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 590/3393 [04:14<23:20,  2.00batch/s, Batch Loss=0.1894, Avg Loss=0.4456, Time Left=23.68 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 591/3393 [04:14<23:02,  2.03batch/s, Batch Loss=0.1894, Avg Loss=0.4456, Time Left=23.68 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 591/3393 [04:14<23:02,  2.03batch/s, Batch Loss=0.2340, Avg Loss=0.4451, Time Left=23.67 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  17%|▏| 592/3393 [04:14<22:51,  2.04batch/s, Batch Loss=0.2340, Avg Loss=0.4451, Time Left=23.67 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 592/3393 [04:15<22:51,  2.04batch/s, Batch Loss=0.5674, Avg Loss=0.4454, Time Left=23.66 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 593/3393 [04:15<23:01,  2.03batch/s, Batch Loss=0.5674, Avg Loss=0.4454, Time Left=23.66 \u001b[A\n",
      "Epoch 1/3 - Training:  17%|▏| 593/3393 [04:15<23:01,  2.03batch/s, Batch Loss=0.3822, Avg Loss=0.4453, Time Left=23.66 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 594/3393 [04:15<22:55,  2.03batch/s, Batch Loss=0.3822, Avg Loss=0.4453, Time Left=23.66 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 594/3393 [04:16<22:55,  2.03batch/s, Batch Loss=0.2974, Avg Loss=0.4450, Time Left=23.65 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 595/3393 [04:16<23:05,  2.02batch/s, Batch Loss=0.2974, Avg Loss=0.4450, Time Left=23.65 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 595/3393 [04:16<23:05,  2.02batch/s, Batch Loss=0.1401, Avg Loss=0.4444, Time Left=23.64 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 596/3393 [04:16<22:55,  2.03batch/s, Batch Loss=0.1401, Avg Loss=0.4444, Time Left=23.64 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 596/3393 [04:17<22:55,  2.03batch/s, Batch Loss=0.3579, Avg Loss=0.4442, Time Left=23.63 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 597/3393 [04:17<22:38,  2.06batch/s, Batch Loss=0.3579, Avg Loss=0.4442, Time Left=23.63 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 597/3393 [04:17<22:38,  2.06batch/s, Batch Loss=0.2751, Avg Loss=0.4439, Time Left=23.62 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 598/3393 [04:17<23:08,  2.01batch/s, Batch Loss=0.2751, Avg Loss=0.4439, Time Left=23.62 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 598/3393 [04:18<23:08,  2.01batch/s, Batch Loss=0.2701, Avg Loss=0.4436, Time Left=23.61 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 599/3393 [04:18<22:43,  2.05batch/s, Batch Loss=0.2701, Avg Loss=0.4436, Time Left=23.61 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 599/3393 [04:18<22:43,  2.05batch/s, Batch Loss=0.4966, Avg Loss=0.4437, Time Left=23.60 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 600/3393 [04:18<22:26,  2.07batch/s, Batch Loss=0.4966, Avg Loss=0.4437, Time Left=23.60 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 600/3393 [04:19<22:26,  2.07batch/s, Batch Loss=0.2058, Avg Loss=0.4432, Time Left=23.59 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 601/3393 [04:19<22:56,  2.03batch/s, Batch Loss=0.2058, Avg Loss=0.4432, Time Left=23.59 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 601/3393 [04:19<22:56,  2.03batch/s, Batch Loss=0.3278, Avg Loss=0.4430, Time Left=23.59 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 602/3393 [04:19<22:50,  2.04batch/s, Batch Loss=0.3278, Avg Loss=0.4430, Time Left=23.59 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 602/3393 [04:20<22:50,  2.04batch/s, Batch Loss=0.3738, Avg Loss=0.4429, Time Left=23.58 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 603/3393 [04:20<23:10,  2.01batch/s, Batch Loss=0.3738, Avg Loss=0.4429, Time Left=23.58 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 603/3393 [04:20<23:10,  2.01batch/s, Batch Loss=0.3299, Avg Loss=0.4427, Time Left=23.57 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 604/3393 [04:20<22:47,  2.04batch/s, Batch Loss=0.3299, Avg Loss=0.4427, Time Left=23.57 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 604/3393 [04:21<22:47,  2.04batch/s, Batch Loss=0.1845, Avg Loss=0.4422, Time Left=23.56 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 605/3393 [04:21<22:44,  2.04batch/s, Batch Loss=0.1845, Avg Loss=0.4422, Time Left=23.56 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 605/3393 [04:21<22:44,  2.04batch/s, Batch Loss=0.3084, Avg Loss=0.4419, Time Left=23.55 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 606/3393 [04:21<22:54,  2.03batch/s, Batch Loss=0.3084, Avg Loss=0.4419, Time Left=23.55 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 606/3393 [04:22<22:54,  2.03batch/s, Batch Loss=0.3075, Avg Loss=0.4417, Time Left=23.54 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 607/3393 [04:22<22:35,  2.05batch/s, Batch Loss=0.3075, Avg Loss=0.4417, Time Left=23.54 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 607/3393 [04:22<22:35,  2.05batch/s, Batch Loss=0.1119, Avg Loss=0.4410, Time Left=23.54 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 608/3393 [04:22<23:14,  2.00batch/s, Batch Loss=0.1119, Avg Loss=0.4410, Time Left=23.54 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 608/3393 [04:23<23:14,  2.00batch/s, Batch Loss=0.2556, Avg Loss=0.4407, Time Left=23.53 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 609/3393 [04:23<22:50,  2.03batch/s, Batch Loss=0.2556, Avg Loss=0.4407, Time Left=23.53 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 609/3393 [04:23<22:50,  2.03batch/s, Batch Loss=0.3145, Avg Loss=0.4405, Time Left=23.52 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 610/3393 [04:23<23:10,  2.00batch/s, Batch Loss=0.3145, Avg Loss=0.4405, Time Left=23.52 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 610/3393 [04:24<23:10,  2.00batch/s, Batch Loss=0.5515, Avg Loss=0.4407, Time Left=23.51 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 611/3393 [04:24<22:46,  2.04batch/s, Batch Loss=0.5515, Avg Loss=0.4407, Time Left=23.51 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 611/3393 [04:24<22:46,  2.04batch/s, Batch Loss=0.2425, Avg Loss=0.4403, Time Left=23.50 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 612/3393 [04:24<22:32,  2.06batch/s, Batch Loss=0.2425, Avg Loss=0.4403, Time Left=23.50 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 612/3393 [04:25<22:32,  2.06batch/s, Batch Loss=0.1380, Avg Loss=0.4397, Time Left=23.50 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 613/3393 [04:25<22:53,  2.02batch/s, Batch Loss=0.1380, Avg Loss=0.4397, Time Left=23.50 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 613/3393 [04:25<22:53,  2.02batch/s, Batch Loss=0.1165, Avg Loss=0.4391, Time Left=23.49 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 614/3393 [04:25<23:01,  2.01batch/s, Batch Loss=0.1165, Avg Loss=0.4391, Time Left=23.49 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 614/3393 [04:26<23:01,  2.01batch/s, Batch Loss=0.6593, Avg Loss=0.4395, Time Left=23.48 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 615/3393 [04:26<23:27,  1.97batch/s, Batch Loss=0.6593, Avg Loss=0.4395, Time Left=23.48 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 615/3393 [04:26<23:27,  1.97batch/s, Batch Loss=0.3536, Avg Loss=0.4394, Time Left=23.47 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 616/3393 [04:26<22:45,  2.03batch/s, Batch Loss=0.3536, Avg Loss=0.4394, Time Left=23.47 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 616/3393 [04:27<22:45,  2.03batch/s, Batch Loss=0.3030, Avg Loss=0.4391, Time Left=23.46 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 617/3393 [04:27<22:40,  2.04batch/s, Batch Loss=0.3030, Avg Loss=0.4391, Time Left=23.46 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 617/3393 [04:27<22:40,  2.04batch/s, Batch Loss=0.6111, Avg Loss=0.4394, Time Left=23.46 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 618/3393 [04:27<22:49,  2.03batch/s, Batch Loss=0.6111, Avg Loss=0.4394, Time Left=23.46 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 618/3393 [04:28<22:49,  2.03batch/s, Batch Loss=0.1396, Avg Loss=0.4389, Time Left=23.45 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 619/3393 [04:28<22:44,  2.03batch/s, Batch Loss=0.1396, Avg Loss=0.4389, Time Left=23.45 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 619/3393 [04:28<22:44,  2.03batch/s, Batch Loss=0.1763, Avg Loss=0.4384, Time Left=23.44 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 620/3393 [04:28<23:05,  2.00batch/s, Batch Loss=0.1763, Avg Loss=0.4384, Time Left=23.44 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 620/3393 [04:29<23:05,  2.00batch/s, Batch Loss=0.1753, Avg Loss=0.4379, Time Left=23.43 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 621/3393 [04:29<22:52,  2.02batch/s, Batch Loss=0.1753, Avg Loss=0.4379, Time Left=23.43 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 621/3393 [04:29<22:52,  2.02batch/s, Batch Loss=0.5075, Avg Loss=0.4381, Time Left=23.43 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 622/3393 [04:29<23:39,  1.95batch/s, Batch Loss=0.5075, Avg Loss=0.4381, Time Left=23.43 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 622/3393 [04:30<23:39,  1.95batch/s, Batch Loss=0.1802, Avg Loss=0.4376, Time Left=23.42 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 623/3393 [04:30<23:25,  1.97batch/s, Batch Loss=0.1802, Avg Loss=0.4376, Time Left=23.42 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 623/3393 [04:30<23:25,  1.97batch/s, Batch Loss=0.4882, Avg Loss=0.4377, Time Left=23.41 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 624/3393 [04:30<23:32,  1.96batch/s, Batch Loss=0.4882, Avg Loss=0.4377, Time Left=23.41 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 624/3393 [04:31<23:32,  1.96batch/s, Batch Loss=0.2889, Avg Loss=0.4374, Time Left=23.40 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  18%|▏| 625/3393 [04:31<23:05,  2.00batch/s, Batch Loss=0.2889, Avg Loss=0.4374, Time Left=23.40 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 625/3393 [04:31<23:05,  2.00batch/s, Batch Loss=0.2940, Avg Loss=0.4371, Time Left=23.40 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 626/3393 [04:31<23:17,  1.98batch/s, Batch Loss=0.2940, Avg Loss=0.4371, Time Left=23.40 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 626/3393 [04:32<23:17,  1.98batch/s, Batch Loss=0.3532, Avg Loss=0.4370, Time Left=23.39 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 627/3393 [04:32<22:48,  2.02batch/s, Batch Loss=0.3532, Avg Loss=0.4370, Time Left=23.39 \u001b[A\n",
      "Epoch 1/3 - Training:  18%|▏| 627/3393 [04:32<22:48,  2.02batch/s, Batch Loss=0.2113, Avg Loss=0.4366, Time Left=23.38 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 628/3393 [04:32<22:30,  2.05batch/s, Batch Loss=0.2113, Avg Loss=0.4366, Time Left=23.38 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 628/3393 [04:33<22:30,  2.05batch/s, Batch Loss=0.7097, Avg Loss=0.4371, Time Left=23.37 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 629/3393 [04:33<22:56,  2.01batch/s, Batch Loss=0.7097, Avg Loss=0.4371, Time Left=23.37 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 629/3393 [04:33<22:56,  2.01batch/s, Batch Loss=0.3732, Avg Loss=0.4370, Time Left=23.36 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 630/3393 [04:33<22:44,  2.02batch/s, Batch Loss=0.3732, Avg Loss=0.4370, Time Left=23.36 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 630/3393 [04:34<22:44,  2.02batch/s, Batch Loss=0.4566, Avg Loss=0.4370, Time Left=23.35 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 631/3393 [04:34<22:22,  2.06batch/s, Batch Loss=0.4566, Avg Loss=0.4370, Time Left=23.35 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 631/3393 [04:34<22:22,  2.06batch/s, Batch Loss=0.1087, Avg Loss=0.4364, Time Left=23.34 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 632/3393 [04:34<22:23,  2.06batch/s, Batch Loss=0.1087, Avg Loss=0.4364, Time Left=23.34 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 632/3393 [04:34<22:23,  2.06batch/s, Batch Loss=0.2401, Avg Loss=0.4361, Time Left=23.34 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 633/3393 [04:34<22:22,  2.06batch/s, Batch Loss=0.2401, Avg Loss=0.4361, Time Left=23.34 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 633/3393 [04:35<22:22,  2.06batch/s, Batch Loss=0.1211, Avg Loss=0.4355, Time Left=23.33 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 634/3393 [04:35<22:47,  2.02batch/s, Batch Loss=0.1211, Avg Loss=0.4355, Time Left=23.33 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 634/3393 [04:35<22:47,  2.02batch/s, Batch Loss=0.2559, Avg Loss=0.4352, Time Left=23.32 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 635/3393 [04:36<22:45,  2.02batch/s, Batch Loss=0.2559, Avg Loss=0.4352, Time Left=23.32 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 635/3393 [04:36<22:45,  2.02batch/s, Batch Loss=0.1736, Avg Loss=0.4347, Time Left=23.31 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 636/3393 [04:36<22:20,  2.06batch/s, Batch Loss=0.1736, Avg Loss=0.4347, Time Left=23.31 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 636/3393 [04:36<22:20,  2.06batch/s, Batch Loss=0.2201, Avg Loss=0.4343, Time Left=23.30 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 637/3393 [04:36<22:33,  2.04batch/s, Batch Loss=0.2201, Avg Loss=0.4343, Time Left=23.30 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 637/3393 [04:37<22:33,  2.04batch/s, Batch Loss=0.3333, Avg Loss=0.4341, Time Left=23.29 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 638/3393 [04:37<22:28,  2.04batch/s, Batch Loss=0.3333, Avg Loss=0.4341, Time Left=23.29 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 638/3393 [04:37<22:28,  2.04batch/s, Batch Loss=0.2628, Avg Loss=0.4338, Time Left=23.29 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 639/3393 [04:37<22:38,  2.03batch/s, Batch Loss=0.2628, Avg Loss=0.4338, Time Left=23.29 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 639/3393 [04:38<22:38,  2.03batch/s, Batch Loss=0.1728, Avg Loss=0.4334, Time Left=23.28 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 640/3393 [04:38<22:45,  2.02batch/s, Batch Loss=0.1728, Avg Loss=0.4334, Time Left=23.28 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 640/3393 [04:38<22:45,  2.02batch/s, Batch Loss=0.3292, Avg Loss=0.4332, Time Left=23.27 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 641/3393 [04:38<22:36,  2.03batch/s, Batch Loss=0.3292, Avg Loss=0.4332, Time Left=23.27 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 641/3393 [04:39<22:36,  2.03batch/s, Batch Loss=0.2364, Avg Loss=0.4328, Time Left=23.26 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 642/3393 [04:39<22:31,  2.04batch/s, Batch Loss=0.2364, Avg Loss=0.4328, Time Left=23.26 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 642/3393 [04:39<22:31,  2.04batch/s, Batch Loss=0.2185, Avg Loss=0.4325, Time Left=23.25 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 643/3393 [04:39<22:15,  2.06batch/s, Batch Loss=0.2185, Avg Loss=0.4325, Time Left=23.25 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 643/3393 [04:40<22:15,  2.06batch/s, Batch Loss=0.1072, Avg Loss=0.4319, Time Left=23.24 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 644/3393 [04:40<22:26,  2.04batch/s, Batch Loss=0.1072, Avg Loss=0.4319, Time Left=23.24 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 644/3393 [04:40<22:26,  2.04batch/s, Batch Loss=0.3758, Avg Loss=0.4318, Time Left=23.23 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 645/3393 [04:40<22:09,  2.07batch/s, Batch Loss=0.3758, Avg Loss=0.4318, Time Left=23.23 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 645/3393 [04:41<22:09,  2.07batch/s, Batch Loss=0.2310, Avg Loss=0.4314, Time Left=23.22 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 646/3393 [04:41<21:58,  2.08batch/s, Batch Loss=0.2310, Avg Loss=0.4314, Time Left=23.22 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 646/3393 [04:41<21:58,  2.08batch/s, Batch Loss=0.2289, Avg Loss=0.4311, Time Left=23.22 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 647/3393 [04:41<22:28,  2.04batch/s, Batch Loss=0.2289, Avg Loss=0.4311, Time Left=23.22 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 647/3393 [04:42<22:28,  2.04batch/s, Batch Loss=0.3327, Avg Loss=0.4309, Time Left=23.21 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 648/3393 [04:42<22:24,  2.04batch/s, Batch Loss=0.3327, Avg Loss=0.4309, Time Left=23.21 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 648/3393 [04:42<22:24,  2.04batch/s, Batch Loss=0.1642, Avg Loss=0.4304, Time Left=23.20 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 649/3393 [04:42<22:34,  2.03batch/s, Batch Loss=0.1642, Avg Loss=0.4304, Time Left=23.20 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 649/3393 [04:43<22:34,  2.03batch/s, Batch Loss=0.3651, Avg Loss=0.4303, Time Left=23.19 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 650/3393 [04:43<22:15,  2.05batch/s, Batch Loss=0.3651, Avg Loss=0.4303, Time Left=23.19 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 650/3393 [04:43<22:15,  2.05batch/s, Batch Loss=0.1978, Avg Loss=0.4299, Time Left=23.18 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 651/3393 [04:43<22:15,  2.05batch/s, Batch Loss=0.1978, Avg Loss=0.4299, Time Left=23.18 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 651/3393 [04:44<22:15,  2.05batch/s, Batch Loss=0.0837, Avg Loss=0.4293, Time Left=23.18 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 652/3393 [04:44<22:41,  2.01batch/s, Batch Loss=0.0837, Avg Loss=0.4293, Time Left=23.18 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 652/3393 [04:44<22:41,  2.01batch/s, Batch Loss=0.4852, Avg Loss=0.4294, Time Left=23.16 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 653/3393 [04:44<22:05,  2.07batch/s, Batch Loss=0.4852, Avg Loss=0.4294, Time Left=23.16 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 653/3393 [04:45<22:05,  2.07batch/s, Batch Loss=0.2691, Avg Loss=0.4291, Time Left=23.16 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 654/3393 [04:45<22:06,  2.07batch/s, Batch Loss=0.2691, Avg Loss=0.4291, Time Left=23.16 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 654/3393 [04:45<22:06,  2.07batch/s, Batch Loss=0.2408, Avg Loss=0.4288, Time Left=23.15 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 655/3393 [04:45<22:30,  2.03batch/s, Batch Loss=0.2408, Avg Loss=0.4288, Time Left=23.15 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 655/3393 [04:46<22:30,  2.03batch/s, Batch Loss=0.2472, Avg Loss=0.4285, Time Left=23.14 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 656/3393 [04:46<22:17,  2.05batch/s, Batch Loss=0.2472, Avg Loss=0.4285, Time Left=23.14 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 656/3393 [04:46<22:17,  2.05batch/s, Batch Loss=0.4236, Avg Loss=0.4285, Time Left=23.13 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 657/3393 [04:46<22:37,  2.02batch/s, Batch Loss=0.4236, Avg Loss=0.4285, Time Left=23.13 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 657/3393 [04:47<22:37,  2.02batch/s, Batch Loss=0.2868, Avg Loss=0.4283, Time Left=23.12 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  19%|▏| 658/3393 [04:47<22:28,  2.03batch/s, Batch Loss=0.2868, Avg Loss=0.4283, Time Left=23.12 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 658/3393 [04:47<22:28,  2.03batch/s, Batch Loss=0.2713, Avg Loss=0.4280, Time Left=23.11 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 659/3393 [04:47<22:10,  2.06batch/s, Batch Loss=0.2713, Avg Loss=0.4280, Time Left=23.11 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 659/3393 [04:48<22:10,  2.06batch/s, Batch Loss=0.2796, Avg Loss=0.4277, Time Left=23.11 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 660/3393 [04:48<22:25,  2.03batch/s, Batch Loss=0.2796, Avg Loss=0.4277, Time Left=23.11 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 660/3393 [04:48<22:25,  2.03batch/s, Batch Loss=0.1558, Avg Loss=0.4273, Time Left=23.10 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 661/3393 [04:48<22:16,  2.04batch/s, Batch Loss=0.1558, Avg Loss=0.4273, Time Left=23.10 \u001b[A\n",
      "Epoch 1/3 - Training:  19%|▏| 661/3393 [04:49<22:16,  2.04batch/s, Batch Loss=0.3068, Avg Loss=0.4271, Time Left=23.09 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 662/3393 [04:49<22:39,  2.01batch/s, Batch Loss=0.3068, Avg Loss=0.4271, Time Left=23.09 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 662/3393 [04:49<22:39,  2.01batch/s, Batch Loss=0.2453, Avg Loss=0.4267, Time Left=23.08 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 663/3393 [04:49<22:30,  2.02batch/s, Batch Loss=0.2453, Avg Loss=0.4267, Time Left=23.08 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 663/3393 [04:50<22:30,  2.02batch/s, Batch Loss=0.4319, Avg Loss=0.4268, Time Left=23.07 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 664/3393 [04:50<22:24,  2.03batch/s, Batch Loss=0.4319, Avg Loss=0.4268, Time Left=23.07 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 664/3393 [04:50<22:24,  2.03batch/s, Batch Loss=0.4782, Avg Loss=0.4268, Time Left=23.07 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 665/3393 [04:50<22:19,  2.04batch/s, Batch Loss=0.4782, Avg Loss=0.4268, Time Left=23.07 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 665/3393 [04:51<22:19,  2.04batch/s, Batch Loss=0.3124, Avg Loss=0.4267, Time Left=23.06 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 666/3393 [04:51<22:04,  2.06batch/s, Batch Loss=0.3124, Avg Loss=0.4267, Time Left=23.06 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 666/3393 [04:51<22:04,  2.06batch/s, Batch Loss=0.3218, Avg Loss=0.4265, Time Left=23.05 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 667/3393 [04:51<22:26,  2.02batch/s, Batch Loss=0.3218, Avg Loss=0.4265, Time Left=23.05 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 667/3393 [04:52<22:26,  2.02batch/s, Batch Loss=0.5724, Avg Loss=0.4267, Time Left=23.04 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 668/3393 [04:52<22:20,  2.03batch/s, Batch Loss=0.5724, Avg Loss=0.4267, Time Left=23.04 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 668/3393 [04:52<22:20,  2.03batch/s, Batch Loss=0.4086, Avg Loss=0.4267, Time Left=23.03 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 669/3393 [04:52<22:07,  2.05batch/s, Batch Loss=0.4086, Avg Loss=0.4267, Time Left=23.03 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 669/3393 [04:53<22:07,  2.05batch/s, Batch Loss=0.1602, Avg Loss=0.4262, Time Left=23.02 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 670/3393 [04:53<22:02,  2.06batch/s, Batch Loss=0.1602, Avg Loss=0.4262, Time Left=23.02 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 670/3393 [04:53<22:02,  2.06batch/s, Batch Loss=0.3662, Avg Loss=0.4261, Time Left=23.01 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 671/3393 [04:53<21:46,  2.08batch/s, Batch Loss=0.3662, Avg Loss=0.4261, Time Left=23.01 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 671/3393 [04:54<21:46,  2.08batch/s, Batch Loss=0.3704, Avg Loss=0.4260, Time Left=23.00 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 672/3393 [04:54<21:40,  2.09batch/s, Batch Loss=0.3704, Avg Loss=0.4260, Time Left=23.00 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 672/3393 [04:54<21:40,  2.09batch/s, Batch Loss=0.2730, Avg Loss=0.4258, Time Left=23.00 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 673/3393 [04:54<22:24,  2.02batch/s, Batch Loss=0.2730, Avg Loss=0.4258, Time Left=23.00 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 673/3393 [04:55<22:24,  2.02batch/s, Batch Loss=0.1521, Avg Loss=0.4253, Time Left=22.99 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 674/3393 [04:55<22:04,  2.05batch/s, Batch Loss=0.1521, Avg Loss=0.4253, Time Left=22.99 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 674/3393 [04:55<22:04,  2.05batch/s, Batch Loss=0.4532, Avg Loss=0.4254, Time Left=22.98 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 675/3393 [04:55<22:42,  1.99batch/s, Batch Loss=0.4532, Avg Loss=0.4254, Time Left=22.98 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 675/3393 [04:56<22:42,  1.99batch/s, Batch Loss=0.5412, Avg Loss=0.4256, Time Left=22.97 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 676/3393 [04:56<22:17,  2.03batch/s, Batch Loss=0.5412, Avg Loss=0.4256, Time Left=22.97 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 676/3393 [04:56<22:17,  2.03batch/s, Batch Loss=0.3601, Avg Loss=0.4255, Time Left=22.97 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 677/3393 [04:56<22:49,  1.98batch/s, Batch Loss=0.3601, Avg Loss=0.4255, Time Left=22.97 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 677/3393 [04:57<22:49,  1.98batch/s, Batch Loss=0.2512, Avg Loss=0.4252, Time Left=22.96 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 678/3393 [04:57<22:14,  2.03batch/s, Batch Loss=0.2512, Avg Loss=0.4252, Time Left=22.96 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 678/3393 [04:57<22:14,  2.03batch/s, Batch Loss=0.5233, Avg Loss=0.4253, Time Left=22.95 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 679/3393 [04:57<23:09,  1.95batch/s, Batch Loss=0.5233, Avg Loss=0.4253, Time Left=22.95 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 679/3393 [04:58<23:09,  1.95batch/s, Batch Loss=0.4101, Avg Loss=0.4253, Time Left=22.95 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 680/3393 [04:58<24:09,  1.87batch/s, Batch Loss=0.4101, Avg Loss=0.4253, Time Left=22.95 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 680/3393 [04:58<24:09,  1.87batch/s, Batch Loss=0.2811, Avg Loss=0.4251, Time Left=22.95 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 681/3393 [04:58<24:29,  1.85batch/s, Batch Loss=0.2811, Avg Loss=0.4251, Time Left=22.95 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 681/3393 [04:59<24:29,  1.85batch/s, Batch Loss=0.3049, Avg Loss=0.4249, Time Left=22.94 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 682/3393 [04:59<24:35,  1.84batch/s, Batch Loss=0.3049, Avg Loss=0.4249, Time Left=22.94 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 682/3393 [04:59<24:35,  1.84batch/s, Batch Loss=0.5508, Avg Loss=0.4251, Time Left=22.93 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 683/3393 [04:59<23:35,  1.91batch/s, Batch Loss=0.5508, Avg Loss=0.4251, Time Left=22.93 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 683/3393 [05:00<23:35,  1.91batch/s, Batch Loss=0.2606, Avg Loss=0.4248, Time Left=22.93 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 684/3393 [05:00<23:15,  1.94batch/s, Batch Loss=0.2606, Avg Loss=0.4248, Time Left=22.93 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 684/3393 [05:00<23:15,  1.94batch/s, Batch Loss=0.5222, Avg Loss=0.4250, Time Left=22.92 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 685/3393 [05:00<22:52,  1.97batch/s, Batch Loss=0.5222, Avg Loss=0.4250, Time Left=22.92 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 685/3393 [05:01<22:52,  1.97batch/s, Batch Loss=0.3121, Avg Loss=0.4248, Time Left=22.91 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 686/3393 [05:01<23:08,  1.95batch/s, Batch Loss=0.3121, Avg Loss=0.4248, Time Left=22.91 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 686/3393 [05:01<23:08,  1.95batch/s, Batch Loss=0.2577, Avg Loss=0.4245, Time Left=22.91 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 687/3393 [05:01<24:24,  1.85batch/s, Batch Loss=0.2577, Avg Loss=0.4245, Time Left=22.91 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 687/3393 [05:02<24:24,  1.85batch/s, Batch Loss=0.0862, Avg Loss=0.4240, Time Left=22.90 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 688/3393 [05:02<23:23,  1.93batch/s, Batch Loss=0.0862, Avg Loss=0.4240, Time Left=22.90 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 688/3393 [05:02<23:23,  1.93batch/s, Batch Loss=0.3938, Avg Loss=0.4239, Time Left=22.89 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 689/3393 [05:02<22:44,  1.98batch/s, Batch Loss=0.3938, Avg Loss=0.4239, Time Left=22.89 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 689/3393 [05:03<22:44,  1.98batch/s, Batch Loss=0.2099, Avg Loss=0.4236, Time Left=22.88 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 690/3393 [05:03<22:45,  1.98batch/s, Batch Loss=0.2099, Avg Loss=0.4236, Time Left=22.88 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 690/3393 [05:03<22:45,  1.98batch/s, Batch Loss=0.4556, Avg Loss=0.4236, Time Left=22.87 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  20%|▏| 691/3393 [05:03<22:00,  2.05batch/s, Batch Loss=0.4556, Avg Loss=0.4236, Time Left=22.87 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 691/3393 [05:04<22:00,  2.05batch/s, Batch Loss=0.1698, Avg Loss=0.4232, Time Left=22.86 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 692/3393 [05:04<21:48,  2.06batch/s, Batch Loss=0.1698, Avg Loss=0.4232, Time Left=22.86 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 692/3393 [05:04<21:48,  2.06batch/s, Batch Loss=0.2183, Avg Loss=0.4229, Time Left=22.86 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 693/3393 [05:04<22:13,  2.02batch/s, Batch Loss=0.2183, Avg Loss=0.4229, Time Left=22.86 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 693/3393 [05:05<22:13,  2.02batch/s, Batch Loss=0.2592, Avg Loss=0.4226, Time Left=22.85 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 694/3393 [05:05<21:51,  2.06batch/s, Batch Loss=0.2592, Avg Loss=0.4226, Time Left=22.85 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 694/3393 [05:05<21:51,  2.06batch/s, Batch Loss=0.2463, Avg Loss=0.4223, Time Left=22.84 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 695/3393 [05:05<21:41,  2.07batch/s, Batch Loss=0.2463, Avg Loss=0.4223, Time Left=22.84 \u001b[A\n",
      "Epoch 1/3 - Training:  20%|▏| 695/3393 [05:06<21:41,  2.07batch/s, Batch Loss=0.2007, Avg Loss=0.4220, Time Left=22.83 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 696/3393 [05:06<22:07,  2.03batch/s, Batch Loss=0.2007, Avg Loss=0.4220, Time Left=22.83 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 696/3393 [05:06<22:07,  2.03batch/s, Batch Loss=0.3627, Avg Loss=0.4219, Time Left=22.82 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 697/3393 [05:06<21:49,  2.06batch/s, Batch Loss=0.3627, Avg Loss=0.4219, Time Left=22.82 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 697/3393 [05:07<21:49,  2.06batch/s, Batch Loss=0.1815, Avg Loss=0.4215, Time Left=22.81 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 698/3393 [05:07<22:15,  2.02batch/s, Batch Loss=0.1815, Avg Loss=0.4215, Time Left=22.81 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 698/3393 [05:07<22:15,  2.02batch/s, Batch Loss=0.3973, Avg Loss=0.4214, Time Left=22.80 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 699/3393 [05:07<22:07,  2.03batch/s, Batch Loss=0.3973, Avg Loss=0.4214, Time Left=22.80 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 699/3393 [05:08<22:07,  2.03batch/s, Batch Loss=0.4700, Avg Loss=0.4215, Time Left=22.79 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 700/3393 [05:08<21:49,  2.06batch/s, Batch Loss=0.4700, Avg Loss=0.4215, Time Left=22.79 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 700/3393 [05:08<21:49,  2.06batch/s, Batch Loss=0.0889, Avg Loss=0.4210, Time Left=22.79 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 701/3393 [05:08<22:14,  2.02batch/s, Batch Loss=0.0889, Avg Loss=0.4210, Time Left=22.79 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 701/3393 [05:09<22:14,  2.02batch/s, Batch Loss=0.1915, Avg Loss=0.4206, Time Left=22.78 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 702/3393 [05:09<21:53,  2.05batch/s, Batch Loss=0.1915, Avg Loss=0.4206, Time Left=22.78 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 702/3393 [05:09<21:53,  2.05batch/s, Batch Loss=0.3135, Avg Loss=0.4205, Time Left=22.77 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 703/3393 [05:09<22:17,  2.01batch/s, Batch Loss=0.3135, Avg Loss=0.4205, Time Left=22.77 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 703/3393 [05:10<22:17,  2.01batch/s, Batch Loss=0.3402, Avg Loss=0.4203, Time Left=22.76 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 704/3393 [05:10<21:43,  2.06batch/s, Batch Loss=0.3402, Avg Loss=0.4203, Time Left=22.76 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 704/3393 [05:10<21:43,  2.06batch/s, Batch Loss=0.0845, Avg Loss=0.4198, Time Left=22.75 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 705/3393 [05:10<22:00,  2.04batch/s, Batch Loss=0.0845, Avg Loss=0.4198, Time Left=22.75 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 705/3393 [05:11<22:00,  2.04batch/s, Batch Loss=0.0414, Avg Loss=0.4192, Time Left=22.74 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 706/3393 [05:11<21:54,  2.04batch/s, Batch Loss=0.0414, Avg Loss=0.4192, Time Left=22.74 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 706/3393 [05:11<21:54,  2.04batch/s, Batch Loss=0.3537, Avg Loss=0.4191, Time Left=22.74 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 707/3393 [05:11<22:00,  2.03batch/s, Batch Loss=0.3537, Avg Loss=0.4191, Time Left=22.74 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 707/3393 [05:12<22:00,  2.03batch/s, Batch Loss=0.2854, Avg Loss=0.4189, Time Left=22.73 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 708/3393 [05:12<22:09,  2.02batch/s, Batch Loss=0.2854, Avg Loss=0.4189, Time Left=22.73 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 708/3393 [05:12<22:09,  2.02batch/s, Batch Loss=0.1686, Avg Loss=0.4185, Time Left=22.72 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 709/3393 [05:12<22:09,  2.02batch/s, Batch Loss=0.1686, Avg Loss=0.4185, Time Left=22.72 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 709/3393 [05:13<22:09,  2.02batch/s, Batch Loss=0.1298, Avg Loss=0.4180, Time Left=22.71 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 710/3393 [05:13<21:54,  2.04batch/s, Batch Loss=0.1298, Avg Loss=0.4180, Time Left=22.71 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 710/3393 [05:13<21:54,  2.04batch/s, Batch Loss=0.4100, Avg Loss=0.4180, Time Left=22.70 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 711/3393 [05:13<21:51,  2.05batch/s, Batch Loss=0.4100, Avg Loss=0.4180, Time Left=22.70 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 711/3393 [05:14<21:51,  2.05batch/s, Batch Loss=0.2829, Avg Loss=0.4178, Time Left=22.69 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 712/3393 [05:14<21:36,  2.07batch/s, Batch Loss=0.2829, Avg Loss=0.4178, Time Left=22.69 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 712/3393 [05:14<21:36,  2.07batch/s, Batch Loss=0.0833, Avg Loss=0.4173, Time Left=22.69 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 713/3393 [05:14<21:51,  2.04batch/s, Batch Loss=0.0833, Avg Loss=0.4173, Time Left=22.69 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 713/3393 [05:15<21:51,  2.04batch/s, Batch Loss=0.1674, Avg Loss=0.4169, Time Left=22.68 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 714/3393 [05:15<21:56,  2.04batch/s, Batch Loss=0.1674, Avg Loss=0.4169, Time Left=22.68 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 714/3393 [05:15<21:56,  2.04batch/s, Batch Loss=0.2046, Avg Loss=0.4165, Time Left=22.67 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 715/3393 [05:15<21:57,  2.03batch/s, Batch Loss=0.2046, Avg Loss=0.4165, Time Left=22.67 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 715/3393 [05:16<21:57,  2.03batch/s, Batch Loss=0.7418, Avg Loss=0.4171, Time Left=22.66 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 716/3393 [05:16<22:09,  2.01batch/s, Batch Loss=0.7418, Avg Loss=0.4171, Time Left=22.66 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 716/3393 [05:16<22:09,  2.01batch/s, Batch Loss=0.3057, Avg Loss=0.4169, Time Left=22.65 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 717/3393 [05:16<21:45,  2.05batch/s, Batch Loss=0.3057, Avg Loss=0.4169, Time Left=22.65 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 717/3393 [05:17<21:45,  2.05batch/s, Batch Loss=0.2522, Avg Loss=0.4166, Time Left=22.65 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 718/3393 [05:17<22:33,  1.98batch/s, Batch Loss=0.2522, Avg Loss=0.4166, Time Left=22.65 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 718/3393 [05:17<22:33,  1.98batch/s, Batch Loss=0.2844, Avg Loss=0.4164, Time Left=22.64 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 719/3393 [05:17<22:35,  1.97batch/s, Batch Loss=0.2844, Avg Loss=0.4164, Time Left=22.64 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 719/3393 [05:18<22:35,  1.97batch/s, Batch Loss=0.1794, Avg Loss=0.4161, Time Left=22.64 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 720/3393 [05:18<23:17,  1.91batch/s, Batch Loss=0.1794, Avg Loss=0.4161, Time Left=22.64 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 720/3393 [05:18<23:17,  1.91batch/s, Batch Loss=0.4801, Avg Loss=0.4162, Time Left=22.63 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 721/3393 [05:18<23:25,  1.90batch/s, Batch Loss=0.4801, Avg Loss=0.4162, Time Left=22.63 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 721/3393 [05:19<23:25,  1.90batch/s, Batch Loss=0.2166, Avg Loss=0.4158, Time Left=22.62 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 722/3393 [05:19<23:05,  1.93batch/s, Batch Loss=0.2166, Avg Loss=0.4158, Time Left=22.62 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 722/3393 [05:19<23:05,  1.93batch/s, Batch Loss=0.4226, Avg Loss=0.4159, Time Left=22.63 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 723/3393 [05:19<24:54,  1.79batch/s, Batch Loss=0.4226, Avg Loss=0.4159, Time Left=22.63 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 723/3393 [05:20<24:54,  1.79batch/s, Batch Loss=0.3994, Avg Loss=0.4158, Time Left=22.62 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  21%|▏| 724/3393 [05:20<24:48,  1.79batch/s, Batch Loss=0.3994, Avg Loss=0.4158, Time Left=22.62 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 724/3393 [05:20<24:48,  1.79batch/s, Batch Loss=0.1488, Avg Loss=0.4154, Time Left=22.61 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 725/3393 [05:20<23:39,  1.88batch/s, Batch Loss=0.1488, Avg Loss=0.4154, Time Left=22.61 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 725/3393 [05:21<23:39,  1.88batch/s, Batch Loss=0.2272, Avg Loss=0.4151, Time Left=22.61 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 726/3393 [05:21<23:57,  1.86batch/s, Batch Loss=0.2272, Avg Loss=0.4151, Time Left=22.61 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 726/3393 [05:21<23:57,  1.86batch/s, Batch Loss=0.4194, Avg Loss=0.4151, Time Left=22.60 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 727/3393 [05:21<23:44,  1.87batch/s, Batch Loss=0.4194, Avg Loss=0.4151, Time Left=22.60 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 727/3393 [05:22<23:44,  1.87batch/s, Batch Loss=0.3660, Avg Loss=0.4151, Time Left=22.60 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 728/3393 [05:22<23:32,  1.89batch/s, Batch Loss=0.3660, Avg Loss=0.4151, Time Left=22.60 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 728/3393 [05:23<23:32,  1.89batch/s, Batch Loss=0.2874, Avg Loss=0.4149, Time Left=22.59 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 729/3393 [05:23<23:32,  1.89batch/s, Batch Loss=0.2874, Avg Loss=0.4149, Time Left=22.59 \u001b[A\n",
      "Epoch 1/3 - Training:  21%|▏| 729/3393 [05:23<23:32,  1.89batch/s, Batch Loss=0.4432, Avg Loss=0.4149, Time Left=22.59 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 730/3393 [05:23<23:58,  1.85batch/s, Batch Loss=0.4432, Avg Loss=0.4149, Time Left=22.59 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 730/3393 [05:24<23:58,  1.85batch/s, Batch Loss=0.2798, Avg Loss=0.4147, Time Left=22.58 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 731/3393 [05:24<23:52,  1.86batch/s, Batch Loss=0.2798, Avg Loss=0.4147, Time Left=22.58 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 731/3393 [05:24<23:52,  1.86batch/s, Batch Loss=0.4187, Avg Loss=0.4147, Time Left=22.58 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 732/3393 [05:24<24:04,  1.84batch/s, Batch Loss=0.4187, Avg Loss=0.4147, Time Left=22.58 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 732/3393 [05:25<24:04,  1.84batch/s, Batch Loss=0.2192, Avg Loss=0.4144, Time Left=22.57 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 733/3393 [05:25<23:22,  1.90batch/s, Batch Loss=0.2192, Avg Loss=0.4144, Time Left=22.57 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 733/3393 [05:25<23:22,  1.90batch/s, Batch Loss=0.1524, Avg Loss=0.4140, Time Left=22.56 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 734/3393 [05:25<23:13,  1.91batch/s, Batch Loss=0.1524, Avg Loss=0.4140, Time Left=22.56 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 734/3393 [05:26<23:13,  1.91batch/s, Batch Loss=0.2328, Avg Loss=0.4137, Time Left=22.55 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 735/3393 [05:26<22:41,  1.95batch/s, Batch Loss=0.2328, Avg Loss=0.4137, Time Left=22.55 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 735/3393 [05:26<22:41,  1.95batch/s, Batch Loss=0.2642, Avg Loss=0.4135, Time Left=22.54 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 736/3393 [05:26<22:26,  1.97batch/s, Batch Loss=0.2642, Avg Loss=0.4135, Time Left=22.54 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 736/3393 [05:27<22:26,  1.97batch/s, Batch Loss=0.3514, Avg Loss=0.4134, Time Left=22.54 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 737/3393 [05:27<22:06,  2.00batch/s, Batch Loss=0.3514, Avg Loss=0.4134, Time Left=22.54 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 737/3393 [05:27<22:06,  2.00batch/s, Batch Loss=0.3646, Avg Loss=0.4133, Time Left=22.53 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 738/3393 [05:27<21:50,  2.03batch/s, Batch Loss=0.3646, Avg Loss=0.4133, Time Left=22.53 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 738/3393 [05:28<21:50,  2.03batch/s, Batch Loss=0.1656, Avg Loss=0.4130, Time Left=22.52 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 739/3393 [05:28<22:00,  2.01batch/s, Batch Loss=0.1656, Avg Loss=0.4130, Time Left=22.52 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 739/3393 [05:28<22:00,  2.01batch/s, Batch Loss=0.1394, Avg Loss=0.4126, Time Left=22.51 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 740/3393 [05:28<22:14,  1.99batch/s, Batch Loss=0.1394, Avg Loss=0.4126, Time Left=22.51 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 740/3393 [05:29<22:14,  1.99batch/s, Batch Loss=0.5839, Avg Loss=0.4128, Time Left=22.51 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 741/3393 [05:29<22:39,  1.95batch/s, Batch Loss=0.5839, Avg Loss=0.4128, Time Left=22.51 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 741/3393 [05:29<22:39,  1.95batch/s, Batch Loss=0.4347, Avg Loss=0.4128, Time Left=22.50 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 742/3393 [05:29<22:27,  1.97batch/s, Batch Loss=0.4347, Avg Loss=0.4128, Time Left=22.50 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 742/3393 [05:30<22:27,  1.97batch/s, Batch Loss=0.0635, Avg Loss=0.4123, Time Left=22.49 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 743/3393 [05:30<22:21,  1.98batch/s, Batch Loss=0.0635, Avg Loss=0.4123, Time Left=22.49 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 743/3393 [05:30<22:21,  1.98batch/s, Batch Loss=0.4233, Avg Loss=0.4123, Time Left=22.48 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 744/3393 [05:30<22:17,  1.98batch/s, Batch Loss=0.4233, Avg Loss=0.4123, Time Left=22.48 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 744/3393 [05:31<22:17,  1.98batch/s, Batch Loss=0.2233, Avg Loss=0.4121, Time Left=22.47 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 745/3393 [05:31<22:12,  1.99batch/s, Batch Loss=0.2233, Avg Loss=0.4121, Time Left=22.47 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 745/3393 [05:31<22:12,  1.99batch/s, Batch Loss=0.0745, Avg Loss=0.4116, Time Left=22.46 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 746/3393 [05:31<21:42,  2.03batch/s, Batch Loss=0.0745, Avg Loss=0.4116, Time Left=22.46 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 746/3393 [05:32<21:42,  2.03batch/s, Batch Loss=0.5817, Avg Loss=0.4118, Time Left=22.46 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 747/3393 [05:32<22:16,  1.98batch/s, Batch Loss=0.5817, Avg Loss=0.4118, Time Left=22.46 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 747/3393 [05:32<22:16,  1.98batch/s, Batch Loss=0.4611, Avg Loss=0.4119, Time Left=22.45 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 748/3393 [05:32<22:01,  2.00batch/s, Batch Loss=0.4611, Avg Loss=0.4119, Time Left=22.45 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 748/3393 [05:33<22:01,  2.00batch/s, Batch Loss=0.2134, Avg Loss=0.4116, Time Left=22.44 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 749/3393 [05:33<22:00,  2.00batch/s, Batch Loss=0.2134, Avg Loss=0.4116, Time Left=22.44 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 749/3393 [05:33<22:00,  2.00batch/s, Batch Loss=0.1953, Avg Loss=0.4113, Time Left=22.44 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 750/3393 [05:33<22:13,  1.98batch/s, Batch Loss=0.1953, Avg Loss=0.4113, Time Left=22.44 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 750/3393 [05:34<22:13,  1.98batch/s, Batch Loss=0.2546, Avg Loss=0.4110, Time Left=22.43 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 751/3393 [05:34<21:49,  2.02batch/s, Batch Loss=0.2546, Avg Loss=0.4110, Time Left=22.43 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 751/3393 [05:34<21:49,  2.02batch/s, Batch Loss=0.4067, Avg Loss=0.4110, Time Left=22.42 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 752/3393 [05:34<21:39,  2.03batch/s, Batch Loss=0.4067, Avg Loss=0.4110, Time Left=22.42 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 752/3393 [05:35<21:39,  2.03batch/s, Batch Loss=0.0930, Avg Loss=0.4106, Time Left=22.41 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 753/3393 [05:35<21:17,  2.07batch/s, Batch Loss=0.0930, Avg Loss=0.4106, Time Left=22.41 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 753/3393 [05:35<21:17,  2.07batch/s, Batch Loss=0.0601, Avg Loss=0.4100, Time Left=22.40 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 754/3393 [05:35<21:21,  2.06batch/s, Batch Loss=0.0601, Avg Loss=0.4100, Time Left=22.40 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 754/3393 [05:36<21:21,  2.06batch/s, Batch Loss=0.3219, Avg Loss=0.4099, Time Left=22.39 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 755/3393 [05:36<21:43,  2.02batch/s, Batch Loss=0.3219, Avg Loss=0.4099, Time Left=22.39 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 755/3393 [05:36<21:43,  2.02batch/s, Batch Loss=0.2282, Avg Loss=0.4096, Time Left=22.38 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 756/3393 [05:36<21:19,  2.06batch/s, Batch Loss=0.2282, Avg Loss=0.4096, Time Left=22.38 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 756/3393 [05:37<21:19,  2.06batch/s, Batch Loss=0.1187, Avg Loss=0.4092, Time Left=22.37 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  22%|▏| 757/3393 [05:37<21:26,  2.05batch/s, Batch Loss=0.1187, Avg Loss=0.4092, Time Left=22.37 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 757/3393 [05:37<21:26,  2.05batch/s, Batch Loss=0.1878, Avg Loss=0.4089, Time Left=22.37 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 758/3393 [05:37<21:57,  2.00batch/s, Batch Loss=0.1878, Avg Loss=0.4089, Time Left=22.37 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 758/3393 [05:38<21:57,  2.00batch/s, Batch Loss=0.3112, Avg Loss=0.4087, Time Left=22.36 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 759/3393 [05:38<21:43,  2.02batch/s, Batch Loss=0.3112, Avg Loss=0.4087, Time Left=22.36 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 759/3393 [05:38<21:43,  2.02batch/s, Batch Loss=0.0699, Avg Loss=0.4083, Time Left=22.35 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 760/3393 [05:38<22:01,  1.99batch/s, Batch Loss=0.0699, Avg Loss=0.4083, Time Left=22.35 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 760/3393 [05:39<22:01,  1.99batch/s, Batch Loss=0.3724, Avg Loss=0.4082, Time Left=22.34 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 761/3393 [05:39<21:23,  2.05batch/s, Batch Loss=0.3724, Avg Loss=0.4082, Time Left=22.34 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 761/3393 [05:39<21:23,  2.05batch/s, Batch Loss=0.2352, Avg Loss=0.4079, Time Left=22.33 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 762/3393 [05:39<21:31,  2.04batch/s, Batch Loss=0.2352, Avg Loss=0.4079, Time Left=22.33 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 762/3393 [05:40<21:31,  2.04batch/s, Batch Loss=0.2309, Avg Loss=0.4077, Time Left=22.32 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 763/3393 [05:40<21:49,  2.01batch/s, Batch Loss=0.2309, Avg Loss=0.4077, Time Left=22.32 \u001b[A\n",
      "Epoch 1/3 - Training:  22%|▏| 763/3393 [05:40<21:49,  2.01batch/s, Batch Loss=0.0548, Avg Loss=0.4072, Time Left=22.31 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 764/3393 [05:40<21:22,  2.05batch/s, Batch Loss=0.0548, Avg Loss=0.4072, Time Left=22.31 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 764/3393 [05:41<21:22,  2.05batch/s, Batch Loss=0.3237, Avg Loss=0.4071, Time Left=22.31 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 765/3393 [05:41<21:51,  2.00batch/s, Batch Loss=0.3237, Avg Loss=0.4071, Time Left=22.31 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 765/3393 [05:41<21:51,  2.00batch/s, Batch Loss=0.1323, Avg Loss=0.4067, Time Left=22.30 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 766/3393 [05:41<21:37,  2.02batch/s, Batch Loss=0.1323, Avg Loss=0.4067, Time Left=22.30 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 766/3393 [05:42<21:37,  2.02batch/s, Batch Loss=0.2171, Avg Loss=0.4064, Time Left=22.29 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 767/3393 [05:42<21:35,  2.03batch/s, Batch Loss=0.2171, Avg Loss=0.4064, Time Left=22.29 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 767/3393 [05:42<21:35,  2.03batch/s, Batch Loss=0.7000, Avg Loss=0.4068, Time Left=22.28 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 768/3393 [05:42<21:35,  2.03batch/s, Batch Loss=0.7000, Avg Loss=0.4068, Time Left=22.28 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 768/3393 [05:43<21:35,  2.03batch/s, Batch Loss=0.4226, Avg Loss=0.4068, Time Left=22.27 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 769/3393 [05:43<21:41,  2.02batch/s, Batch Loss=0.4226, Avg Loss=0.4068, Time Left=22.27 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 769/3393 [05:43<21:41,  2.02batch/s, Batch Loss=0.3066, Avg Loss=0.4067, Time Left=22.27 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 770/3393 [05:43<22:03,  1.98batch/s, Batch Loss=0.3066, Avg Loss=0.4067, Time Left=22.27 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 770/3393 [05:44<22:03,  1.98batch/s, Batch Loss=0.3749, Avg Loss=0.4066, Time Left=22.26 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 771/3393 [05:44<22:43,  1.92batch/s, Batch Loss=0.3749, Avg Loss=0.4066, Time Left=22.26 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 771/3393 [05:44<22:43,  1.92batch/s, Batch Loss=0.2607, Avg Loss=0.4064, Time Left=22.26 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 772/3393 [05:44<23:07,  1.89batch/s, Batch Loss=0.2607, Avg Loss=0.4064, Time Left=22.26 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 772/3393 [05:45<23:07,  1.89batch/s, Batch Loss=0.5520, Avg Loss=0.4066, Time Left=22.25 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 773/3393 [05:45<23:01,  1.90batch/s, Batch Loss=0.5520, Avg Loss=0.4066, Time Left=22.25 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 773/3393 [05:45<23:01,  1.90batch/s, Batch Loss=0.2717, Avg Loss=0.4064, Time Left=22.25 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 774/3393 [05:45<23:00,  1.90batch/s, Batch Loss=0.2717, Avg Loss=0.4064, Time Left=22.25 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 774/3393 [05:46<23:00,  1.90batch/s, Batch Loss=0.4177, Avg Loss=0.4065, Time Left=22.24 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 775/3393 [05:46<23:00,  1.90batch/s, Batch Loss=0.4177, Avg Loss=0.4065, Time Left=22.24 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 775/3393 [05:46<23:00,  1.90batch/s, Batch Loss=0.1839, Avg Loss=0.4061, Time Left=22.23 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 776/3393 [05:46<23:10,  1.88batch/s, Batch Loss=0.1839, Avg Loss=0.4061, Time Left=22.23 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 776/3393 [05:47<23:10,  1.88batch/s, Batch Loss=0.3608, Avg Loss=0.4061, Time Left=22.22 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 777/3393 [05:47<22:08,  1.97batch/s, Batch Loss=0.3608, Avg Loss=0.4061, Time Left=22.22 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 777/3393 [05:47<22:08,  1.97batch/s, Batch Loss=0.4033, Avg Loss=0.4061, Time Left=22.22 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 778/3393 [05:47<22:34,  1.93batch/s, Batch Loss=0.4033, Avg Loss=0.4061, Time Left=22.22 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 778/3393 [05:48<22:34,  1.93batch/s, Batch Loss=0.2125, Avg Loss=0.4058, Time Left=22.21 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 779/3393 [05:48<22:47,  1.91batch/s, Batch Loss=0.2125, Avg Loss=0.4058, Time Left=22.21 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 779/3393 [05:48<22:47,  1.91batch/s, Batch Loss=0.3996, Avg Loss=0.4058, Time Left=22.20 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 780/3393 [05:48<22:22,  1.95batch/s, Batch Loss=0.3996, Avg Loss=0.4058, Time Left=22.20 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 780/3393 [05:49<22:22,  1.95batch/s, Batch Loss=0.5939, Avg Loss=0.4061, Time Left=22.20 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 781/3393 [05:49<21:54,  1.99batch/s, Batch Loss=0.5939, Avg Loss=0.4061, Time Left=22.20 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 781/3393 [05:49<21:54,  1.99batch/s, Batch Loss=0.1687, Avg Loss=0.4057, Time Left=22.19 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 782/3393 [05:49<21:28,  2.03batch/s, Batch Loss=0.1687, Avg Loss=0.4057, Time Left=22.19 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 782/3393 [05:50<21:28,  2.03batch/s, Batch Loss=0.1862, Avg Loss=0.4054, Time Left=22.18 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 783/3393 [05:50<21:48,  2.00batch/s, Batch Loss=0.1862, Avg Loss=0.4054, Time Left=22.18 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 783/3393 [05:50<21:48,  2.00batch/s, Batch Loss=0.1392, Avg Loss=0.4050, Time Left=22.17 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 784/3393 [05:50<21:46,  2.00batch/s, Batch Loss=0.1392, Avg Loss=0.4050, Time Left=22.17 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 784/3393 [05:51<21:46,  2.00batch/s, Batch Loss=0.2126, Avg Loss=0.4048, Time Left=22.16 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 785/3393 [05:51<21:47,  1.99batch/s, Batch Loss=0.2126, Avg Loss=0.4048, Time Left=22.16 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 785/3393 [05:51<21:47,  1.99batch/s, Batch Loss=0.2877, Avg Loss=0.4046, Time Left=22.15 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 786/3393 [05:51<21:23,  2.03batch/s, Batch Loss=0.2877, Avg Loss=0.4046, Time Left=22.15 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 786/3393 [05:52<21:23,  2.03batch/s, Batch Loss=0.2520, Avg Loss=0.4044, Time Left=22.14 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 787/3393 [05:52<21:08,  2.05batch/s, Batch Loss=0.2520, Avg Loss=0.4044, Time Left=22.14 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 787/3393 [05:52<21:08,  2.05batch/s, Batch Loss=0.1516, Avg Loss=0.4040, Time Left=22.14 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 788/3393 [05:52<21:42,  2.00batch/s, Batch Loss=0.1516, Avg Loss=0.4040, Time Left=22.14 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 788/3393 [05:53<21:42,  2.00batch/s, Batch Loss=0.2163, Avg Loss=0.4038, Time Left=22.13 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 789/3393 [05:53<21:18,  2.04batch/s, Batch Loss=0.2163, Avg Loss=0.4038, Time Left=22.13 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 789/3393 [05:53<21:18,  2.04batch/s, Batch Loss=0.3659, Avg Loss=0.4037, Time Left=22.12 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  23%|▏| 790/3393 [05:53<21:41,  2.00batch/s, Batch Loss=0.3659, Avg Loss=0.4037, Time Left=22.12 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 790/3393 [05:54<21:41,  2.00batch/s, Batch Loss=0.1521, Avg Loss=0.4034, Time Left=22.11 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 791/3393 [05:54<21:28,  2.02batch/s, Batch Loss=0.1521, Avg Loss=0.4034, Time Left=22.11 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 791/3393 [05:54<21:28,  2.02batch/s, Batch Loss=0.2772, Avg Loss=0.4032, Time Left=22.10 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 792/3393 [05:54<21:20,  2.03batch/s, Batch Loss=0.2772, Avg Loss=0.4032, Time Left=22.10 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 792/3393 [05:55<21:20,  2.03batch/s, Batch Loss=0.3457, Avg Loss=0.4031, Time Left=22.10 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 793/3393 [05:55<21:27,  2.02batch/s, Batch Loss=0.3457, Avg Loss=0.4031, Time Left=22.10 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 793/3393 [05:55<21:27,  2.02batch/s, Batch Loss=0.2316, Avg Loss=0.4029, Time Left=22.09 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 794/3393 [05:55<21:19,  2.03batch/s, Batch Loss=0.2316, Avg Loss=0.4029, Time Left=22.09 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 794/3393 [05:56<21:19,  2.03batch/s, Batch Loss=0.1574, Avg Loss=0.4025, Time Left=22.08 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 795/3393 [05:56<21:27,  2.02batch/s, Batch Loss=0.1574, Avg Loss=0.4025, Time Left=22.08 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 795/3393 [05:56<21:27,  2.02batch/s, Batch Loss=0.3380, Avg Loss=0.4024, Time Left=22.07 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 796/3393 [05:56<21:07,  2.05batch/s, Batch Loss=0.3380, Avg Loss=0.4024, Time Left=22.07 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 796/3393 [05:57<21:07,  2.05batch/s, Batch Loss=0.2658, Avg Loss=0.4022, Time Left=22.06 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 797/3393 [05:57<20:42,  2.09batch/s, Batch Loss=0.2658, Avg Loss=0.4022, Time Left=22.06 \u001b[A\n",
      "Epoch 1/3 - Training:  23%|▏| 797/3393 [05:57<20:42,  2.09batch/s, Batch Loss=0.1521, Avg Loss=0.4019, Time Left=22.05 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 798/3393 [05:57<21:12,  2.04batch/s, Batch Loss=0.1521, Avg Loss=0.4019, Time Left=22.05 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 798/3393 [05:58<21:12,  2.04batch/s, Batch Loss=0.3221, Avg Loss=0.4018, Time Left=22.04 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 799/3393 [05:58<21:22,  2.02batch/s, Batch Loss=0.3221, Avg Loss=0.4018, Time Left=22.04 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 799/3393 [05:58<21:22,  2.02batch/s, Batch Loss=0.1030, Avg Loss=0.4014, Time Left=22.04 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 800/3393 [05:58<21:58,  1.97batch/s, Batch Loss=0.1030, Avg Loss=0.4014, Time Left=22.04 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 800/3393 [05:59<21:58,  1.97batch/s, Batch Loss=0.4701, Avg Loss=0.4015, Time Left=22.03 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 801/3393 [05:59<21:33,  2.00batch/s, Batch Loss=0.4701, Avg Loss=0.4015, Time Left=22.03 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 801/3393 [05:59<21:33,  2.00batch/s, Batch Loss=0.1719, Avg Loss=0.4012, Time Left=22.02 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 802/3393 [05:59<21:58,  1.97batch/s, Batch Loss=0.1719, Avg Loss=0.4012, Time Left=22.02 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 802/3393 [06:00<21:58,  1.97batch/s, Batch Loss=0.2048, Avg Loss=0.4009, Time Left=22.02 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 803/3393 [06:00<21:57,  1.97batch/s, Batch Loss=0.2048, Avg Loss=0.4009, Time Left=22.02 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 803/3393 [06:00<21:57,  1.97batch/s, Batch Loss=0.2522, Avg Loss=0.4007, Time Left=22.01 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 804/3393 [06:00<22:12,  1.94batch/s, Batch Loss=0.2522, Avg Loss=0.4007, Time Left=22.01 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 804/3393 [06:01<22:12,  1.94batch/s, Batch Loss=0.2020, Avg Loss=0.4004, Time Left=22.00 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 805/3393 [06:01<21:39,  1.99batch/s, Batch Loss=0.2020, Avg Loss=0.4004, Time Left=22.00 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 805/3393 [06:01<21:39,  1.99batch/s, Batch Loss=0.3816, Avg Loss=0.4004, Time Left=21.99 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 806/3393 [06:01<22:03,  1.95batch/s, Batch Loss=0.3816, Avg Loss=0.4004, Time Left=21.99 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 806/3393 [06:02<22:03,  1.95batch/s, Batch Loss=0.2018, Avg Loss=0.4001, Time Left=21.98 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 807/3393 [06:02<21:28,  2.01batch/s, Batch Loss=0.2018, Avg Loss=0.4001, Time Left=21.98 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 807/3393 [06:02<21:28,  2.01batch/s, Batch Loss=0.2039, Avg Loss=0.3998, Time Left=21.98 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 808/3393 [06:02<21:56,  1.96batch/s, Batch Loss=0.2039, Avg Loss=0.3998, Time Left=21.98 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 808/3393 [06:03<21:56,  1.96batch/s, Batch Loss=0.1654, Avg Loss=0.3995, Time Left=21.97 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 809/3393 [06:03<21:14,  2.03batch/s, Batch Loss=0.1654, Avg Loss=0.3995, Time Left=21.97 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 809/3393 [06:03<21:14,  2.03batch/s, Batch Loss=0.0939, Avg Loss=0.3991, Time Left=21.96 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 810/3393 [06:03<21:07,  2.04batch/s, Batch Loss=0.0939, Avg Loss=0.3991, Time Left=21.96 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 810/3393 [06:04<21:07,  2.04batch/s, Batch Loss=0.2836, Avg Loss=0.3989, Time Left=21.95 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 811/3393 [06:04<21:28,  2.00batch/s, Batch Loss=0.2836, Avg Loss=0.3989, Time Left=21.95 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 811/3393 [06:04<21:28,  2.00batch/s, Batch Loss=0.0987, Avg Loss=0.3985, Time Left=21.94 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 812/3393 [06:04<21:18,  2.02batch/s, Batch Loss=0.0987, Avg Loss=0.3985, Time Left=21.94 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 812/3393 [06:05<21:18,  2.02batch/s, Batch Loss=0.2010, Avg Loss=0.3983, Time Left=21.93 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 813/3393 [06:05<21:12,  2.03batch/s, Batch Loss=0.2010, Avg Loss=0.3983, Time Left=21.93 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 813/3393 [06:05<21:12,  2.03batch/s, Batch Loss=0.1874, Avg Loss=0.3980, Time Left=21.92 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 814/3393 [06:05<20:53,  2.06batch/s, Batch Loss=0.1874, Avg Loss=0.3980, Time Left=21.92 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 814/3393 [06:06<20:53,  2.06batch/s, Batch Loss=0.1805, Avg Loss=0.3977, Time Left=21.91 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 815/3393 [06:06<20:42,  2.07batch/s, Batch Loss=0.1805, Avg Loss=0.3977, Time Left=21.91 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 815/3393 [06:06<20:42,  2.07batch/s, Batch Loss=0.2042, Avg Loss=0.3974, Time Left=21.91 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 816/3393 [06:06<21:33,  1.99batch/s, Batch Loss=0.2042, Avg Loss=0.3974, Time Left=21.91 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 816/3393 [06:07<21:33,  1.99batch/s, Batch Loss=0.2092, Avg Loss=0.3972, Time Left=21.90 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 817/3393 [06:07<21:32,  1.99batch/s, Batch Loss=0.2092, Avg Loss=0.3972, Time Left=21.90 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 817/3393 [06:07<21:32,  1.99batch/s, Batch Loss=0.2970, Avg Loss=0.3970, Time Left=21.89 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 818/3393 [06:07<21:20,  2.01batch/s, Batch Loss=0.2970, Avg Loss=0.3970, Time Left=21.89 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 818/3393 [06:08<21:20,  2.01batch/s, Batch Loss=0.3695, Avg Loss=0.3970, Time Left=21.88 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 819/3393 [06:08<21:03,  2.04batch/s, Batch Loss=0.3695, Avg Loss=0.3970, Time Left=21.88 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 819/3393 [06:08<21:03,  2.04batch/s, Batch Loss=0.1494, Avg Loss=0.3967, Time Left=21.87 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 820/3393 [06:08<20:30,  2.09batch/s, Batch Loss=0.1494, Avg Loss=0.3967, Time Left=21.87 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 820/3393 [06:09<20:30,  2.09batch/s, Batch Loss=0.2962, Avg Loss=0.3965, Time Left=21.86 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 821/3393 [06:09<21:00,  2.04batch/s, Batch Loss=0.2962, Avg Loss=0.3965, Time Left=21.86 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 821/3393 [06:09<21:00,  2.04batch/s, Batch Loss=0.3194, Avg Loss=0.3964, Time Left=21.86 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 822/3393 [06:09<20:57,  2.05batch/s, Batch Loss=0.3194, Avg Loss=0.3964, Time Left=21.86 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 822/3393 [06:10<20:57,  2.05batch/s, Batch Loss=0.3542, Avg Loss=0.3964, Time Left=21.85 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  24%|▏| 823/3393 [06:10<20:53,  2.05batch/s, Batch Loss=0.3542, Avg Loss=0.3964, Time Left=21.85 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 823/3393 [06:10<20:53,  2.05batch/s, Batch Loss=0.0941, Avg Loss=0.3960, Time Left=21.84 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 824/3393 [06:10<21:05,  2.03batch/s, Batch Loss=0.0941, Avg Loss=0.3960, Time Left=21.84 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 824/3393 [06:11<21:05,  2.03batch/s, Batch Loss=0.0869, Avg Loss=0.3956, Time Left=21.83 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 825/3393 [06:11<20:48,  2.06batch/s, Batch Loss=0.0869, Avg Loss=0.3956, Time Left=21.83 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 825/3393 [06:11<20:48,  2.06batch/s, Batch Loss=0.3029, Avg Loss=0.3954, Time Left=21.82 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 826/3393 [06:11<21:13,  2.02batch/s, Batch Loss=0.3029, Avg Loss=0.3954, Time Left=21.82 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 826/3393 [06:12<21:13,  2.02batch/s, Batch Loss=0.2694, Avg Loss=0.3953, Time Left=21.81 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 827/3393 [06:12<21:08,  2.02batch/s, Batch Loss=0.2694, Avg Loss=0.3953, Time Left=21.81 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 827/3393 [06:12<21:08,  2.02batch/s, Batch Loss=0.3069, Avg Loss=0.3952, Time Left=21.80 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 828/3393 [06:12<20:47,  2.06batch/s, Batch Loss=0.3069, Avg Loss=0.3952, Time Left=21.80 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 828/3393 [06:13<20:47,  2.06batch/s, Batch Loss=0.2414, Avg Loss=0.3949, Time Left=21.80 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 829/3393 [06:13<21:10,  2.02batch/s, Batch Loss=0.2414, Avg Loss=0.3949, Time Left=21.80 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 829/3393 [06:13<21:10,  2.02batch/s, Batch Loss=0.1002, Avg Loss=0.3946, Time Left=21.79 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 830/3393 [06:13<21:02,  2.03batch/s, Batch Loss=0.1002, Avg Loss=0.3946, Time Left=21.79 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 830/3393 [06:14<21:02,  2.03batch/s, Batch Loss=0.1211, Avg Loss=0.3942, Time Left=21.78 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 831/3393 [06:14<21:22,  2.00batch/s, Batch Loss=0.1211, Avg Loss=0.3942, Time Left=21.78 \u001b[A\n",
      "Epoch 1/3 - Training:  24%|▏| 831/3393 [06:14<21:22,  2.00batch/s, Batch Loss=0.1760, Avg Loss=0.3939, Time Left=21.77 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 832/3393 [06:14<21:22,  2.00batch/s, Batch Loss=0.1760, Avg Loss=0.3939, Time Left=21.77 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 832/3393 [06:15<21:22,  2.00batch/s, Batch Loss=0.1236, Avg Loss=0.3935, Time Left=21.77 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 833/3393 [06:15<21:30,  1.98batch/s, Batch Loss=0.1236, Avg Loss=0.3935, Time Left=21.77 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 833/3393 [06:15<21:30,  1.98batch/s, Batch Loss=0.1332, Avg Loss=0.3932, Time Left=21.76 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 834/3393 [06:15<20:45,  2.05batch/s, Batch Loss=0.1332, Avg Loss=0.3932, Time Left=21.76 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 834/3393 [06:15<20:45,  2.05batch/s, Batch Loss=0.1566, Avg Loss=0.3929, Time Left=21.75 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 835/3393 [06:15<20:54,  2.04batch/s, Batch Loss=0.1566, Avg Loss=0.3929, Time Left=21.75 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 835/3393 [06:16<20:54,  2.04batch/s, Batch Loss=0.1908, Avg Loss=0.3926, Time Left=21.74 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 836/3393 [06:16<21:04,  2.02batch/s, Batch Loss=0.1908, Avg Loss=0.3926, Time Left=21.74 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 836/3393 [06:16<21:04,  2.02batch/s, Batch Loss=0.1809, Avg Loss=0.3923, Time Left=21.73 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 837/3393 [06:16<20:45,  2.05batch/s, Batch Loss=0.1809, Avg Loss=0.3923, Time Left=21.73 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 837/3393 [06:17<20:45,  2.05batch/s, Batch Loss=0.4232, Avg Loss=0.3924, Time Left=21.72 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 838/3393 [06:17<21:08,  2.01batch/s, Batch Loss=0.4232, Avg Loss=0.3924, Time Left=21.72 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 838/3393 [06:17<21:08,  2.01batch/s, Batch Loss=0.1918, Avg Loss=0.3921, Time Left=21.71 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 839/3393 [06:17<21:00,  2.03batch/s, Batch Loss=0.1918, Avg Loss=0.3921, Time Left=21.71 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 839/3393 [06:18<21:00,  2.03batch/s, Batch Loss=0.1534, Avg Loss=0.3918, Time Left=21.71 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 840/3393 [06:18<21:18,  2.00batch/s, Batch Loss=0.1534, Avg Loss=0.3918, Time Left=21.71 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 840/3393 [06:18<21:18,  2.00batch/s, Batch Loss=0.1866, Avg Loss=0.3915, Time Left=21.70 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 841/3393 [06:18<20:43,  2.05batch/s, Batch Loss=0.1866, Avg Loss=0.3915, Time Left=21.70 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 841/3393 [06:19<20:43,  2.05batch/s, Batch Loss=0.1146, Avg Loss=0.3912, Time Left=21.69 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 842/3393 [06:19<20:31,  2.07batch/s, Batch Loss=0.1146, Avg Loss=0.3912, Time Left=21.69 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 842/3393 [06:19<20:31,  2.07batch/s, Batch Loss=0.1338, Avg Loss=0.3908, Time Left=21.68 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 843/3393 [06:19<20:43,  2.05batch/s, Batch Loss=0.1338, Avg Loss=0.3908, Time Left=21.68 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 843/3393 [06:20<20:43,  2.05batch/s, Batch Loss=0.0973, Avg Loss=0.3905, Time Left=21.67 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 844/3393 [06:20<20:31,  2.07batch/s, Batch Loss=0.0973, Avg Loss=0.3905, Time Left=21.67 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 844/3393 [06:20<20:31,  2.07batch/s, Batch Loss=0.3216, Avg Loss=0.3904, Time Left=21.66 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 845/3393 [06:20<20:33,  2.07batch/s, Batch Loss=0.3216, Avg Loss=0.3904, Time Left=21.66 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 845/3393 [06:21<20:33,  2.07batch/s, Batch Loss=0.3126, Avg Loss=0.3903, Time Left=21.65 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 846/3393 [06:21<20:36,  2.06batch/s, Batch Loss=0.3126, Avg Loss=0.3903, Time Left=21.65 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 846/3393 [06:21<20:36,  2.06batch/s, Batch Loss=0.5222, Avg Loss=0.3904, Time Left=21.64 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 847/3393 [06:21<20:46,  2.04batch/s, Batch Loss=0.5222, Avg Loss=0.3904, Time Left=21.64 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 847/3393 [06:22<20:46,  2.04batch/s, Batch Loss=0.1055, Avg Loss=0.3901, Time Left=21.64 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 848/3393 [06:22<20:43,  2.05batch/s, Batch Loss=0.1055, Avg Loss=0.3901, Time Left=21.64 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▏| 848/3393 [06:22<20:43,  2.05batch/s, Batch Loss=0.2855, Avg Loss=0.3899, Time Left=21.63 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 849/3393 [06:22<20:33,  2.06batch/s, Batch Loss=0.2855, Avg Loss=0.3899, Time Left=21.63 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 849/3393 [06:23<20:33,  2.06batch/s, Batch Loss=0.1061, Avg Loss=0.3896, Time Left=21.62 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 850/3393 [06:23<20:17,  2.09batch/s, Batch Loss=0.1061, Avg Loss=0.3896, Time Left=21.62 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 850/3393 [06:23<20:17,  2.09batch/s, Batch Loss=0.4375, Avg Loss=0.3896, Time Left=21.61 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 851/3393 [06:23<20:22,  2.08batch/s, Batch Loss=0.4375, Avg Loss=0.3896, Time Left=21.61 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 851/3393 [06:24<20:22,  2.08batch/s, Batch Loss=0.1333, Avg Loss=0.3893, Time Left=21.60 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 852/3393 [06:24<20:39,  2.05batch/s, Batch Loss=0.1333, Avg Loss=0.3893, Time Left=21.60 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 852/3393 [06:24<20:39,  2.05batch/s, Batch Loss=0.1429, Avg Loss=0.3890, Time Left=21.59 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 853/3393 [06:24<20:37,  2.05batch/s, Batch Loss=0.1429, Avg Loss=0.3890, Time Left=21.59 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 853/3393 [06:25<20:37,  2.05batch/s, Batch Loss=0.3199, Avg Loss=0.3889, Time Left=21.58 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 854/3393 [06:25<20:48,  2.03batch/s, Batch Loss=0.3199, Avg Loss=0.3889, Time Left=21.58 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 854/3393 [06:25<20:48,  2.03batch/s, Batch Loss=0.1326, Avg Loss=0.3886, Time Left=21.57 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 855/3393 [06:25<20:45,  2.04batch/s, Batch Loss=0.1326, Avg Loss=0.3886, Time Left=21.57 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 855/3393 [06:26<20:45,  2.04batch/s, Batch Loss=0.7838, Avg Loss=0.3891, Time Left=21.56 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  25%|▎| 856/3393 [06:26<20:28,  2.07batch/s, Batch Loss=0.7838, Avg Loss=0.3891, Time Left=21.56 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 856/3393 [06:26<20:28,  2.07batch/s, Batch Loss=0.4209, Avg Loss=0.3891, Time Left=21.56 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 857/3393 [06:26<20:53,  2.02batch/s, Batch Loss=0.4209, Avg Loss=0.3891, Time Left=21.56 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 857/3393 [06:27<20:53,  2.02batch/s, Batch Loss=0.1628, Avg Loss=0.3888, Time Left=21.55 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 858/3393 [06:27<20:43,  2.04batch/s, Batch Loss=0.1628, Avg Loss=0.3888, Time Left=21.55 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 858/3393 [06:27<20:43,  2.04batch/s, Batch Loss=0.1352, Avg Loss=0.3885, Time Left=21.54 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 859/3393 [06:27<20:45,  2.03batch/s, Batch Loss=0.1352, Avg Loss=0.3885, Time Left=21.54 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 859/3393 [06:28<20:45,  2.03batch/s, Batch Loss=0.1676, Avg Loss=0.3882, Time Left=21.53 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 860/3393 [06:28<20:39,  2.04batch/s, Batch Loss=0.1676, Avg Loss=0.3882, Time Left=21.53 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 860/3393 [06:28<20:39,  2.04batch/s, Batch Loss=0.1328, Avg Loss=0.3879, Time Left=21.52 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 861/3393 [06:28<20:36,  2.05batch/s, Batch Loss=0.1328, Avg Loss=0.3879, Time Left=21.52 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 861/3393 [06:29<20:36,  2.05batch/s, Batch Loss=0.2661, Avg Loss=0.3877, Time Left=21.51 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 862/3393 [06:29<20:34,  2.05batch/s, Batch Loss=0.2661, Avg Loss=0.3877, Time Left=21.51 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 862/3393 [06:29<20:34,  2.05batch/s, Batch Loss=0.3923, Avg Loss=0.3877, Time Left=21.51 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 863/3393 [06:29<20:44,  2.03batch/s, Batch Loss=0.3923, Avg Loss=0.3877, Time Left=21.51 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 863/3393 [06:30<20:44,  2.03batch/s, Batch Loss=0.0925, Avg Loss=0.3874, Time Left=21.50 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 864/3393 [06:30<20:51,  2.02batch/s, Batch Loss=0.0925, Avg Loss=0.3874, Time Left=21.50 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 864/3393 [06:30<20:51,  2.02batch/s, Batch Loss=0.3778, Avg Loss=0.3874, Time Left=21.49 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 865/3393 [06:30<20:46,  2.03batch/s, Batch Loss=0.3778, Avg Loss=0.3874, Time Left=21.49 \u001b[A\n",
      "Epoch 1/3 - Training:  25%|▎| 865/3393 [06:31<20:46,  2.03batch/s, Batch Loss=0.2108, Avg Loss=0.3871, Time Left=21.48 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 866/3393 [06:31<20:27,  2.06batch/s, Batch Loss=0.2108, Avg Loss=0.3871, Time Left=21.48 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 866/3393 [06:31<20:27,  2.06batch/s, Batch Loss=0.0178, Avg Loss=0.3867, Time Left=21.47 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 867/3393 [06:31<20:28,  2.06batch/s, Batch Loss=0.0178, Avg Loss=0.3867, Time Left=21.47 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 867/3393 [06:32<20:28,  2.06batch/s, Batch Loss=0.1139, Avg Loss=0.3863, Time Left=21.46 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 868/3393 [06:32<20:17,  2.07batch/s, Batch Loss=0.1139, Avg Loss=0.3863, Time Left=21.46 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 868/3393 [06:32<20:17,  2.07batch/s, Batch Loss=0.1717, Avg Loss=0.3861, Time Left=21.45 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 869/3393 [06:32<20:19,  2.07batch/s, Batch Loss=0.1717, Avg Loss=0.3861, Time Left=21.45 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 869/3393 [06:33<20:19,  2.07batch/s, Batch Loss=0.1306, Avg Loss=0.3857, Time Left=21.44 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 870/3393 [06:33<20:28,  2.05batch/s, Batch Loss=0.1306, Avg Loss=0.3857, Time Left=21.44 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 870/3393 [06:33<20:28,  2.05batch/s, Batch Loss=0.1349, Avg Loss=0.3854, Time Left=21.43 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 871/3393 [06:33<20:08,  2.09batch/s, Batch Loss=0.1349, Avg Loss=0.3854, Time Left=21.43 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 871/3393 [06:34<20:08,  2.09batch/s, Batch Loss=0.1693, Avg Loss=0.3851, Time Left=21.43 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 872/3393 [06:34<20:12,  2.08batch/s, Batch Loss=0.1693, Avg Loss=0.3851, Time Left=21.43 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 872/3393 [06:34<20:12,  2.08batch/s, Batch Loss=0.2624, Avg Loss=0.3850, Time Left=21.42 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 873/3393 [06:34<20:17,  2.07batch/s, Batch Loss=0.2624, Avg Loss=0.3850, Time Left=21.42 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 873/3393 [06:35<20:17,  2.07batch/s, Batch Loss=0.2655, Avg Loss=0.3848, Time Left=21.41 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 874/3393 [06:35<20:18,  2.07batch/s, Batch Loss=0.2655, Avg Loss=0.3848, Time Left=21.41 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 874/3393 [06:35<20:18,  2.07batch/s, Batch Loss=0.0270, Avg Loss=0.3844, Time Left=21.40 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 875/3393 [06:35<20:42,  2.03batch/s, Batch Loss=0.0270, Avg Loss=0.3844, Time Left=21.40 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 875/3393 [06:36<20:42,  2.03batch/s, Batch Loss=0.3532, Avg Loss=0.3844, Time Left=21.39 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 876/3393 [06:36<20:36,  2.04batch/s, Batch Loss=0.3532, Avg Loss=0.3844, Time Left=21.39 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 876/3393 [06:36<20:36,  2.04batch/s, Batch Loss=0.1700, Avg Loss=0.3841, Time Left=21.38 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 877/3393 [06:36<20:21,  2.06batch/s, Batch Loss=0.1700, Avg Loss=0.3841, Time Left=21.38 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 877/3393 [06:36<20:21,  2.06batch/s, Batch Loss=0.2830, Avg Loss=0.3840, Time Left=21.38 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 878/3393 [06:36<20:34,  2.04batch/s, Batch Loss=0.2830, Avg Loss=0.3840, Time Left=21.38 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 878/3393 [06:37<20:34,  2.04batch/s, Batch Loss=0.3452, Avg Loss=0.3839, Time Left=21.37 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 879/3393 [06:37<20:30,  2.04batch/s, Batch Loss=0.3452, Avg Loss=0.3839, Time Left=21.37 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 879/3393 [06:37<20:30,  2.04batch/s, Batch Loss=0.3237, Avg Loss=0.3838, Time Left=21.36 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 880/3393 [06:37<20:40,  2.03batch/s, Batch Loss=0.3237, Avg Loss=0.3838, Time Left=21.36 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 880/3393 [06:38<20:40,  2.03batch/s, Batch Loss=0.1537, Avg Loss=0.3836, Time Left=21.35 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 881/3393 [06:38<20:44,  2.02batch/s, Batch Loss=0.1537, Avg Loss=0.3836, Time Left=21.35 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 881/3393 [06:38<20:44,  2.02batch/s, Batch Loss=0.1150, Avg Loss=0.3832, Time Left=21.34 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 882/3393 [06:38<20:49,  2.01batch/s, Batch Loss=0.1150, Avg Loss=0.3832, Time Left=21.34 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 882/3393 [06:39<20:49,  2.01batch/s, Batch Loss=0.6679, Avg Loss=0.3836, Time Left=21.33 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 883/3393 [06:39<20:29,  2.04batch/s, Batch Loss=0.6679, Avg Loss=0.3836, Time Left=21.33 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 883/3393 [06:39<20:29,  2.04batch/s, Batch Loss=0.4020, Avg Loss=0.3836, Time Left=21.32 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 884/3393 [06:39<20:18,  2.06batch/s, Batch Loss=0.4020, Avg Loss=0.3836, Time Left=21.32 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 884/3393 [06:40<20:18,  2.06batch/s, Batch Loss=0.2606, Avg Loss=0.3834, Time Left=21.32 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 885/3393 [06:40<20:49,  2.01batch/s, Batch Loss=0.2606, Avg Loss=0.3834, Time Left=21.32 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 885/3393 [06:40<20:49,  2.01batch/s, Batch Loss=0.2239, Avg Loss=0.3832, Time Left=21.31 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 886/3393 [06:40<20:16,  2.06batch/s, Batch Loss=0.2239, Avg Loss=0.3832, Time Left=21.31 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 886/3393 [06:41<20:16,  2.06batch/s, Batch Loss=0.0735, Avg Loss=0.3829, Time Left=21.30 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 887/3393 [06:41<20:17,  2.06batch/s, Batch Loss=0.0735, Avg Loss=0.3829, Time Left=21.30 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 887/3393 [06:41<20:17,  2.06batch/s, Batch Loss=0.2690, Avg Loss=0.3827, Time Left=21.29 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 888/3393 [06:41<20:28,  2.04batch/s, Batch Loss=0.2690, Avg Loss=0.3827, Time Left=21.29 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 888/3393 [06:42<20:28,  2.04batch/s, Batch Loss=0.2715, Avg Loss=0.3826, Time Left=21.28 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  26%|▎| 889/3393 [06:42<20:24,  2.04batch/s, Batch Loss=0.2715, Avg Loss=0.3826, Time Left=21.28 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 889/3393 [06:42<20:24,  2.04batch/s, Batch Loss=0.0707, Avg Loss=0.3822, Time Left=21.27 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 890/3393 [06:42<20:45,  2.01batch/s, Batch Loss=0.0707, Avg Loss=0.3822, Time Left=21.27 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 890/3393 [06:43<20:45,  2.01batch/s, Batch Loss=0.1148, Avg Loss=0.3819, Time Left=21.27 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 891/3393 [06:43<20:25,  2.04batch/s, Batch Loss=0.1148, Avg Loss=0.3819, Time Left=21.27 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 891/3393 [06:43<20:25,  2.04batch/s, Batch Loss=0.1540, Avg Loss=0.3816, Time Left=21.26 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 892/3393 [06:43<20:21,  2.05batch/s, Batch Loss=0.1540, Avg Loss=0.3816, Time Left=21.26 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 892/3393 [06:44<20:21,  2.05batch/s, Batch Loss=0.7289, Avg Loss=0.3820, Time Left=21.25 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 893/3393 [06:44<20:49,  2.00batch/s, Batch Loss=0.7289, Avg Loss=0.3820, Time Left=21.25 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 893/3393 [06:44<20:49,  2.00batch/s, Batch Loss=0.4975, Avg Loss=0.3822, Time Left=21.24 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 894/3393 [06:44<20:22,  2.04batch/s, Batch Loss=0.4975, Avg Loss=0.3822, Time Left=21.24 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 894/3393 [06:45<20:22,  2.04batch/s, Batch Loss=0.1824, Avg Loss=0.3819, Time Left=21.23 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 895/3393 [06:45<20:43,  2.01batch/s, Batch Loss=0.1824, Avg Loss=0.3819, Time Left=21.23 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 895/3393 [06:45<20:43,  2.01batch/s, Batch Loss=0.0680, Avg Loss=0.3815, Time Left=21.22 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 896/3393 [06:45<20:11,  2.06batch/s, Batch Loss=0.0680, Avg Loss=0.3815, Time Left=21.22 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 896/3393 [06:46<20:11,  2.06batch/s, Batch Loss=0.1994, Avg Loss=0.3813, Time Left=21.21 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 897/3393 [06:46<20:14,  2.06batch/s, Batch Loss=0.1994, Avg Loss=0.3813, Time Left=21.21 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 897/3393 [06:46<20:14,  2.06batch/s, Batch Loss=0.1485, Avg Loss=0.3810, Time Left=21.21 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 898/3393 [06:46<20:36,  2.02batch/s, Batch Loss=0.1485, Avg Loss=0.3810, Time Left=21.21 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 898/3393 [06:47<20:36,  2.02batch/s, Batch Loss=0.2120, Avg Loss=0.3808, Time Left=21.20 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 899/3393 [06:47<20:15,  2.05batch/s, Batch Loss=0.2120, Avg Loss=0.3808, Time Left=21.20 \u001b[A\n",
      "Epoch 1/3 - Training:  26%|▎| 899/3393 [06:47<20:15,  2.05batch/s, Batch Loss=0.1678, Avg Loss=0.3806, Time Left=21.19 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 900/3393 [06:47<21:02,  1.97batch/s, Batch Loss=0.1678, Avg Loss=0.3806, Time Left=21.19 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 900/3393 [06:48<21:02,  1.97batch/s, Batch Loss=0.2876, Avg Loss=0.3805, Time Left=21.19 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 901/3393 [06:48<21:09,  1.96batch/s, Batch Loss=0.2876, Avg Loss=0.3805, Time Left=21.19 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 901/3393 [06:48<21:09,  1.96batch/s, Batch Loss=0.1497, Avg Loss=0.3802, Time Left=21.18 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 902/3393 [06:48<20:53,  1.99batch/s, Batch Loss=0.1497, Avg Loss=0.3802, Time Left=21.18 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 902/3393 [06:49<20:53,  1.99batch/s, Batch Loss=0.1541, Avg Loss=0.3799, Time Left=21.17 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 903/3393 [06:49<20:57,  1.98batch/s, Batch Loss=0.1541, Avg Loss=0.3799, Time Left=21.17 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 903/3393 [06:49<20:57,  1.98batch/s, Batch Loss=0.4099, Avg Loss=0.3799, Time Left=21.16 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 904/3393 [06:49<21:18,  1.95batch/s, Batch Loss=0.4099, Avg Loss=0.3799, Time Left=21.16 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 904/3393 [06:50<21:18,  1.95batch/s, Batch Loss=0.1727, Avg Loss=0.3797, Time Left=21.15 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 905/3393 [06:50<21:08,  1.96batch/s, Batch Loss=0.1727, Avg Loss=0.3797, Time Left=21.15 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 905/3393 [06:50<21:08,  1.96batch/s, Batch Loss=0.4229, Avg Loss=0.3797, Time Left=21.15 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 906/3393 [06:50<21:36,  1.92batch/s, Batch Loss=0.4229, Avg Loss=0.3797, Time Left=21.15 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 906/3393 [06:51<21:36,  1.92batch/s, Batch Loss=0.0791, Avg Loss=0.3794, Time Left=21.14 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 907/3393 [06:51<20:58,  1.97batch/s, Batch Loss=0.0791, Avg Loss=0.3794, Time Left=21.14 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 907/3393 [06:51<20:58,  1.97batch/s, Batch Loss=0.0370, Avg Loss=0.3790, Time Left=21.13 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 908/3393 [06:51<21:09,  1.96batch/s, Batch Loss=0.0370, Avg Loss=0.3790, Time Left=21.13 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 908/3393 [06:52<21:09,  1.96batch/s, Batch Loss=0.0464, Avg Loss=0.3786, Time Left=21.12 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 909/3393 [06:52<20:56,  1.98batch/s, Batch Loss=0.0464, Avg Loss=0.3786, Time Left=21.12 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 909/3393 [06:52<20:56,  1.98batch/s, Batch Loss=0.3049, Avg Loss=0.3785, Time Left=21.12 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 910/3393 [06:52<20:51,  1.98batch/s, Batch Loss=0.3049, Avg Loss=0.3785, Time Left=21.12 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 910/3393 [06:53<20:51,  1.98batch/s, Batch Loss=0.2105, Avg Loss=0.3783, Time Left=21.11 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 911/3393 [06:53<20:38,  2.00batch/s, Batch Loss=0.2105, Avg Loss=0.3783, Time Left=21.11 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 911/3393 [06:53<20:38,  2.00batch/s, Batch Loss=0.6355, Avg Loss=0.3786, Time Left=21.10 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 912/3393 [06:53<20:25,  2.02batch/s, Batch Loss=0.6355, Avg Loss=0.3786, Time Left=21.10 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 912/3393 [06:54<20:25,  2.02batch/s, Batch Loss=0.7531, Avg Loss=0.3790, Time Left=21.09 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 913/3393 [06:54<20:17,  2.04batch/s, Batch Loss=0.7531, Avg Loss=0.3790, Time Left=21.09 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 913/3393 [06:54<20:17,  2.04batch/s, Batch Loss=0.2077, Avg Loss=0.3788, Time Left=21.08 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 914/3393 [06:54<20:12,  2.04batch/s, Batch Loss=0.2077, Avg Loss=0.3788, Time Left=21.08 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 914/3393 [06:55<20:12,  2.04batch/s, Batch Loss=0.2165, Avg Loss=0.3786, Time Left=21.08 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 915/3393 [06:55<20:42,  2.00batch/s, Batch Loss=0.2165, Avg Loss=0.3786, Time Left=21.08 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 915/3393 [06:55<20:42,  2.00batch/s, Batch Loss=0.2516, Avg Loss=0.3785, Time Left=21.07 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 916/3393 [06:55<20:18,  2.03batch/s, Batch Loss=0.2516, Avg Loss=0.3785, Time Left=21.07 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 916/3393 [06:56<20:18,  2.03batch/s, Batch Loss=0.1468, Avg Loss=0.3782, Time Left=21.06 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 917/3393 [06:56<20:00,  2.06batch/s, Batch Loss=0.1468, Avg Loss=0.3782, Time Left=21.06 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 917/3393 [06:56<20:00,  2.06batch/s, Batch Loss=0.3762, Avg Loss=0.3782, Time Left=21.05 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 918/3393 [06:56<20:36,  2.00batch/s, Batch Loss=0.3762, Avg Loss=0.3782, Time Left=21.05 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 918/3393 [06:57<20:36,  2.00batch/s, Batch Loss=0.3146, Avg Loss=0.3781, Time Left=21.04 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 919/3393 [06:57<20:22,  2.02batch/s, Batch Loss=0.3146, Avg Loss=0.3781, Time Left=21.04 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 919/3393 [06:57<20:22,  2.02batch/s, Batch Loss=0.3164, Avg Loss=0.3781, Time Left=21.03 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 920/3393 [06:57<20:49,  1.98batch/s, Batch Loss=0.3164, Avg Loss=0.3781, Time Left=21.03 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 920/3393 [06:58<20:49,  1.98batch/s, Batch Loss=0.3293, Avg Loss=0.3780, Time Left=21.03 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 921/3393 [06:58<20:44,  1.99batch/s, Batch Loss=0.3293, Avg Loss=0.3780, Time Left=21.03 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 921/3393 [06:58<20:44,  1.99batch/s, Batch Loss=0.2903, Avg Loss=0.3779, Time Left=21.02 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  27%|▎| 922/3393 [06:58<21:17,  1.93batch/s, Batch Loss=0.2903, Avg Loss=0.3779, Time Left=21.02 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 922/3393 [06:59<21:17,  1.93batch/s, Batch Loss=0.4753, Avg Loss=0.3780, Time Left=21.02 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 923/3393 [06:59<21:47,  1.89batch/s, Batch Loss=0.4753, Avg Loss=0.3780, Time Left=21.02 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 923/3393 [07:00<21:47,  1.89batch/s, Batch Loss=0.3972, Avg Loss=0.3780, Time Left=21.01 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 924/3393 [07:00<22:47,  1.81batch/s, Batch Loss=0.3972, Avg Loss=0.3780, Time Left=21.01 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 924/3393 [07:00<22:47,  1.81batch/s, Batch Loss=0.3403, Avg Loss=0.3780, Time Left=21.01 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 925/3393 [07:00<22:46,  1.81batch/s, Batch Loss=0.3403, Avg Loss=0.3780, Time Left=21.01 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 925/3393 [07:01<22:46,  1.81batch/s, Batch Loss=0.2909, Avg Loss=0.3779, Time Left=21.00 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 926/3393 [07:01<22:03,  1.86batch/s, Batch Loss=0.2909, Avg Loss=0.3779, Time Left=21.00 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 926/3393 [07:01<22:03,  1.86batch/s, Batch Loss=0.2498, Avg Loss=0.3777, Time Left=20.99 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 927/3393 [07:01<21:19,  1.93batch/s, Batch Loss=0.2498, Avg Loss=0.3777, Time Left=20.99 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 927/3393 [07:02<21:19,  1.93batch/s, Batch Loss=0.1733, Avg Loss=0.3775, Time Left=20.98 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 928/3393 [07:02<20:32,  2.00batch/s, Batch Loss=0.1733, Avg Loss=0.3775, Time Left=20.98 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 928/3393 [07:02<20:32,  2.00batch/s, Batch Loss=0.1512, Avg Loss=0.3772, Time Left=20.97 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 929/3393 [07:02<20:46,  1.98batch/s, Batch Loss=0.1512, Avg Loss=0.3772, Time Left=20.97 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 929/3393 [07:03<20:46,  1.98batch/s, Batch Loss=0.3419, Avg Loss=0.3772, Time Left=20.96 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 930/3393 [07:03<20:26,  2.01batch/s, Batch Loss=0.3419, Avg Loss=0.3772, Time Left=20.96 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 930/3393 [07:03<20:26,  2.01batch/s, Batch Loss=0.2707, Avg Loss=0.3771, Time Left=20.96 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 931/3393 [07:03<20:39,  1.99batch/s, Batch Loss=0.2707, Avg Loss=0.3771, Time Left=20.96 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 931/3393 [07:04<20:39,  1.99batch/s, Batch Loss=0.3261, Avg Loss=0.3770, Time Left=20.95 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 932/3393 [07:04<20:32,  2.00batch/s, Batch Loss=0.3261, Avg Loss=0.3770, Time Left=20.95 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 932/3393 [07:04<20:32,  2.00batch/s, Batch Loss=0.2154, Avg Loss=0.3768, Time Left=20.94 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 933/3393 [07:04<20:41,  1.98batch/s, Batch Loss=0.2154, Avg Loss=0.3768, Time Left=20.94 \u001b[A\n",
      "Epoch 1/3 - Training:  27%|▎| 933/3393 [07:05<20:41,  1.98batch/s, Batch Loss=0.1397, Avg Loss=0.3765, Time Left=20.93 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 934/3393 [07:05<20:15,  2.02batch/s, Batch Loss=0.1397, Avg Loss=0.3765, Time Left=20.93 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 934/3393 [07:05<20:15,  2.02batch/s, Batch Loss=0.1451, Avg Loss=0.3763, Time Left=20.92 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 935/3393 [07:05<20:16,  2.02batch/s, Batch Loss=0.1451, Avg Loss=0.3763, Time Left=20.92 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 935/3393 [07:06<20:16,  2.02batch/s, Batch Loss=0.1579, Avg Loss=0.3760, Time Left=20.92 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 936/3393 [07:06<20:17,  2.02batch/s, Batch Loss=0.1579, Avg Loss=0.3760, Time Left=20.92 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 936/3393 [07:06<20:17,  2.02batch/s, Batch Loss=0.0563, Avg Loss=0.3756, Time Left=20.91 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 937/3393 [07:06<20:27,  2.00batch/s, Batch Loss=0.0563, Avg Loss=0.3756, Time Left=20.91 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 937/3393 [07:07<20:27,  2.00batch/s, Batch Loss=0.5236, Avg Loss=0.3758, Time Left=20.90 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 938/3393 [07:07<20:52,  1.96batch/s, Batch Loss=0.5236, Avg Loss=0.3758, Time Left=20.90 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 938/3393 [07:07<20:52,  1.96batch/s, Batch Loss=0.2037, Avg Loss=0.3756, Time Left=20.90 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 939/3393 [07:07<21:33,  1.90batch/s, Batch Loss=0.2037, Avg Loss=0.3756, Time Left=20.90 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 939/3393 [07:08<21:33,  1.90batch/s, Batch Loss=0.1086, Avg Loss=0.3753, Time Left=20.89 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 940/3393 [07:08<21:49,  1.87batch/s, Batch Loss=0.1086, Avg Loss=0.3753, Time Left=20.89 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 940/3393 [07:08<21:49,  1.87batch/s, Batch Loss=0.2936, Avg Loss=0.3752, Time Left=20.89 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 941/3393 [07:08<21:55,  1.86batch/s, Batch Loss=0.2936, Avg Loss=0.3752, Time Left=20.89 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 941/3393 [07:09<21:55,  1.86batch/s, Batch Loss=0.4506, Avg Loss=0.3753, Time Left=20.88 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 942/3393 [07:09<23:21,  1.75batch/s, Batch Loss=0.4506, Avg Loss=0.3753, Time Left=20.88 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 942/3393 [07:09<23:21,  1.75batch/s, Batch Loss=0.2485, Avg Loss=0.3752, Time Left=20.88 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 943/3393 [07:09<23:19,  1.75batch/s, Batch Loss=0.2485, Avg Loss=0.3752, Time Left=20.88 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 943/3393 [07:10<23:19,  1.75batch/s, Batch Loss=0.5028, Avg Loss=0.3753, Time Left=20.87 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 944/3393 [07:10<23:10,  1.76batch/s, Batch Loss=0.5028, Avg Loss=0.3753, Time Left=20.87 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 944/3393 [07:11<23:10,  1.76batch/s, Batch Loss=0.4311, Avg Loss=0.3754, Time Left=20.87 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 945/3393 [07:11<22:35,  1.81batch/s, Batch Loss=0.4311, Avg Loss=0.3754, Time Left=20.87 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 945/3393 [07:11<22:35,  1.81batch/s, Batch Loss=0.0546, Avg Loss=0.3750, Time Left=20.86 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 946/3393 [07:11<22:40,  1.80batch/s, Batch Loss=0.0546, Avg Loss=0.3750, Time Left=20.86 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 946/3393 [07:12<22:40,  1.80batch/s, Batch Loss=0.1644, Avg Loss=0.3748, Time Left=20.85 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 947/3393 [07:12<22:08,  1.84batch/s, Batch Loss=0.1644, Avg Loss=0.3748, Time Left=20.85 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 947/3393 [07:12<22:08,  1.84batch/s, Batch Loss=0.2880, Avg Loss=0.3747, Time Left=20.85 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 948/3393 [07:12<22:13,  1.83batch/s, Batch Loss=0.2880, Avg Loss=0.3747, Time Left=20.85 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 948/3393 [07:13<22:13,  1.83batch/s, Batch Loss=0.1040, Avg Loss=0.3743, Time Left=20.85 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 949/3393 [07:13<22:39,  1.80batch/s, Batch Loss=0.1040, Avg Loss=0.3743, Time Left=20.85 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 949/3393 [07:13<22:39,  1.80batch/s, Batch Loss=0.1773, Avg Loss=0.3741, Time Left=20.84 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 950/3393 [07:13<21:34,  1.89batch/s, Batch Loss=0.1773, Avg Loss=0.3741, Time Left=20.84 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 950/3393 [07:14<21:34,  1.89batch/s, Batch Loss=0.2351, Avg Loss=0.3740, Time Left=20.83 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 951/3393 [07:14<21:46,  1.87batch/s, Batch Loss=0.2351, Avg Loss=0.3740, Time Left=20.83 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 951/3393 [07:14<21:46,  1.87batch/s, Batch Loss=0.1718, Avg Loss=0.3737, Time Left=20.82 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 952/3393 [07:14<21:37,  1.88batch/s, Batch Loss=0.1718, Avg Loss=0.3737, Time Left=20.82 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 952/3393 [07:15<21:37,  1.88batch/s, Batch Loss=0.2793, Avg Loss=0.3736, Time Left=20.82 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 953/3393 [07:15<21:43,  1.87batch/s, Batch Loss=0.2793, Avg Loss=0.3736, Time Left=20.82 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 953/3393 [07:15<21:43,  1.87batch/s, Batch Loss=0.6861, Avg Loss=0.3740, Time Left=20.81 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 954/3393 [07:15<21:20,  1.91batch/s, Batch Loss=0.6861, Avg Loss=0.3740, Time Left=20.81 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 954/3393 [07:16<21:20,  1.91batch/s, Batch Loss=0.2256, Avg Loss=0.3738, Time Left=20.80 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  28%|▎| 955/3393 [07:16<20:31,  1.98batch/s, Batch Loss=0.2256, Avg Loss=0.3738, Time Left=20.80 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 955/3393 [07:16<20:31,  1.98batch/s, Batch Loss=0.1562, Avg Loss=0.3736, Time Left=20.79 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 956/3393 [07:16<20:20,  2.00batch/s, Batch Loss=0.1562, Avg Loss=0.3736, Time Left=20.79 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 956/3393 [07:17<20:20,  2.00batch/s, Batch Loss=0.3044, Avg Loss=0.3735, Time Left=20.78 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 957/3393 [07:17<20:21,  1.99batch/s, Batch Loss=0.3044, Avg Loss=0.3735, Time Left=20.78 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 957/3393 [07:17<20:21,  1.99batch/s, Batch Loss=0.2609, Avg Loss=0.3734, Time Left=20.77 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 958/3393 [07:17<19:59,  2.03batch/s, Batch Loss=0.2609, Avg Loss=0.3734, Time Left=20.77 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 958/3393 [07:18<19:59,  2.03batch/s, Batch Loss=0.1542, Avg Loss=0.3731, Time Left=20.77 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 959/3393 [07:18<20:30,  1.98batch/s, Batch Loss=0.1542, Avg Loss=0.3731, Time Left=20.77 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 959/3393 [07:18<20:30,  1.98batch/s, Batch Loss=0.0568, Avg Loss=0.3728, Time Left=20.76 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 960/3393 [07:18<20:14,  2.00batch/s, Batch Loss=0.0568, Avg Loss=0.3728, Time Left=20.76 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 960/3393 [07:19<20:14,  2.00batch/s, Batch Loss=0.4409, Avg Loss=0.3728, Time Left=20.75 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 961/3393 [07:19<20:08,  2.01batch/s, Batch Loss=0.4409, Avg Loss=0.3728, Time Left=20.75 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 961/3393 [07:19<20:08,  2.01batch/s, Batch Loss=0.2737, Avg Loss=0.3727, Time Left=20.74 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 962/3393 [07:19<19:53,  2.04batch/s, Batch Loss=0.2737, Avg Loss=0.3727, Time Left=20.74 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 962/3393 [07:20<19:53,  2.04batch/s, Batch Loss=0.0426, Avg Loss=0.3723, Time Left=20.73 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 963/3393 [07:20<19:21,  2.09batch/s, Batch Loss=0.0426, Avg Loss=0.3723, Time Left=20.73 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 963/3393 [07:20<19:21,  2.09batch/s, Batch Loss=0.2509, Avg Loss=0.3722, Time Left=20.72 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 964/3393 [07:20<19:33,  2.07batch/s, Batch Loss=0.2509, Avg Loss=0.3722, Time Left=20.72 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 964/3393 [07:21<19:33,  2.07batch/s, Batch Loss=0.3941, Avg Loss=0.3722, Time Left=20.71 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 965/3393 [07:21<19:24,  2.09batch/s, Batch Loss=0.3941, Avg Loss=0.3722, Time Left=20.71 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 965/3393 [07:21<19:24,  2.09batch/s, Batch Loss=0.2874, Avg Loss=0.3721, Time Left=20.70 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 966/3393 [07:21<19:16,  2.10batch/s, Batch Loss=0.2874, Avg Loss=0.3721, Time Left=20.70 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 966/3393 [07:22<19:16,  2.10batch/s, Batch Loss=0.1930, Avg Loss=0.3719, Time Left=20.69 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 967/3393 [07:22<19:23,  2.08batch/s, Batch Loss=0.1930, Avg Loss=0.3719, Time Left=20.69 \u001b[A\n",
      "Epoch 1/3 - Training:  28%|▎| 967/3393 [07:22<19:23,  2.08batch/s, Batch Loss=0.2285, Avg Loss=0.3718, Time Left=20.68 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 968/3393 [07:22<19:17,  2.10batch/s, Batch Loss=0.2285, Avg Loss=0.3718, Time Left=20.68 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 968/3393 [07:23<19:17,  2.10batch/s, Batch Loss=0.6633, Avg Loss=0.3721, Time Left=20.67 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 969/3393 [07:23<19:13,  2.10batch/s, Batch Loss=0.6633, Avg Loss=0.3721, Time Left=20.67 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 969/3393 [07:23<19:13,  2.10batch/s, Batch Loss=0.1419, Avg Loss=0.3718, Time Left=20.67 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 970/3393 [07:23<19:19,  2.09batch/s, Batch Loss=0.1419, Avg Loss=0.3718, Time Left=20.67 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 970/3393 [07:24<19:19,  2.09batch/s, Batch Loss=0.5913, Avg Loss=0.3721, Time Left=20.66 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 971/3393 [07:24<19:36,  2.06batch/s, Batch Loss=0.5913, Avg Loss=0.3721, Time Left=20.66 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 971/3393 [07:24<19:36,  2.06batch/s, Batch Loss=0.2656, Avg Loss=0.3720, Time Left=20.65 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 972/3393 [07:24<19:36,  2.06batch/s, Batch Loss=0.2656, Avg Loss=0.3720, Time Left=20.65 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 972/3393 [07:25<19:36,  2.06batch/s, Batch Loss=0.1166, Avg Loss=0.3717, Time Left=20.64 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 973/3393 [07:25<20:04,  2.01batch/s, Batch Loss=0.1166, Avg Loss=0.3717, Time Left=20.64 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 973/3393 [07:25<20:04,  2.01batch/s, Batch Loss=0.2333, Avg Loss=0.3715, Time Left=20.63 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 974/3393 [07:25<19:54,  2.03batch/s, Batch Loss=0.2333, Avg Loss=0.3715, Time Left=20.63 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 974/3393 [07:26<19:54,  2.03batch/s, Batch Loss=0.2830, Avg Loss=0.3714, Time Left=20.62 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 975/3393 [07:26<19:31,  2.06batch/s, Batch Loss=0.2830, Avg Loss=0.3714, Time Left=20.62 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 975/3393 [07:26<19:31,  2.06batch/s, Batch Loss=0.2112, Avg Loss=0.3713, Time Left=20.62 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 976/3393 [07:26<19:43,  2.04batch/s, Batch Loss=0.2112, Avg Loss=0.3713, Time Left=20.62 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 976/3393 [07:27<19:43,  2.04batch/s, Batch Loss=0.1035, Avg Loss=0.3710, Time Left=20.61 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 977/3393 [07:27<19:42,  2.04batch/s, Batch Loss=0.1035, Avg Loss=0.3710, Time Left=20.61 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 977/3393 [07:27<19:42,  2.04batch/s, Batch Loss=0.2202, Avg Loss=0.3708, Time Left=20.60 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 978/3393 [07:27<20:01,  2.01batch/s, Batch Loss=0.2202, Avg Loss=0.3708, Time Left=20.60 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 978/3393 [07:28<20:01,  2.01batch/s, Batch Loss=0.0735, Avg Loss=0.3705, Time Left=20.59 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 979/3393 [07:28<19:53,  2.02batch/s, Batch Loss=0.0735, Avg Loss=0.3705, Time Left=20.59 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 979/3393 [07:28<19:53,  2.02batch/s, Batch Loss=0.4181, Avg Loss=0.3705, Time Left=20.58 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 980/3393 [07:28<19:47,  2.03batch/s, Batch Loss=0.4181, Avg Loss=0.3705, Time Left=20.58 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 980/3393 [07:28<19:47,  2.03batch/s, Batch Loss=0.2485, Avg Loss=0.3704, Time Left=20.57 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 981/3393 [07:28<19:38,  2.05batch/s, Batch Loss=0.2485, Avg Loss=0.3704, Time Left=20.57 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 981/3393 [07:29<19:38,  2.05batch/s, Batch Loss=0.1479, Avg Loss=0.3701, Time Left=20.56 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 982/3393 [07:29<19:30,  2.06batch/s, Batch Loss=0.1479, Avg Loss=0.3701, Time Left=20.56 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 982/3393 [07:29<19:30,  2.06batch/s, Batch Loss=0.1402, Avg Loss=0.3699, Time Left=20.56 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 983/3393 [07:29<19:54,  2.02batch/s, Batch Loss=0.1402, Avg Loss=0.3699, Time Left=20.56 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 983/3393 [07:30<19:54,  2.02batch/s, Batch Loss=0.1924, Avg Loss=0.3697, Time Left=20.55 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 984/3393 [07:30<19:46,  2.03batch/s, Batch Loss=0.1924, Avg Loss=0.3697, Time Left=20.55 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 984/3393 [07:30<19:46,  2.03batch/s, Batch Loss=0.1781, Avg Loss=0.3695, Time Left=20.54 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 985/3393 [07:30<19:51,  2.02batch/s, Batch Loss=0.1781, Avg Loss=0.3695, Time Left=20.54 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 985/3393 [07:31<19:51,  2.02batch/s, Batch Loss=0.2332, Avg Loss=0.3693, Time Left=20.53 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 986/3393 [07:31<19:47,  2.03batch/s, Batch Loss=0.2332, Avg Loss=0.3693, Time Left=20.53 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 986/3393 [07:31<19:47,  2.03batch/s, Batch Loss=0.2329, Avg Loss=0.3692, Time Left=20.52 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 987/3393 [07:31<19:32,  2.05batch/s, Batch Loss=0.2329, Avg Loss=0.3692, Time Left=20.52 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 987/3393 [07:32<19:32,  2.05batch/s, Batch Loss=0.7142, Avg Loss=0.3696, Time Left=20.51 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  29%|▎| 988/3393 [07:32<19:50,  2.02batch/s, Batch Loss=0.7142, Avg Loss=0.3696, Time Left=20.51 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 988/3393 [07:32<19:50,  2.02batch/s, Batch Loss=0.2512, Avg Loss=0.3694, Time Left=20.50 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 989/3393 [07:32<19:43,  2.03batch/s, Batch Loss=0.2512, Avg Loss=0.3694, Time Left=20.50 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 989/3393 [07:33<19:43,  2.03batch/s, Batch Loss=0.2756, Avg Loss=0.3693, Time Left=20.50 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 990/3393 [07:33<19:38,  2.04batch/s, Batch Loss=0.2756, Avg Loss=0.3693, Time Left=20.50 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 990/3393 [07:33<19:38,  2.04batch/s, Batch Loss=0.1749, Avg Loss=0.3691, Time Left=20.49 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 991/3393 [07:33<19:34,  2.04batch/s, Batch Loss=0.1749, Avg Loss=0.3691, Time Left=20.49 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 991/3393 [07:34<19:34,  2.04batch/s, Batch Loss=0.2411, Avg Loss=0.3690, Time Left=20.48 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 992/3393 [07:34<19:32,  2.05batch/s, Batch Loss=0.2411, Avg Loss=0.3690, Time Left=20.48 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 992/3393 [07:34<19:32,  2.05batch/s, Batch Loss=0.1694, Avg Loss=0.3688, Time Left=20.47 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 993/3393 [07:34<19:53,  2.01batch/s, Batch Loss=0.1694, Avg Loss=0.3688, Time Left=20.47 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 993/3393 [07:35<19:53,  2.01batch/s, Batch Loss=0.2379, Avg Loss=0.3686, Time Left=20.46 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 994/3393 [07:35<19:45,  2.02batch/s, Batch Loss=0.2379, Avg Loss=0.3686, Time Left=20.46 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 994/3393 [07:35<19:45,  2.02batch/s, Batch Loss=0.1018, Avg Loss=0.3683, Time Left=20.45 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 995/3393 [07:35<19:28,  2.05batch/s, Batch Loss=0.1018, Avg Loss=0.3683, Time Left=20.45 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 995/3393 [07:36<19:28,  2.05batch/s, Batch Loss=0.3363, Avg Loss=0.3683, Time Left=20.45 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 996/3393 [07:36<19:39,  2.03batch/s, Batch Loss=0.3363, Avg Loss=0.3683, Time Left=20.45 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 996/3393 [07:36<19:39,  2.03batch/s, Batch Loss=0.3943, Avg Loss=0.3683, Time Left=20.44 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 997/3393 [07:36<19:34,  2.04batch/s, Batch Loss=0.3943, Avg Loss=0.3683, Time Left=20.44 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 997/3393 [07:37<19:34,  2.04batch/s, Batch Loss=0.3321, Avg Loss=0.3683, Time Left=20.43 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 998/3393 [07:37<19:43,  2.02batch/s, Batch Loss=0.3321, Avg Loss=0.3683, Time Left=20.43 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 998/3393 [07:37<19:43,  2.02batch/s, Batch Loss=0.1322, Avg Loss=0.3680, Time Left=20.42 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 999/3393 [07:37<19:25,  2.05batch/s, Batch Loss=0.1322, Avg Loss=0.3680, Time Left=20.42 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 999/3393 [07:38<19:25,  2.05batch/s, Batch Loss=0.0654, Avg Loss=0.3677, Time Left=20.41 \u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 1000/3393 [07:38<19:17,  2.07batch/s, Batch Loss=0.0654, Avg Loss=0.3677, Time Left=20.41\u001b[A\n",
      "Epoch 1/3 - Training:  29%|▎| 1000/3393 [07:38<19:17,  2.07batch/s, Batch Loss=0.1063, Avg Loss=0.3674, Time Left=20.40\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1001/3393 [07:38<19:44,  2.02batch/s, Batch Loss=0.1063, Avg Loss=0.3674, Time Left=20.40\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1001/3393 [07:39<19:44,  2.02batch/s, Batch Loss=0.0786, Avg Loss=0.3671, Time Left=20.39\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1002/3393 [07:39<19:31,  2.04batch/s, Batch Loss=0.0786, Avg Loss=0.3671, Time Left=20.39\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1002/3393 [07:39<19:31,  2.04batch/s, Batch Loss=0.0699, Avg Loss=0.3668, Time Left=20.39\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1003/3393 [07:39<19:49,  2.01batch/s, Batch Loss=0.0699, Avg Loss=0.3668, Time Left=20.39\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1003/3393 [07:40<19:49,  2.01batch/s, Batch Loss=0.1788, Avg Loss=0.3666, Time Left=20.38\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1004/3393 [07:40<19:40,  2.02batch/s, Batch Loss=0.1788, Avg Loss=0.3666, Time Left=20.38\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1004/3393 [07:40<19:40,  2.02batch/s, Batch Loss=0.0615, Avg Loss=0.3662, Time Left=20.37\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1005/3393 [07:40<19:12,  2.07batch/s, Batch Loss=0.0615, Avg Loss=0.3662, Time Left=20.37\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1005/3393 [07:41<19:12,  2.07batch/s, Batch Loss=0.4184, Avg Loss=0.3663, Time Left=20.36\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1006/3393 [07:41<19:36,  2.03batch/s, Batch Loss=0.4184, Avg Loss=0.3663, Time Left=20.36\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1006/3393 [07:41<19:36,  2.03batch/s, Batch Loss=0.4812, Avg Loss=0.3664, Time Left=20.35\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1007/3393 [07:41<19:20,  2.06batch/s, Batch Loss=0.4812, Avg Loss=0.3664, Time Left=20.35\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1007/3393 [07:42<19:20,  2.06batch/s, Batch Loss=0.2642, Avg Loss=0.3663, Time Left=20.34\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1008/3393 [07:42<19:42,  2.02batch/s, Batch Loss=0.2642, Avg Loss=0.3663, Time Left=20.34\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1008/3393 [07:42<19:42,  2.02batch/s, Batch Loss=0.2711, Avg Loss=0.3662, Time Left=20.34\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1009/3393 [07:42<19:36,  2.03batch/s, Batch Loss=0.2711, Avg Loss=0.3662, Time Left=20.34\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1009/3393 [07:43<19:36,  2.03batch/s, Batch Loss=0.0550, Avg Loss=0.3659, Time Left=20.33\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1010/3393 [07:43<19:19,  2.06batch/s, Batch Loss=0.0550, Avg Loss=0.3659, Time Left=20.33\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1010/3393 [07:43<19:19,  2.06batch/s, Batch Loss=0.1964, Avg Loss=0.3657, Time Left=20.32\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1011/3393 [07:43<19:19,  2.05batch/s, Batch Loss=0.1964, Avg Loss=0.3657, Time Left=20.32\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1011/3393 [07:44<19:19,  2.05batch/s, Batch Loss=0.1807, Avg Loss=0.3655, Time Left=20.31\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1012/3393 [07:44<19:06,  2.08batch/s, Batch Loss=0.1807, Avg Loss=0.3655, Time Left=20.31\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1012/3393 [07:44<19:06,  2.08batch/s, Batch Loss=0.3928, Avg Loss=0.3655, Time Left=20.30\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1013/3393 [07:44<19:32,  2.03batch/s, Batch Loss=0.3928, Avg Loss=0.3655, Time Left=20.30\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1013/3393 [07:45<19:32,  2.03batch/s, Batch Loss=0.1662, Avg Loss=0.3653, Time Left=20.29\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1014/3393 [07:45<19:30,  2.03batch/s, Batch Loss=0.1662, Avg Loss=0.3653, Time Left=20.29\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1014/3393 [07:45<19:30,  2.03batch/s, Batch Loss=0.3524, Avg Loss=0.3653, Time Left=20.28\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1015/3393 [07:45<19:11,  2.07batch/s, Batch Loss=0.3524, Avg Loss=0.3653, Time Left=20.28\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1015/3393 [07:46<19:11,  2.07batch/s, Batch Loss=0.6743, Avg Loss=0.3656, Time Left=20.28\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1016/3393 [07:46<19:44,  2.01batch/s, Batch Loss=0.6743, Avg Loss=0.3656, Time Left=20.28\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1016/3393 [07:46<19:44,  2.01batch/s, Batch Loss=0.1384, Avg Loss=0.3654, Time Left=20.27\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1017/3393 [07:46<19:36,  2.02batch/s, Batch Loss=0.1384, Avg Loss=0.3654, Time Left=20.27\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1017/3393 [07:47<19:36,  2.02batch/s, Batch Loss=0.3610, Avg Loss=0.3654, Time Left=20.26\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1018/3393 [07:47<19:46,  2.00batch/s, Batch Loss=0.3610, Avg Loss=0.3654, Time Left=20.26\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1018/3393 [07:47<19:46,  2.00batch/s, Batch Loss=0.4629, Avg Loss=0.3655, Time Left=20.25\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1019/3393 [07:47<19:53,  1.99batch/s, Batch Loss=0.4629, Avg Loss=0.3655, Time Left=20.25\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1019/3393 [07:48<19:53,  1.99batch/s, Batch Loss=0.3667, Avg Loss=0.3655, Time Left=20.25\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1020/3393 [07:48<20:03,  1.97batch/s, Batch Loss=0.3667, Avg Loss=0.3655, Time Left=20.25\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1020/3393 [07:48<20:03,  1.97batch/s, Batch Loss=0.1031, Avg Loss=0.3652, Time Left=20.24\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  30%|▎| 1021/3393 [07:48<19:48,  2.00batch/s, Batch Loss=0.1031, Avg Loss=0.3652, Time Left=20.24\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1021/3393 [07:49<19:48,  2.00batch/s, Batch Loss=0.2453, Avg Loss=0.3651, Time Left=20.23\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1022/3393 [07:49<20:00,  1.98batch/s, Batch Loss=0.2453, Avg Loss=0.3651, Time Left=20.23\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1022/3393 [07:49<20:00,  1.98batch/s, Batch Loss=0.0945, Avg Loss=0.3648, Time Left=20.22\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1023/3393 [07:49<19:57,  1.98batch/s, Batch Loss=0.0945, Avg Loss=0.3648, Time Left=20.22\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1023/3393 [07:50<19:57,  1.98batch/s, Batch Loss=0.2206, Avg Loss=0.3646, Time Left=20.21\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1024/3393 [07:50<19:32,  2.02batch/s, Batch Loss=0.2206, Avg Loss=0.3646, Time Left=20.21\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1024/3393 [07:50<19:32,  2.02batch/s, Batch Loss=0.3663, Avg Loss=0.3646, Time Left=20.20\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1025/3393 [07:50<19:25,  2.03batch/s, Batch Loss=0.3663, Avg Loss=0.3646, Time Left=20.20\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1025/3393 [07:51<19:25,  2.03batch/s, Batch Loss=0.2123, Avg Loss=0.3645, Time Left=20.19\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1026/3393 [07:51<18:59,  2.08batch/s, Batch Loss=0.2123, Avg Loss=0.3645, Time Left=20.19\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1026/3393 [07:51<18:59,  2.08batch/s, Batch Loss=0.1524, Avg Loss=0.3643, Time Left=20.18\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1027/3393 [07:51<18:50,  2.09batch/s, Batch Loss=0.1524, Avg Loss=0.3643, Time Left=20.18\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1027/3393 [07:52<18:50,  2.09batch/s, Batch Loss=0.2093, Avg Loss=0.3641, Time Left=20.18\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1028/3393 [07:52<19:30,  2.02batch/s, Batch Loss=0.2093, Avg Loss=0.3641, Time Left=20.18\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1028/3393 [07:52<19:30,  2.02batch/s, Batch Loss=0.1146, Avg Loss=0.3638, Time Left=20.17\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1029/3393 [07:52<19:16,  2.04batch/s, Batch Loss=0.1146, Avg Loss=0.3638, Time Left=20.17\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1029/3393 [07:53<19:16,  2.04batch/s, Batch Loss=0.2148, Avg Loss=0.3637, Time Left=20.16\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1030/3393 [07:53<19:31,  2.02batch/s, Batch Loss=0.2148, Avg Loss=0.3637, Time Left=20.16\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1030/3393 [07:53<19:31,  2.02batch/s, Batch Loss=0.1349, Avg Loss=0.3634, Time Left=20.15\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1031/3393 [07:53<19:24,  2.03batch/s, Batch Loss=0.1349, Avg Loss=0.3634, Time Left=20.15\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1031/3393 [07:54<19:24,  2.03batch/s, Batch Loss=0.1974, Avg Loss=0.3633, Time Left=20.14\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1032/3393 [07:54<19:30,  2.02batch/s, Batch Loss=0.1974, Avg Loss=0.3633, Time Left=20.14\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1032/3393 [07:54<19:30,  2.02batch/s, Batch Loss=0.0608, Avg Loss=0.3630, Time Left=20.14\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1033/3393 [07:54<19:21,  2.03batch/s, Batch Loss=0.0608, Avg Loss=0.3630, Time Left=20.14\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1033/3393 [07:55<19:21,  2.03batch/s, Batch Loss=0.2404, Avg Loss=0.3628, Time Left=20.13\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1034/3393 [07:55<19:17,  2.04batch/s, Batch Loss=0.2404, Avg Loss=0.3628, Time Left=20.13\u001b[A\n",
      "Epoch 1/3 - Training:  30%|▎| 1034/3393 [07:55<19:17,  2.04batch/s, Batch Loss=0.1392, Avg Loss=0.3626, Time Left=20.12\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1035/3393 [07:55<19:54,  1.97batch/s, Batch Loss=0.1392, Avg Loss=0.3626, Time Left=20.12\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1035/3393 [07:56<19:54,  1.97batch/s, Batch Loss=0.3421, Avg Loss=0.3626, Time Left=20.11\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1036/3393 [07:56<19:34,  2.01batch/s, Batch Loss=0.3421, Avg Loss=0.3626, Time Left=20.11\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1036/3393 [07:56<19:34,  2.01batch/s, Batch Loss=0.2047, Avg Loss=0.3624, Time Left=20.10\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1037/3393 [07:56<19:59,  1.96batch/s, Batch Loss=0.2047, Avg Loss=0.3624, Time Left=20.10\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1037/3393 [07:57<19:59,  1.96batch/s, Batch Loss=0.5174, Avg Loss=0.3626, Time Left=20.10\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1038/3393 [07:57<19:33,  2.01batch/s, Batch Loss=0.5174, Avg Loss=0.3626, Time Left=20.10\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1038/3393 [07:57<19:33,  2.01batch/s, Batch Loss=0.2704, Avg Loss=0.3625, Time Left=20.09\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1039/3393 [07:57<19:43,  1.99batch/s, Batch Loss=0.2704, Avg Loss=0.3625, Time Left=20.09\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1039/3393 [07:58<19:43,  1.99batch/s, Batch Loss=0.3370, Avg Loss=0.3624, Time Left=20.08\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1040/3393 [07:58<19:38,  2.00batch/s, Batch Loss=0.3370, Avg Loss=0.3624, Time Left=20.08\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1040/3393 [07:58<19:38,  2.00batch/s, Batch Loss=0.0954, Avg Loss=0.3622, Time Left=20.07\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1041/3393 [07:58<19:54,  1.97batch/s, Batch Loss=0.0954, Avg Loss=0.3622, Time Left=20.07\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1041/3393 [07:59<19:54,  1.97batch/s, Batch Loss=0.1042, Avg Loss=0.3619, Time Left=20.06\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1042/3393 [07:59<19:39,  1.99batch/s, Batch Loss=0.1042, Avg Loss=0.3619, Time Left=20.06\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1042/3393 [07:59<19:39,  1.99batch/s, Batch Loss=0.4085, Avg Loss=0.3620, Time Left=20.05\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1043/3393 [07:59<19:17,  2.03batch/s, Batch Loss=0.4085, Avg Loss=0.3620, Time Left=20.05\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1043/3393 [08:00<19:17,  2.03batch/s, Batch Loss=0.4130, Avg Loss=0.3620, Time Left=20.05\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1044/3393 [08:00<19:36,  2.00batch/s, Batch Loss=0.4130, Avg Loss=0.3620, Time Left=20.05\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1044/3393 [08:00<19:36,  2.00batch/s, Batch Loss=0.3741, Avg Loss=0.3620, Time Left=20.04\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1045/3393 [08:00<19:23,  2.02batch/s, Batch Loss=0.3741, Avg Loss=0.3620, Time Left=20.04\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1045/3393 [08:01<19:23,  2.02batch/s, Batch Loss=0.2489, Avg Loss=0.3619, Time Left=20.03\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1046/3393 [08:01<19:24,  2.02batch/s, Batch Loss=0.2489, Avg Loss=0.3619, Time Left=20.03\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1046/3393 [08:01<19:24,  2.02batch/s, Batch Loss=0.3521, Avg Loss=0.3619, Time Left=20.02\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1047/3393 [08:01<19:00,  2.06batch/s, Batch Loss=0.3521, Avg Loss=0.3619, Time Left=20.02\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1047/3393 [08:02<19:00,  2.06batch/s, Batch Loss=0.0622, Avg Loss=0.3616, Time Left=20.01\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1048/3393 [08:02<18:46,  2.08batch/s, Batch Loss=0.0622, Avg Loss=0.3616, Time Left=20.01\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1048/3393 [08:02<18:46,  2.08batch/s, Batch Loss=0.1519, Avg Loss=0.3614, Time Left=20.00\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1049/3393 [08:02<19:13,  2.03batch/s, Batch Loss=0.1519, Avg Loss=0.3614, Time Left=20.00\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1049/3393 [08:03<19:13,  2.03batch/s, Batch Loss=0.0457, Avg Loss=0.3610, Time Left=20.00\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1050/3393 [08:03<19:08,  2.04batch/s, Batch Loss=0.0457, Avg Loss=0.3610, Time Left=20.00\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1050/3393 [08:03<19:08,  2.04batch/s, Batch Loss=0.5883, Avg Loss=0.3613, Time Left=19.99\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1051/3393 [08:03<19:36,  1.99batch/s, Batch Loss=0.5883, Avg Loss=0.3613, Time Left=19.99\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1051/3393 [08:04<19:36,  1.99batch/s, Batch Loss=0.2479, Avg Loss=0.3612, Time Left=19.98\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1052/3393 [08:04<19:26,  2.01batch/s, Batch Loss=0.2479, Avg Loss=0.3612, Time Left=19.98\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1052/3393 [08:04<19:26,  2.01batch/s, Batch Loss=0.2670, Avg Loss=0.3611, Time Left=19.97\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1053/3393 [08:04<19:38,  1.99batch/s, Batch Loss=0.2670, Avg Loss=0.3611, Time Left=19.97\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1053/3393 [08:05<19:38,  1.99batch/s, Batch Loss=0.2091, Avg Loss=0.3609, Time Left=19.96\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  31%|▎| 1054/3393 [08:05<19:16,  2.02batch/s, Batch Loss=0.2091, Avg Loss=0.3609, Time Left=19.96\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1054/3393 [08:05<19:16,  2.02batch/s, Batch Loss=0.1274, Avg Loss=0.3607, Time Left=19.95\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1055/3393 [08:05<18:58,  2.05batch/s, Batch Loss=0.1274, Avg Loss=0.3607, Time Left=19.95\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1055/3393 [08:06<18:58,  2.05batch/s, Batch Loss=0.0621, Avg Loss=0.3604, Time Left=19.95\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1056/3393 [08:06<19:09,  2.03batch/s, Batch Loss=0.0621, Avg Loss=0.3604, Time Left=19.95\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1056/3393 [08:06<19:09,  2.03batch/s, Batch Loss=0.2647, Avg Loss=0.3603, Time Left=19.94\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1057/3393 [08:06<18:54,  2.06batch/s, Batch Loss=0.2647, Avg Loss=0.3603, Time Left=19.94\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1057/3393 [08:06<18:54,  2.06batch/s, Batch Loss=0.3620, Avg Loss=0.3603, Time Left=19.93\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1058/3393 [08:06<18:53,  2.06batch/s, Batch Loss=0.3620, Avg Loss=0.3603, Time Left=19.93\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1058/3393 [08:07<18:53,  2.06batch/s, Batch Loss=0.3437, Avg Loss=0.3603, Time Left=19.92\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1059/3393 [08:07<19:05,  2.04batch/s, Batch Loss=0.3437, Avg Loss=0.3603, Time Left=19.92\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1059/3393 [08:07<19:05,  2.04batch/s, Batch Loss=0.1945, Avg Loss=0.3601, Time Left=19.91\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1060/3393 [08:07<18:52,  2.06batch/s, Batch Loss=0.1945, Avg Loss=0.3601, Time Left=19.91\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1060/3393 [08:08<18:52,  2.06batch/s, Batch Loss=0.2568, Avg Loss=0.3600, Time Left=19.90\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1061/3393 [08:08<19:01,  2.04batch/s, Batch Loss=0.2568, Avg Loss=0.3600, Time Left=19.90\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1061/3393 [08:08<19:01,  2.04batch/s, Batch Loss=0.5785, Avg Loss=0.3602, Time Left=19.89\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1062/3393 [08:08<18:57,  2.05batch/s, Batch Loss=0.5785, Avg Loss=0.3602, Time Left=19.89\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1062/3393 [08:09<18:57,  2.05batch/s, Batch Loss=0.3458, Avg Loss=0.3602, Time Left=19.88\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1063/3393 [08:09<18:35,  2.09batch/s, Batch Loss=0.3458, Avg Loss=0.3602, Time Left=19.88\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1063/3393 [08:09<18:35,  2.09batch/s, Batch Loss=0.2176, Avg Loss=0.3600, Time Left=19.88\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1064/3393 [08:09<19:12,  2.02batch/s, Batch Loss=0.2176, Avg Loss=0.3600, Time Left=19.88\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1064/3393 [08:10<19:12,  2.02batch/s, Batch Loss=0.2344, Avg Loss=0.3599, Time Left=19.87\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1065/3393 [08:10<18:55,  2.05batch/s, Batch Loss=0.2344, Avg Loss=0.3599, Time Left=19.87\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1065/3393 [08:10<18:55,  2.05batch/s, Batch Loss=0.3113, Avg Loss=0.3599, Time Left=19.86\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1066/3393 [08:10<18:54,  2.05batch/s, Batch Loss=0.3113, Avg Loss=0.3599, Time Left=19.86\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1066/3393 [08:11<18:54,  2.05batch/s, Batch Loss=0.4507, Avg Loss=0.3600, Time Left=19.85\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1067/3393 [08:11<18:53,  2.05batch/s, Batch Loss=0.4507, Avg Loss=0.3600, Time Left=19.85\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1067/3393 [08:11<18:53,  2.05batch/s, Batch Loss=0.2466, Avg Loss=0.3598, Time Left=19.84\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1068/3393 [08:11<18:53,  2.05batch/s, Batch Loss=0.2466, Avg Loss=0.3598, Time Left=19.84\u001b[A\n",
      "Epoch 1/3 - Training:  31%|▎| 1068/3393 [08:12<18:53,  2.05batch/s, Batch Loss=0.1215, Avg Loss=0.3596, Time Left=19.83\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1069/3393 [08:12<19:14,  2.01batch/s, Batch Loss=0.1215, Avg Loss=0.3596, Time Left=19.83\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1069/3393 [08:12<19:14,  2.01batch/s, Batch Loss=0.4914, Avg Loss=0.3597, Time Left=19.83\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1070/3393 [08:12<19:03,  2.03batch/s, Batch Loss=0.4914, Avg Loss=0.3597, Time Left=19.83\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1070/3393 [08:13<19:03,  2.03batch/s, Batch Loss=0.4911, Avg Loss=0.3599, Time Left=19.82\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1071/3393 [08:13<18:51,  2.05batch/s, Batch Loss=0.4911, Avg Loss=0.3599, Time Left=19.82\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1071/3393 [08:13<18:51,  2.05batch/s, Batch Loss=0.2352, Avg Loss=0.3597, Time Left=19.81\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1072/3393 [08:13<18:52,  2.05batch/s, Batch Loss=0.2352, Avg Loss=0.3597, Time Left=19.81\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1072/3393 [08:14<18:52,  2.05batch/s, Batch Loss=0.1404, Avg Loss=0.3595, Time Left=19.80\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1073/3393 [08:14<18:26,  2.10batch/s, Batch Loss=0.1404, Avg Loss=0.3595, Time Left=19.80\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1073/3393 [08:14<18:26,  2.10batch/s, Batch Loss=0.1513, Avg Loss=0.3593, Time Left=19.79\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1074/3393 [08:14<18:30,  2.09batch/s, Batch Loss=0.1513, Avg Loss=0.3593, Time Left=19.79\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1074/3393 [08:15<18:30,  2.09batch/s, Batch Loss=0.1478, Avg Loss=0.3591, Time Left=19.78\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1075/3393 [08:15<18:38,  2.07batch/s, Batch Loss=0.1478, Avg Loss=0.3591, Time Left=19.78\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1075/3393 [08:15<18:38,  2.07batch/s, Batch Loss=0.0883, Avg Loss=0.3588, Time Left=19.77\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1076/3393 [08:15<18:27,  2.09batch/s, Batch Loss=0.0883, Avg Loss=0.3588, Time Left=19.77\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1076/3393 [08:16<18:27,  2.09batch/s, Batch Loss=0.2427, Avg Loss=0.3587, Time Left=19.76\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1077/3393 [08:16<18:33,  2.08batch/s, Batch Loss=0.2427, Avg Loss=0.3587, Time Left=19.76\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1077/3393 [08:16<18:33,  2.08batch/s, Batch Loss=0.0837, Avg Loss=0.3584, Time Left=19.75\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1078/3393 [08:16<18:36,  2.07batch/s, Batch Loss=0.0837, Avg Loss=0.3584, Time Left=19.75\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1078/3393 [08:17<18:36,  2.07batch/s, Batch Loss=0.4807, Avg Loss=0.3586, Time Left=19.75\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1079/3393 [08:17<18:27,  2.09batch/s, Batch Loss=0.4807, Avg Loss=0.3586, Time Left=19.75\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1079/3393 [08:17<18:27,  2.09batch/s, Batch Loss=0.0483, Avg Loss=0.3583, Time Left=19.74\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1080/3393 [08:17<18:55,  2.04batch/s, Batch Loss=0.0483, Avg Loss=0.3583, Time Left=19.74\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1080/3393 [08:18<18:55,  2.04batch/s, Batch Loss=0.1343, Avg Loss=0.3580, Time Left=19.73\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1081/3393 [08:18<18:40,  2.06batch/s, Batch Loss=0.1343, Avg Loss=0.3580, Time Left=19.73\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1081/3393 [08:18<18:40,  2.06batch/s, Batch Loss=0.4404, Avg Loss=0.3581, Time Left=19.72\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1082/3393 [08:18<18:41,  2.06batch/s, Batch Loss=0.4404, Avg Loss=0.3581, Time Left=19.72\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1082/3393 [08:19<18:41,  2.06batch/s, Batch Loss=0.2654, Avg Loss=0.3580, Time Left=19.71\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1083/3393 [08:19<19:03,  2.02batch/s, Batch Loss=0.2654, Avg Loss=0.3580, Time Left=19.71\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1083/3393 [08:19<19:03,  2.02batch/s, Batch Loss=0.1699, Avg Loss=0.3578, Time Left=19.70\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1084/3393 [08:19<19:03,  2.02batch/s, Batch Loss=0.1699, Avg Loss=0.3578, Time Left=19.70\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1084/3393 [08:20<19:03,  2.02batch/s, Batch Loss=0.3826, Avg Loss=0.3579, Time Left=19.70\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1085/3393 [08:20<19:01,  2.02batch/s, Batch Loss=0.3826, Avg Loss=0.3579, Time Left=19.70\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1085/3393 [08:20<19:01,  2.02batch/s, Batch Loss=0.3938, Avg Loss=0.3579, Time Left=19.69\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1086/3393 [08:20<18:56,  2.03batch/s, Batch Loss=0.3938, Avg Loss=0.3579, Time Left=19.69\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1086/3393 [08:21<18:56,  2.03batch/s, Batch Loss=0.1798, Avg Loss=0.3577, Time Left=19.68\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  32%|▎| 1087/3393 [08:21<19:10,  2.00batch/s, Batch Loss=0.1798, Avg Loss=0.3577, Time Left=19.68\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1087/3393 [08:21<19:10,  2.00batch/s, Batch Loss=0.1057, Avg Loss=0.3575, Time Left=19.67\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1088/3393 [08:21<18:51,  2.04batch/s, Batch Loss=0.1057, Avg Loss=0.3575, Time Left=19.67\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1088/3393 [08:22<18:51,  2.04batch/s, Batch Loss=0.1582, Avg Loss=0.3573, Time Left=19.66\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1089/3393 [08:22<18:48,  2.04batch/s, Batch Loss=0.1582, Avg Loss=0.3573, Time Left=19.66\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1089/3393 [08:22<18:48,  2.04batch/s, Batch Loss=0.3487, Avg Loss=0.3573, Time Left=19.65\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1090/3393 [08:22<18:56,  2.03batch/s, Batch Loss=0.3487, Avg Loss=0.3573, Time Left=19.65\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1090/3393 [08:23<18:56,  2.03batch/s, Batch Loss=0.5097, Avg Loss=0.3574, Time Left=19.64\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1091/3393 [08:23<18:40,  2.05batch/s, Batch Loss=0.5097, Avg Loss=0.3574, Time Left=19.64\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1091/3393 [08:23<18:40,  2.05batch/s, Batch Loss=0.2828, Avg Loss=0.3573, Time Left=19.64\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1092/3393 [08:23<18:50,  2.03batch/s, Batch Loss=0.2828, Avg Loss=0.3573, Time Left=19.64\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1092/3393 [08:24<18:50,  2.03batch/s, Batch Loss=0.1880, Avg Loss=0.3572, Time Left=19.63\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1093/3393 [08:24<18:36,  2.06batch/s, Batch Loss=0.1880, Avg Loss=0.3572, Time Left=19.63\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1093/3393 [08:24<18:36,  2.06batch/s, Batch Loss=0.3600, Avg Loss=0.3572, Time Left=19.62\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1094/3393 [08:24<18:36,  2.06batch/s, Batch Loss=0.3600, Avg Loss=0.3572, Time Left=19.62\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1094/3393 [08:25<18:36,  2.06batch/s, Batch Loss=0.2363, Avg Loss=0.3571, Time Left=19.61\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1095/3393 [08:25<18:57,  2.02batch/s, Batch Loss=0.2363, Avg Loss=0.3571, Time Left=19.61\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1095/3393 [08:25<18:57,  2.02batch/s, Batch Loss=0.3340, Avg Loss=0.3570, Time Left=19.60\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1096/3393 [08:25<18:43,  2.05batch/s, Batch Loss=0.3340, Avg Loss=0.3570, Time Left=19.60\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1096/3393 [08:26<18:43,  2.05batch/s, Batch Loss=0.1492, Avg Loss=0.3568, Time Left=19.59\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1097/3393 [08:26<18:34,  2.06batch/s, Batch Loss=0.1492, Avg Loss=0.3568, Time Left=19.59\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1097/3393 [08:26<18:34,  2.06batch/s, Batch Loss=0.2112, Avg Loss=0.3567, Time Left=19.59\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1098/3393 [08:26<18:39,  2.05batch/s, Batch Loss=0.2112, Avg Loss=0.3567, Time Left=19.59\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1098/3393 [08:27<18:39,  2.05batch/s, Batch Loss=0.1570, Avg Loss=0.3565, Time Left=19.58\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1099/3393 [08:27<18:37,  2.05batch/s, Batch Loss=0.1570, Avg Loss=0.3565, Time Left=19.58\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1099/3393 [08:27<18:37,  2.05batch/s, Batch Loss=0.1032, Avg Loss=0.3562, Time Left=19.57\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1100/3393 [08:27<18:24,  2.08batch/s, Batch Loss=0.1032, Avg Loss=0.3562, Time Left=19.57\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1100/3393 [08:27<18:24,  2.08batch/s, Batch Loss=0.0846, Avg Loss=0.3560, Time Left=19.56\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1101/3393 [08:27<18:38,  2.05batch/s, Batch Loss=0.0846, Avg Loss=0.3560, Time Left=19.56\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1101/3393 [08:28<18:38,  2.05batch/s, Batch Loss=0.6878, Avg Loss=0.3563, Time Left=19.55\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1102/3393 [08:28<18:26,  2.07batch/s, Batch Loss=0.6878, Avg Loss=0.3563, Time Left=19.55\u001b[A\n",
      "Epoch 1/3 - Training:  32%|▎| 1102/3393 [08:28<18:26,  2.07batch/s, Batch Loss=0.1080, Avg Loss=0.3561, Time Left=19.54\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1103/3393 [08:28<18:49,  2.03batch/s, Batch Loss=0.1080, Avg Loss=0.3561, Time Left=19.54\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1103/3393 [08:29<18:49,  2.03batch/s, Batch Loss=0.3675, Avg Loss=0.3561, Time Left=19.53\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1104/3393 [08:29<18:55,  2.02batch/s, Batch Loss=0.3675, Avg Loss=0.3561, Time Left=19.53\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1104/3393 [08:29<18:55,  2.02batch/s, Batch Loss=0.0732, Avg Loss=0.3558, Time Left=19.53\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1105/3393 [08:29<18:37,  2.05batch/s, Batch Loss=0.0732, Avg Loss=0.3558, Time Left=19.53\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1105/3393 [08:30<18:37,  2.05batch/s, Batch Loss=0.2251, Avg Loss=0.3557, Time Left=19.52\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1106/3393 [08:30<18:36,  2.05batch/s, Batch Loss=0.2251, Avg Loss=0.3557, Time Left=19.52\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1106/3393 [08:30<18:36,  2.05batch/s, Batch Loss=0.2681, Avg Loss=0.3556, Time Left=19.51\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1107/3393 [08:30<18:46,  2.03batch/s, Batch Loss=0.2681, Avg Loss=0.3556, Time Left=19.51\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1107/3393 [08:31<18:46,  2.03batch/s, Batch Loss=0.1925, Avg Loss=0.3554, Time Left=19.50\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1108/3393 [08:31<19:01,  2.00batch/s, Batch Loss=0.1925, Avg Loss=0.3554, Time Left=19.50\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1108/3393 [08:31<19:01,  2.00batch/s, Batch Loss=0.1930, Avg Loss=0.3553, Time Left=19.49\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1109/3393 [08:31<18:42,  2.04batch/s, Batch Loss=0.1930, Avg Loss=0.3553, Time Left=19.49\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1109/3393 [08:32<18:42,  2.04batch/s, Batch Loss=0.2054, Avg Loss=0.3551, Time Left=19.48\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1110/3393 [08:32<18:34,  2.05batch/s, Batch Loss=0.2054, Avg Loss=0.3551, Time Left=19.48\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1110/3393 [08:32<18:34,  2.05batch/s, Batch Loss=0.1208, Avg Loss=0.3549, Time Left=19.47\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1111/3393 [08:32<18:45,  2.03batch/s, Batch Loss=0.1208, Avg Loss=0.3549, Time Left=19.47\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1111/3393 [08:33<18:45,  2.03batch/s, Batch Loss=0.1981, Avg Loss=0.3548, Time Left=19.47\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1112/3393 [08:33<18:31,  2.05batch/s, Batch Loss=0.1981, Avg Loss=0.3548, Time Left=19.47\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1112/3393 [08:33<18:31,  2.05batch/s, Batch Loss=0.0862, Avg Loss=0.3545, Time Left=19.46\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1113/3393 [08:33<18:50,  2.02batch/s, Batch Loss=0.0862, Avg Loss=0.3545, Time Left=19.46\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1113/3393 [08:34<18:50,  2.02batch/s, Batch Loss=0.1531, Avg Loss=0.3543, Time Left=19.45\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1114/3393 [08:34<18:54,  2.01batch/s, Batch Loss=0.1531, Avg Loss=0.3543, Time Left=19.45\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1114/3393 [08:34<18:54,  2.01batch/s, Batch Loss=0.2452, Avg Loss=0.3542, Time Left=19.44\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1115/3393 [08:34<18:35,  2.04batch/s, Batch Loss=0.2452, Avg Loss=0.3542, Time Left=19.44\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1115/3393 [08:35<18:35,  2.04batch/s, Batch Loss=0.3095, Avg Loss=0.3541, Time Left=19.43\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1116/3393 [08:35<18:34,  2.04batch/s, Batch Loss=0.3095, Avg Loss=0.3541, Time Left=19.43\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1116/3393 [08:35<18:34,  2.04batch/s, Batch Loss=0.3752, Avg Loss=0.3542, Time Left=19.42\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1117/3393 [08:35<18:30,  2.05batch/s, Batch Loss=0.3752, Avg Loss=0.3542, Time Left=19.42\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1117/3393 [08:36<18:30,  2.05batch/s, Batch Loss=0.2948, Avg Loss=0.3541, Time Left=19.42\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1118/3393 [08:36<18:50,  2.01batch/s, Batch Loss=0.2948, Avg Loss=0.3541, Time Left=19.42\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1118/3393 [08:36<18:50,  2.01batch/s, Batch Loss=0.1209, Avg Loss=0.3539, Time Left=19.41\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1119/3393 [08:36<18:43,  2.02batch/s, Batch Loss=0.1209, Avg Loss=0.3539, Time Left=19.41\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1119/3393 [08:37<18:43,  2.02batch/s, Batch Loss=0.2666, Avg Loss=0.3538, Time Left=19.40\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  33%|▎| 1120/3393 [08:37<18:48,  2.01batch/s, Batch Loss=0.2666, Avg Loss=0.3538, Time Left=19.40\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1120/3393 [08:37<18:48,  2.01batch/s, Batch Loss=0.2525, Avg Loss=0.3537, Time Left=19.39\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1121/3393 [08:37<18:31,  2.04batch/s, Batch Loss=0.2525, Avg Loss=0.3537, Time Left=19.39\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1121/3393 [08:38<18:31,  2.04batch/s, Batch Loss=0.1142, Avg Loss=0.3535, Time Left=19.38\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1122/3393 [08:38<18:39,  2.03batch/s, Batch Loss=0.1142, Avg Loss=0.3535, Time Left=19.38\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1122/3393 [08:38<18:39,  2.03batch/s, Batch Loss=0.2188, Avg Loss=0.3534, Time Left=19.38\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1123/3393 [08:38<19:05,  1.98batch/s, Batch Loss=0.2188, Avg Loss=0.3534, Time Left=19.38\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1123/3393 [08:39<19:05,  1.98batch/s, Batch Loss=0.1870, Avg Loss=0.3532, Time Left=19.37\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1124/3393 [08:39<18:52,  2.00batch/s, Batch Loss=0.1870, Avg Loss=0.3532, Time Left=19.37\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1124/3393 [08:39<18:52,  2.00batch/s, Batch Loss=0.1433, Avg Loss=0.3530, Time Left=19.36\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1125/3393 [08:39<18:54,  2.00batch/s, Batch Loss=0.1433, Avg Loss=0.3530, Time Left=19.36\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1125/3393 [08:40<18:54,  2.00batch/s, Batch Loss=0.2859, Avg Loss=0.3529, Time Left=19.35\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1126/3393 [08:40<18:58,  1.99batch/s, Batch Loss=0.2859, Avg Loss=0.3529, Time Left=19.35\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1126/3393 [08:40<18:58,  1.99batch/s, Batch Loss=0.1903, Avg Loss=0.3528, Time Left=19.34\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1127/3393 [08:40<19:08,  1.97batch/s, Batch Loss=0.1903, Avg Loss=0.3528, Time Left=19.34\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1127/3393 [08:41<19:08,  1.97batch/s, Batch Loss=0.1067, Avg Loss=0.3525, Time Left=19.34\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1128/3393 [08:41<18:51,  2.00batch/s, Batch Loss=0.1067, Avg Loss=0.3525, Time Left=19.34\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1128/3393 [08:41<18:51,  2.00batch/s, Batch Loss=0.0749, Avg Loss=0.3523, Time Left=19.33\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1129/3393 [08:41<19:13,  1.96batch/s, Batch Loss=0.0749, Avg Loss=0.3523, Time Left=19.33\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1129/3393 [08:42<19:13,  1.96batch/s, Batch Loss=0.2899, Avg Loss=0.3522, Time Left=19.32\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1130/3393 [08:42<19:08,  1.97batch/s, Batch Loss=0.2899, Avg Loss=0.3522, Time Left=19.32\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1130/3393 [08:42<19:08,  1.97batch/s, Batch Loss=0.2536, Avg Loss=0.3521, Time Left=19.31\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1131/3393 [08:42<19:14,  1.96batch/s, Batch Loss=0.2536, Avg Loss=0.3521, Time Left=19.31\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1131/3393 [08:43<19:14,  1.96batch/s, Batch Loss=0.4367, Avg Loss=0.3522, Time Left=19.30\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1132/3393 [08:43<18:57,  1.99batch/s, Batch Loss=0.4367, Avg Loss=0.3522, Time Left=19.30\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1132/3393 [08:43<18:57,  1.99batch/s, Batch Loss=0.0887, Avg Loss=0.3520, Time Left=19.30\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1133/3393 [08:43<19:06,  1.97batch/s, Batch Loss=0.0887, Avg Loss=0.3520, Time Left=19.30\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1133/3393 [08:44<19:06,  1.97batch/s, Batch Loss=0.0880, Avg Loss=0.3517, Time Left=19.29\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1134/3393 [08:44<18:54,  1.99batch/s, Batch Loss=0.0880, Avg Loss=0.3517, Time Left=19.29\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1134/3393 [08:44<18:54,  1.99batch/s, Batch Loss=0.2715, Avg Loss=0.3516, Time Left=19.28\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1135/3393 [08:44<18:30,  2.03batch/s, Batch Loss=0.2715, Avg Loss=0.3516, Time Left=19.28\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1135/3393 [08:45<18:30,  2.03batch/s, Batch Loss=0.2174, Avg Loss=0.3515, Time Left=19.27\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1136/3393 [08:45<18:58,  1.98batch/s, Batch Loss=0.2174, Avg Loss=0.3515, Time Left=19.27\u001b[A\n",
      "Epoch 1/3 - Training:  33%|▎| 1136/3393 [08:45<18:58,  1.98batch/s, Batch Loss=0.1194, Avg Loss=0.3513, Time Left=19.26\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1137/3393 [08:45<18:45,  2.00batch/s, Batch Loss=0.1194, Avg Loss=0.3513, Time Left=19.26\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1137/3393 [08:46<18:45,  2.00batch/s, Batch Loss=0.1374, Avg Loss=0.3511, Time Left=19.26\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1138/3393 [08:46<18:58,  1.98batch/s, Batch Loss=0.1374, Avg Loss=0.3511, Time Left=19.26\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1138/3393 [08:46<18:58,  1.98batch/s, Batch Loss=0.0991, Avg Loss=0.3508, Time Left=19.25\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1139/3393 [08:46<18:56,  1.98batch/s, Batch Loss=0.0991, Avg Loss=0.3508, Time Left=19.25\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1139/3393 [08:47<18:56,  1.98batch/s, Batch Loss=0.0756, Avg Loss=0.3506, Time Left=19.24\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1140/3393 [08:47<19:05,  1.97batch/s, Batch Loss=0.0756, Avg Loss=0.3506, Time Left=19.24\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1140/3393 [08:47<19:05,  1.97batch/s, Batch Loss=0.2823, Avg Loss=0.3505, Time Left=19.23\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1141/3393 [08:47<18:50,  1.99batch/s, Batch Loss=0.2823, Avg Loss=0.3505, Time Left=19.23\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1141/3393 [08:48<18:50,  1.99batch/s, Batch Loss=0.1507, Avg Loss=0.3503, Time Left=19.23\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1142/3393 [08:48<18:59,  1.97batch/s, Batch Loss=0.1507, Avg Loss=0.3503, Time Left=19.23\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1142/3393 [08:48<18:59,  1.97batch/s, Batch Loss=0.0786, Avg Loss=0.3501, Time Left=19.22\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1143/3393 [08:48<18:46,  2.00batch/s, Batch Loss=0.0786, Avg Loss=0.3501, Time Left=19.22\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1143/3393 [08:49<18:46,  2.00batch/s, Batch Loss=0.1624, Avg Loss=0.3499, Time Left=19.21\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1144/3393 [08:49<18:35,  2.02batch/s, Batch Loss=0.1624, Avg Loss=0.3499, Time Left=19.21\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1144/3393 [08:49<18:35,  2.02batch/s, Batch Loss=0.3016, Avg Loss=0.3499, Time Left=19.20\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1145/3393 [08:49<18:36,  2.01batch/s, Batch Loss=0.3016, Avg Loss=0.3499, Time Left=19.20\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1145/3393 [08:50<18:36,  2.01batch/s, Batch Loss=0.1192, Avg Loss=0.3496, Time Left=19.19\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1146/3393 [08:50<18:34,  2.02batch/s, Batch Loss=0.1192, Avg Loss=0.3496, Time Left=19.19\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1146/3393 [08:50<18:34,  2.02batch/s, Batch Loss=0.1561, Avg Loss=0.3495, Time Left=19.18\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1147/3393 [08:50<18:35,  2.01batch/s, Batch Loss=0.1561, Avg Loss=0.3495, Time Left=19.18\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1147/3393 [08:51<18:35,  2.01batch/s, Batch Loss=0.1649, Avg Loss=0.3493, Time Left=19.17\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1148/3393 [08:51<18:38,  2.01batch/s, Batch Loss=0.1649, Avg Loss=0.3493, Time Left=19.17\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1148/3393 [08:51<18:38,  2.01batch/s, Batch Loss=0.1088, Avg Loss=0.3491, Time Left=19.17\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1149/3393 [08:51<18:40,  2.00batch/s, Batch Loss=0.1088, Avg Loss=0.3491, Time Left=19.17\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1149/3393 [08:52<18:40,  2.00batch/s, Batch Loss=0.2786, Avg Loss=0.3490, Time Left=19.16\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1150/3393 [08:52<18:21,  2.04batch/s, Batch Loss=0.2786, Avg Loss=0.3490, Time Left=19.16\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1150/3393 [08:52<18:21,  2.04batch/s, Batch Loss=0.1342, Avg Loss=0.3488, Time Left=19.15\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1151/3393 [08:52<18:16,  2.04batch/s, Batch Loss=0.1342, Avg Loss=0.3488, Time Left=19.15\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1151/3393 [08:53<18:16,  2.04batch/s, Batch Loss=0.3556, Avg Loss=0.3488, Time Left=19.14\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1152/3393 [08:53<18:56,  1.97batch/s, Batch Loss=0.3556, Avg Loss=0.3488, Time Left=19.14\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1152/3393 [08:53<18:56,  1.97batch/s, Batch Loss=0.0399, Avg Loss=0.3485, Time Left=19.13\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  34%|▎| 1153/3393 [08:53<18:48,  1.99batch/s, Batch Loss=0.0399, Avg Loss=0.3485, Time Left=19.13\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1153/3393 [08:54<18:48,  1.99batch/s, Batch Loss=0.4765, Avg Loss=0.3486, Time Left=19.13\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1154/3393 [08:54<18:40,  2.00batch/s, Batch Loss=0.4765, Avg Loss=0.3486, Time Left=19.13\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1154/3393 [08:54<18:40,  2.00batch/s, Batch Loss=0.1738, Avg Loss=0.3485, Time Left=19.12\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1155/3393 [08:54<18:31,  2.01batch/s, Batch Loss=0.1738, Avg Loss=0.3485, Time Left=19.12\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1155/3393 [08:55<18:31,  2.01batch/s, Batch Loss=0.1129, Avg Loss=0.3483, Time Left=19.11\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1156/3393 [08:55<18:55,  1.97batch/s, Batch Loss=0.1129, Avg Loss=0.3483, Time Left=19.11\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1156/3393 [08:55<18:55,  1.97batch/s, Batch Loss=0.1300, Avg Loss=0.3481, Time Left=19.10\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1157/3393 [08:55<18:41,  1.99batch/s, Batch Loss=0.1300, Avg Loss=0.3481, Time Left=19.10\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1157/3393 [08:56<18:41,  1.99batch/s, Batch Loss=0.1350, Avg Loss=0.3479, Time Left=19.09\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1158/3393 [08:56<18:52,  1.97batch/s, Batch Loss=0.1350, Avg Loss=0.3479, Time Left=19.09\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1158/3393 [08:56<18:52,  1.97batch/s, Batch Loss=0.1736, Avg Loss=0.3477, Time Left=19.09\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1159/3393 [08:56<18:45,  1.98batch/s, Batch Loss=0.1736, Avg Loss=0.3477, Time Left=19.09\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1159/3393 [08:57<18:45,  1.98batch/s, Batch Loss=0.0709, Avg Loss=0.3474, Time Left=19.08\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1160/3393 [08:57<18:36,  2.00batch/s, Batch Loss=0.0709, Avg Loss=0.3474, Time Left=19.08\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1160/3393 [08:57<18:36,  2.00batch/s, Batch Loss=0.1805, Avg Loss=0.3473, Time Left=19.07\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1161/3393 [08:57<18:15,  2.04batch/s, Batch Loss=0.1805, Avg Loss=0.3473, Time Left=19.07\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1161/3393 [08:58<18:15,  2.04batch/s, Batch Loss=0.2004, Avg Loss=0.3472, Time Left=19.06\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1162/3393 [08:58<18:26,  2.02batch/s, Batch Loss=0.2004, Avg Loss=0.3472, Time Left=19.06\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1162/3393 [08:58<18:26,  2.02batch/s, Batch Loss=0.8105, Avg Loss=0.3476, Time Left=19.05\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1163/3393 [08:58<18:29,  2.01batch/s, Batch Loss=0.8105, Avg Loss=0.3476, Time Left=19.05\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1163/3393 [08:59<18:29,  2.01batch/s, Batch Loss=0.0853, Avg Loss=0.3473, Time Left=19.04\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1164/3393 [08:59<18:30,  2.01batch/s, Batch Loss=0.0853, Avg Loss=0.3473, Time Left=19.04\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1164/3393 [08:59<18:30,  2.01batch/s, Batch Loss=0.0826, Avg Loss=0.3471, Time Left=19.04\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1165/3393 [08:59<18:41,  1.99batch/s, Batch Loss=0.0826, Avg Loss=0.3471, Time Left=19.04\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1165/3393 [09:00<18:41,  1.99batch/s, Batch Loss=0.3209, Avg Loss=0.3471, Time Left=19.03\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1166/3393 [09:00<18:39,  1.99batch/s, Batch Loss=0.3209, Avg Loss=0.3471, Time Left=19.03\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1166/3393 [09:00<18:39,  1.99batch/s, Batch Loss=0.4590, Avg Loss=0.3472, Time Left=19.02\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1167/3393 [09:00<18:38,  1.99batch/s, Batch Loss=0.4590, Avg Loss=0.3472, Time Left=19.02\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1167/3393 [09:01<18:38,  1.99batch/s, Batch Loss=0.3013, Avg Loss=0.3471, Time Left=19.01\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1168/3393 [09:01<18:27,  2.01batch/s, Batch Loss=0.3013, Avg Loss=0.3471, Time Left=19.01\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1168/3393 [09:01<18:27,  2.01batch/s, Batch Loss=0.0980, Avg Loss=0.3469, Time Left=19.00\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1169/3393 [09:01<18:20,  2.02batch/s, Batch Loss=0.0980, Avg Loss=0.3469, Time Left=19.00\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1169/3393 [09:02<18:20,  2.02batch/s, Batch Loss=0.1077, Avg Loss=0.3467, Time Left=19.00\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1170/3393 [09:02<18:44,  1.98batch/s, Batch Loss=0.1077, Avg Loss=0.3467, Time Left=19.00\u001b[A\n",
      "Epoch 1/3 - Training:  34%|▎| 1170/3393 [09:02<18:44,  1.98batch/s, Batch Loss=0.1989, Avg Loss=0.3466, Time Left=18.99\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1171/3393 [09:02<18:31,  2.00batch/s, Batch Loss=0.1989, Avg Loss=0.3466, Time Left=18.99\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1171/3393 [09:03<18:31,  2.00batch/s, Batch Loss=0.4319, Avg Loss=0.3466, Time Left=18.98\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1172/3393 [09:03<18:56,  1.95batch/s, Batch Loss=0.4319, Avg Loss=0.3466, Time Left=18.98\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1172/3393 [09:03<18:56,  1.95batch/s, Batch Loss=0.3745, Avg Loss=0.3467, Time Left=18.97\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1173/3393 [09:03<18:35,  1.99batch/s, Batch Loss=0.3745, Avg Loss=0.3467, Time Left=18.97\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1173/3393 [09:04<18:35,  1.99batch/s, Batch Loss=0.1856, Avg Loss=0.3465, Time Left=18.96\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1174/3393 [09:04<18:24,  2.01batch/s, Batch Loss=0.1856, Avg Loss=0.3465, Time Left=18.96\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1174/3393 [09:04<18:24,  2.01batch/s, Batch Loss=0.1623, Avg Loss=0.3463, Time Left=18.96\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1175/3393 [09:04<18:27,  2.00batch/s, Batch Loss=0.1623, Avg Loss=0.3463, Time Left=18.96\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1175/3393 [09:05<18:27,  2.00batch/s, Batch Loss=0.1889, Avg Loss=0.3462, Time Left=18.95\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1176/3393 [09:05<18:08,  2.04batch/s, Batch Loss=0.1889, Avg Loss=0.3462, Time Left=18.95\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1176/3393 [09:05<18:08,  2.04batch/s, Batch Loss=0.1508, Avg Loss=0.3460, Time Left=18.94\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1177/3393 [09:05<18:04,  2.04batch/s, Batch Loss=0.1508, Avg Loss=0.3460, Time Left=18.94\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1177/3393 [09:06<18:04,  2.04batch/s, Batch Loss=0.1957, Avg Loss=0.3459, Time Left=18.93\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1178/3393 [09:06<18:13,  2.03batch/s, Batch Loss=0.1957, Avg Loss=0.3459, Time Left=18.93\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1178/3393 [09:06<18:13,  2.03batch/s, Batch Loss=0.2104, Avg Loss=0.3458, Time Left=18.92\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1179/3393 [09:06<18:17,  2.02batch/s, Batch Loss=0.2104, Avg Loss=0.3458, Time Left=18.92\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1179/3393 [09:07<18:17,  2.02batch/s, Batch Loss=0.0862, Avg Loss=0.3455, Time Left=18.91\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1180/3393 [09:07<18:03,  2.04batch/s, Batch Loss=0.0862, Avg Loss=0.3455, Time Left=18.91\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1180/3393 [09:07<18:03,  2.04batch/s, Batch Loss=0.3930, Avg Loss=0.3456, Time Left=18.91\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1181/3393 [09:07<18:08,  2.03batch/s, Batch Loss=0.3930, Avg Loss=0.3456, Time Left=18.91\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1181/3393 [09:08<18:08,  2.03batch/s, Batch Loss=0.2292, Avg Loss=0.3455, Time Left=18.90\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1182/3393 [09:08<17:53,  2.06batch/s, Batch Loss=0.2292, Avg Loss=0.3455, Time Left=18.90\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1182/3393 [09:08<17:53,  2.06batch/s, Batch Loss=0.2563, Avg Loss=0.3454, Time Left=18.89\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1183/3393 [09:08<18:04,  2.04batch/s, Batch Loss=0.2563, Avg Loss=0.3454, Time Left=18.89\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1183/3393 [09:09<18:04,  2.04batch/s, Batch Loss=0.1072, Avg Loss=0.3452, Time Left=18.88\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1184/3393 [09:09<18:21,  2.00batch/s, Batch Loss=0.1072, Avg Loss=0.3452, Time Left=18.88\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1184/3393 [09:09<18:21,  2.00batch/s, Batch Loss=0.2693, Avg Loss=0.3451, Time Left=18.87\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1185/3393 [09:09<18:13,  2.02batch/s, Batch Loss=0.2693, Avg Loss=0.3451, Time Left=18.87\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1185/3393 [09:10<18:13,  2.02batch/s, Batch Loss=0.1735, Avg Loss=0.3449, Time Left=18.86\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  35%|▎| 1186/3393 [09:10<17:55,  2.05batch/s, Batch Loss=0.1735, Avg Loss=0.3449, Time Left=18.86\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1186/3393 [09:10<17:55,  2.05batch/s, Batch Loss=0.3725, Avg Loss=0.3450, Time Left=18.85\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1187/3393 [09:10<17:55,  2.05batch/s, Batch Loss=0.3725, Avg Loss=0.3450, Time Left=18.85\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1187/3393 [09:11<17:55,  2.05batch/s, Batch Loss=0.4922, Avg Loss=0.3451, Time Left=18.85\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1188/3393 [09:11<17:34,  2.09batch/s, Batch Loss=0.4922, Avg Loss=0.3451, Time Left=18.85\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1188/3393 [09:11<17:34,  2.09batch/s, Batch Loss=0.2582, Avg Loss=0.3450, Time Left=18.84\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1189/3393 [09:11<17:44,  2.07batch/s, Batch Loss=0.2582, Avg Loss=0.3450, Time Left=18.84\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1189/3393 [09:12<17:44,  2.07batch/s, Batch Loss=0.1677, Avg Loss=0.3449, Time Left=18.83\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1190/3393 [09:12<17:51,  2.06batch/s, Batch Loss=0.1677, Avg Loss=0.3449, Time Left=18.83\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1190/3393 [09:12<17:51,  2.06batch/s, Batch Loss=0.2345, Avg Loss=0.3448, Time Left=18.82\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1191/3393 [09:12<17:50,  2.06batch/s, Batch Loss=0.2345, Avg Loss=0.3448, Time Left=18.82\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1191/3393 [09:13<17:50,  2.06batch/s, Batch Loss=0.3128, Avg Loss=0.3447, Time Left=18.81\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1192/3393 [09:13<18:10,  2.02batch/s, Batch Loss=0.3128, Avg Loss=0.3447, Time Left=18.81\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1192/3393 [09:13<18:10,  2.02batch/s, Batch Loss=0.2254, Avg Loss=0.3446, Time Left=18.80\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1193/3393 [09:13<18:07,  2.02batch/s, Batch Loss=0.2254, Avg Loss=0.3446, Time Left=18.80\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1193/3393 [09:14<18:07,  2.02batch/s, Batch Loss=0.1756, Avg Loss=0.3445, Time Left=18.80\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1194/3393 [09:14<18:09,  2.02batch/s, Batch Loss=0.1756, Avg Loss=0.3445, Time Left=18.80\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1194/3393 [09:14<18:09,  2.02batch/s, Batch Loss=0.4020, Avg Loss=0.3445, Time Left=18.79\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1195/3393 [09:14<17:52,  2.05batch/s, Batch Loss=0.4020, Avg Loss=0.3445, Time Left=18.79\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1195/3393 [09:15<17:52,  2.05batch/s, Batch Loss=0.2588, Avg Loss=0.3445, Time Left=18.78\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1196/3393 [09:15<17:40,  2.07batch/s, Batch Loss=0.2588, Avg Loss=0.3445, Time Left=18.78\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1196/3393 [09:15<17:40,  2.07batch/s, Batch Loss=0.2844, Avg Loss=0.3444, Time Left=18.77\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1197/3393 [09:15<18:12,  2.01batch/s, Batch Loss=0.2844, Avg Loss=0.3444, Time Left=18.77\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1197/3393 [09:16<18:12,  2.01batch/s, Batch Loss=0.1665, Avg Loss=0.3442, Time Left=18.76\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1198/3393 [09:16<18:08,  2.02batch/s, Batch Loss=0.1665, Avg Loss=0.3442, Time Left=18.76\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1198/3393 [09:16<18:08,  2.02batch/s, Batch Loss=0.2642, Avg Loss=0.3442, Time Left=18.75\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1199/3393 [09:16<18:19,  2.00batch/s, Batch Loss=0.2642, Avg Loss=0.3442, Time Left=18.75\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1199/3393 [09:17<18:19,  2.00batch/s, Batch Loss=0.1589, Avg Loss=0.3440, Time Left=18.74\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1200/3393 [09:17<17:48,  2.05batch/s, Batch Loss=0.1589, Avg Loss=0.3440, Time Left=18.74\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1200/3393 [09:17<17:48,  2.05batch/s, Batch Loss=0.1727, Avg Loss=0.3439, Time Left=18.74\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1201/3393 [09:17<17:48,  2.05batch/s, Batch Loss=0.1727, Avg Loss=0.3439, Time Left=18.74\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1201/3393 [09:18<17:48,  2.05batch/s, Batch Loss=0.1690, Avg Loss=0.3437, Time Left=18.73\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1202/3393 [09:18<18:07,  2.01batch/s, Batch Loss=0.1690, Avg Loss=0.3437, Time Left=18.73\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1202/3393 [09:18<18:07,  2.01batch/s, Batch Loss=0.1603, Avg Loss=0.3435, Time Left=18.72\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1203/3393 [09:18<18:00,  2.03batch/s, Batch Loss=0.1603, Avg Loss=0.3435, Time Left=18.72\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1203/3393 [09:19<18:00,  2.03batch/s, Batch Loss=0.0495, Avg Loss=0.3433, Time Left=18.71\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1204/3393 [09:19<18:15,  2.00batch/s, Batch Loss=0.0495, Avg Loss=0.3433, Time Left=18.71\u001b[A\n",
      "Epoch 1/3 - Training:  35%|▎| 1204/3393 [09:19<18:15,  2.00batch/s, Batch Loss=0.0883, Avg Loss=0.3430, Time Left=18.70\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1205/3393 [09:19<18:05,  2.02batch/s, Batch Loss=0.0883, Avg Loss=0.3430, Time Left=18.70\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1205/3393 [09:20<18:05,  2.02batch/s, Batch Loss=0.1720, Avg Loss=0.3429, Time Left=18.69\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1206/3393 [09:20<17:49,  2.05batch/s, Batch Loss=0.1720, Avg Loss=0.3429, Time Left=18.69\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1206/3393 [09:20<17:49,  2.05batch/s, Batch Loss=0.1305, Avg Loss=0.3427, Time Left=18.69\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1207/3393 [09:20<18:02,  2.02batch/s, Batch Loss=0.1305, Avg Loss=0.3427, Time Left=18.69\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1207/3393 [09:21<18:02,  2.02batch/s, Batch Loss=0.0193, Avg Loss=0.3424, Time Left=18.68\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1208/3393 [09:21<17:51,  2.04batch/s, Batch Loss=0.0193, Avg Loss=0.3424, Time Left=18.68\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1208/3393 [09:21<17:51,  2.04batch/s, Batch Loss=0.2453, Avg Loss=0.3423, Time Left=18.67\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1209/3393 [09:21<18:08,  2.01batch/s, Batch Loss=0.2453, Avg Loss=0.3423, Time Left=18.67\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1209/3393 [09:22<18:08,  2.01batch/s, Batch Loss=0.2872, Avg Loss=0.3423, Time Left=18.66\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1210/3393 [09:22<17:58,  2.02batch/s, Batch Loss=0.2872, Avg Loss=0.3423, Time Left=18.66\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1210/3393 [09:22<17:58,  2.02batch/s, Batch Loss=0.0545, Avg Loss=0.3420, Time Left=18.65\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1211/3393 [09:22<18:15,  1.99batch/s, Batch Loss=0.0545, Avg Loss=0.3420, Time Left=18.65\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1211/3393 [09:23<18:15,  1.99batch/s, Batch Loss=0.3549, Avg Loss=0.3420, Time Left=18.65\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1212/3393 [09:23<17:54,  2.03batch/s, Batch Loss=0.3549, Avg Loss=0.3420, Time Left=18.65\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1212/3393 [09:23<17:54,  2.03batch/s, Batch Loss=0.2214, Avg Loss=0.3419, Time Left=18.64\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1213/3393 [09:23<17:49,  2.04batch/s, Batch Loss=0.2214, Avg Loss=0.3419, Time Left=18.64\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1213/3393 [09:24<17:49,  2.04batch/s, Batch Loss=0.0950, Avg Loss=0.3417, Time Left=18.63\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1214/3393 [09:24<18:06,  2.00batch/s, Batch Loss=0.0950, Avg Loss=0.3417, Time Left=18.63\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1214/3393 [09:24<18:06,  2.00batch/s, Batch Loss=0.4176, Avg Loss=0.3418, Time Left=18.62\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1215/3393 [09:24<18:08,  2.00batch/s, Batch Loss=0.4176, Avg Loss=0.3418, Time Left=18.62\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1215/3393 [09:25<18:08,  2.00batch/s, Batch Loss=0.1885, Avg Loss=0.3417, Time Left=18.61\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1216/3393 [09:25<17:59,  2.02batch/s, Batch Loss=0.1885, Avg Loss=0.3417, Time Left=18.61\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1216/3393 [09:25<17:59,  2.02batch/s, Batch Loss=0.1399, Avg Loss=0.3415, Time Left=18.60\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1217/3393 [09:25<18:03,  2.01batch/s, Batch Loss=0.1399, Avg Loss=0.3415, Time Left=18.60\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1217/3393 [09:26<18:03,  2.01batch/s, Batch Loss=0.0618, Avg Loss=0.3412, Time Left=18.60\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1218/3393 [09:26<17:43,  2.04batch/s, Batch Loss=0.0618, Avg Loss=0.3412, Time Left=18.60\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1218/3393 [09:26<17:43,  2.04batch/s, Batch Loss=0.1962, Avg Loss=0.3411, Time Left=18.59\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  36%|▎| 1219/3393 [09:26<17:42,  2.05batch/s, Batch Loss=0.1962, Avg Loss=0.3411, Time Left=18.59\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1219/3393 [09:27<17:42,  2.05batch/s, Batch Loss=0.2074, Avg Loss=0.3410, Time Left=18.58\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1220/3393 [09:27<17:51,  2.03batch/s, Batch Loss=0.2074, Avg Loss=0.3410, Time Left=18.58\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1220/3393 [09:27<17:51,  2.03batch/s, Batch Loss=0.2271, Avg Loss=0.3409, Time Left=18.57\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1221/3393 [09:27<17:56,  2.02batch/s, Batch Loss=0.2271, Avg Loss=0.3409, Time Left=18.57\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1221/3393 [09:28<17:56,  2.02batch/s, Batch Loss=0.1592, Avg Loss=0.3407, Time Left=18.56\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1222/3393 [09:28<17:56,  2.02batch/s, Batch Loss=0.1592, Avg Loss=0.3407, Time Left=18.56\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1222/3393 [09:28<17:56,  2.02batch/s, Batch Loss=0.4469, Avg Loss=0.3408, Time Left=18.55\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1223/3393 [09:28<17:45,  2.04batch/s, Batch Loss=0.4469, Avg Loss=0.3408, Time Left=18.55\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1223/3393 [09:28<17:45,  2.04batch/s, Batch Loss=0.1924, Avg Loss=0.3407, Time Left=18.54\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1224/3393 [09:28<17:29,  2.07batch/s, Batch Loss=0.1924, Avg Loss=0.3407, Time Left=18.54\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1224/3393 [09:29<17:29,  2.07batch/s, Batch Loss=0.2483, Avg Loss=0.3406, Time Left=18.54\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1225/3393 [09:29<17:31,  2.06batch/s, Batch Loss=0.2483, Avg Loss=0.3406, Time Left=18.54\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1225/3393 [09:30<17:31,  2.06batch/s, Batch Loss=0.3139, Avg Loss=0.3406, Time Left=18.53\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1226/3393 [09:30<18:01,  2.00batch/s, Batch Loss=0.3139, Avg Loss=0.3406, Time Left=18.53\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1226/3393 [09:30<18:01,  2.00batch/s, Batch Loss=0.2363, Avg Loss=0.3405, Time Left=18.52\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1227/3393 [09:30<17:42,  2.04batch/s, Batch Loss=0.2363, Avg Loss=0.3405, Time Left=18.52\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1227/3393 [09:30<17:42,  2.04batch/s, Batch Loss=0.1426, Avg Loss=0.3403, Time Left=18.51\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1228/3393 [09:30<17:39,  2.04batch/s, Batch Loss=0.1426, Avg Loss=0.3403, Time Left=18.51\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1228/3393 [09:31<17:39,  2.04batch/s, Batch Loss=0.4047, Avg Loss=0.3404, Time Left=18.50\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1229/3393 [09:31<17:47,  2.03batch/s, Batch Loss=0.4047, Avg Loss=0.3404, Time Left=18.50\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1229/3393 [09:31<17:47,  2.03batch/s, Batch Loss=0.2724, Avg Loss=0.3403, Time Left=18.49\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1230/3393 [09:31<17:32,  2.05batch/s, Batch Loss=0.2724, Avg Loss=0.3403, Time Left=18.49\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1230/3393 [09:32<17:32,  2.05batch/s, Batch Loss=0.1472, Avg Loss=0.3402, Time Left=18.49\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1231/3393 [09:32<17:52,  2.02batch/s, Batch Loss=0.1472, Avg Loss=0.3402, Time Left=18.49\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1231/3393 [09:32<17:52,  2.02batch/s, Batch Loss=0.1352, Avg Loss=0.3400, Time Left=18.48\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1232/3393 [09:32<17:45,  2.03batch/s, Batch Loss=0.1352, Avg Loss=0.3400, Time Left=18.48\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1232/3393 [09:33<17:45,  2.03batch/s, Batch Loss=0.3538, Avg Loss=0.3400, Time Left=18.47\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1233/3393 [09:33<17:30,  2.06batch/s, Batch Loss=0.3538, Avg Loss=0.3400, Time Left=18.47\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1233/3393 [09:33<17:30,  2.06batch/s, Batch Loss=0.4146, Avg Loss=0.3401, Time Left=18.46\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1234/3393 [09:33<17:40,  2.04batch/s, Batch Loss=0.4146, Avg Loss=0.3401, Time Left=18.46\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1234/3393 [09:34<17:40,  2.04batch/s, Batch Loss=0.1640, Avg Loss=0.3399, Time Left=18.45\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1235/3393 [09:34<17:46,  2.02batch/s, Batch Loss=0.1640, Avg Loss=0.3399, Time Left=18.45\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1235/3393 [09:34<17:46,  2.02batch/s, Batch Loss=0.1242, Avg Loss=0.3397, Time Left=18.45\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1236/3393 [09:34<17:54,  2.01batch/s, Batch Loss=0.1242, Avg Loss=0.3397, Time Left=18.45\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1236/3393 [09:35<17:54,  2.01batch/s, Batch Loss=0.1455, Avg Loss=0.3396, Time Left=18.44\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1237/3393 [09:35<17:43,  2.03batch/s, Batch Loss=0.1455, Avg Loss=0.3396, Time Left=18.44\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1237/3393 [09:35<17:43,  2.03batch/s, Batch Loss=0.0744, Avg Loss=0.3393, Time Left=18.43\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1238/3393 [09:35<17:37,  2.04batch/s, Batch Loss=0.0744, Avg Loss=0.3393, Time Left=18.43\u001b[A\n",
      "Epoch 1/3 - Training:  36%|▎| 1238/3393 [09:36<17:37,  2.04batch/s, Batch Loss=0.2854, Avg Loss=0.3393, Time Left=18.42\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1239/3393 [09:36<17:34,  2.04batch/s, Batch Loss=0.2854, Avg Loss=0.3393, Time Left=18.42\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1239/3393 [09:36<17:34,  2.04batch/s, Batch Loss=0.3509, Avg Loss=0.3393, Time Left=18.41\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1240/3393 [09:36<17:32,  2.05batch/s, Batch Loss=0.3509, Avg Loss=0.3393, Time Left=18.41\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1240/3393 [09:37<17:32,  2.05batch/s, Batch Loss=0.0958, Avg Loss=0.3391, Time Left=18.40\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1241/3393 [09:37<17:50,  2.01batch/s, Batch Loss=0.0958, Avg Loss=0.3391, Time Left=18.40\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1241/3393 [09:37<17:50,  2.01batch/s, Batch Loss=0.1371, Avg Loss=0.3389, Time Left=18.39\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1242/3393 [09:37<17:32,  2.04batch/s, Batch Loss=0.1371, Avg Loss=0.3389, Time Left=18.39\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1242/3393 [09:38<17:32,  2.04batch/s, Batch Loss=0.4591, Avg Loss=0.3390, Time Left=18.39\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1243/3393 [09:38<17:30,  2.05batch/s, Batch Loss=0.4591, Avg Loss=0.3390, Time Left=18.39\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1243/3393 [09:38<17:30,  2.05batch/s, Batch Loss=0.1684, Avg Loss=0.3389, Time Left=18.38\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1244/3393 [09:38<17:19,  2.07batch/s, Batch Loss=0.1684, Avg Loss=0.3389, Time Left=18.38\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1244/3393 [09:39<17:19,  2.07batch/s, Batch Loss=0.3440, Avg Loss=0.3389, Time Left=18.37\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1245/3393 [09:39<17:11,  2.08batch/s, Batch Loss=0.3440, Avg Loss=0.3389, Time Left=18.37\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1245/3393 [09:39<17:11,  2.08batch/s, Batch Loss=0.1160, Avg Loss=0.3387, Time Left=18.36\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1246/3393 [09:39<17:03,  2.10batch/s, Batch Loss=0.1160, Avg Loss=0.3387, Time Left=18.36\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1246/3393 [09:40<17:03,  2.10batch/s, Batch Loss=0.6240, Avg Loss=0.3389, Time Left=18.35\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1247/3393 [09:40<17:29,  2.05batch/s, Batch Loss=0.6240, Avg Loss=0.3389, Time Left=18.35\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1247/3393 [09:40<17:29,  2.05batch/s, Batch Loss=0.1316, Avg Loss=0.3387, Time Left=18.34\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1248/3393 [09:40<17:33,  2.04batch/s, Batch Loss=0.1316, Avg Loss=0.3387, Time Left=18.34\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1248/3393 [09:41<17:33,  2.04batch/s, Batch Loss=0.1906, Avg Loss=0.3386, Time Left=18.34\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1249/3393 [09:41<17:46,  2.01batch/s, Batch Loss=0.1906, Avg Loss=0.3386, Time Left=18.34\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1249/3393 [09:41<17:46,  2.01batch/s, Batch Loss=0.1683, Avg Loss=0.3385, Time Left=18.33\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1250/3393 [09:41<17:36,  2.03batch/s, Batch Loss=0.1683, Avg Loss=0.3385, Time Left=18.33\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1250/3393 [09:42<17:36,  2.03batch/s, Batch Loss=0.0938, Avg Loss=0.3383, Time Left=18.32\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1251/3393 [09:42<17:42,  2.02batch/s, Batch Loss=0.0938, Avg Loss=0.3383, Time Left=18.32\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1251/3393 [09:42<17:42,  2.02batch/s, Batch Loss=0.1365, Avg Loss=0.3381, Time Left=18.31\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  37%|▎| 1252/3393 [09:42<17:28,  2.04batch/s, Batch Loss=0.1365, Avg Loss=0.3381, Time Left=18.31\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1252/3393 [09:43<17:28,  2.04batch/s, Batch Loss=0.3557, Avg Loss=0.3381, Time Left=18.30\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1253/3393 [09:43<17:22,  2.05batch/s, Batch Loss=0.3557, Avg Loss=0.3381, Time Left=18.30\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1253/3393 [09:43<17:22,  2.05batch/s, Batch Loss=0.2742, Avg Loss=0.3381, Time Left=18.29\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1254/3393 [09:43<17:40,  2.02batch/s, Batch Loss=0.2742, Avg Loss=0.3381, Time Left=18.29\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1254/3393 [09:44<17:40,  2.02batch/s, Batch Loss=0.1689, Avg Loss=0.3379, Time Left=18.28\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1255/3393 [09:44<17:46,  2.01batch/s, Batch Loss=0.1689, Avg Loss=0.3379, Time Left=18.28\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1255/3393 [09:44<17:46,  2.01batch/s, Batch Loss=0.3624, Avg Loss=0.3379, Time Left=18.28\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1256/3393 [09:44<17:46,  2.00batch/s, Batch Loss=0.3624, Avg Loss=0.3379, Time Left=18.28\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1256/3393 [09:45<17:46,  2.00batch/s, Batch Loss=0.2888, Avg Loss=0.3379, Time Left=18.27\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1257/3393 [09:45<17:48,  2.00batch/s, Batch Loss=0.2888, Avg Loss=0.3379, Time Left=18.27\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1257/3393 [09:45<17:48,  2.00batch/s, Batch Loss=0.1439, Avg Loss=0.3377, Time Left=18.26\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1258/3393 [09:45<17:42,  2.01batch/s, Batch Loss=0.1439, Avg Loss=0.3377, Time Left=18.26\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1258/3393 [09:46<17:42,  2.01batch/s, Batch Loss=0.3318, Avg Loss=0.3377, Time Left=18.25\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1259/3393 [09:46<17:21,  2.05batch/s, Batch Loss=0.3318, Avg Loss=0.3377, Time Left=18.25\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1259/3393 [09:46<17:21,  2.05batch/s, Batch Loss=0.2381, Avg Loss=0.3376, Time Left=18.24\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1260/3393 [09:46<17:20,  2.05batch/s, Batch Loss=0.2381, Avg Loss=0.3376, Time Left=18.24\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1260/3393 [09:47<17:20,  2.05batch/s, Batch Loss=0.1321, Avg Loss=0.3375, Time Left=18.24\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1261/3393 [09:47<17:38,  2.01batch/s, Batch Loss=0.1321, Avg Loss=0.3375, Time Left=18.24\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1261/3393 [09:47<17:38,  2.01batch/s, Batch Loss=0.0786, Avg Loss=0.3372, Time Left=18.23\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1262/3393 [09:47<17:47,  2.00batch/s, Batch Loss=0.0786, Avg Loss=0.3372, Time Left=18.23\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1262/3393 [09:48<17:47,  2.00batch/s, Batch Loss=0.4210, Avg Loss=0.3373, Time Left=18.22\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1263/3393 [09:48<17:52,  1.99batch/s, Batch Loss=0.4210, Avg Loss=0.3373, Time Left=18.22\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1263/3393 [09:48<17:52,  1.99batch/s, Batch Loss=0.1999, Avg Loss=0.3372, Time Left=18.21\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1264/3393 [09:48<17:50,  1.99batch/s, Batch Loss=0.1999, Avg Loss=0.3372, Time Left=18.21\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1264/3393 [09:49<17:50,  1.99batch/s, Batch Loss=0.1718, Avg Loss=0.3371, Time Left=18.20\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1265/3393 [09:49<17:31,  2.02batch/s, Batch Loss=0.1718, Avg Loss=0.3371, Time Left=18.20\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1265/3393 [09:49<17:31,  2.02batch/s, Batch Loss=0.1501, Avg Loss=0.3369, Time Left=18.19\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1266/3393 [09:49<17:14,  2.06batch/s, Batch Loss=0.1501, Avg Loss=0.3369, Time Left=18.19\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1266/3393 [09:50<17:14,  2.06batch/s, Batch Loss=0.2206, Avg Loss=0.3368, Time Left=18.18\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1267/3393 [09:50<17:10,  2.06batch/s, Batch Loss=0.2206, Avg Loss=0.3368, Time Left=18.18\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1267/3393 [09:50<17:10,  2.06batch/s, Batch Loss=0.3087, Avg Loss=0.3368, Time Left=18.18\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1268/3393 [09:50<17:13,  2.06batch/s, Batch Loss=0.3087, Avg Loss=0.3368, Time Left=18.18\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1268/3393 [09:51<17:13,  2.06batch/s, Batch Loss=0.4363, Avg Loss=0.3369, Time Left=18.17\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1269/3393 [09:51<17:03,  2.08batch/s, Batch Loss=0.4363, Avg Loss=0.3369, Time Left=18.17\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1269/3393 [09:51<17:03,  2.08batch/s, Batch Loss=0.2212, Avg Loss=0.3368, Time Left=18.16\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1270/3393 [09:51<17:06,  2.07batch/s, Batch Loss=0.2212, Avg Loss=0.3368, Time Left=18.16\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1270/3393 [09:52<17:06,  2.07batch/s, Batch Loss=0.3253, Avg Loss=0.3368, Time Left=18.15\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1271/3393 [09:52<17:27,  2.02batch/s, Batch Loss=0.3253, Avg Loss=0.3368, Time Left=18.15\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1271/3393 [09:52<17:27,  2.02batch/s, Batch Loss=0.2609, Avg Loss=0.3367, Time Left=18.14\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1272/3393 [09:52<17:22,  2.03batch/s, Batch Loss=0.2609, Avg Loss=0.3367, Time Left=18.14\u001b[A\n",
      "Epoch 1/3 - Training:  37%|▎| 1272/3393 [09:53<17:22,  2.03batch/s, Batch Loss=0.2323, Avg Loss=0.3366, Time Left=18.14\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1273/3393 [09:53<17:39,  2.00batch/s, Batch Loss=0.2323, Avg Loss=0.3366, Time Left=18.14\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1273/3393 [09:53<17:39,  2.00batch/s, Batch Loss=0.4476, Avg Loss=0.3367, Time Left=18.13\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1274/3393 [09:53<17:19,  2.04batch/s, Batch Loss=0.4476, Avg Loss=0.3367, Time Left=18.13\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1274/3393 [09:54<17:19,  2.04batch/s, Batch Loss=0.2298, Avg Loss=0.3366, Time Left=18.12\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1275/3393 [09:54<17:17,  2.04batch/s, Batch Loss=0.2298, Avg Loss=0.3366, Time Left=18.12\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1275/3393 [09:54<17:17,  2.04batch/s, Batch Loss=0.1437, Avg Loss=0.3365, Time Left=18.11\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1276/3393 [09:54<17:45,  1.99batch/s, Batch Loss=0.1437, Avg Loss=0.3365, Time Left=18.11\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1276/3393 [09:55<17:45,  1.99batch/s, Batch Loss=0.3393, Avg Loss=0.3365, Time Left=18.10\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1277/3393 [09:55<17:33,  2.01batch/s, Batch Loss=0.3393, Avg Loss=0.3365, Time Left=18.10\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1277/3393 [09:55<17:33,  2.01batch/s, Batch Loss=0.2348, Avg Loss=0.3364, Time Left=18.09\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1278/3393 [09:55<17:45,  1.99batch/s, Batch Loss=0.2348, Avg Loss=0.3364, Time Left=18.09\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1278/3393 [09:56<17:45,  1.99batch/s, Batch Loss=0.2058, Avg Loss=0.3363, Time Left=18.09\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1279/3393 [09:56<17:25,  2.02batch/s, Batch Loss=0.2058, Avg Loss=0.3363, Time Left=18.09\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1279/3393 [09:56<17:25,  2.02batch/s, Batch Loss=0.1737, Avg Loss=0.3361, Time Left=18.08\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1280/3393 [09:56<17:28,  2.02batch/s, Batch Loss=0.1737, Avg Loss=0.3361, Time Left=18.08\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1280/3393 [09:57<17:28,  2.02batch/s, Batch Loss=0.0699, Avg Loss=0.3359, Time Left=18.07\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1281/3393 [09:57<17:23,  2.02batch/s, Batch Loss=0.0699, Avg Loss=0.3359, Time Left=18.07\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1281/3393 [09:57<17:23,  2.02batch/s, Batch Loss=0.2411, Avg Loss=0.3358, Time Left=18.06\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1282/3393 [09:57<17:05,  2.06batch/s, Batch Loss=0.2411, Avg Loss=0.3358, Time Left=18.06\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1282/3393 [09:58<17:05,  2.06batch/s, Batch Loss=0.1544, Avg Loss=0.3357, Time Left=18.05\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1283/3393 [09:58<17:29,  2.01batch/s, Batch Loss=0.1544, Avg Loss=0.3357, Time Left=18.05\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1283/3393 [09:58<17:29,  2.01batch/s, Batch Loss=0.1148, Avg Loss=0.3355, Time Left=18.04\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1284/3393 [09:58<17:23,  2.02batch/s, Batch Loss=0.1148, Avg Loss=0.3355, Time Left=18.04\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1284/3393 [09:59<17:23,  2.02batch/s, Batch Loss=0.2999, Avg Loss=0.3355, Time Left=18.04\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  38%|▍| 1285/3393 [09:59<17:23,  2.02batch/s, Batch Loss=0.2999, Avg Loss=0.3355, Time Left=18.04\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1285/3393 [09:59<17:23,  2.02batch/s, Batch Loss=0.0534, Avg Loss=0.3352, Time Left=18.03\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1286/3393 [09:59<17:22,  2.02batch/s, Batch Loss=0.0534, Avg Loss=0.3352, Time Left=18.03\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1286/3393 [10:00<17:22,  2.02batch/s, Batch Loss=0.2254, Avg Loss=0.3351, Time Left=18.02\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1287/3393 [10:00<17:08,  2.05batch/s, Batch Loss=0.2254, Avg Loss=0.3351, Time Left=18.02\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1287/3393 [10:00<17:08,  2.05batch/s, Batch Loss=0.0849, Avg Loss=0.3349, Time Left=18.01\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1288/3393 [10:00<17:29,  2.01batch/s, Batch Loss=0.0849, Avg Loss=0.3349, Time Left=18.01\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1288/3393 [10:01<17:29,  2.01batch/s, Batch Loss=0.1361, Avg Loss=0.3348, Time Left=18.00\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1289/3393 [10:01<17:22,  2.02batch/s, Batch Loss=0.1361, Avg Loss=0.3348, Time Left=18.00\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1289/3393 [10:01<17:22,  2.02batch/s, Batch Loss=0.1641, Avg Loss=0.3346, Time Left=18.00\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1290/3393 [10:01<17:35,  1.99batch/s, Batch Loss=0.1641, Avg Loss=0.3346, Time Left=18.00\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1290/3393 [10:02<17:35,  1.99batch/s, Batch Loss=0.2008, Avg Loss=0.3345, Time Left=17.99\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1291/3393 [10:02<17:24,  2.01batch/s, Batch Loss=0.2008, Avg Loss=0.3345, Time Left=17.99\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1291/3393 [10:02<17:24,  2.01batch/s, Batch Loss=0.1162, Avg Loss=0.3343, Time Left=17.98\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1292/3393 [10:02<17:08,  2.04batch/s, Batch Loss=0.1162, Avg Loss=0.3343, Time Left=17.98\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1292/3393 [10:02<17:08,  2.04batch/s, Batch Loss=0.1850, Avg Loss=0.3342, Time Left=17.97\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1293/3393 [10:02<17:08,  2.04batch/s, Batch Loss=0.1850, Avg Loss=0.3342, Time Left=17.97\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1293/3393 [10:03<17:08,  2.04batch/s, Batch Loss=0.1954, Avg Loss=0.3341, Time Left=17.96\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1294/3393 [10:03<17:03,  2.05batch/s, Batch Loss=0.1954, Avg Loss=0.3341, Time Left=17.96\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1294/3393 [10:04<17:03,  2.05batch/s, Batch Loss=0.0906, Avg Loss=0.3339, Time Left=17.95\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1295/3393 [10:04<17:31,  2.00batch/s, Batch Loss=0.0906, Avg Loss=0.3339, Time Left=17.95\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1295/3393 [10:04<17:31,  2.00batch/s, Batch Loss=0.1821, Avg Loss=0.3338, Time Left=17.94\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1296/3393 [10:04<17:10,  2.03batch/s, Batch Loss=0.1821, Avg Loss=0.3338, Time Left=17.94\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1296/3393 [10:04<17:10,  2.03batch/s, Batch Loss=0.1915, Avg Loss=0.3337, Time Left=17.94\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1297/3393 [10:04<17:08,  2.04batch/s, Batch Loss=0.1915, Avg Loss=0.3337, Time Left=17.94\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1297/3393 [10:05<17:08,  2.04batch/s, Batch Loss=0.1143, Avg Loss=0.3335, Time Left=17.93\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1298/3393 [10:05<17:05,  2.04batch/s, Batch Loss=0.1143, Avg Loss=0.3335, Time Left=17.93\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1298/3393 [10:05<17:05,  2.04batch/s, Batch Loss=0.3364, Avg Loss=0.3335, Time Left=17.92\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1299/3393 [10:05<16:52,  2.07batch/s, Batch Loss=0.3364, Avg Loss=0.3335, Time Left=17.92\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1299/3393 [10:06<16:52,  2.07batch/s, Batch Loss=0.7728, Avg Loss=0.3338, Time Left=17.91\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1300/3393 [10:06<17:04,  2.04batch/s, Batch Loss=0.7728, Avg Loss=0.3338, Time Left=17.91\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1300/3393 [10:06<17:04,  2.04batch/s, Batch Loss=0.0676, Avg Loss=0.3336, Time Left=17.90\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1301/3393 [10:06<16:52,  2.07batch/s, Batch Loss=0.0676, Avg Loss=0.3336, Time Left=17.90\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1301/3393 [10:07<16:52,  2.07batch/s, Batch Loss=0.3011, Avg Loss=0.3336, Time Left=17.89\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1302/3393 [10:07<16:54,  2.06batch/s, Batch Loss=0.3011, Avg Loss=0.3336, Time Left=17.89\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1302/3393 [10:07<16:54,  2.06batch/s, Batch Loss=0.2181, Avg Loss=0.3335, Time Left=17.88\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1303/3393 [10:07<17:03,  2.04batch/s, Batch Loss=0.2181, Avg Loss=0.3335, Time Left=17.88\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1303/3393 [10:08<17:03,  2.04batch/s, Batch Loss=0.0542, Avg Loss=0.3333, Time Left=17.88\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1304/3393 [10:08<16:51,  2.07batch/s, Batch Loss=0.0542, Avg Loss=0.3333, Time Left=17.88\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1304/3393 [10:08<16:51,  2.07batch/s, Batch Loss=0.1702, Avg Loss=0.3331, Time Left=17.87\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1305/3393 [10:08<16:58,  2.05batch/s, Batch Loss=0.1702, Avg Loss=0.3331, Time Left=17.87\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1305/3393 [10:09<16:58,  2.05batch/s, Batch Loss=0.1281, Avg Loss=0.3330, Time Left=17.86\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1306/3393 [10:09<16:52,  2.06batch/s, Batch Loss=0.1281, Avg Loss=0.3330, Time Left=17.86\u001b[A\n",
      "Epoch 1/3 - Training:  38%|▍| 1306/3393 [10:09<16:52,  2.06batch/s, Batch Loss=0.2879, Avg Loss=0.3329, Time Left=17.85\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1307/3393 [10:09<16:40,  2.09batch/s, Batch Loss=0.2879, Avg Loss=0.3329, Time Left=17.85\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1307/3393 [10:10<16:40,  2.09batch/s, Batch Loss=0.2477, Avg Loss=0.3329, Time Left=17.84\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1308/3393 [10:10<16:44,  2.08batch/s, Batch Loss=0.2477, Avg Loss=0.3329, Time Left=17.84\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1308/3393 [10:10<16:44,  2.08batch/s, Batch Loss=0.5083, Avg Loss=0.3330, Time Left=17.83\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1309/3393 [10:10<16:46,  2.07batch/s, Batch Loss=0.5083, Avg Loss=0.3330, Time Left=17.83\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1309/3393 [10:11<16:46,  2.07batch/s, Batch Loss=0.1570, Avg Loss=0.3329, Time Left=17.82\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1310/3393 [10:11<16:48,  2.07batch/s, Batch Loss=0.1570, Avg Loss=0.3329, Time Left=17.82\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1310/3393 [10:11<16:48,  2.07batch/s, Batch Loss=0.0654, Avg Loss=0.3327, Time Left=17.82\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1311/3393 [10:11<17:09,  2.02batch/s, Batch Loss=0.0654, Avg Loss=0.3327, Time Left=17.82\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1311/3393 [10:12<17:09,  2.02batch/s, Batch Loss=0.2531, Avg Loss=0.3326, Time Left=17.81\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1312/3393 [10:12<17:03,  2.03batch/s, Batch Loss=0.2531, Avg Loss=0.3326, Time Left=17.81\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1312/3393 [10:12<17:03,  2.03batch/s, Batch Loss=0.4817, Avg Loss=0.3327, Time Left=17.80\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1313/3393 [10:12<16:49,  2.06batch/s, Batch Loss=0.4817, Avg Loss=0.3327, Time Left=17.80\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1313/3393 [10:13<16:49,  2.06batch/s, Batch Loss=0.2572, Avg Loss=0.3327, Time Left=17.79\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1314/3393 [10:13<16:50,  2.06batch/s, Batch Loss=0.2572, Avg Loss=0.3327, Time Left=17.79\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1314/3393 [10:13<16:50,  2.06batch/s, Batch Loss=0.1170, Avg Loss=0.3325, Time Left=17.78\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1315/3393 [10:13<16:49,  2.06batch/s, Batch Loss=0.1170, Avg Loss=0.3325, Time Left=17.78\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1315/3393 [10:14<16:49,  2.06batch/s, Batch Loss=0.3499, Avg Loss=0.3325, Time Left=17.77\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1316/3393 [10:14<17:09,  2.02batch/s, Batch Loss=0.3499, Avg Loss=0.3325, Time Left=17.77\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1316/3393 [10:14<17:09,  2.02batch/s, Batch Loss=0.3494, Avg Loss=0.3325, Time Left=17.77\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1317/3393 [10:14<16:49,  2.06batch/s, Batch Loss=0.3494, Avg Loss=0.3325, Time Left=17.77\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1317/3393 [10:15<16:49,  2.06batch/s, Batch Loss=0.2969, Avg Loss=0.3325, Time Left=17.76\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  39%|▍| 1318/3393 [10:15<16:34,  2.09batch/s, Batch Loss=0.2969, Avg Loss=0.3325, Time Left=17.76\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1318/3393 [10:15<16:34,  2.09batch/s, Batch Loss=0.1715, Avg Loss=0.3323, Time Left=17.75\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1319/3393 [10:15<16:57,  2.04batch/s, Batch Loss=0.1715, Avg Loss=0.3323, Time Left=17.75\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1319/3393 [10:16<16:57,  2.04batch/s, Batch Loss=0.2609, Avg Loss=0.3323, Time Left=17.74\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1320/3393 [10:16<16:54,  2.04batch/s, Batch Loss=0.2609, Avg Loss=0.3323, Time Left=17.74\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1320/3393 [10:16<16:54,  2.04batch/s, Batch Loss=0.0984, Avg Loss=0.3321, Time Left=17.73\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1321/3393 [10:16<17:11,  2.01batch/s, Batch Loss=0.0984, Avg Loss=0.3321, Time Left=17.73\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1321/3393 [10:17<17:11,  2.01batch/s, Batch Loss=0.0926, Avg Loss=0.3319, Time Left=17.72\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1322/3393 [10:17<16:54,  2.04batch/s, Batch Loss=0.0926, Avg Loss=0.3319, Time Left=17.72\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1322/3393 [10:17<16:54,  2.04batch/s, Batch Loss=0.1030, Avg Loss=0.3317, Time Left=17.72\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1323/3393 [10:17<16:51,  2.05batch/s, Batch Loss=0.1030, Avg Loss=0.3317, Time Left=17.72\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1323/3393 [10:18<16:51,  2.05batch/s, Batch Loss=0.1675, Avg Loss=0.3316, Time Left=17.71\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1324/3393 [10:18<16:59,  2.03batch/s, Batch Loss=0.1675, Avg Loss=0.3316, Time Left=17.71\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1324/3393 [10:18<16:59,  2.03batch/s, Batch Loss=0.1246, Avg Loss=0.3314, Time Left=17.70\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1325/3393 [10:18<16:43,  2.06batch/s, Batch Loss=0.1246, Avg Loss=0.3314, Time Left=17.70\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1325/3393 [10:19<16:43,  2.06batch/s, Batch Loss=0.2360, Avg Loss=0.3314, Time Left=17.69\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1326/3393 [10:19<16:52,  2.04batch/s, Batch Loss=0.2360, Avg Loss=0.3314, Time Left=17.69\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1326/3393 [10:19<16:52,  2.04batch/s, Batch Loss=0.4140, Avg Loss=0.3314, Time Left=17.68\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1327/3393 [10:19<16:44,  2.06batch/s, Batch Loss=0.4140, Avg Loss=0.3314, Time Left=17.68\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1327/3393 [10:20<16:44,  2.06batch/s, Batch Loss=0.3832, Avg Loss=0.3315, Time Left=17.67\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1328/3393 [10:20<16:52,  2.04batch/s, Batch Loss=0.3832, Avg Loss=0.3315, Time Left=17.67\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1328/3393 [10:20<16:52,  2.04batch/s, Batch Loss=0.1424, Avg Loss=0.3313, Time Left=17.67\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1329/3393 [10:20<17:11,  2.00batch/s, Batch Loss=0.1424, Avg Loss=0.3313, Time Left=17.67\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1329/3393 [10:21<17:11,  2.00batch/s, Batch Loss=0.1830, Avg Loss=0.3312, Time Left=17.66\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1330/3393 [10:21<16:40,  2.06batch/s, Batch Loss=0.1830, Avg Loss=0.3312, Time Left=17.66\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1330/3393 [10:21<16:40,  2.06batch/s, Batch Loss=0.1132, Avg Loss=0.3310, Time Left=17.65\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1331/3393 [10:21<16:50,  2.04batch/s, Batch Loss=0.1132, Avg Loss=0.3310, Time Left=17.65\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1331/3393 [10:22<16:50,  2.04batch/s, Batch Loss=0.0746, Avg Loss=0.3308, Time Left=17.64\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1332/3393 [10:22<16:38,  2.06batch/s, Batch Loss=0.0746, Avg Loss=0.3308, Time Left=17.64\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1332/3393 [10:22<16:38,  2.06batch/s, Batch Loss=0.1463, Avg Loss=0.3307, Time Left=17.63\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1333/3393 [10:22<16:29,  2.08batch/s, Batch Loss=0.1463, Avg Loss=0.3307, Time Left=17.63\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1333/3393 [10:22<16:29,  2.08batch/s, Batch Loss=0.3977, Avg Loss=0.3307, Time Left=17.62\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1334/3393 [10:22<16:33,  2.07batch/s, Batch Loss=0.3977, Avg Loss=0.3307, Time Left=17.62\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1334/3393 [10:23<16:33,  2.07batch/s, Batch Loss=0.2226, Avg Loss=0.3306, Time Left=17.61\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1335/3393 [10:23<16:43,  2.05batch/s, Batch Loss=0.2226, Avg Loss=0.3306, Time Left=17.61\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1335/3393 [10:23<16:43,  2.05batch/s, Batch Loss=0.2115, Avg Loss=0.3305, Time Left=17.60\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1336/3393 [10:23<16:33,  2.07batch/s, Batch Loss=0.2115, Avg Loss=0.3305, Time Left=17.60\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1336/3393 [10:24<16:33,  2.07batch/s, Batch Loss=0.3307, Avg Loss=0.3305, Time Left=17.60\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1337/3393 [10:24<17:04,  2.01batch/s, Batch Loss=0.3307, Avg Loss=0.3305, Time Left=17.60\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1337/3393 [10:24<17:04,  2.01batch/s, Batch Loss=0.0507, Avg Loss=0.3303, Time Left=17.59\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1338/3393 [10:24<16:56,  2.02batch/s, Batch Loss=0.0507, Avg Loss=0.3303, Time Left=17.59\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1338/3393 [10:25<16:56,  2.02batch/s, Batch Loss=0.3500, Avg Loss=0.3303, Time Left=17.58\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1339/3393 [10:25<16:41,  2.05batch/s, Batch Loss=0.3500, Avg Loss=0.3303, Time Left=17.58\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1339/3393 [10:25<16:41,  2.05batch/s, Batch Loss=0.2893, Avg Loss=0.3303, Time Left=17.57\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1340/3393 [10:25<16:31,  2.07batch/s, Batch Loss=0.2893, Avg Loss=0.3303, Time Left=17.57\u001b[A\n",
      "Epoch 1/3 - Training:  39%|▍| 1340/3393 [10:26<16:31,  2.07batch/s, Batch Loss=0.2055, Avg Loss=0.3302, Time Left=17.56\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1341/3393 [10:26<16:32,  2.07batch/s, Batch Loss=0.2055, Avg Loss=0.3302, Time Left=17.56\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1341/3393 [10:26<16:32,  2.07batch/s, Batch Loss=0.1206, Avg Loss=0.3300, Time Left=17.55\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1342/3393 [10:26<16:26,  2.08batch/s, Batch Loss=0.1206, Avg Loss=0.3300, Time Left=17.55\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1342/3393 [10:27<16:26,  2.08batch/s, Batch Loss=0.1684, Avg Loss=0.3299, Time Left=17.55\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1343/3393 [10:27<16:27,  2.08batch/s, Batch Loss=0.1684, Avg Loss=0.3299, Time Left=17.55\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1343/3393 [10:27<16:27,  2.08batch/s, Batch Loss=0.0795, Avg Loss=0.3297, Time Left=17.54\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1344/3393 [10:27<16:29,  2.07batch/s, Batch Loss=0.0795, Avg Loss=0.3297, Time Left=17.54\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1344/3393 [10:28<16:29,  2.07batch/s, Batch Loss=0.1515, Avg Loss=0.3296, Time Left=17.53\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1345/3393 [10:28<16:58,  2.01batch/s, Batch Loss=0.1515, Avg Loss=0.3296, Time Left=17.53\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1345/3393 [10:28<16:58,  2.01batch/s, Batch Loss=0.1541, Avg Loss=0.3294, Time Left=17.52\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1346/3393 [10:28<16:42,  2.04batch/s, Batch Loss=0.1541, Avg Loss=0.3294, Time Left=17.52\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1346/3393 [10:29<16:42,  2.04batch/s, Batch Loss=0.1590, Avg Loss=0.3293, Time Left=17.51\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1347/3393 [10:29<17:08,  1.99batch/s, Batch Loss=0.1590, Avg Loss=0.3293, Time Left=17.51\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1347/3393 [10:29<17:08,  1.99batch/s, Batch Loss=0.0720, Avg Loss=0.3291, Time Left=17.50\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1348/3393 [10:29<16:49,  2.03batch/s, Batch Loss=0.0720, Avg Loss=0.3291, Time Left=17.50\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1348/3393 [10:30<16:49,  2.03batch/s, Batch Loss=0.0989, Avg Loss=0.3289, Time Left=17.50\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1349/3393 [10:30<16:43,  2.04batch/s, Batch Loss=0.0989, Avg Loss=0.3289, Time Left=17.50\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1349/3393 [10:30<16:43,  2.04batch/s, Batch Loss=0.2529, Avg Loss=0.3289, Time Left=17.49\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1350/3393 [10:30<16:59,  2.00batch/s, Batch Loss=0.2529, Avg Loss=0.3289, Time Left=17.49\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1350/3393 [10:31<16:59,  2.00batch/s, Batch Loss=0.1741, Avg Loss=0.3287, Time Left=17.48\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  40%|▍| 1351/3393 [10:31<17:01,  2.00batch/s, Batch Loss=0.1741, Avg Loss=0.3287, Time Left=17.48\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1351/3393 [10:31<17:01,  2.00batch/s, Batch Loss=0.2742, Avg Loss=0.3287, Time Left=17.47\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1352/3393 [10:31<17:11,  1.98batch/s, Batch Loss=0.2742, Avg Loss=0.3287, Time Left=17.47\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1352/3393 [10:32<17:11,  1.98batch/s, Batch Loss=0.1605, Avg Loss=0.3286, Time Left=17.46\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1353/3393 [10:32<16:50,  2.02batch/s, Batch Loss=0.1605, Avg Loss=0.3286, Time Left=17.46\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1353/3393 [10:32<16:50,  2.02batch/s, Batch Loss=0.0580, Avg Loss=0.3283, Time Left=17.46\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1354/3393 [10:32<17:03,  1.99batch/s, Batch Loss=0.0580, Avg Loss=0.3283, Time Left=17.46\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1354/3393 [10:33<17:03,  1.99batch/s, Batch Loss=0.1935, Avg Loss=0.3282, Time Left=17.45\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1355/3393 [10:33<17:04,  1.99batch/s, Batch Loss=0.1935, Avg Loss=0.3282, Time Left=17.45\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1355/3393 [10:33<17:04,  1.99batch/s, Batch Loss=0.0443, Avg Loss=0.3280, Time Left=17.44\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1356/3393 [10:33<16:54,  2.01batch/s, Batch Loss=0.0443, Avg Loss=0.3280, Time Left=17.44\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1356/3393 [10:34<16:54,  2.01batch/s, Batch Loss=0.0533, Avg Loss=0.3278, Time Left=17.43\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1357/3393 [10:34<16:36,  2.04batch/s, Batch Loss=0.0533, Avg Loss=0.3278, Time Left=17.43\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1357/3393 [10:34<16:36,  2.04batch/s, Batch Loss=0.1704, Avg Loss=0.3277, Time Left=17.42\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1358/3393 [10:34<16:35,  2.04batch/s, Batch Loss=0.1704, Avg Loss=0.3277, Time Left=17.42\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1358/3393 [10:35<16:35,  2.04batch/s, Batch Loss=0.2913, Avg Loss=0.3277, Time Left=17.41\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1359/3393 [10:35<16:52,  2.01batch/s, Batch Loss=0.2913, Avg Loss=0.3277, Time Left=17.41\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1359/3393 [10:35<16:52,  2.01batch/s, Batch Loss=0.1321, Avg Loss=0.3275, Time Left=17.41\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1360/3393 [10:35<16:52,  2.01batch/s, Batch Loss=0.1321, Avg Loss=0.3275, Time Left=17.41\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1360/3393 [10:36<16:52,  2.01batch/s, Batch Loss=0.0830, Avg Loss=0.3273, Time Left=17.40\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1361/3393 [10:36<17:03,  1.98batch/s, Batch Loss=0.0830, Avg Loss=0.3273, Time Left=17.40\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1361/3393 [10:36<17:03,  1.98batch/s, Batch Loss=0.1250, Avg Loss=0.3272, Time Left=17.39\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1362/3393 [10:36<16:52,  2.01batch/s, Batch Loss=0.1250, Avg Loss=0.3272, Time Left=17.39\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1362/3393 [10:37<16:52,  2.01batch/s, Batch Loss=0.0596, Avg Loss=0.3269, Time Left=17.38\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1363/3393 [10:37<17:13,  1.96batch/s, Batch Loss=0.0596, Avg Loss=0.3269, Time Left=17.38\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1363/3393 [10:37<17:13,  1.96batch/s, Batch Loss=0.2590, Avg Loss=0.3269, Time Left=17.37\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1364/3393 [10:37<16:40,  2.03batch/s, Batch Loss=0.2590, Avg Loss=0.3269, Time Left=17.37\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1364/3393 [10:38<16:40,  2.03batch/s, Batch Loss=0.5618, Avg Loss=0.3271, Time Left=17.37\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1365/3393 [10:38<16:35,  2.04batch/s, Batch Loss=0.5618, Avg Loss=0.3271, Time Left=17.37\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1365/3393 [10:38<16:35,  2.04batch/s, Batch Loss=0.1178, Avg Loss=0.3269, Time Left=17.36\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1366/3393 [10:38<16:42,  2.02batch/s, Batch Loss=0.1178, Avg Loss=0.3269, Time Left=17.36\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1366/3393 [10:39<16:42,  2.02batch/s, Batch Loss=0.6078, Avg Loss=0.3271, Time Left=17.35\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1367/3393 [10:39<16:46,  2.01batch/s, Batch Loss=0.6078, Avg Loss=0.3271, Time Left=17.35\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1367/3393 [10:39<16:46,  2.01batch/s, Batch Loss=0.1004, Avg Loss=0.3270, Time Left=17.34\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1368/3393 [10:39<16:58,  1.99batch/s, Batch Loss=0.1004, Avg Loss=0.3270, Time Left=17.34\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1368/3393 [10:40<16:58,  1.99batch/s, Batch Loss=0.0833, Avg Loss=0.3268, Time Left=17.33\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1369/3393 [10:40<16:57,  1.99batch/s, Batch Loss=0.0833, Avg Loss=0.3268, Time Left=17.33\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1369/3393 [10:40<16:57,  1.99batch/s, Batch Loss=0.5256, Avg Loss=0.3269, Time Left=17.32\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1370/3393 [10:40<16:37,  2.03batch/s, Batch Loss=0.5256, Avg Loss=0.3269, Time Left=17.32\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1370/3393 [10:41<16:37,  2.03batch/s, Batch Loss=0.2479, Avg Loss=0.3269, Time Left=17.32\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1371/3393 [10:41<16:24,  2.05batch/s, Batch Loss=0.2479, Avg Loss=0.3269, Time Left=17.32\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1371/3393 [10:41<16:24,  2.05batch/s, Batch Loss=0.0899, Avg Loss=0.3267, Time Left=17.31\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1372/3393 [10:41<16:13,  2.08batch/s, Batch Loss=0.0899, Avg Loss=0.3267, Time Left=17.31\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1372/3393 [10:42<16:13,  2.08batch/s, Batch Loss=0.1288, Avg Loss=0.3265, Time Left=17.30\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1373/3393 [10:42<16:34,  2.03batch/s, Batch Loss=0.1288, Avg Loss=0.3265, Time Left=17.30\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1373/3393 [10:42<16:34,  2.03batch/s, Batch Loss=0.1968, Avg Loss=0.3264, Time Left=17.29\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1374/3393 [10:42<16:23,  2.05batch/s, Batch Loss=0.1968, Avg Loss=0.3264, Time Left=17.29\u001b[A\n",
      "Epoch 1/3 - Training:  40%|▍| 1374/3393 [10:43<16:23,  2.05batch/s, Batch Loss=0.2627, Avg Loss=0.3264, Time Left=17.28\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1375/3393 [10:43<16:19,  2.06batch/s, Batch Loss=0.2627, Avg Loss=0.3264, Time Left=17.28\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1375/3393 [10:43<16:19,  2.06batch/s, Batch Loss=0.4579, Avg Loss=0.3265, Time Left=17.27\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1376/3393 [10:43<16:31,  2.03batch/s, Batch Loss=0.4579, Avg Loss=0.3265, Time Left=17.27\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1376/3393 [10:44<16:31,  2.03batch/s, Batch Loss=0.2040, Avg Loss=0.3264, Time Left=17.26\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1377/3393 [10:44<16:26,  2.04batch/s, Batch Loss=0.2040, Avg Loss=0.3264, Time Left=17.26\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1377/3393 [10:44<16:26,  2.04batch/s, Batch Loss=0.1368, Avg Loss=0.3262, Time Left=17.26\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1378/3393 [10:44<16:42,  2.01batch/s, Batch Loss=0.1368, Avg Loss=0.3262, Time Left=17.26\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1378/3393 [10:45<16:42,  2.01batch/s, Batch Loss=0.1980, Avg Loss=0.3261, Time Left=17.25\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1379/3393 [10:45<16:25,  2.04batch/s, Batch Loss=0.1980, Avg Loss=0.3261, Time Left=17.25\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1379/3393 [10:45<16:25,  2.04batch/s, Batch Loss=0.3442, Avg Loss=0.3262, Time Left=17.24\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1380/3393 [10:45<16:23,  2.05batch/s, Batch Loss=0.3442, Avg Loss=0.3262, Time Left=17.24\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1380/3393 [10:46<16:23,  2.05batch/s, Batch Loss=0.2845, Avg Loss=0.3261, Time Left=17.23\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1381/3393 [10:46<16:31,  2.03batch/s, Batch Loss=0.2845, Avg Loss=0.3261, Time Left=17.23\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1381/3393 [10:46<16:31,  2.03batch/s, Batch Loss=0.1347, Avg Loss=0.3260, Time Left=17.22\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1382/3393 [10:46<16:36,  2.02batch/s, Batch Loss=0.1347, Avg Loss=0.3260, Time Left=17.22\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1382/3393 [10:47<16:36,  2.02batch/s, Batch Loss=0.0700, Avg Loss=0.3258, Time Left=17.22\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1383/3393 [10:47<16:49,  1.99batch/s, Batch Loss=0.0700, Avg Loss=0.3258, Time Left=17.22\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1383/3393 [10:47<16:49,  1.99batch/s, Batch Loss=0.1724, Avg Loss=0.3257, Time Left=17.21\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  41%|▍| 1384/3393 [10:47<16:40,  2.01batch/s, Batch Loss=0.1724, Avg Loss=0.3257, Time Left=17.21\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1384/3393 [10:48<16:40,  2.01batch/s, Batch Loss=0.2026, Avg Loss=0.3256, Time Left=17.20\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1385/3393 [10:48<16:33,  2.02batch/s, Batch Loss=0.2026, Avg Loss=0.3256, Time Left=17.20\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1385/3393 [10:48<16:33,  2.02batch/s, Batch Loss=0.0813, Avg Loss=0.3254, Time Left=17.19\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1386/3393 [10:48<16:25,  2.04batch/s, Batch Loss=0.0813, Avg Loss=0.3254, Time Left=17.19\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1386/3393 [10:49<16:25,  2.04batch/s, Batch Loss=0.2471, Avg Loss=0.3253, Time Left=17.18\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1387/3393 [10:49<16:15,  2.06batch/s, Batch Loss=0.2471, Avg Loss=0.3253, Time Left=17.18\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1387/3393 [10:49<16:15,  2.06batch/s, Batch Loss=0.1655, Avg Loss=0.3252, Time Left=17.17\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1388/3393 [10:49<16:29,  2.03batch/s, Batch Loss=0.1655, Avg Loss=0.3252, Time Left=17.17\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1388/3393 [10:50<16:29,  2.03batch/s, Batch Loss=0.2047, Avg Loss=0.3251, Time Left=17.17\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1389/3393 [10:50<16:20,  2.04batch/s, Batch Loss=0.2047, Avg Loss=0.3251, Time Left=17.17\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1389/3393 [10:50<16:20,  2.04batch/s, Batch Loss=0.1842, Avg Loss=0.3250, Time Left=17.16\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1390/3393 [10:50<16:28,  2.03batch/s, Batch Loss=0.1842, Avg Loss=0.3250, Time Left=17.16\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1390/3393 [10:51<16:28,  2.03batch/s, Batch Loss=0.1822, Avg Loss=0.3249, Time Left=17.15\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1391/3393 [10:51<16:39,  2.00batch/s, Batch Loss=0.1822, Avg Loss=0.3249, Time Left=17.15\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1391/3393 [10:51<16:39,  2.00batch/s, Batch Loss=0.0684, Avg Loss=0.3247, Time Left=17.14\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1392/3393 [10:51<16:31,  2.02batch/s, Batch Loss=0.0684, Avg Loss=0.3247, Time Left=17.14\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1392/3393 [10:52<16:31,  2.02batch/s, Batch Loss=0.2039, Avg Loss=0.3246, Time Left=17.13\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1393/3393 [10:52<16:43,  1.99batch/s, Batch Loss=0.2039, Avg Loss=0.3246, Time Left=17.13\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1393/3393 [10:52<16:43,  1.99batch/s, Batch Loss=0.2323, Avg Loss=0.3245, Time Left=17.12\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1394/3393 [10:52<16:34,  2.01batch/s, Batch Loss=0.2323, Avg Loss=0.3245, Time Left=17.12\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1394/3393 [10:53<16:34,  2.01batch/s, Batch Loss=0.3279, Avg Loss=0.3245, Time Left=17.12\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1395/3393 [10:53<16:43,  1.99batch/s, Batch Loss=0.3279, Avg Loss=0.3245, Time Left=17.12\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1395/3393 [10:53<16:43,  1.99batch/s, Batch Loss=0.1263, Avg Loss=0.3244, Time Left=17.11\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1396/3393 [10:53<16:27,  2.02batch/s, Batch Loss=0.1263, Avg Loss=0.3244, Time Left=17.11\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1396/3393 [10:54<16:27,  2.02batch/s, Batch Loss=0.2237, Avg Loss=0.3243, Time Left=17.10\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1397/3393 [10:54<16:22,  2.03batch/s, Batch Loss=0.2237, Avg Loss=0.3243, Time Left=17.10\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1397/3393 [10:54<16:22,  2.03batch/s, Batch Loss=0.4276, Avg Loss=0.3244, Time Left=17.09\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1398/3393 [10:54<16:27,  2.02batch/s, Batch Loss=0.4276, Avg Loss=0.3244, Time Left=17.09\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1398/3393 [10:55<16:27,  2.02batch/s, Batch Loss=0.1674, Avg Loss=0.3243, Time Left=17.08\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1399/3393 [10:55<16:13,  2.05batch/s, Batch Loss=0.1674, Avg Loss=0.3243, Time Left=17.08\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1399/3393 [10:55<16:13,  2.05batch/s, Batch Loss=0.2035, Avg Loss=0.3242, Time Left=17.08\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1400/3393 [10:55<16:28,  2.02batch/s, Batch Loss=0.2035, Avg Loss=0.3242, Time Left=17.08\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1400/3393 [10:56<16:28,  2.02batch/s, Batch Loss=0.2406, Avg Loss=0.3241, Time Left=17.07\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1401/3393 [10:56<16:23,  2.03batch/s, Batch Loss=0.2406, Avg Loss=0.3241, Time Left=17.07\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1401/3393 [10:56<16:23,  2.03batch/s, Batch Loss=0.1613, Avg Loss=0.3240, Time Left=17.06\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1402/3393 [10:56<16:55,  1.96batch/s, Batch Loss=0.1613, Avg Loss=0.3240, Time Left=17.06\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1402/3393 [10:57<16:55,  1.96batch/s, Batch Loss=0.1619, Avg Loss=0.3239, Time Left=17.05\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1403/3393 [10:57<16:51,  1.97batch/s, Batch Loss=0.1619, Avg Loss=0.3239, Time Left=17.05\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1403/3393 [10:57<16:51,  1.97batch/s, Batch Loss=0.2666, Avg Loss=0.3238, Time Left=17.04\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1404/3393 [10:57<16:56,  1.96batch/s, Batch Loss=0.2666, Avg Loss=0.3238, Time Left=17.04\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1404/3393 [10:58<16:56,  1.96batch/s, Batch Loss=0.0130, Avg Loss=0.3236, Time Left=17.04\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1405/3393 [10:58<16:41,  1.98batch/s, Batch Loss=0.0130, Avg Loss=0.3236, Time Left=17.04\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1405/3393 [10:58<16:41,  1.98batch/s, Batch Loss=0.4905, Avg Loss=0.3237, Time Left=17.03\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1406/3393 [10:58<16:21,  2.02batch/s, Batch Loss=0.4905, Avg Loss=0.3237, Time Left=17.03\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1406/3393 [10:59<16:21,  2.02batch/s, Batch Loss=0.1919, Avg Loss=0.3236, Time Left=17.02\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1407/3393 [10:59<16:13,  2.04batch/s, Batch Loss=0.1919, Avg Loss=0.3236, Time Left=17.02\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1407/3393 [10:59<16:13,  2.04batch/s, Batch Loss=0.0976, Avg Loss=0.3235, Time Left=17.01\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1408/3393 [10:59<16:03,  2.06batch/s, Batch Loss=0.0976, Avg Loss=0.3235, Time Left=17.01\u001b[A\n",
      "Epoch 1/3 - Training:  41%|▍| 1408/3393 [11:00<16:03,  2.06batch/s, Batch Loss=0.2391, Avg Loss=0.3234, Time Left=17.00\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1409/3393 [11:00<16:20,  2.02batch/s, Batch Loss=0.2391, Avg Loss=0.3234, Time Left=17.00\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1409/3393 [11:00<16:20,  2.02batch/s, Batch Loss=0.1696, Avg Loss=0.3233, Time Left=16.99\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1410/3393 [11:00<16:07,  2.05batch/s, Batch Loss=0.1696, Avg Loss=0.3233, Time Left=16.99\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1410/3393 [11:01<16:07,  2.05batch/s, Batch Loss=0.1791, Avg Loss=0.3232, Time Left=16.98\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1411/3393 [11:01<15:57,  2.07batch/s, Batch Loss=0.1791, Avg Loss=0.3232, Time Left=16.98\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1411/3393 [11:01<15:57,  2.07batch/s, Batch Loss=0.2140, Avg Loss=0.3231, Time Left=16.98\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1412/3393 [11:01<16:07,  2.05batch/s, Batch Loss=0.2140, Avg Loss=0.3231, Time Left=16.98\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1412/3393 [11:02<16:07,  2.05batch/s, Batch Loss=0.2524, Avg Loss=0.3230, Time Left=16.97\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1413/3393 [11:02<15:56,  2.07batch/s, Batch Loss=0.2524, Avg Loss=0.3230, Time Left=16.97\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1413/3393 [11:02<15:56,  2.07batch/s, Batch Loss=0.0956, Avg Loss=0.3229, Time Left=16.96\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1414/3393 [11:02<15:58,  2.06batch/s, Batch Loss=0.0956, Avg Loss=0.3229, Time Left=16.96\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1414/3393 [11:03<15:58,  2.06batch/s, Batch Loss=0.1035, Avg Loss=0.3227, Time Left=16.95\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1415/3393 [11:03<16:17,  2.02batch/s, Batch Loss=0.1035, Avg Loss=0.3227, Time Left=16.95\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1415/3393 [11:03<16:17,  2.02batch/s, Batch Loss=0.4168, Avg Loss=0.3228, Time Left=16.94\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1416/3393 [11:03<16:15,  2.03batch/s, Batch Loss=0.4168, Avg Loss=0.3228, Time Left=16.94\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1416/3393 [11:04<16:15,  2.03batch/s, Batch Loss=0.2107, Avg Loss=0.3227, Time Left=16.93\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  42%|▍| 1417/3393 [11:04<16:17,  2.02batch/s, Batch Loss=0.2107, Avg Loss=0.3227, Time Left=16.93\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1417/3393 [11:04<16:17,  2.02batch/s, Batch Loss=0.1334, Avg Loss=0.3225, Time Left=16.93\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1418/3393 [11:04<16:24,  2.01batch/s, Batch Loss=0.1334, Avg Loss=0.3225, Time Left=16.93\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1418/3393 [11:04<16:24,  2.01batch/s, Batch Loss=0.0562, Avg Loss=0.3223, Time Left=16.92\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1419/3393 [11:04<16:05,  2.04batch/s, Batch Loss=0.0562, Avg Loss=0.3223, Time Left=16.92\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1419/3393 [11:05<16:05,  2.04batch/s, Batch Loss=0.1331, Avg Loss=0.3222, Time Left=16.91\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1420/3393 [11:05<15:53,  2.07batch/s, Batch Loss=0.1331, Avg Loss=0.3222, Time Left=16.91\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1420/3393 [11:05<15:53,  2.07batch/s, Batch Loss=0.1141, Avg Loss=0.3220, Time Left=16.90\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1421/3393 [11:05<16:04,  2.05batch/s, Batch Loss=0.1141, Avg Loss=0.3220, Time Left=16.90\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1421/3393 [11:06<16:04,  2.05batch/s, Batch Loss=0.0964, Avg Loss=0.3219, Time Left=16.89\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1422/3393 [11:06<16:11,  2.03batch/s, Batch Loss=0.0964, Avg Loss=0.3219, Time Left=16.89\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1422/3393 [11:06<16:11,  2.03batch/s, Batch Loss=0.2253, Avg Loss=0.3218, Time Left=16.88\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1423/3393 [11:06<16:15,  2.02batch/s, Batch Loss=0.2253, Avg Loss=0.3218, Time Left=16.88\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1423/3393 [11:07<16:15,  2.02batch/s, Batch Loss=0.2144, Avg Loss=0.3217, Time Left=16.88\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1424/3393 [11:07<16:19,  2.01batch/s, Batch Loss=0.2144, Avg Loss=0.3217, Time Left=16.88\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1424/3393 [11:07<16:19,  2.01batch/s, Batch Loss=0.1425, Avg Loss=0.3216, Time Left=16.87\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1425/3393 [11:07<16:03,  2.04batch/s, Batch Loss=0.1425, Avg Loss=0.3216, Time Left=16.87\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1425/3393 [11:08<16:03,  2.04batch/s, Batch Loss=0.2098, Avg Loss=0.3215, Time Left=16.86\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1426/3393 [11:08<16:01,  2.05batch/s, Batch Loss=0.2098, Avg Loss=0.3215, Time Left=16.86\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1426/3393 [11:08<16:01,  2.05batch/s, Batch Loss=0.1131, Avg Loss=0.3214, Time Left=16.85\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1427/3393 [11:08<16:27,  1.99batch/s, Batch Loss=0.1131, Avg Loss=0.3214, Time Left=16.85\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1427/3393 [11:09<16:27,  1.99batch/s, Batch Loss=0.1225, Avg Loss=0.3212, Time Left=16.84\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1428/3393 [11:09<16:17,  2.01batch/s, Batch Loss=0.1225, Avg Loss=0.3212, Time Left=16.84\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1428/3393 [11:09<16:17,  2.01batch/s, Batch Loss=0.2113, Avg Loss=0.3211, Time Left=16.84\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1429/3393 [11:09<16:21,  2.00batch/s, Batch Loss=0.2113, Avg Loss=0.3211, Time Left=16.84\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1429/3393 [11:10<16:21,  2.00batch/s, Batch Loss=0.3499, Avg Loss=0.3212, Time Left=16.83\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1430/3393 [11:10<16:21,  2.00batch/s, Batch Loss=0.3499, Avg Loss=0.3212, Time Left=16.83\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1430/3393 [11:10<16:21,  2.00batch/s, Batch Loss=0.5806, Avg Loss=0.3213, Time Left=16.82\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1431/3393 [11:10<16:12,  2.02batch/s, Batch Loss=0.5806, Avg Loss=0.3213, Time Left=16.82\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1431/3393 [11:11<16:12,  2.02batch/s, Batch Loss=0.2867, Avg Loss=0.3213, Time Left=16.81\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1432/3393 [11:11<16:00,  2.04batch/s, Batch Loss=0.2867, Avg Loss=0.3213, Time Left=16.81\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1432/3393 [11:11<16:00,  2.04batch/s, Batch Loss=0.3224, Avg Loss=0.3213, Time Left=16.80\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1433/3393 [11:11<15:46,  2.07batch/s, Batch Loss=0.3224, Avg Loss=0.3213, Time Left=16.80\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1433/3393 [11:12<15:46,  2.07batch/s, Batch Loss=0.2612, Avg Loss=0.3213, Time Left=16.79\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1434/3393 [11:12<15:46,  2.07batch/s, Batch Loss=0.2612, Avg Loss=0.3213, Time Left=16.79\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1434/3393 [11:12<15:46,  2.07batch/s, Batch Loss=0.1682, Avg Loss=0.3212, Time Left=16.78\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1435/3393 [11:12<15:38,  2.09batch/s, Batch Loss=0.1682, Avg Loss=0.3212, Time Left=16.78\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1435/3393 [11:13<15:38,  2.09batch/s, Batch Loss=0.0989, Avg Loss=0.3210, Time Left=16.77\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1436/3393 [11:13<15:36,  2.09batch/s, Batch Loss=0.0989, Avg Loss=0.3210, Time Left=16.77\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1436/3393 [11:13<15:36,  2.09batch/s, Batch Loss=0.4082, Avg Loss=0.3211, Time Left=16.77\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1437/3393 [11:13<15:28,  2.11batch/s, Batch Loss=0.4082, Avg Loss=0.3211, Time Left=16.77\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1437/3393 [11:14<15:28,  2.11batch/s, Batch Loss=0.1045, Avg Loss=0.3209, Time Left=16.76\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1438/3393 [11:14<15:44,  2.07batch/s, Batch Loss=0.1045, Avg Loss=0.3209, Time Left=16.76\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1438/3393 [11:14<15:44,  2.07batch/s, Batch Loss=0.3532, Avg Loss=0.3209, Time Left=16.75\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1439/3393 [11:14<15:46,  2.06batch/s, Batch Loss=0.3532, Avg Loss=0.3209, Time Left=16.75\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1439/3393 [11:15<15:46,  2.06batch/s, Batch Loss=0.2493, Avg Loss=0.3209, Time Left=16.74\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1440/3393 [11:15<16:05,  2.02batch/s, Batch Loss=0.2493, Avg Loss=0.3209, Time Left=16.74\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1440/3393 [11:15<16:05,  2.02batch/s, Batch Loss=0.3020, Avg Loss=0.3209, Time Left=16.73\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1441/3393 [11:15<16:00,  2.03batch/s, Batch Loss=0.3020, Avg Loss=0.3209, Time Left=16.73\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1441/3393 [11:16<16:00,  2.03batch/s, Batch Loss=0.1423, Avg Loss=0.3207, Time Left=16.72\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1442/3393 [11:16<16:08,  2.01batch/s, Batch Loss=0.1423, Avg Loss=0.3207, Time Left=16.72\u001b[A\n",
      "Epoch 1/3 - Training:  42%|▍| 1442/3393 [11:16<16:08,  2.01batch/s, Batch Loss=0.2682, Avg Loss=0.3207, Time Left=16.72\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1443/3393 [11:16<15:49,  2.05batch/s, Batch Loss=0.2682, Avg Loss=0.3207, Time Left=16.72\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1443/3393 [11:17<15:49,  2.05batch/s, Batch Loss=0.0937, Avg Loss=0.3205, Time Left=16.71\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1444/3393 [11:17<15:50,  2.05batch/s, Batch Loss=0.0937, Avg Loss=0.3205, Time Left=16.71\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1444/3393 [11:17<15:50,  2.05batch/s, Batch Loss=0.1394, Avg Loss=0.3204, Time Left=16.70\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1445/3393 [11:17<16:07,  2.01batch/s, Batch Loss=0.1394, Avg Loss=0.3204, Time Left=16.70\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1445/3393 [11:18<16:07,  2.01batch/s, Batch Loss=0.2633, Avg Loss=0.3204, Time Left=16.69\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1446/3393 [11:18<15:50,  2.05batch/s, Batch Loss=0.2633, Avg Loss=0.3204, Time Left=16.69\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1446/3393 [11:18<15:50,  2.05batch/s, Batch Loss=0.1641, Avg Loss=0.3202, Time Left=16.68\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1447/3393 [11:18<15:51,  2.04batch/s, Batch Loss=0.1641, Avg Loss=0.3202, Time Left=16.68\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1447/3393 [11:19<15:51,  2.04batch/s, Batch Loss=0.2924, Avg Loss=0.3202, Time Left=16.67\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1448/3393 [11:19<15:45,  2.06batch/s, Batch Loss=0.2924, Avg Loss=0.3202, Time Left=16.67\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1448/3393 [11:19<15:45,  2.06batch/s, Batch Loss=0.2875, Avg Loss=0.3202, Time Left=16.67\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1449/3393 [11:19<15:46,  2.05batch/s, Batch Loss=0.2875, Avg Loss=0.3202, Time Left=16.67\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1449/3393 [11:20<15:46,  2.05batch/s, Batch Loss=0.1206, Avg Loss=0.3200, Time Left=16.66\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  43%|▍| 1450/3393 [11:20<16:02,  2.02batch/s, Batch Loss=0.1206, Avg Loss=0.3200, Time Left=16.66\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1450/3393 [11:20<16:02,  2.02batch/s, Batch Loss=0.0803, Avg Loss=0.3199, Time Left=16.65\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1451/3393 [11:20<16:07,  2.01batch/s, Batch Loss=0.0803, Avg Loss=0.3199, Time Left=16.65\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1451/3393 [11:21<16:07,  2.01batch/s, Batch Loss=0.3431, Avg Loss=0.3199, Time Left=16.64\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1452/3393 [11:21<16:26,  1.97batch/s, Batch Loss=0.3431, Avg Loss=0.3199, Time Left=16.64\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1452/3393 [11:21<16:26,  1.97batch/s, Batch Loss=0.2014, Avg Loss=0.3198, Time Left=16.63\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1453/3393 [11:21<16:14,  1.99batch/s, Batch Loss=0.2014, Avg Loss=0.3198, Time Left=16.63\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1453/3393 [11:22<16:14,  1.99batch/s, Batch Loss=0.2011, Avg Loss=0.3197, Time Left=16.63\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1454/3393 [11:22<16:04,  2.01batch/s, Batch Loss=0.2011, Avg Loss=0.3197, Time Left=16.63\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1454/3393 [11:22<16:04,  2.01batch/s, Batch Loss=0.2349, Avg Loss=0.3197, Time Left=16.62\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1455/3393 [11:22<15:58,  2.02batch/s, Batch Loss=0.2349, Avg Loss=0.3197, Time Left=16.62\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1455/3393 [11:23<15:58,  2.02batch/s, Batch Loss=0.1977, Avg Loss=0.3196, Time Left=16.61\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1456/3393 [11:23<16:01,  2.01batch/s, Batch Loss=0.1977, Avg Loss=0.3196, Time Left=16.61\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1456/3393 [11:23<16:01,  2.01batch/s, Batch Loss=0.1379, Avg Loss=0.3194, Time Left=16.60\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1457/3393 [11:23<16:13,  1.99batch/s, Batch Loss=0.1379, Avg Loss=0.3194, Time Left=16.60\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1457/3393 [11:24<16:13,  1.99batch/s, Batch Loss=0.0721, Avg Loss=0.3193, Time Left=16.59\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1458/3393 [11:24<16:12,  1.99batch/s, Batch Loss=0.0721, Avg Loss=0.3193, Time Left=16.59\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1458/3393 [11:24<16:12,  1.99batch/s, Batch Loss=0.1359, Avg Loss=0.3191, Time Left=16.59\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1459/3393 [11:24<16:21,  1.97batch/s, Batch Loss=0.1359, Avg Loss=0.3191, Time Left=16.59\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1459/3393 [11:25<16:21,  1.97batch/s, Batch Loss=0.1046, Avg Loss=0.3190, Time Left=16.58\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1460/3393 [11:25<16:08,  2.00batch/s, Batch Loss=0.1046, Avg Loss=0.3190, Time Left=16.58\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1460/3393 [11:25<16:08,  2.00batch/s, Batch Loss=0.1931, Avg Loss=0.3189, Time Left=16.57\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1461/3393 [11:25<16:17,  1.98batch/s, Batch Loss=0.1931, Avg Loss=0.3189, Time Left=16.57\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1461/3393 [11:26<16:17,  1.98batch/s, Batch Loss=0.2066, Avg Loss=0.3188, Time Left=16.56\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1462/3393 [11:26<15:57,  2.02batch/s, Batch Loss=0.2066, Avg Loss=0.3188, Time Left=16.56\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1462/3393 [11:26<15:57,  2.02batch/s, Batch Loss=0.1153, Avg Loss=0.3187, Time Left=16.55\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1463/3393 [11:26<15:50,  2.03batch/s, Batch Loss=0.1153, Avg Loss=0.3187, Time Left=16.55\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1463/3393 [11:27<15:50,  2.03batch/s, Batch Loss=0.0778, Avg Loss=0.3185, Time Left=16.54\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1464/3393 [11:27<15:54,  2.02batch/s, Batch Loss=0.0778, Avg Loss=0.3185, Time Left=16.54\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1464/3393 [11:27<15:54,  2.02batch/s, Batch Loss=0.5504, Avg Loss=0.3186, Time Left=16.54\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1465/3393 [11:27<15:50,  2.03batch/s, Batch Loss=0.5504, Avg Loss=0.3186, Time Left=16.54\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1465/3393 [11:28<15:50,  2.03batch/s, Batch Loss=0.1046, Avg Loss=0.3185, Time Left=16.53\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1466/3393 [11:28<15:55,  2.02batch/s, Batch Loss=0.1046, Avg Loss=0.3185, Time Left=16.53\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1466/3393 [11:28<15:55,  2.02batch/s, Batch Loss=0.3027, Avg Loss=0.3185, Time Left=16.52\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1467/3393 [11:28<15:57,  2.01batch/s, Batch Loss=0.3027, Avg Loss=0.3185, Time Left=16.52\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1467/3393 [11:29<15:57,  2.01batch/s, Batch Loss=0.1581, Avg Loss=0.3184, Time Left=16.51\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1468/3393 [11:29<16:18,  1.97batch/s, Batch Loss=0.1581, Avg Loss=0.3184, Time Left=16.51\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1468/3393 [11:29<16:18,  1.97batch/s, Batch Loss=0.2230, Avg Loss=0.3183, Time Left=16.50\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1469/3393 [11:29<15:55,  2.01batch/s, Batch Loss=0.2230, Avg Loss=0.3183, Time Left=16.50\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1469/3393 [11:30<15:55,  2.01batch/s, Batch Loss=0.1622, Avg Loss=0.3182, Time Left=16.49\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1470/3393 [11:30<15:59,  2.00batch/s, Batch Loss=0.1622, Avg Loss=0.3182, Time Left=16.49\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1470/3393 [11:30<15:59,  2.00batch/s, Batch Loss=0.1407, Avg Loss=0.3181, Time Left=16.49\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1471/3393 [11:30<15:42,  2.04batch/s, Batch Loss=0.1407, Avg Loss=0.3181, Time Left=16.49\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1471/3393 [11:31<15:42,  2.04batch/s, Batch Loss=0.5202, Avg Loss=0.3182, Time Left=16.48\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1472/3393 [11:31<15:37,  2.05batch/s, Batch Loss=0.5202, Avg Loss=0.3182, Time Left=16.48\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1472/3393 [11:31<15:37,  2.05batch/s, Batch Loss=0.1214, Avg Loss=0.3181, Time Left=16.47\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1473/3393 [11:31<15:55,  2.01batch/s, Batch Loss=0.1214, Avg Loss=0.3181, Time Left=16.47\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1473/3393 [11:32<15:55,  2.01batch/s, Batch Loss=0.1437, Avg Loss=0.3179, Time Left=16.46\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1474/3393 [11:32<15:48,  2.02batch/s, Batch Loss=0.1437, Avg Loss=0.3179, Time Left=16.46\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1474/3393 [11:32<15:48,  2.02batch/s, Batch Loss=0.0875, Avg Loss=0.3178, Time Left=16.45\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1475/3393 [11:32<16:11,  1.97batch/s, Batch Loss=0.0875, Avg Loss=0.3178, Time Left=16.45\u001b[A\n",
      "Epoch 1/3 - Training:  43%|▍| 1475/3393 [11:33<16:11,  1.97batch/s, Batch Loss=0.3944, Avg Loss=0.3178, Time Left=16.45\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1476/3393 [11:33<15:59,  2.00batch/s, Batch Loss=0.3944, Avg Loss=0.3178, Time Left=16.45\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1476/3393 [11:33<15:59,  2.00batch/s, Batch Loss=0.6786, Avg Loss=0.3181, Time Left=16.44\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1477/3393 [11:33<16:17,  1.96batch/s, Batch Loss=0.6786, Avg Loss=0.3181, Time Left=16.44\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1477/3393 [11:34<16:17,  1.96batch/s, Batch Loss=0.0877, Avg Loss=0.3179, Time Left=16.43\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1478/3393 [11:34<16:06,  1.98batch/s, Batch Loss=0.0877, Avg Loss=0.3179, Time Left=16.43\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1478/3393 [11:34<16:06,  1.98batch/s, Batch Loss=0.0760, Avg Loss=0.3177, Time Left=16.42\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1479/3393 [11:34<15:44,  2.03batch/s, Batch Loss=0.0760, Avg Loss=0.3177, Time Left=16.42\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1479/3393 [11:35<15:44,  2.03batch/s, Batch Loss=0.2106, Avg Loss=0.3177, Time Left=16.41\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1480/3393 [11:35<15:39,  2.04batch/s, Batch Loss=0.2106, Avg Loss=0.3177, Time Left=16.41\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1480/3393 [11:35<15:39,  2.04batch/s, Batch Loss=0.2197, Avg Loss=0.3176, Time Left=16.40\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1481/3393 [11:35<15:30,  2.06batch/s, Batch Loss=0.2197, Avg Loss=0.3176, Time Left=16.40\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1481/3393 [11:36<15:30,  2.06batch/s, Batch Loss=0.1630, Avg Loss=0.3175, Time Left=16.40\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1482/3393 [11:36<15:25,  2.06batch/s, Batch Loss=0.1630, Avg Loss=0.3175, Time Left=16.40\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1482/3393 [11:36<15:25,  2.06batch/s, Batch Loss=0.1010, Avg Loss=0.3173, Time Left=16.39\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  44%|▍| 1483/3393 [11:36<15:44,  2.02batch/s, Batch Loss=0.1010, Avg Loss=0.3173, Time Left=16.39\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1483/3393 [11:37<15:44,  2.02batch/s, Batch Loss=0.2582, Avg Loss=0.3173, Time Left=16.38\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1484/3393 [11:37<15:39,  2.03batch/s, Batch Loss=0.2582, Avg Loss=0.3173, Time Left=16.38\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1484/3393 [11:37<15:39,  2.03batch/s, Batch Loss=0.1634, Avg Loss=0.3172, Time Left=16.37\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1485/3393 [11:37<16:02,  1.98batch/s, Batch Loss=0.1634, Avg Loss=0.3172, Time Left=16.37\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1485/3393 [11:38<16:02,  1.98batch/s, Batch Loss=0.2249, Avg Loss=0.3171, Time Left=16.36\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1486/3393 [11:38<15:52,  2.00batch/s, Batch Loss=0.2249, Avg Loss=0.3171, Time Left=16.36\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1486/3393 [11:38<15:52,  2.00batch/s, Batch Loss=0.1100, Avg Loss=0.3170, Time Left=16.36\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1487/3393 [11:38<15:53,  2.00batch/s, Batch Loss=0.1100, Avg Loss=0.3170, Time Left=16.36\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1487/3393 [11:39<15:53,  2.00batch/s, Batch Loss=0.1520, Avg Loss=0.3169, Time Left=16.35\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1488/3393 [11:39<15:56,  1.99batch/s, Batch Loss=0.1520, Avg Loss=0.3169, Time Left=16.35\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1488/3393 [11:39<15:56,  1.99batch/s, Batch Loss=0.0998, Avg Loss=0.3167, Time Left=16.34\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1489/3393 [11:39<16:02,  1.98batch/s, Batch Loss=0.0998, Avg Loss=0.3167, Time Left=16.34\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1489/3393 [11:40<16:02,  1.98batch/s, Batch Loss=0.1074, Avg Loss=0.3166, Time Left=16.33\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1490/3393 [11:40<15:51,  2.00batch/s, Batch Loss=0.1074, Avg Loss=0.3166, Time Left=16.33\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1490/3393 [11:40<15:51,  2.00batch/s, Batch Loss=0.1494, Avg Loss=0.3164, Time Left=16.32\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1491/3393 [11:40<15:42,  2.02batch/s, Batch Loss=0.1494, Avg Loss=0.3164, Time Left=16.32\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1491/3393 [11:41<15:42,  2.02batch/s, Batch Loss=0.1881, Avg Loss=0.3163, Time Left=16.31\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1492/3393 [11:41<15:28,  2.05batch/s, Batch Loss=0.1881, Avg Loss=0.3163, Time Left=16.31\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1492/3393 [11:41<15:28,  2.05batch/s, Batch Loss=0.3950, Avg Loss=0.3164, Time Left=16.31\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1493/3393 [11:41<15:39,  2.02batch/s, Batch Loss=0.3950, Avg Loss=0.3164, Time Left=16.31\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1493/3393 [11:42<15:39,  2.02batch/s, Batch Loss=0.4800, Avg Loss=0.3165, Time Left=16.30\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1494/3393 [11:42<15:49,  2.00batch/s, Batch Loss=0.4800, Avg Loss=0.3165, Time Left=16.30\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1494/3393 [11:42<15:49,  2.00batch/s, Batch Loss=0.1213, Avg Loss=0.3164, Time Left=16.29\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1495/3393 [11:42<15:39,  2.02batch/s, Batch Loss=0.1213, Avg Loss=0.3164, Time Left=16.29\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1495/3393 [11:43<15:39,  2.02batch/s, Batch Loss=0.1063, Avg Loss=0.3162, Time Left=16.28\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1496/3393 [11:43<15:52,  1.99batch/s, Batch Loss=0.1063, Avg Loss=0.3162, Time Left=16.28\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1496/3393 [11:43<15:52,  1.99batch/s, Batch Loss=0.0449, Avg Loss=0.3160, Time Left=16.27\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1497/3393 [11:43<15:55,  1.99batch/s, Batch Loss=0.0449, Avg Loss=0.3160, Time Left=16.27\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1497/3393 [11:44<15:55,  1.99batch/s, Batch Loss=0.4219, Avg Loss=0.3161, Time Left=16.27\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1498/3393 [11:44<15:59,  1.98batch/s, Batch Loss=0.4219, Avg Loss=0.3161, Time Left=16.27\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1498/3393 [11:44<15:59,  1.98batch/s, Batch Loss=0.0273, Avg Loss=0.3159, Time Left=16.26\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1499/3393 [11:44<15:47,  2.00batch/s, Batch Loss=0.0273, Avg Loss=0.3159, Time Left=16.26\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1499/3393 [11:45<15:47,  2.00batch/s, Batch Loss=0.4827, Avg Loss=0.3160, Time Left=16.25\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1500/3393 [11:45<15:47,  2.00batch/s, Batch Loss=0.4827, Avg Loss=0.3160, Time Left=16.25\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1500/3393 [11:45<15:47,  2.00batch/s, Batch Loss=0.1513, Avg Loss=0.3159, Time Left=16.24\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1501/3393 [11:45<15:38,  2.02batch/s, Batch Loss=0.1513, Avg Loss=0.3159, Time Left=16.24\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1501/3393 [11:46<15:38,  2.02batch/s, Batch Loss=0.3651, Avg Loss=0.3159, Time Left=16.23\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1502/3393 [11:46<15:33,  2.03batch/s, Batch Loss=0.3651, Avg Loss=0.3159, Time Left=16.23\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1502/3393 [11:46<15:33,  2.03batch/s, Batch Loss=0.1249, Avg Loss=0.3158, Time Left=16.23\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1503/3393 [11:46<15:55,  1.98batch/s, Batch Loss=0.1249, Avg Loss=0.3158, Time Left=16.23\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1503/3393 [11:47<15:55,  1.98batch/s, Batch Loss=0.1771, Avg Loss=0.3157, Time Left=16.22\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1504/3393 [11:47<15:44,  2.00batch/s, Batch Loss=0.1771, Avg Loss=0.3157, Time Left=16.22\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1504/3393 [11:47<15:44,  2.00batch/s, Batch Loss=0.4009, Avg Loss=0.3158, Time Left=16.21\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1505/3393 [11:47<15:54,  1.98batch/s, Batch Loss=0.4009, Avg Loss=0.3158, Time Left=16.21\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1505/3393 [11:48<15:54,  1.98batch/s, Batch Loss=0.3508, Avg Loss=0.3158, Time Left=16.20\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1506/3393 [11:48<15:34,  2.02batch/s, Batch Loss=0.3508, Avg Loss=0.3158, Time Left=16.20\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1506/3393 [11:48<15:34,  2.02batch/s, Batch Loss=0.1137, Avg Loss=0.3157, Time Left=16.19\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1507/3393 [11:48<15:29,  2.03batch/s, Batch Loss=0.1137, Avg Loss=0.3157, Time Left=16.19\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1507/3393 [11:49<15:29,  2.03batch/s, Batch Loss=0.3936, Avg Loss=0.3157, Time Left=16.18\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1508/3393 [11:49<15:24,  2.04batch/s, Batch Loss=0.3936, Avg Loss=0.3157, Time Left=16.18\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1508/3393 [11:49<15:24,  2.04batch/s, Batch Loss=0.1489, Avg Loss=0.3156, Time Left=16.18\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1509/3393 [11:49<15:36,  2.01batch/s, Batch Loss=0.1489, Avg Loss=0.3156, Time Left=16.18\u001b[A\n",
      "Epoch 1/3 - Training:  44%|▍| 1509/3393 [11:50<15:36,  2.01batch/s, Batch Loss=0.1679, Avg Loss=0.3155, Time Left=16.17\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1510/3393 [11:50<15:37,  2.01batch/s, Batch Loss=0.1679, Avg Loss=0.3155, Time Left=16.17\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1510/3393 [11:50<15:37,  2.01batch/s, Batch Loss=0.1956, Avg Loss=0.3154, Time Left=16.16\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1511/3393 [11:50<15:09,  2.07batch/s, Batch Loss=0.1956, Avg Loss=0.3154, Time Left=16.16\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1511/3393 [11:51<15:09,  2.07batch/s, Batch Loss=0.1417, Avg Loss=0.3153, Time Left=16.15\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1512/3393 [11:51<15:09,  2.07batch/s, Batch Loss=0.1417, Avg Loss=0.3153, Time Left=16.15\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1512/3393 [11:51<15:09,  2.07batch/s, Batch Loss=0.4444, Avg Loss=0.3154, Time Left=16.14\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1513/3393 [11:51<15:37,  2.01batch/s, Batch Loss=0.4444, Avg Loss=0.3154, Time Left=16.14\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1513/3393 [11:52<15:37,  2.01batch/s, Batch Loss=0.4622, Avg Loss=0.3155, Time Left=16.13\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1514/3393 [11:52<15:30,  2.02batch/s, Batch Loss=0.4622, Avg Loss=0.3155, Time Left=16.13\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1514/3393 [11:52<15:30,  2.02batch/s, Batch Loss=0.1788, Avg Loss=0.3154, Time Left=16.13\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1515/3393 [11:52<15:50,  1.98batch/s, Batch Loss=0.1788, Avg Loss=0.3154, Time Left=16.13\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1515/3393 [11:53<15:50,  1.98batch/s, Batch Loss=0.0915, Avg Loss=0.3152, Time Left=16.12\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  45%|▍| 1516/3393 [11:53<15:39,  2.00batch/s, Batch Loss=0.0915, Avg Loss=0.3152, Time Left=16.12\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1516/3393 [11:53<15:39,  2.00batch/s, Batch Loss=0.1119, Avg Loss=0.3151, Time Left=16.11\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1517/3393 [11:53<15:57,  1.96batch/s, Batch Loss=0.1119, Avg Loss=0.3151, Time Left=16.11\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1517/3393 [11:54<15:57,  1.96batch/s, Batch Loss=0.1891, Avg Loss=0.3150, Time Left=16.10\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1518/3393 [11:54<15:43,  1.99batch/s, Batch Loss=0.1891, Avg Loss=0.3150, Time Left=16.10\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1518/3393 [11:54<15:43,  1.99batch/s, Batch Loss=0.1898, Avg Loss=0.3149, Time Left=16.09\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1519/3393 [11:54<15:51,  1.97batch/s, Batch Loss=0.1898, Avg Loss=0.3149, Time Left=16.09\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1519/3393 [11:55<15:51,  1.97batch/s, Batch Loss=0.3080, Avg Loss=0.3149, Time Left=16.09\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1520/3393 [11:55<15:48,  1.98batch/s, Batch Loss=0.3080, Avg Loss=0.3149, Time Left=16.09\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1520/3393 [11:55<15:48,  1.98batch/s, Batch Loss=0.2540, Avg Loss=0.3149, Time Left=16.08\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1521/3393 [11:55<15:54,  1.96batch/s, Batch Loss=0.2540, Avg Loss=0.3149, Time Left=16.08\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1521/3393 [11:56<15:54,  1.96batch/s, Batch Loss=0.1190, Avg Loss=0.3147, Time Left=16.07\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1522/3393 [11:56<15:49,  1.97batch/s, Batch Loss=0.1190, Avg Loss=0.3147, Time Left=16.07\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1522/3393 [11:56<15:49,  1.97batch/s, Batch Loss=0.2179, Avg Loss=0.3147, Time Left=16.06\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1523/3393 [11:56<15:54,  1.96batch/s, Batch Loss=0.2179, Avg Loss=0.3147, Time Left=16.06\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1523/3393 [11:57<15:54,  1.96batch/s, Batch Loss=0.3354, Avg Loss=0.3147, Time Left=16.05\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1524/3393 [11:57<15:23,  2.02batch/s, Batch Loss=0.3354, Avg Loss=0.3147, Time Left=16.05\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1524/3393 [11:57<15:23,  2.02batch/s, Batch Loss=0.1653, Avg Loss=0.3146, Time Left=16.05\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1525/3393 [11:57<15:18,  2.03batch/s, Batch Loss=0.1653, Avg Loss=0.3146, Time Left=16.05\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1525/3393 [11:58<15:18,  2.03batch/s, Batch Loss=0.1417, Avg Loss=0.3145, Time Left=16.04\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1526/3393 [11:58<15:56,  1.95batch/s, Batch Loss=0.1417, Avg Loss=0.3145, Time Left=16.04\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1526/3393 [11:58<15:56,  1.95batch/s, Batch Loss=0.1163, Avg Loss=0.3143, Time Left=16.03\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1527/3393 [11:58<15:44,  1.98batch/s, Batch Loss=0.1163, Avg Loss=0.3143, Time Left=16.03\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1527/3393 [11:59<15:44,  1.98batch/s, Batch Loss=0.3057, Avg Loss=0.3143, Time Left=16.02\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1528/3393 [11:59<15:32,  2.00batch/s, Batch Loss=0.3057, Avg Loss=0.3143, Time Left=16.02\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1528/3393 [11:59<15:32,  2.00batch/s, Batch Loss=0.0522, Avg Loss=0.3141, Time Left=16.01\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1529/3393 [11:59<15:45,  1.97batch/s, Batch Loss=0.0522, Avg Loss=0.3141, Time Left=16.01\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1529/3393 [12:00<15:45,  1.97batch/s, Batch Loss=0.1104, Avg Loss=0.3140, Time Left=16.01\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1530/3393 [12:00<16:05,  1.93batch/s, Batch Loss=0.1104, Avg Loss=0.3140, Time Left=16.01\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1530/3393 [12:00<16:05,  1.93batch/s, Batch Loss=0.1678, Avg Loss=0.3139, Time Left=16.00\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1531/3393 [12:00<16:04,  1.93batch/s, Batch Loss=0.1678, Avg Loss=0.3139, Time Left=16.00\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1531/3393 [12:01<16:04,  1.93batch/s, Batch Loss=0.3862, Avg Loss=0.3139, Time Left=15.99\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1532/3393 [12:01<15:44,  1.97batch/s, Batch Loss=0.3862, Avg Loss=0.3139, Time Left=15.99\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1532/3393 [12:01<15:44,  1.97batch/s, Batch Loss=0.1730, Avg Loss=0.3138, Time Left=15.98\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1533/3393 [12:01<15:24,  2.01batch/s, Batch Loss=0.1730, Avg Loss=0.3138, Time Left=15.98\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1533/3393 [12:02<15:24,  2.01batch/s, Batch Loss=0.0138, Avg Loss=0.3136, Time Left=15.97\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1534/3393 [12:02<15:35,  1.99batch/s, Batch Loss=0.0138, Avg Loss=0.3136, Time Left=15.97\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1534/3393 [12:02<15:35,  1.99batch/s, Batch Loss=0.1526, Avg Loss=0.3135, Time Left=15.97\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1535/3393 [12:02<15:35,  1.99batch/s, Batch Loss=0.1526, Avg Loss=0.3135, Time Left=15.97\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1535/3393 [12:03<15:35,  1.99batch/s, Batch Loss=0.2949, Avg Loss=0.3135, Time Left=15.96\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1536/3393 [12:03<15:42,  1.97batch/s, Batch Loss=0.2949, Avg Loss=0.3135, Time Left=15.96\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1536/3393 [12:03<15:42,  1.97batch/s, Batch Loss=0.0655, Avg Loss=0.3134, Time Left=15.95\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1537/3393 [12:03<15:22,  2.01batch/s, Batch Loss=0.0655, Avg Loss=0.3134, Time Left=15.95\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1537/3393 [12:04<15:22,  2.01batch/s, Batch Loss=0.1580, Avg Loss=0.3132, Time Left=15.94\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1538/3393 [12:04<15:23,  2.01batch/s, Batch Loss=0.1580, Avg Loss=0.3132, Time Left=15.94\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1538/3393 [12:04<15:23,  2.01batch/s, Batch Loss=0.2233, Avg Loss=0.3132, Time Left=15.93\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1539/3393 [12:04<15:16,  2.02batch/s, Batch Loss=0.2233, Avg Loss=0.3132, Time Left=15.93\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1539/3393 [12:05<15:16,  2.02batch/s, Batch Loss=0.1725, Avg Loss=0.3131, Time Left=15.92\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1540/3393 [12:05<15:11,  2.03batch/s, Batch Loss=0.1725, Avg Loss=0.3131, Time Left=15.92\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1540/3393 [12:05<15:11,  2.03batch/s, Batch Loss=0.0417, Avg Loss=0.3129, Time Left=15.92\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1541/3393 [12:05<15:16,  2.02batch/s, Batch Loss=0.0417, Avg Loss=0.3129, Time Left=15.92\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1541/3393 [12:06<15:16,  2.02batch/s, Batch Loss=0.1222, Avg Loss=0.3128, Time Left=15.91\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1542/3393 [12:06<15:03,  2.05batch/s, Batch Loss=0.1222, Avg Loss=0.3128, Time Left=15.91\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1542/3393 [12:06<15:03,  2.05batch/s, Batch Loss=0.1235, Avg Loss=0.3126, Time Left=15.90\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1543/3393 [12:06<15:07,  2.04batch/s, Batch Loss=0.1235, Avg Loss=0.3126, Time Left=15.90\u001b[A\n",
      "Epoch 1/3 - Training:  45%|▍| 1543/3393 [12:07<15:07,  2.04batch/s, Batch Loss=0.5257, Avg Loss=0.3128, Time Left=15.89\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1544/3393 [12:07<15:06,  2.04batch/s, Batch Loss=0.5257, Avg Loss=0.3128, Time Left=15.89\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1544/3393 [12:07<15:06,  2.04batch/s, Batch Loss=0.2360, Avg Loss=0.3127, Time Left=15.88\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1545/3393 [12:07<14:57,  2.06batch/s, Batch Loss=0.2360, Avg Loss=0.3127, Time Left=15.88\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1545/3393 [12:08<14:57,  2.06batch/s, Batch Loss=0.0757, Avg Loss=0.3126, Time Left=15.87\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1546/3393 [12:08<14:50,  2.07batch/s, Batch Loss=0.0757, Avg Loss=0.3126, Time Left=15.87\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1546/3393 [12:08<14:50,  2.07batch/s, Batch Loss=0.1595, Avg Loss=0.3125, Time Left=15.86\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1547/3393 [12:08<14:49,  2.08batch/s, Batch Loss=0.1595, Avg Loss=0.3125, Time Left=15.86\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1547/3393 [12:09<14:49,  2.08batch/s, Batch Loss=0.2386, Avg Loss=0.3124, Time Left=15.86\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1548/3393 [12:09<15:00,  2.05batch/s, Batch Loss=0.2386, Avg Loss=0.3124, Time Left=15.86\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1548/3393 [12:09<15:00,  2.05batch/s, Batch Loss=0.1980, Avg Loss=0.3123, Time Left=15.85\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  46%|▍| 1549/3393 [12:09<15:16,  2.01batch/s, Batch Loss=0.1980, Avg Loss=0.3123, Time Left=15.85\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1549/3393 [12:10<15:16,  2.01batch/s, Batch Loss=0.2989, Avg Loss=0.3123, Time Left=15.84\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1550/3393 [12:10<14:52,  2.07batch/s, Batch Loss=0.2989, Avg Loss=0.3123, Time Left=15.84\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1550/3393 [12:10<14:52,  2.07batch/s, Batch Loss=0.1605, Avg Loss=0.3122, Time Left=15.83\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1551/3393 [12:10<15:02,  2.04batch/s, Batch Loss=0.1605, Avg Loss=0.3122, Time Left=15.83\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1551/3393 [12:10<15:02,  2.04batch/s, Batch Loss=0.3554, Avg Loss=0.3123, Time Left=15.82\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1552/3393 [12:11<15:09,  2.03batch/s, Batch Loss=0.3554, Avg Loss=0.3123, Time Left=15.82\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1552/3393 [12:11<15:09,  2.03batch/s, Batch Loss=0.2373, Avg Loss=0.3122, Time Left=15.82\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1553/3393 [12:11<15:04,  2.03batch/s, Batch Loss=0.2373, Avg Loss=0.3122, Time Left=15.82\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1553/3393 [12:12<15:04,  2.03batch/s, Batch Loss=0.3014, Avg Loss=0.3122, Time Left=15.81\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1554/3393 [12:12<15:21,  2.00batch/s, Batch Loss=0.3014, Avg Loss=0.3122, Time Left=15.81\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1554/3393 [12:12<15:21,  2.00batch/s, Batch Loss=0.3577, Avg Loss=0.3122, Time Left=15.80\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1555/3393 [12:12<15:10,  2.02batch/s, Batch Loss=0.3577, Avg Loss=0.3122, Time Left=15.80\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1555/3393 [12:13<15:10,  2.02batch/s, Batch Loss=0.1525, Avg Loss=0.3121, Time Left=15.79\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1556/3393 [12:13<15:30,  1.97batch/s, Batch Loss=0.1525, Avg Loss=0.3121, Time Left=15.79\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1556/3393 [12:13<15:30,  1.97batch/s, Batch Loss=0.0347, Avg Loss=0.3119, Time Left=15.78\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1557/3393 [12:13<15:15,  2.01batch/s, Batch Loss=0.0347, Avg Loss=0.3119, Time Left=15.78\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1557/3393 [12:14<15:15,  2.01batch/s, Batch Loss=0.1193, Avg Loss=0.3118, Time Left=15.78\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1558/3393 [12:14<15:24,  1.99batch/s, Batch Loss=0.1193, Avg Loss=0.3118, Time Left=15.78\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1558/3393 [12:14<15:24,  1.99batch/s, Batch Loss=0.1417, Avg Loss=0.3117, Time Left=15.77\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1559/3393 [12:14<15:10,  2.01batch/s, Batch Loss=0.1417, Avg Loss=0.3117, Time Left=15.77\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1559/3393 [12:15<15:10,  2.01batch/s, Batch Loss=0.1854, Avg Loss=0.3116, Time Left=15.76\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1560/3393 [12:15<15:12,  2.01batch/s, Batch Loss=0.1854, Avg Loss=0.3116, Time Left=15.76\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1560/3393 [12:15<15:12,  2.01batch/s, Batch Loss=0.0528, Avg Loss=0.3114, Time Left=15.75\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1561/3393 [12:15<15:08,  2.02batch/s, Batch Loss=0.0528, Avg Loss=0.3114, Time Left=15.75\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1561/3393 [12:16<15:08,  2.02batch/s, Batch Loss=0.4013, Avg Loss=0.3115, Time Left=15.74\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1562/3393 [12:16<15:09,  2.01batch/s, Batch Loss=0.4013, Avg Loss=0.3115, Time Left=15.74\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1562/3393 [12:16<15:09,  2.01batch/s, Batch Loss=0.1333, Avg Loss=0.3114, Time Left=15.73\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1563/3393 [12:16<15:02,  2.03batch/s, Batch Loss=0.1333, Avg Loss=0.3114, Time Left=15.73\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1563/3393 [12:16<15:02,  2.03batch/s, Batch Loss=0.7102, Avg Loss=0.3116, Time Left=15.72\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1564/3393 [12:16<14:49,  2.06batch/s, Batch Loss=0.7102, Avg Loss=0.3116, Time Left=15.72\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1564/3393 [12:17<14:49,  2.06batch/s, Batch Loss=0.3644, Avg Loss=0.3117, Time Left=15.72\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1565/3393 [12:17<14:49,  2.05batch/s, Batch Loss=0.3644, Avg Loss=0.3117, Time Left=15.72\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1565/3393 [12:17<14:49,  2.05batch/s, Batch Loss=0.2170, Avg Loss=0.3116, Time Left=15.71\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1566/3393 [12:17<15:15,  1.99batch/s, Batch Loss=0.2170, Avg Loss=0.3116, Time Left=15.71\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1566/3393 [12:18<15:15,  1.99batch/s, Batch Loss=0.1339, Avg Loss=0.3115, Time Left=15.70\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1567/3393 [12:18<15:05,  2.02batch/s, Batch Loss=0.1339, Avg Loss=0.3115, Time Left=15.70\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1567/3393 [12:18<15:05,  2.02batch/s, Batch Loss=0.2323, Avg Loss=0.3114, Time Left=15.69\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1568/3393 [12:18<14:59,  2.03batch/s, Batch Loss=0.2323, Avg Loss=0.3114, Time Left=15.69\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1568/3393 [12:19<14:59,  2.03batch/s, Batch Loss=0.1719, Avg Loss=0.3113, Time Left=15.68\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1569/3393 [12:19<14:56,  2.03batch/s, Batch Loss=0.1719, Avg Loss=0.3113, Time Left=15.68\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1569/3393 [12:19<14:56,  2.03batch/s, Batch Loss=0.0892, Avg Loss=0.3112, Time Left=15.67\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1570/3393 [12:19<14:44,  2.06batch/s, Batch Loss=0.0892, Avg Loss=0.3112, Time Left=15.67\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1570/3393 [12:20<14:44,  2.06batch/s, Batch Loss=0.3314, Avg Loss=0.3112, Time Left=15.67\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1571/3393 [12:20<14:53,  2.04batch/s, Batch Loss=0.3314, Avg Loss=0.3112, Time Left=15.67\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1571/3393 [12:20<14:53,  2.04batch/s, Batch Loss=0.0827, Avg Loss=0.3111, Time Left=15.66\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1572/3393 [12:20<14:33,  2.09batch/s, Batch Loss=0.0827, Avg Loss=0.3111, Time Left=15.66\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1572/3393 [12:21<14:33,  2.09batch/s, Batch Loss=0.2658, Avg Loss=0.3110, Time Left=15.65\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1573/3393 [12:21<14:45,  2.06batch/s, Batch Loss=0.2658, Avg Loss=0.3110, Time Left=15.65\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1573/3393 [12:21<14:45,  2.06batch/s, Batch Loss=0.3900, Avg Loss=0.3111, Time Left=15.64\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1574/3393 [12:21<14:36,  2.08batch/s, Batch Loss=0.3900, Avg Loss=0.3111, Time Left=15.64\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1574/3393 [12:22<14:36,  2.08batch/s, Batch Loss=0.1716, Avg Loss=0.3110, Time Left=15.63\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1575/3393 [12:22<14:21,  2.11batch/s, Batch Loss=0.1716, Avg Loss=0.3110, Time Left=15.63\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1575/3393 [12:22<14:21,  2.11batch/s, Batch Loss=0.2560, Avg Loss=0.3110, Time Left=15.62\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1576/3393 [12:22<14:27,  2.09batch/s, Batch Loss=0.2560, Avg Loss=0.3110, Time Left=15.62\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1576/3393 [12:23<14:27,  2.09batch/s, Batch Loss=0.2053, Avg Loss=0.3109, Time Left=15.61\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1577/3393 [12:23<14:31,  2.08batch/s, Batch Loss=0.2053, Avg Loss=0.3109, Time Left=15.61\u001b[A\n",
      "Epoch 1/3 - Training:  46%|▍| 1577/3393 [12:23<14:31,  2.08batch/s, Batch Loss=0.3916, Avg Loss=0.3109, Time Left=15.61\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1578/3393 [12:23<14:25,  2.10batch/s, Batch Loss=0.3916, Avg Loss=0.3109, Time Left=15.61\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1578/3393 [12:24<14:25,  2.10batch/s, Batch Loss=0.0879, Avg Loss=0.3108, Time Left=15.60\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1579/3393 [12:24<14:31,  2.08batch/s, Batch Loss=0.0879, Avg Loss=0.3108, Time Left=15.60\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1579/3393 [12:24<14:31,  2.08batch/s, Batch Loss=0.1313, Avg Loss=0.3107, Time Left=15.59\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1580/3393 [12:24<14:34,  2.07batch/s, Batch Loss=0.1313, Avg Loss=0.3107, Time Left=15.59\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1580/3393 [12:25<14:34,  2.07batch/s, Batch Loss=0.2065, Avg Loss=0.3106, Time Left=15.58\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1581/3393 [12:25<14:27,  2.09batch/s, Batch Loss=0.2065, Avg Loss=0.3106, Time Left=15.58\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1581/3393 [12:25<14:27,  2.09batch/s, Batch Loss=0.1920, Avg Loss=0.3105, Time Left=15.57\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  47%|▍| 1582/3393 [12:25<14:56,  2.02batch/s, Batch Loss=0.1920, Avg Loss=0.3105, Time Left=15.57\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1582/3393 [12:26<14:56,  2.02batch/s, Batch Loss=0.2979, Avg Loss=0.3105, Time Left=15.56\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1583/3393 [12:26<14:51,  2.03batch/s, Batch Loss=0.2979, Avg Loss=0.3105, Time Left=15.56\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1583/3393 [12:26<14:51,  2.03batch/s, Batch Loss=0.4106, Avg Loss=0.3106, Time Left=15.56\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1584/3393 [12:26<15:04,  2.00batch/s, Batch Loss=0.4106, Avg Loss=0.3106, Time Left=15.56\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1584/3393 [12:27<15:04,  2.00batch/s, Batch Loss=0.1728, Avg Loss=0.3105, Time Left=15.55\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1585/3393 [12:27<14:54,  2.02batch/s, Batch Loss=0.1728, Avg Loss=0.3105, Time Left=15.55\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1585/3393 [12:27<14:54,  2.02batch/s, Batch Loss=0.0837, Avg Loss=0.3103, Time Left=15.54\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1586/3393 [12:27<14:59,  2.01batch/s, Batch Loss=0.0837, Avg Loss=0.3103, Time Left=15.54\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1586/3393 [12:28<14:59,  2.01batch/s, Batch Loss=0.0496, Avg Loss=0.3102, Time Left=15.53\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1587/3393 [12:28<14:45,  2.04batch/s, Batch Loss=0.0496, Avg Loss=0.3102, Time Left=15.53\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1587/3393 [12:28<14:45,  2.04batch/s, Batch Loss=0.0772, Avg Loss=0.3100, Time Left=15.52\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1588/3393 [12:28<14:36,  2.06batch/s, Batch Loss=0.0772, Avg Loss=0.3100, Time Left=15.52\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1588/3393 [12:29<14:36,  2.06batch/s, Batch Loss=0.1584, Avg Loss=0.3099, Time Left=15.51\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1589/3393 [12:29<14:33,  2.06batch/s, Batch Loss=0.1584, Avg Loss=0.3099, Time Left=15.51\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1589/3393 [12:29<14:33,  2.06batch/s, Batch Loss=0.1721, Avg Loss=0.3098, Time Left=15.51\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1590/3393 [12:29<14:33,  2.06batch/s, Batch Loss=0.1721, Avg Loss=0.3098, Time Left=15.51\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1590/3393 [12:30<14:33,  2.06batch/s, Batch Loss=0.1352, Avg Loss=0.3097, Time Left=15.50\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1591/3393 [12:30<14:17,  2.10batch/s, Batch Loss=0.1352, Avg Loss=0.3097, Time Left=15.50\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1591/3393 [12:30<14:17,  2.10batch/s, Batch Loss=0.1990, Avg Loss=0.3096, Time Left=15.49\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1592/3393 [12:30<14:14,  2.11batch/s, Batch Loss=0.1990, Avg Loss=0.3096, Time Left=15.49\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1592/3393 [12:31<14:14,  2.11batch/s, Batch Loss=0.0566, Avg Loss=0.3095, Time Left=15.48\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1593/3393 [12:31<14:46,  2.03batch/s, Batch Loss=0.0566, Avg Loss=0.3095, Time Left=15.48\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1593/3393 [12:31<14:46,  2.03batch/s, Batch Loss=0.2168, Avg Loss=0.3094, Time Left=15.47\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1594/3393 [12:31<14:41,  2.04batch/s, Batch Loss=0.2168, Avg Loss=0.3094, Time Left=15.47\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1594/3393 [12:32<14:41,  2.04batch/s, Batch Loss=0.3845, Avg Loss=0.3095, Time Left=15.46\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1595/3393 [12:32<14:40,  2.04batch/s, Batch Loss=0.3845, Avg Loss=0.3095, Time Left=15.46\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1595/3393 [12:32<14:40,  2.04batch/s, Batch Loss=0.4877, Avg Loss=0.3096, Time Left=15.45\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1596/3393 [12:32<14:28,  2.07batch/s, Batch Loss=0.4877, Avg Loss=0.3096, Time Left=15.45\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1596/3393 [12:33<14:28,  2.07batch/s, Batch Loss=0.1000, Avg Loss=0.3094, Time Left=15.45\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1597/3393 [12:33<14:30,  2.06batch/s, Batch Loss=0.1000, Avg Loss=0.3094, Time Left=15.45\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1597/3393 [12:33<14:30,  2.06batch/s, Batch Loss=0.1339, Avg Loss=0.3093, Time Left=15.44\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1598/3393 [12:33<14:31,  2.06batch/s, Batch Loss=0.1339, Avg Loss=0.3093, Time Left=15.44\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1598/3393 [12:33<14:31,  2.06batch/s, Batch Loss=0.5423, Avg Loss=0.3095, Time Left=15.43\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1599/3393 [12:33<14:30,  2.06batch/s, Batch Loss=0.5423, Avg Loss=0.3095, Time Left=15.43\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1599/3393 [12:34<14:30,  2.06batch/s, Batch Loss=0.0596, Avg Loss=0.3093, Time Left=15.42\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1600/3393 [12:34<14:30,  2.06batch/s, Batch Loss=0.0596, Avg Loss=0.3093, Time Left=15.42\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1600/3393 [12:34<14:30,  2.06batch/s, Batch Loss=0.4477, Avg Loss=0.3094, Time Left=15.41\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1601/3393 [12:34<14:22,  2.08batch/s, Batch Loss=0.4477, Avg Loss=0.3094, Time Left=15.41\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1601/3393 [12:35<14:22,  2.08batch/s, Batch Loss=0.2336, Avg Loss=0.3093, Time Left=15.40\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1602/3393 [12:35<14:24,  2.07batch/s, Batch Loss=0.2336, Avg Loss=0.3093, Time Left=15.40\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1602/3393 [12:35<14:24,  2.07batch/s, Batch Loss=0.1845, Avg Loss=0.3093, Time Left=15.40\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1603/3393 [12:35<14:51,  2.01batch/s, Batch Loss=0.1845, Avg Loss=0.3093, Time Left=15.40\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1603/3393 [12:36<14:51,  2.01batch/s, Batch Loss=0.1579, Avg Loss=0.3092, Time Left=15.39\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1604/3393 [12:36<14:43,  2.03batch/s, Batch Loss=0.1579, Avg Loss=0.3092, Time Left=15.39\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1604/3393 [12:36<14:43,  2.03batch/s, Batch Loss=0.3717, Avg Loss=0.3092, Time Left=15.38\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1605/3393 [12:36<14:32,  2.05batch/s, Batch Loss=0.3717, Avg Loss=0.3092, Time Left=15.38\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1605/3393 [12:37<14:32,  2.05batch/s, Batch Loss=0.2212, Avg Loss=0.3091, Time Left=15.37\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1606/3393 [12:37<14:36,  2.04batch/s, Batch Loss=0.2212, Avg Loss=0.3091, Time Left=15.37\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1606/3393 [12:37<14:36,  2.04batch/s, Batch Loss=0.1774, Avg Loss=0.3091, Time Left=15.36\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1607/3393 [12:37<14:20,  2.08batch/s, Batch Loss=0.1774, Avg Loss=0.3091, Time Left=15.36\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1607/3393 [12:38<14:20,  2.08batch/s, Batch Loss=0.1914, Avg Loss=0.3090, Time Left=15.35\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1608/3393 [12:38<14:22,  2.07batch/s, Batch Loss=0.1914, Avg Loss=0.3090, Time Left=15.35\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1608/3393 [12:38<14:22,  2.07batch/s, Batch Loss=0.2579, Avg Loss=0.3090, Time Left=15.35\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1609/3393 [12:38<14:20,  2.07batch/s, Batch Loss=0.2579, Avg Loss=0.3090, Time Left=15.35\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1609/3393 [12:39<14:20,  2.07batch/s, Batch Loss=0.4216, Avg Loss=0.3090, Time Left=15.34\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1610/3393 [12:39<14:09,  2.10batch/s, Batch Loss=0.4216, Avg Loss=0.3090, Time Left=15.34\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1610/3393 [12:39<14:09,  2.10batch/s, Batch Loss=0.4382, Avg Loss=0.3091, Time Left=15.33\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1611/3393 [12:39<14:12,  2.09batch/s, Batch Loss=0.4382, Avg Loss=0.3091, Time Left=15.33\u001b[A\n",
      "Epoch 1/3 - Training:  47%|▍| 1611/3393 [12:40<14:12,  2.09batch/s, Batch Loss=0.1242, Avg Loss=0.3090, Time Left=15.32\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1612/3393 [12:40<14:08,  2.10batch/s, Batch Loss=0.1242, Avg Loss=0.3090, Time Left=15.32\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1612/3393 [12:40<14:08,  2.10batch/s, Batch Loss=0.0749, Avg Loss=0.3088, Time Left=15.31\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1613/3393 [12:40<14:04,  2.11batch/s, Batch Loss=0.0749, Avg Loss=0.3088, Time Left=15.31\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1613/3393 [12:41<14:04,  2.11batch/s, Batch Loss=0.3336, Avg Loss=0.3089, Time Left=15.30\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1614/3393 [12:41<14:19,  2.07batch/s, Batch Loss=0.3336, Avg Loss=0.3089, Time Left=15.30\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1614/3393 [12:41<14:19,  2.07batch/s, Batch Loss=0.2594, Avg Loss=0.3088, Time Left=15.29\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  48%|▍| 1615/3393 [12:41<14:20,  2.07batch/s, Batch Loss=0.2594, Avg Loss=0.3088, Time Left=15.29\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1615/3393 [12:42<14:20,  2.07batch/s, Batch Loss=0.1060, Avg Loss=0.3087, Time Left=15.29\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1616/3393 [12:42<14:21,  2.06batch/s, Batch Loss=0.1060, Avg Loss=0.3087, Time Left=15.29\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1616/3393 [12:42<14:21,  2.06batch/s, Batch Loss=0.1848, Avg Loss=0.3086, Time Left=15.28\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1617/3393 [12:42<14:38,  2.02batch/s, Batch Loss=0.1848, Avg Loss=0.3086, Time Left=15.28\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1617/3393 [12:43<14:38,  2.02batch/s, Batch Loss=0.1196, Avg Loss=0.3085, Time Left=15.27\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1618/3393 [12:43<14:33,  2.03batch/s, Batch Loss=0.1196, Avg Loss=0.3085, Time Left=15.27\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1618/3393 [12:43<14:33,  2.03batch/s, Batch Loss=0.3417, Avg Loss=0.3085, Time Left=15.26\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1619/3393 [12:43<14:55,  1.98batch/s, Batch Loss=0.3417, Avg Loss=0.3085, Time Left=15.26\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1619/3393 [12:44<14:55,  1.98batch/s, Batch Loss=0.0863, Avg Loss=0.3084, Time Left=15.25\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1620/3393 [12:44<14:45,  2.00batch/s, Batch Loss=0.0863, Avg Loss=0.3084, Time Left=15.25\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1620/3393 [12:44<14:45,  2.00batch/s, Batch Loss=0.4362, Avg Loss=0.3084, Time Left=15.25\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1621/3393 [12:44<14:54,  1.98batch/s, Batch Loss=0.4362, Avg Loss=0.3084, Time Left=15.25\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1621/3393 [12:45<14:54,  1.98batch/s, Batch Loss=0.0831, Avg Loss=0.3083, Time Left=15.24\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1622/3393 [12:45<14:35,  2.02batch/s, Batch Loss=0.0831, Avg Loss=0.3083, Time Left=15.24\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1622/3393 [12:45<14:35,  2.02batch/s, Batch Loss=0.4228, Avg Loss=0.3084, Time Left=15.23\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1623/3393 [12:45<14:23,  2.05batch/s, Batch Loss=0.4228, Avg Loss=0.3084, Time Left=15.23\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1623/3393 [12:46<14:23,  2.05batch/s, Batch Loss=0.2867, Avg Loss=0.3084, Time Left=15.22\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1624/3393 [12:46<14:46,  2.00batch/s, Batch Loss=0.2867, Avg Loss=0.3084, Time Left=15.22\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1624/3393 [12:46<14:46,  2.00batch/s, Batch Loss=0.3881, Avg Loss=0.3084, Time Left=15.21\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1625/3393 [12:46<14:37,  2.01batch/s, Batch Loss=0.3881, Avg Loss=0.3084, Time Left=15.21\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1625/3393 [12:47<14:37,  2.01batch/s, Batch Loss=0.0858, Avg Loss=0.3083, Time Left=15.20\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1626/3393 [12:47<14:47,  1.99batch/s, Batch Loss=0.0858, Avg Loss=0.3083, Time Left=15.20\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1626/3393 [12:47<14:47,  1.99batch/s, Batch Loss=0.1258, Avg Loss=0.3082, Time Left=15.20\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1627/3393 [12:47<14:48,  1.99batch/s, Batch Loss=0.1258, Avg Loss=0.3082, Time Left=15.20\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1627/3393 [12:48<14:48,  1.99batch/s, Batch Loss=0.1430, Avg Loss=0.3080, Time Left=15.19\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1628/3393 [12:48<14:55,  1.97batch/s, Batch Loss=0.1430, Avg Loss=0.3080, Time Left=15.19\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1628/3393 [12:48<14:55,  1.97batch/s, Batch Loss=0.1567, Avg Loss=0.3079, Time Left=15.18\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1629/3393 [12:48<14:51,  1.98batch/s, Batch Loss=0.1567, Avg Loss=0.3079, Time Left=15.18\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1629/3393 [12:49<14:51,  1.98batch/s, Batch Loss=0.0856, Avg Loss=0.3078, Time Left=15.17\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1630/3393 [12:49<14:56,  1.97batch/s, Batch Loss=0.0856, Avg Loss=0.3078, Time Left=15.17\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1630/3393 [12:49<14:56,  1.97batch/s, Batch Loss=0.1665, Avg Loss=0.3077, Time Left=15.16\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1631/3393 [12:49<14:53,  1.97batch/s, Batch Loss=0.1665, Avg Loss=0.3077, Time Left=15.16\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1631/3393 [12:50<14:53,  1.97batch/s, Batch Loss=0.3306, Avg Loss=0.3077, Time Left=15.16\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1632/3393 [12:50<14:58,  1.96batch/s, Batch Loss=0.3306, Avg Loss=0.3077, Time Left=15.16\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1632/3393 [12:50<14:58,  1.96batch/s, Batch Loss=0.1161, Avg Loss=0.3076, Time Left=15.15\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1633/3393 [12:50<14:29,  2.02batch/s, Batch Loss=0.1161, Avg Loss=0.3076, Time Left=15.15\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1633/3393 [12:51<14:29,  2.02batch/s, Batch Loss=0.1037, Avg Loss=0.3075, Time Left=15.14\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1634/3393 [12:51<14:17,  2.05batch/s, Batch Loss=0.1037, Avg Loss=0.3075, Time Left=15.14\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1634/3393 [12:51<14:17,  2.05batch/s, Batch Loss=0.4476, Avg Loss=0.3076, Time Left=15.13\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1635/3393 [12:51<14:41,  1.99batch/s, Batch Loss=0.4476, Avg Loss=0.3076, Time Left=15.13\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1635/3393 [12:52<14:41,  1.99batch/s, Batch Loss=0.0885, Avg Loss=0.3074, Time Left=15.12\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1636/3393 [12:52<14:32,  2.01batch/s, Batch Loss=0.0885, Avg Loss=0.3074, Time Left=15.12\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1636/3393 [12:52<14:32,  2.01batch/s, Batch Loss=0.1036, Avg Loss=0.3073, Time Left=15.12\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1637/3393 [12:52<14:42,  1.99batch/s, Batch Loss=0.1036, Avg Loss=0.3073, Time Left=15.12\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1637/3393 [12:53<14:42,  1.99batch/s, Batch Loss=0.1797, Avg Loss=0.3072, Time Left=15.11\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1638/3393 [12:53<14:25,  2.03batch/s, Batch Loss=0.1797, Avg Loss=0.3072, Time Left=15.11\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1638/3393 [12:53<14:25,  2.03batch/s, Batch Loss=0.1910, Avg Loss=0.3071, Time Left=15.10\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1639/3393 [12:53<14:21,  2.04batch/s, Batch Loss=0.1910, Avg Loss=0.3071, Time Left=15.10\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1639/3393 [12:54<14:21,  2.04batch/s, Batch Loss=0.2267, Avg Loss=0.3071, Time Left=15.09\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1640/3393 [12:54<14:24,  2.03batch/s, Batch Loss=0.2267, Avg Loss=0.3071, Time Left=15.09\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1640/3393 [12:54<14:24,  2.03batch/s, Batch Loss=0.1334, Avg Loss=0.3070, Time Left=15.08\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1641/3393 [12:54<14:24,  2.03batch/s, Batch Loss=0.1334, Avg Loss=0.3070, Time Left=15.08\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1641/3393 [12:55<14:24,  2.03batch/s, Batch Loss=0.1283, Avg Loss=0.3069, Time Left=15.07\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1642/3393 [12:55<14:35,  2.00batch/s, Batch Loss=0.1283, Avg Loss=0.3069, Time Left=15.07\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1642/3393 [12:55<14:35,  2.00batch/s, Batch Loss=0.2128, Avg Loss=0.3068, Time Left=15.07\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1643/3393 [12:55<14:27,  2.02batch/s, Batch Loss=0.2128, Avg Loss=0.3068, Time Left=15.07\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1643/3393 [12:56<14:27,  2.02batch/s, Batch Loss=0.1538, Avg Loss=0.3067, Time Left=15.06\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1644/3393 [12:56<14:46,  1.97batch/s, Batch Loss=0.1538, Avg Loss=0.3067, Time Left=15.06\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1644/3393 [12:56<14:46,  1.97batch/s, Batch Loss=0.2813, Avg Loss=0.3067, Time Left=15.05\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1645/3393 [12:56<14:36,  1.99batch/s, Batch Loss=0.2813, Avg Loss=0.3067, Time Left=15.05\u001b[A\n",
      "Epoch 1/3 - Training:  48%|▍| 1645/3393 [12:57<14:36,  1.99batch/s, Batch Loss=0.3384, Avg Loss=0.3067, Time Left=15.04\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1646/3393 [12:57<14:31,  2.00batch/s, Batch Loss=0.3384, Avg Loss=0.3067, Time Left=15.04\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1646/3393 [12:57<14:31,  2.00batch/s, Batch Loss=0.1268, Avg Loss=0.3066, Time Left=15.03\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1647/3393 [12:57<14:20,  2.03batch/s, Batch Loss=0.1268, Avg Loss=0.3066, Time Left=15.03\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1647/3393 [12:58<14:20,  2.03batch/s, Batch Loss=0.3213, Avg Loss=0.3066, Time Left=15.02\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  49%|▍| 1648/3393 [12:58<14:16,  2.04batch/s, Batch Loss=0.3213, Avg Loss=0.3066, Time Left=15.02\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1648/3393 [12:58<14:16,  2.04batch/s, Batch Loss=0.0968, Avg Loss=0.3065, Time Left=15.02\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1649/3393 [12:58<14:29,  2.01batch/s, Batch Loss=0.0968, Avg Loss=0.3065, Time Left=15.02\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1649/3393 [12:59<14:29,  2.01batch/s, Batch Loss=0.2570, Avg Loss=0.3064, Time Left=15.01\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1650/3393 [12:59<14:23,  2.02batch/s, Batch Loss=0.2570, Avg Loss=0.3064, Time Left=15.01\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1650/3393 [12:59<14:23,  2.02batch/s, Batch Loss=0.1632, Avg Loss=0.3063, Time Left=15.00\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1651/3393 [12:59<14:35,  1.99batch/s, Batch Loss=0.1632, Avg Loss=0.3063, Time Left=15.00\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1651/3393 [13:00<14:35,  1.99batch/s, Batch Loss=0.0425, Avg Loss=0.3062, Time Left=14.99\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1652/3393 [13:00<14:09,  2.05batch/s, Batch Loss=0.0425, Avg Loss=0.3062, Time Left=14.99\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1652/3393 [13:00<14:09,  2.05batch/s, Batch Loss=0.3219, Avg Loss=0.3062, Time Left=14.98\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1653/3393 [13:00<14:15,  2.03batch/s, Batch Loss=0.3219, Avg Loss=0.3062, Time Left=14.98\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1653/3393 [13:01<14:15,  2.03batch/s, Batch Loss=0.0346, Avg Loss=0.3060, Time Left=14.97\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1654/3393 [13:01<14:13,  2.04batch/s, Batch Loss=0.0346, Avg Loss=0.3060, Time Left=14.97\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1654/3393 [13:01<14:13,  2.04batch/s, Batch Loss=0.0856, Avg Loss=0.3059, Time Left=14.97\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1655/3393 [13:01<14:19,  2.02batch/s, Batch Loss=0.0856, Avg Loss=0.3059, Time Left=14.97\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1655/3393 [13:02<14:19,  2.02batch/s, Batch Loss=0.3459, Avg Loss=0.3059, Time Left=14.96\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1656/3393 [13:02<14:38,  1.98batch/s, Batch Loss=0.3459, Avg Loss=0.3059, Time Left=14.96\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1656/3393 [13:02<14:38,  1.98batch/s, Batch Loss=0.0642, Avg Loss=0.3057, Time Left=14.95\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1657/3393 [13:02<14:30,  1.99batch/s, Batch Loss=0.0642, Avg Loss=0.3057, Time Left=14.95\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1657/3393 [13:03<14:30,  1.99batch/s, Batch Loss=0.2944, Avg Loss=0.3057, Time Left=14.94\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1658/3393 [13:03<14:19,  2.02batch/s, Batch Loss=0.2944, Avg Loss=0.3057, Time Left=14.94\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1658/3393 [13:03<14:19,  2.02batch/s, Batch Loss=0.0425, Avg Loss=0.3056, Time Left=14.93\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1659/3393 [13:03<14:14,  2.03batch/s, Batch Loss=0.0425, Avg Loss=0.3056, Time Left=14.93\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1659/3393 [13:04<14:14,  2.03batch/s, Batch Loss=0.1533, Avg Loss=0.3055, Time Left=14.92\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1660/3393 [13:04<14:10,  2.04batch/s, Batch Loss=0.1533, Avg Loss=0.3055, Time Left=14.92\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1660/3393 [13:04<14:10,  2.04batch/s, Batch Loss=0.4289, Avg Loss=0.3056, Time Left=14.92\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1661/3393 [13:04<14:32,  1.99batch/s, Batch Loss=0.4289, Avg Loss=0.3056, Time Left=14.92\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1661/3393 [13:05<14:32,  1.99batch/s, Batch Loss=0.0234, Avg Loss=0.3054, Time Left=14.91\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1662/3393 [13:05<14:22,  2.01batch/s, Batch Loss=0.0234, Avg Loss=0.3054, Time Left=14.91\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1662/3393 [13:05<14:22,  2.01batch/s, Batch Loss=0.3973, Avg Loss=0.3054, Time Left=14.90\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1663/3393 [13:05<14:31,  1.98batch/s, Batch Loss=0.3973, Avg Loss=0.3054, Time Left=14.90\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1663/3393 [13:06<14:31,  1.98batch/s, Batch Loss=0.6179, Avg Loss=0.3056, Time Left=14.89\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1664/3393 [13:06<14:22,  2.00batch/s, Batch Loss=0.6179, Avg Loss=0.3056, Time Left=14.89\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1664/3393 [13:06<14:22,  2.00batch/s, Batch Loss=0.1334, Avg Loss=0.3055, Time Left=14.89\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1665/3393 [13:06<14:40,  1.96batch/s, Batch Loss=0.1334, Avg Loss=0.3055, Time Left=14.89\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1665/3393 [13:07<14:40,  1.96batch/s, Batch Loss=0.2599, Avg Loss=0.3055, Time Left=14.88\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1666/3393 [13:07<14:27,  1.99batch/s, Batch Loss=0.2599, Avg Loss=0.3055, Time Left=14.88\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1666/3393 [13:07<14:27,  1.99batch/s, Batch Loss=0.2870, Avg Loss=0.3055, Time Left=14.87\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1667/3393 [13:07<14:27,  1.99batch/s, Batch Loss=0.2870, Avg Loss=0.3055, Time Left=14.87\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1667/3393 [13:08<14:27,  1.99batch/s, Batch Loss=0.2513, Avg Loss=0.3055, Time Left=14.86\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1668/3393 [13:08<14:10,  2.03batch/s, Batch Loss=0.2513, Avg Loss=0.3055, Time Left=14.86\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1668/3393 [13:08<14:10,  2.03batch/s, Batch Loss=0.0855, Avg Loss=0.3053, Time Left=14.85\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1669/3393 [13:08<14:06,  2.04batch/s, Batch Loss=0.0855, Avg Loss=0.3053, Time Left=14.85\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1669/3393 [13:09<14:06,  2.04batch/s, Batch Loss=0.1821, Avg Loss=0.3052, Time Left=14.84\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1670/3393 [13:09<14:03,  2.04batch/s, Batch Loss=0.1821, Avg Loss=0.3052, Time Left=14.84\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1670/3393 [13:09<14:03,  2.04batch/s, Batch Loss=0.2422, Avg Loss=0.3052, Time Left=14.83\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1671/3393 [13:09<14:01,  2.05batch/s, Batch Loss=0.2422, Avg Loss=0.3052, Time Left=14.83\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1671/3393 [13:10<14:01,  2.05batch/s, Batch Loss=0.0915, Avg Loss=0.3051, Time Left=14.83\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1672/3393 [13:10<13:58,  2.05batch/s, Batch Loss=0.0915, Avg Loss=0.3051, Time Left=14.83\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1672/3393 [13:10<13:58,  2.05batch/s, Batch Loss=0.1073, Avg Loss=0.3049, Time Left=14.82\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1673/3393 [13:10<14:06,  2.03batch/s, Batch Loss=0.1073, Avg Loss=0.3049, Time Left=14.82\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1673/3393 [13:11<14:06,  2.03batch/s, Batch Loss=0.1645, Avg Loss=0.3049, Time Left=14.81\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1674/3393 [13:11<14:03,  2.04batch/s, Batch Loss=0.1645, Avg Loss=0.3049, Time Left=14.81\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1674/3393 [13:11<14:03,  2.04batch/s, Batch Loss=0.1472, Avg Loss=0.3048, Time Left=14.80\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1675/3393 [13:11<14:16,  2.01batch/s, Batch Loss=0.1472, Avg Loss=0.3048, Time Left=14.80\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1675/3393 [13:12<14:16,  2.01batch/s, Batch Loss=0.1257, Avg Loss=0.3046, Time Left=14.79\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1676/3393 [13:12<14:17,  2.00batch/s, Batch Loss=0.1257, Avg Loss=0.3046, Time Left=14.79\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1676/3393 [13:12<14:17,  2.00batch/s, Batch Loss=0.2565, Avg Loss=0.3046, Time Left=14.79\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1677/3393 [13:12<14:19,  2.00batch/s, Batch Loss=0.2565, Avg Loss=0.3046, Time Left=14.79\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1677/3393 [13:13<14:19,  2.00batch/s, Batch Loss=0.1411, Avg Loss=0.3045, Time Left=14.78\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1678/3393 [13:13<14:02,  2.04batch/s, Batch Loss=0.1411, Avg Loss=0.3045, Time Left=14.78\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1678/3393 [13:13<14:02,  2.04batch/s, Batch Loss=0.1948, Avg Loss=0.3044, Time Left=14.77\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1679/3393 [13:13<13:59,  2.04batch/s, Batch Loss=0.1948, Avg Loss=0.3044, Time Left=14.77\u001b[A\n",
      "Epoch 1/3 - Training:  49%|▍| 1679/3393 [13:14<13:59,  2.04batch/s, Batch Loss=0.0514, Avg Loss=0.3043, Time Left=14.76\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1680/3393 [13:14<14:13,  2.01batch/s, Batch Loss=0.0514, Avg Loss=0.3043, Time Left=14.76\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1680/3393 [13:14<14:13,  2.01batch/s, Batch Loss=0.1243, Avg Loss=0.3042, Time Left=14.75\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  50%|▍| 1681/3393 [13:14<14:15,  2.00batch/s, Batch Loss=0.1243, Avg Loss=0.3042, Time Left=14.75\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1681/3393 [13:15<14:15,  2.00batch/s, Batch Loss=0.0795, Avg Loss=0.3040, Time Left=14.74\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1682/3393 [13:15<14:08,  2.02batch/s, Batch Loss=0.0795, Avg Loss=0.3040, Time Left=14.74\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1682/3393 [13:15<14:08,  2.02batch/s, Batch Loss=0.1924, Avg Loss=0.3040, Time Left=14.74\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1683/3393 [13:15<13:54,  2.05batch/s, Batch Loss=0.1924, Avg Loss=0.3040, Time Left=14.74\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1683/3393 [13:16<13:54,  2.05batch/s, Batch Loss=0.1409, Avg Loss=0.3039, Time Left=14.73\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1684/3393 [13:16<14:02,  2.03batch/s, Batch Loss=0.1409, Avg Loss=0.3039, Time Left=14.73\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1684/3393 [13:16<14:02,  2.03batch/s, Batch Loss=0.3789, Avg Loss=0.3039, Time Left=14.72\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1685/3393 [13:16<13:58,  2.04batch/s, Batch Loss=0.3789, Avg Loss=0.3039, Time Left=14.72\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1685/3393 [13:17<13:58,  2.04batch/s, Batch Loss=0.3459, Avg Loss=0.3039, Time Left=14.71\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1686/3393 [13:17<14:03,  2.02batch/s, Batch Loss=0.3459, Avg Loss=0.3039, Time Left=14.71\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1686/3393 [13:17<14:03,  2.02batch/s, Batch Loss=0.1367, Avg Loss=0.3038, Time Left=14.70\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1687/3393 [13:17<14:14,  2.00batch/s, Batch Loss=0.1367, Avg Loss=0.3038, Time Left=14.70\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1687/3393 [13:18<14:14,  2.00batch/s, Batch Loss=0.3597, Avg Loss=0.3039, Time Left=14.69\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1688/3393 [13:18<13:52,  2.05batch/s, Batch Loss=0.3597, Avg Loss=0.3039, Time Left=14.69\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1688/3393 [13:18<13:52,  2.05batch/s, Batch Loss=0.0984, Avg Loss=0.3037, Time Left=14.69\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1689/3393 [13:18<13:57,  2.03batch/s, Batch Loss=0.0984, Avg Loss=0.3037, Time Left=14.69\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1689/3393 [13:18<13:57,  2.03batch/s, Batch Loss=0.1401, Avg Loss=0.3036, Time Left=14.68\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1690/3393 [13:18<13:46,  2.06batch/s, Batch Loss=0.1401, Avg Loss=0.3036, Time Left=14.68\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1690/3393 [13:19<13:46,  2.06batch/s, Batch Loss=0.0950, Avg Loss=0.3035, Time Left=14.67\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1691/3393 [13:19<13:45,  2.06batch/s, Batch Loss=0.0950, Avg Loss=0.3035, Time Left=14.67\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1691/3393 [13:19<13:45,  2.06batch/s, Batch Loss=0.2010, Avg Loss=0.3034, Time Left=14.66\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1692/3393 [13:19<14:10,  2.00batch/s, Batch Loss=0.2010, Avg Loss=0.3034, Time Left=14.66\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1692/3393 [13:20<14:10,  2.00batch/s, Batch Loss=0.1566, Avg Loss=0.3034, Time Left=14.65\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1693/3393 [13:20<14:03,  2.02batch/s, Batch Loss=0.1566, Avg Loss=0.3034, Time Left=14.65\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1693/3393 [13:21<14:03,  2.02batch/s, Batch Loss=0.2234, Avg Loss=0.3033, Time Left=14.65\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1694/3393 [13:21<14:22,  1.97batch/s, Batch Loss=0.2234, Avg Loss=0.3033, Time Left=14.65\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1694/3393 [13:21<14:22,  1.97batch/s, Batch Loss=0.1440, Avg Loss=0.3032, Time Left=14.64\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1695/3393 [13:21<14:18,  1.98batch/s, Batch Loss=0.1440, Avg Loss=0.3032, Time Left=14.64\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1695/3393 [13:22<14:18,  1.98batch/s, Batch Loss=0.1131, Avg Loss=0.3031, Time Left=14.63\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1696/3393 [13:22<14:24,  1.96batch/s, Batch Loss=0.1131, Avg Loss=0.3031, Time Left=14.63\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▍| 1696/3393 [13:22<14:24,  1.96batch/s, Batch Loss=0.1915, Avg Loss=0.3030, Time Left=14.62\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1697/3393 [13:22<14:18,  1.98batch/s, Batch Loss=0.1915, Avg Loss=0.3030, Time Left=14.62\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1697/3393 [13:22<14:18,  1.98batch/s, Batch Loss=0.1967, Avg Loss=0.3030, Time Left=14.61\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1698/3393 [13:22<13:53,  2.03batch/s, Batch Loss=0.1967, Avg Loss=0.3030, Time Left=14.61\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1698/3393 [13:23<13:53,  2.03batch/s, Batch Loss=0.2205, Avg Loss=0.3029, Time Left=14.60\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1699/3393 [13:23<13:51,  2.04batch/s, Batch Loss=0.2205, Avg Loss=0.3029, Time Left=14.60\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1699/3393 [13:23<13:51,  2.04batch/s, Batch Loss=0.1436, Avg Loss=0.3028, Time Left=14.60\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1700/3393 [13:23<13:48,  2.04batch/s, Batch Loss=0.1436, Avg Loss=0.3028, Time Left=14.60\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1700/3393 [13:24<13:48,  2.04batch/s, Batch Loss=0.0805, Avg Loss=0.3027, Time Left=14.59\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1701/3393 [13:24<13:45,  2.05batch/s, Batch Loss=0.0805, Avg Loss=0.3027, Time Left=14.59\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1701/3393 [13:24<13:45,  2.05batch/s, Batch Loss=0.1363, Avg Loss=0.3026, Time Left=14.58\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1702/3393 [13:24<13:30,  2.09batch/s, Batch Loss=0.1363, Avg Loss=0.3026, Time Left=14.58\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1702/3393 [13:25<13:30,  2.09batch/s, Batch Loss=0.1151, Avg Loss=0.3024, Time Left=14.57\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1703/3393 [13:25<13:31,  2.08batch/s, Batch Loss=0.1151, Avg Loss=0.3024, Time Left=14.57\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1703/3393 [13:25<13:31,  2.08batch/s, Batch Loss=0.0585, Avg Loss=0.3023, Time Left=14.56\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1704/3393 [13:25<13:58,  2.01batch/s, Batch Loss=0.0585, Avg Loss=0.3023, Time Left=14.56\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1704/3393 [13:26<13:58,  2.01batch/s, Batch Loss=0.1351, Avg Loss=0.3022, Time Left=14.55\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1705/3393 [13:26<13:58,  2.01batch/s, Batch Loss=0.1351, Avg Loss=0.3022, Time Left=14.55\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1705/3393 [13:26<13:58,  2.01batch/s, Batch Loss=0.2207, Avg Loss=0.3021, Time Left=14.55\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1706/3393 [13:26<13:46,  2.04batch/s, Batch Loss=0.2207, Avg Loss=0.3021, Time Left=14.55\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1706/3393 [13:27<13:46,  2.04batch/s, Batch Loss=0.4517, Avg Loss=0.3022, Time Left=14.54\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1707/3393 [13:27<13:44,  2.05batch/s, Batch Loss=0.4517, Avg Loss=0.3022, Time Left=14.54\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1707/3393 [13:27<13:44,  2.05batch/s, Batch Loss=0.0729, Avg Loss=0.3021, Time Left=14.53\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1708/3393 [13:27<13:42,  2.05batch/s, Batch Loss=0.0729, Avg Loss=0.3021, Time Left=14.53\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1708/3393 [13:28<13:42,  2.05batch/s, Batch Loss=0.0922, Avg Loss=0.3020, Time Left=14.52\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1709/3393 [13:28<13:57,  2.01batch/s, Batch Loss=0.0922, Avg Loss=0.3020, Time Left=14.52\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1709/3393 [13:28<13:57,  2.01batch/s, Batch Loss=0.2115, Avg Loss=0.3019, Time Left=14.51\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1710/3393 [13:28<13:43,  2.04batch/s, Batch Loss=0.2115, Avg Loss=0.3019, Time Left=14.51\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1710/3393 [13:29<13:43,  2.04batch/s, Batch Loss=0.0753, Avg Loss=0.3018, Time Left=14.50\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1711/3393 [13:29<13:49,  2.03batch/s, Batch Loss=0.0753, Avg Loss=0.3018, Time Left=14.50\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1711/3393 [13:29<13:49,  2.03batch/s, Batch Loss=0.2851, Avg Loss=0.3018, Time Left=14.50\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1712/3393 [13:29<13:38,  2.05batch/s, Batch Loss=0.2851, Avg Loss=0.3018, Time Left=14.50\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1712/3393 [13:30<13:38,  2.05batch/s, Batch Loss=0.1594, Avg Loss=0.3017, Time Left=14.49\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1713/3393 [13:30<13:29,  2.08batch/s, Batch Loss=0.1594, Avg Loss=0.3017, Time Left=14.49\u001b[A\n",
      "Epoch 1/3 - Training:  50%|▌| 1713/3393 [13:30<13:29,  2.08batch/s, Batch Loss=0.3553, Avg Loss=0.3017, Time Left=14.48\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  51%|▌| 1714/3393 [13:30<13:55,  2.01batch/s, Batch Loss=0.3553, Avg Loss=0.3017, Time Left=14.48\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1714/3393 [13:31<13:55,  2.01batch/s, Batch Loss=0.1950, Avg Loss=0.3016, Time Left=14.47\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1715/3393 [13:31<13:56,  2.01batch/s, Batch Loss=0.1950, Avg Loss=0.3016, Time Left=14.47\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1715/3393 [13:31<13:56,  2.01batch/s, Batch Loss=0.1508, Avg Loss=0.3016, Time Left=14.46\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1716/3393 [13:31<14:05,  1.98batch/s, Batch Loss=0.1508, Avg Loss=0.3016, Time Left=14.46\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1716/3393 [13:32<14:05,  1.98batch/s, Batch Loss=0.1463, Avg Loss=0.3015, Time Left=14.46\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1717/3393 [13:32<14:07,  1.98batch/s, Batch Loss=0.1463, Avg Loss=0.3015, Time Left=14.46\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1717/3393 [13:32<14:07,  1.98batch/s, Batch Loss=0.1138, Avg Loss=0.3013, Time Left=14.45\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1718/3393 [13:32<13:54,  2.01batch/s, Batch Loss=0.1138, Avg Loss=0.3013, Time Left=14.45\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1718/3393 [13:33<13:54,  2.01batch/s, Batch Loss=0.2957, Avg Loss=0.3013, Time Left=14.44\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1719/3393 [13:33<13:39,  2.04batch/s, Batch Loss=0.2957, Avg Loss=0.3013, Time Left=14.44\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1719/3393 [13:33<13:39,  2.04batch/s, Batch Loss=0.1081, Avg Loss=0.3012, Time Left=14.43\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1720/3393 [13:33<13:29,  2.07batch/s, Batch Loss=0.1081, Avg Loss=0.3012, Time Left=14.43\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1720/3393 [13:34<13:29,  2.07batch/s, Batch Loss=0.0410, Avg Loss=0.3011, Time Left=14.42\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1721/3393 [13:34<13:45,  2.02batch/s, Batch Loss=0.0410, Avg Loss=0.3011, Time Left=14.42\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1721/3393 [13:34<13:45,  2.02batch/s, Batch Loss=0.2989, Avg Loss=0.3011, Time Left=14.41\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1722/3393 [13:34<13:41,  2.03batch/s, Batch Loss=0.2989, Avg Loss=0.3011, Time Left=14.41\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1722/3393 [13:35<13:41,  2.03batch/s, Batch Loss=0.3209, Avg Loss=0.3011, Time Left=14.40\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1723/3393 [13:35<13:31,  2.06batch/s, Batch Loss=0.3209, Avg Loss=0.3011, Time Left=14.40\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1723/3393 [13:35<13:31,  2.06batch/s, Batch Loss=0.0527, Avg Loss=0.3009, Time Left=14.40\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1724/3393 [13:35<13:46,  2.02batch/s, Batch Loss=0.0527, Avg Loss=0.3009, Time Left=14.40\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1724/3393 [13:36<13:46,  2.02batch/s, Batch Loss=0.1855, Avg Loss=0.3009, Time Left=14.39\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1725/3393 [13:36<13:42,  2.03batch/s, Batch Loss=0.1855, Avg Loss=0.3009, Time Left=14.39\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1725/3393 [13:36<13:42,  2.03batch/s, Batch Loss=0.3875, Avg Loss=0.3009, Time Left=14.38\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1726/3393 [13:36<13:46,  2.02batch/s, Batch Loss=0.3875, Avg Loss=0.3009, Time Left=14.38\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1726/3393 [13:37<13:46,  2.02batch/s, Batch Loss=0.3380, Avg Loss=0.3009, Time Left=14.37\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1727/3393 [13:37<13:33,  2.05batch/s, Batch Loss=0.3380, Avg Loss=0.3009, Time Left=14.37\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1727/3393 [13:37<13:33,  2.05batch/s, Batch Loss=0.0610, Avg Loss=0.3008, Time Left=14.36\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1728/3393 [13:37<13:39,  2.03batch/s, Batch Loss=0.0610, Avg Loss=0.3008, Time Left=14.36\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1728/3393 [13:38<13:39,  2.03batch/s, Batch Loss=0.1571, Avg Loss=0.3007, Time Left=14.35\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1729/3393 [13:38<13:36,  2.04batch/s, Batch Loss=0.1571, Avg Loss=0.3007, Time Left=14.35\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1729/3393 [13:38<13:36,  2.04batch/s, Batch Loss=0.1898, Avg Loss=0.3006, Time Left=14.35\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1730/3393 [13:38<13:46,  2.01batch/s, Batch Loss=0.1898, Avg Loss=0.3006, Time Left=14.35\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1730/3393 [13:39<13:46,  2.01batch/s, Batch Loss=0.1499, Avg Loss=0.3005, Time Left=14.34\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1731/3393 [13:39<13:45,  2.01batch/s, Batch Loss=0.1499, Avg Loss=0.3005, Time Left=14.34\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1731/3393 [13:39<13:45,  2.01batch/s, Batch Loss=0.0993, Avg Loss=0.3004, Time Left=14.33\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1732/3393 [13:39<13:21,  2.07batch/s, Batch Loss=0.0993, Avg Loss=0.3004, Time Left=14.33\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1732/3393 [13:40<13:21,  2.07batch/s, Batch Loss=0.0507, Avg Loss=0.3003, Time Left=14.32\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1733/3393 [13:40<13:24,  2.06batch/s, Batch Loss=0.0507, Avg Loss=0.3003, Time Left=14.32\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1733/3393 [13:40<13:24,  2.06batch/s, Batch Loss=0.0706, Avg Loss=0.3001, Time Left=14.31\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1734/3393 [13:40<13:39,  2.02batch/s, Batch Loss=0.0706, Avg Loss=0.3001, Time Left=14.31\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1734/3393 [13:41<13:39,  2.02batch/s, Batch Loss=0.1323, Avg Loss=0.3000, Time Left=14.30\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1735/3393 [13:41<13:43,  2.01batch/s, Batch Loss=0.1323, Avg Loss=0.3000, Time Left=14.30\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1735/3393 [13:41<13:43,  2.01batch/s, Batch Loss=0.1538, Avg Loss=0.2999, Time Left=14.30\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1736/3393 [13:41<13:45,  2.01batch/s, Batch Loss=0.1538, Avg Loss=0.2999, Time Left=14.30\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1736/3393 [13:42<13:45,  2.01batch/s, Batch Loss=0.2254, Avg Loss=0.2999, Time Left=14.29\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1737/3393 [13:42<13:31,  2.04batch/s, Batch Loss=0.2254, Avg Loss=0.2999, Time Left=14.29\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1737/3393 [13:42<13:31,  2.04batch/s, Batch Loss=0.0578, Avg Loss=0.2997, Time Left=14.28\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1738/3393 [13:42<13:29,  2.04batch/s, Batch Loss=0.0578, Avg Loss=0.2997, Time Left=14.28\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1738/3393 [13:43<13:29,  2.04batch/s, Batch Loss=0.4219, Avg Loss=0.2998, Time Left=14.27\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1739/3393 [13:43<13:28,  2.05batch/s, Batch Loss=0.4219, Avg Loss=0.2998, Time Left=14.27\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1739/3393 [13:43<13:28,  2.05batch/s, Batch Loss=0.3109, Avg Loss=0.2998, Time Left=14.26\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1740/3393 [13:43<13:26,  2.05batch/s, Batch Loss=0.3109, Avg Loss=0.2998, Time Left=14.26\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1740/3393 [13:44<13:26,  2.05batch/s, Batch Loss=0.0071, Avg Loss=0.2997, Time Left=14.26\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1741/3393 [13:44<13:47,  2.00batch/s, Batch Loss=0.0071, Avg Loss=0.2997, Time Left=14.26\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1741/3393 [13:44<13:47,  2.00batch/s, Batch Loss=0.2014, Avg Loss=0.2996, Time Left=14.25\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1742/3393 [13:44<13:47,  1.99batch/s, Batch Loss=0.2014, Avg Loss=0.2996, Time Left=14.25\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1742/3393 [13:45<13:47,  1.99batch/s, Batch Loss=0.1010, Avg Loss=0.2995, Time Left=14.24\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1743/3393 [13:45<14:03,  1.96batch/s, Batch Loss=0.1010, Avg Loss=0.2995, Time Left=14.24\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1743/3393 [13:45<14:03,  1.96batch/s, Batch Loss=0.0109, Avg Loss=0.2993, Time Left=14.23\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1744/3393 [13:45<13:42,  2.01batch/s, Batch Loss=0.0109, Avg Loss=0.2993, Time Left=14.23\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1744/3393 [13:46<13:42,  2.01batch/s, Batch Loss=0.3695, Avg Loss=0.2993, Time Left=14.22\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1745/3393 [13:46<13:41,  2.01batch/s, Batch Loss=0.3695, Avg Loss=0.2993, Time Left=14.22\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1745/3393 [13:46<13:41,  2.01batch/s, Batch Loss=0.3664, Avg Loss=0.2994, Time Left=14.21\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1746/3393 [13:46<13:31,  2.03batch/s, Batch Loss=0.3664, Avg Loss=0.2994, Time Left=14.21\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1746/3393 [13:47<13:31,  2.03batch/s, Batch Loss=0.2167, Avg Loss=0.2993, Time Left=14.21\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  51%|▌| 1747/3393 [13:47<13:27,  2.04batch/s, Batch Loss=0.2167, Avg Loss=0.2993, Time Left=14.21\u001b[A\n",
      "Epoch 1/3 - Training:  51%|▌| 1747/3393 [13:47<13:27,  2.04batch/s, Batch Loss=0.3012, Avg Loss=0.2993, Time Left=14.20\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1748/3393 [13:47<13:38,  2.01batch/s, Batch Loss=0.3012, Avg Loss=0.2993, Time Left=14.20\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1748/3393 [13:48<13:38,  2.01batch/s, Batch Loss=0.1407, Avg Loss=0.2992, Time Left=14.19\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1749/3393 [13:48<13:33,  2.02batch/s, Batch Loss=0.1407, Avg Loss=0.2992, Time Left=14.19\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1749/3393 [13:48<13:33,  2.02batch/s, Batch Loss=0.0818, Avg Loss=0.2991, Time Left=14.18\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1750/3393 [13:48<13:50,  1.98batch/s, Batch Loss=0.0818, Avg Loss=0.2991, Time Left=14.18\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1750/3393 [13:49<13:50,  1.98batch/s, Batch Loss=0.3135, Avg Loss=0.2991, Time Left=14.17\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1751/3393 [13:49<13:41,  2.00batch/s, Batch Loss=0.3135, Avg Loss=0.2991, Time Left=14.17\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1751/3393 [13:49<13:41,  2.00batch/s, Batch Loss=0.1108, Avg Loss=0.2990, Time Left=14.17\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1752/3393 [13:49<14:01,  1.95batch/s, Batch Loss=0.1108, Avg Loss=0.2990, Time Left=14.17\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1752/3393 [13:50<14:01,  1.95batch/s, Batch Loss=0.2429, Avg Loss=0.2990, Time Left=14.16\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1753/3393 [13:50<13:50,  1.97batch/s, Batch Loss=0.2429, Avg Loss=0.2990, Time Left=14.16\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1753/3393 [13:50<13:50,  1.97batch/s, Batch Loss=0.2628, Avg Loss=0.2990, Time Left=14.15\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1754/3393 [13:50<14:03,  1.94batch/s, Batch Loss=0.2628, Avg Loss=0.2990, Time Left=14.15\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1754/3393 [13:51<14:03,  1.94batch/s, Batch Loss=0.2459, Avg Loss=0.2989, Time Left=14.14\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1755/3393 [13:51<13:49,  1.97batch/s, Batch Loss=0.2459, Avg Loss=0.2989, Time Left=14.14\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1755/3393 [13:51<13:49,  1.97batch/s, Batch Loss=0.5092, Avg Loss=0.2990, Time Left=14.13\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1756/3393 [13:51<14:04,  1.94batch/s, Batch Loss=0.5092, Avg Loss=0.2990, Time Left=14.13\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1756/3393 [13:52<14:04,  1.94batch/s, Batch Loss=0.1554, Avg Loss=0.2990, Time Left=14.13\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1757/3393 [13:52<13:47,  1.98batch/s, Batch Loss=0.1554, Avg Loss=0.2990, Time Left=14.13\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1757/3393 [13:52<13:47,  1.98batch/s, Batch Loss=0.0943, Avg Loss=0.2988, Time Left=14.12\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1758/3393 [13:52<14:01,  1.94batch/s, Batch Loss=0.0943, Avg Loss=0.2988, Time Left=14.12\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1758/3393 [13:53<14:01,  1.94batch/s, Batch Loss=0.2170, Avg Loss=0.2988, Time Left=14.11\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1759/3393 [13:53<13:47,  1.97batch/s, Batch Loss=0.2170, Avg Loss=0.2988, Time Left=14.11\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1759/3393 [13:53<13:47,  1.97batch/s, Batch Loss=0.2191, Avg Loss=0.2987, Time Left=14.10\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1760/3393 [13:53<13:59,  1.95batch/s, Batch Loss=0.2191, Avg Loss=0.2987, Time Left=14.10\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1760/3393 [13:54<13:59,  1.95batch/s, Batch Loss=0.1088, Avg Loss=0.2986, Time Left=14.09\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1761/3393 [13:54<13:53,  1.96batch/s, Batch Loss=0.1088, Avg Loss=0.2986, Time Left=14.09\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1761/3393 [13:54<13:53,  1.96batch/s, Batch Loss=0.2379, Avg Loss=0.2986, Time Left=14.09\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1762/3393 [13:54<13:56,  1.95batch/s, Batch Loss=0.2379, Avg Loss=0.2986, Time Left=14.09\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1762/3393 [13:55<13:56,  1.95batch/s, Batch Loss=0.2859, Avg Loss=0.2986, Time Left=14.08\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1763/3393 [13:55<13:43,  1.98batch/s, Batch Loss=0.2859, Avg Loss=0.2986, Time Left=14.08\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1763/3393 [13:55<13:43,  1.98batch/s, Batch Loss=0.4168, Avg Loss=0.2987, Time Left=14.07\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1764/3393 [13:55<13:32,  2.00batch/s, Batch Loss=0.4168, Avg Loss=0.2987, Time Left=14.07\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1764/3393 [13:56<13:32,  2.00batch/s, Batch Loss=0.2441, Avg Loss=0.2986, Time Left=14.06\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1765/3393 [13:56<13:26,  2.02batch/s, Batch Loss=0.2441, Avg Loss=0.2986, Time Left=14.06\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1765/3393 [13:56<13:26,  2.02batch/s, Batch Loss=0.2760, Avg Loss=0.2986, Time Left=14.05\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1766/3393 [13:56<13:24,  2.02batch/s, Batch Loss=0.2760, Avg Loss=0.2986, Time Left=14.05\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1766/3393 [13:57<13:24,  2.02batch/s, Batch Loss=0.1204, Avg Loss=0.2985, Time Left=14.05\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1767/3393 [13:57<13:32,  2.00batch/s, Batch Loss=0.1204, Avg Loss=0.2985, Time Left=14.05\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1767/3393 [13:57<13:32,  2.00batch/s, Batch Loss=0.1515, Avg Loss=0.2984, Time Left=14.04\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1768/3393 [13:57<13:25,  2.02batch/s, Batch Loss=0.1515, Avg Loss=0.2984, Time Left=14.04\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1768/3393 [13:58<13:25,  2.02batch/s, Batch Loss=0.1380, Avg Loss=0.2983, Time Left=14.03\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1769/3393 [13:58<13:37,  1.99batch/s, Batch Loss=0.1380, Avg Loss=0.2983, Time Left=14.03\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1769/3393 [13:58<13:37,  1.99batch/s, Batch Loss=0.0413, Avg Loss=0.2982, Time Left=14.02\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1770/3393 [13:58<13:11,  2.05batch/s, Batch Loss=0.0413, Avg Loss=0.2982, Time Left=14.02\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1770/3393 [13:59<13:11,  2.05batch/s, Batch Loss=0.3776, Avg Loss=0.2982, Time Left=14.01\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1771/3393 [13:59<13:10,  2.05batch/s, Batch Loss=0.3776, Avg Loss=0.2982, Time Left=14.01\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1771/3393 [13:59<13:10,  2.05batch/s, Batch Loss=0.4136, Avg Loss=0.2983, Time Left=14.00\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1772/3393 [13:59<13:28,  2.00batch/s, Batch Loss=0.4136, Avg Loss=0.2983, Time Left=14.00\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1772/3393 [14:00<13:28,  2.00batch/s, Batch Loss=0.2749, Avg Loss=0.2983, Time Left=14.00\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1773/3393 [14:00<13:38,  1.98batch/s, Batch Loss=0.2749, Avg Loss=0.2983, Time Left=14.00\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1773/3393 [14:00<13:38,  1.98batch/s, Batch Loss=0.3934, Avg Loss=0.2983, Time Left=13.99\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1774/3393 [14:00<13:49,  1.95batch/s, Batch Loss=0.3934, Avg Loss=0.2983, Time Left=13.99\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1774/3393 [14:01<13:49,  1.95batch/s, Batch Loss=0.2883, Avg Loss=0.2983, Time Left=13.98\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1775/3393 [14:01<13:33,  1.99batch/s, Batch Loss=0.2883, Avg Loss=0.2983, Time Left=13.98\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1775/3393 [14:01<13:33,  1.99batch/s, Batch Loss=0.2058, Avg Loss=0.2983, Time Left=13.97\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1776/3393 [14:01<13:32,  1.99batch/s, Batch Loss=0.2058, Avg Loss=0.2983, Time Left=13.97\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1776/3393 [14:02<13:32,  1.99batch/s, Batch Loss=0.1791, Avg Loss=0.2982, Time Left=13.96\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1777/3393 [14:02<13:13,  2.04batch/s, Batch Loss=0.1791, Avg Loss=0.2982, Time Left=13.96\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1777/3393 [14:02<13:13,  2.04batch/s, Batch Loss=0.2395, Avg Loss=0.2982, Time Left=13.95\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1778/3393 [14:02<13:18,  2.02batch/s, Batch Loss=0.2395, Avg Loss=0.2982, Time Left=13.95\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1778/3393 [14:03<13:18,  2.02batch/s, Batch Loss=0.0913, Avg Loss=0.2980, Time Left=13.95\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1779/3393 [14:03<13:20,  2.02batch/s, Batch Loss=0.0913, Avg Loss=0.2980, Time Left=13.95\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1779/3393 [14:03<13:20,  2.02batch/s, Batch Loss=0.1812, Avg Loss=0.2980, Time Left=13.94\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  52%|▌| 1780/3393 [14:03<13:20,  2.01batch/s, Batch Loss=0.1812, Avg Loss=0.2980, Time Left=13.94\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1780/3393 [14:04<13:20,  2.01batch/s, Batch Loss=0.1368, Avg Loss=0.2979, Time Left=13.93\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1781/3393 [14:04<13:15,  2.03batch/s, Batch Loss=0.1368, Avg Loss=0.2979, Time Left=13.93\u001b[A\n",
      "Epoch 1/3 - Training:  52%|▌| 1781/3393 [14:04<13:15,  2.03batch/s, Batch Loss=0.1518, Avg Loss=0.2978, Time Left=13.92\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1782/3393 [14:04<13:51,  1.94batch/s, Batch Loss=0.1518, Avg Loss=0.2978, Time Left=13.92\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1782/3393 [14:05<13:51,  1.94batch/s, Batch Loss=0.2420, Avg Loss=0.2978, Time Left=13.92\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1783/3393 [14:05<14:07,  1.90batch/s, Batch Loss=0.2420, Avg Loss=0.2978, Time Left=13.92\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1783/3393 [14:05<14:07,  1.90batch/s, Batch Loss=0.3332, Avg Loss=0.2978, Time Left=13.91\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1784/3393 [14:05<13:59,  1.92batch/s, Batch Loss=0.3332, Avg Loss=0.2978, Time Left=13.91\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1784/3393 [14:06<13:59,  1.92batch/s, Batch Loss=0.5145, Avg Loss=0.2979, Time Left=13.90\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1785/3393 [14:06<13:37,  1.97batch/s, Batch Loss=0.5145, Avg Loss=0.2979, Time Left=13.90\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1785/3393 [14:06<13:37,  1.97batch/s, Batch Loss=0.2364, Avg Loss=0.2979, Time Left=13.89\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1786/3393 [14:06<14:29,  1.85batch/s, Batch Loss=0.2364, Avg Loss=0.2979, Time Left=13.89\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1786/3393 [14:07<14:29,  1.85batch/s, Batch Loss=0.1872, Avg Loss=0.2978, Time Left=13.88\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1787/3393 [14:07<13:58,  1.92batch/s, Batch Loss=0.1872, Avg Loss=0.2978, Time Left=13.88\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1787/3393 [14:07<13:58,  1.92batch/s, Batch Loss=0.0987, Avg Loss=0.2977, Time Left=13.88\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1788/3393 [14:07<14:00,  1.91batch/s, Batch Loss=0.0987, Avg Loss=0.2977, Time Left=13.88\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1788/3393 [14:08<14:00,  1.91batch/s, Batch Loss=0.1863, Avg Loss=0.2976, Time Left=13.87\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1789/3393 [14:08<13:47,  1.94batch/s, Batch Loss=0.1863, Avg Loss=0.2976, Time Left=13.87\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1789/3393 [14:08<13:47,  1.94batch/s, Batch Loss=0.1043, Avg Loss=0.2975, Time Left=13.86\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1790/3393 [14:08<13:53,  1.92batch/s, Batch Loss=0.1043, Avg Loss=0.2975, Time Left=13.86\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1790/3393 [14:09<13:53,  1.92batch/s, Batch Loss=0.1871, Avg Loss=0.2975, Time Left=13.85\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1791/3393 [14:09<13:29,  1.98batch/s, Batch Loss=0.1871, Avg Loss=0.2975, Time Left=13.85\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1791/3393 [14:09<13:29,  1.98batch/s, Batch Loss=0.2571, Avg Loss=0.2974, Time Left=13.84\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1792/3393 [14:09<13:29,  1.98batch/s, Batch Loss=0.2571, Avg Loss=0.2974, Time Left=13.84\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1792/3393 [14:10<13:29,  1.98batch/s, Batch Loss=0.3346, Avg Loss=0.2974, Time Left=13.84\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1793/3393 [14:10<13:48,  1.93batch/s, Batch Loss=0.3346, Avg Loss=0.2974, Time Left=13.84\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1793/3393 [14:11<13:48,  1.93batch/s, Batch Loss=0.2798, Avg Loss=0.2974, Time Left=13.83\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1794/3393 [14:11<13:59,  1.90batch/s, Batch Loss=0.2798, Avg Loss=0.2974, Time Left=13.83\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1794/3393 [14:11<13:59,  1.90batch/s, Batch Loss=0.0774, Avg Loss=0.2973, Time Left=13.82\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1795/3393 [14:11<13:36,  1.96batch/s, Batch Loss=0.0774, Avg Loss=0.2973, Time Left=13.82\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1795/3393 [14:12<13:36,  1.96batch/s, Batch Loss=0.1867, Avg Loss=0.2972, Time Left=13.81\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1796/3393 [14:12<14:00,  1.90batch/s, Batch Loss=0.1867, Avg Loss=0.2972, Time Left=13.81\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1796/3393 [14:12<14:00,  1.90batch/s, Batch Loss=0.1685, Avg Loss=0.2972, Time Left=13.81\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1797/3393 [14:12<14:19,  1.86batch/s, Batch Loss=0.1685, Avg Loss=0.2972, Time Left=13.81\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1797/3393 [14:13<14:19,  1.86batch/s, Batch Loss=0.0848, Avg Loss=0.2970, Time Left=13.80\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1798/3393 [14:13<13:45,  1.93batch/s, Batch Loss=0.0848, Avg Loss=0.2970, Time Left=13.80\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1798/3393 [14:13<13:45,  1.93batch/s, Batch Loss=0.3003, Avg Loss=0.2970, Time Left=13.79\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1799/3393 [14:13<13:53,  1.91batch/s, Batch Loss=0.3003, Avg Loss=0.2970, Time Left=13.79\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1799/3393 [14:14<13:53,  1.91batch/s, Batch Loss=0.1855, Avg Loss=0.2970, Time Left=13.78\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1800/3393 [14:14<13:32,  1.96batch/s, Batch Loss=0.1855, Avg Loss=0.2970, Time Left=13.78\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1800/3393 [14:14<13:32,  1.96batch/s, Batch Loss=0.0954, Avg Loss=0.2969, Time Left=13.77\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1801/3393 [14:14<13:15,  2.00batch/s, Batch Loss=0.0954, Avg Loss=0.2969, Time Left=13.77\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1801/3393 [14:15<13:15,  2.00batch/s, Batch Loss=0.1803, Avg Loss=0.2968, Time Left=13.76\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1802/3393 [14:15<13:18,  1.99batch/s, Batch Loss=0.1803, Avg Loss=0.2968, Time Left=13.76\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1802/3393 [14:15<13:18,  1.99batch/s, Batch Loss=0.0986, Avg Loss=0.2967, Time Left=13.76\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1803/3393 [14:15<13:09,  2.01batch/s, Batch Loss=0.0986, Avg Loss=0.2967, Time Left=13.76\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1803/3393 [14:16<13:09,  2.01batch/s, Batch Loss=0.1369, Avg Loss=0.2966, Time Left=13.75\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1804/3393 [14:16<13:11,  2.01batch/s, Batch Loss=0.1369, Avg Loss=0.2966, Time Left=13.75\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1804/3393 [14:16<13:11,  2.01batch/s, Batch Loss=0.0753, Avg Loss=0.2965, Time Left=13.74\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1805/3393 [14:16<12:48,  2.07batch/s, Batch Loss=0.0753, Avg Loss=0.2965, Time Left=13.74\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1805/3393 [14:17<12:48,  2.07batch/s, Batch Loss=0.0869, Avg Loss=0.2963, Time Left=13.73\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1806/3393 [14:17<12:47,  2.07batch/s, Batch Loss=0.0869, Avg Loss=0.2963, Time Left=13.73\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1806/3393 [14:17<12:47,  2.07batch/s, Batch Loss=0.3124, Avg Loss=0.2964, Time Left=13.72\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1807/3393 [14:17<13:02,  2.03batch/s, Batch Loss=0.3124, Avg Loss=0.2964, Time Left=13.72\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1807/3393 [14:18<13:02,  2.03batch/s, Batch Loss=0.1395, Avg Loss=0.2963, Time Left=13.71\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1808/3393 [14:18<13:05,  2.02batch/s, Batch Loss=0.1395, Avg Loss=0.2963, Time Left=13.71\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1808/3393 [14:18<13:05,  2.02batch/s, Batch Loss=0.3047, Avg Loss=0.2963, Time Left=13.71\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1809/3393 [14:18<13:13,  2.00batch/s, Batch Loss=0.3047, Avg Loss=0.2963, Time Left=13.71\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1809/3393 [14:19<13:13,  2.00batch/s, Batch Loss=0.0684, Avg Loss=0.2961, Time Left=13.70\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1810/3393 [14:19<13:06,  2.01batch/s, Batch Loss=0.0684, Avg Loss=0.2961, Time Left=13.70\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1810/3393 [14:19<13:06,  2.01batch/s, Batch Loss=0.1027, Avg Loss=0.2960, Time Left=13.69\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1811/3393 [14:19<13:22,  1.97batch/s, Batch Loss=0.1027, Avg Loss=0.2960, Time Left=13.69\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1811/3393 [14:20<13:22,  1.97batch/s, Batch Loss=0.1784, Avg Loss=0.2960, Time Left=13.68\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1812/3393 [14:20<13:11,  2.00batch/s, Batch Loss=0.1784, Avg Loss=0.2960, Time Left=13.68\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1812/3393 [14:20<13:11,  2.00batch/s, Batch Loss=0.1327, Avg Loss=0.2959, Time Left=13.67\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  53%|▌| 1813/3393 [14:20<13:26,  1.96batch/s, Batch Loss=0.1327, Avg Loss=0.2959, Time Left=13.67\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1813/3393 [14:21<13:26,  1.96batch/s, Batch Loss=0.1183, Avg Loss=0.2958, Time Left=13.67\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1814/3393 [14:21<13:21,  1.97batch/s, Batch Loss=0.1183, Avg Loss=0.2958, Time Left=13.67\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1814/3393 [14:21<13:21,  1.97batch/s, Batch Loss=0.0269, Avg Loss=0.2956, Time Left=13.66\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1815/3393 [14:21<13:24,  1.96batch/s, Batch Loss=0.0269, Avg Loss=0.2956, Time Left=13.66\u001b[A\n",
      "Epoch 1/3 - Training:  53%|▌| 1815/3393 [14:22<13:24,  1.96batch/s, Batch Loss=0.0973, Avg Loss=0.2955, Time Left=13.65\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1816/3393 [14:22<12:57,  2.03batch/s, Batch Loss=0.0973, Avg Loss=0.2955, Time Left=13.65\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1816/3393 [14:22<12:57,  2.03batch/s, Batch Loss=0.0751, Avg Loss=0.2954, Time Left=13.64\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1817/3393 [14:22<12:58,  2.02batch/s, Batch Loss=0.0751, Avg Loss=0.2954, Time Left=13.64\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1817/3393 [14:23<12:58,  2.02batch/s, Batch Loss=0.1559, Avg Loss=0.2953, Time Left=13.63\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1818/3393 [14:23<12:54,  2.03batch/s, Batch Loss=0.1559, Avg Loss=0.2953, Time Left=13.63\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1818/3393 [14:23<12:54,  2.03batch/s, Batch Loss=0.2758, Avg Loss=0.2953, Time Left=13.62\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1819/3393 [14:23<12:42,  2.06batch/s, Batch Loss=0.2758, Avg Loss=0.2953, Time Left=13.62\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1819/3393 [14:23<12:42,  2.06batch/s, Batch Loss=0.0675, Avg Loss=0.2951, Time Left=13.62\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1820/3393 [14:23<12:36,  2.08batch/s, Batch Loss=0.0675, Avg Loss=0.2951, Time Left=13.62\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1820/3393 [14:24<12:36,  2.08batch/s, Batch Loss=0.0412, Avg Loss=0.2950, Time Left=13.61\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1821/3393 [14:24<12:42,  2.06batch/s, Batch Loss=0.0412, Avg Loss=0.2950, Time Left=13.61\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1821/3393 [14:24<12:42,  2.06batch/s, Batch Loss=0.3928, Avg Loss=0.2951, Time Left=13.60\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1822/3393 [14:24<12:33,  2.08batch/s, Batch Loss=0.3928, Avg Loss=0.2951, Time Left=13.60\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1822/3393 [14:25<12:33,  2.08batch/s, Batch Loss=0.0770, Avg Loss=0.2949, Time Left=13.59\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1823/3393 [14:25<12:35,  2.08batch/s, Batch Loss=0.0770, Avg Loss=0.2949, Time Left=13.59\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1823/3393 [14:25<12:35,  2.08batch/s, Batch Loss=0.0425, Avg Loss=0.2948, Time Left=13.58\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1824/3393 [14:25<12:57,  2.02batch/s, Batch Loss=0.0425, Avg Loss=0.2948, Time Left=13.58\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1824/3393 [14:26<12:57,  2.02batch/s, Batch Loss=0.0360, Avg Loss=0.2946, Time Left=13.57\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1825/3393 [14:26<12:52,  2.03batch/s, Batch Loss=0.0360, Avg Loss=0.2946, Time Left=13.57\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1825/3393 [14:26<12:52,  2.03batch/s, Batch Loss=0.0085, Avg Loss=0.2945, Time Left=13.57\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1826/3393 [14:26<12:55,  2.02batch/s, Batch Loss=0.0085, Avg Loss=0.2945, Time Left=13.57\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1826/3393 [14:27<12:55,  2.02batch/s, Batch Loss=0.1096, Avg Loss=0.2944, Time Left=13.56\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1827/3393 [14:27<12:35,  2.07batch/s, Batch Loss=0.1096, Avg Loss=0.2944, Time Left=13.56\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1827/3393 [14:27<12:35,  2.07batch/s, Batch Loss=0.2798, Avg Loss=0.2944, Time Left=13.55\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1828/3393 [14:27<12:39,  2.06batch/s, Batch Loss=0.2798, Avg Loss=0.2944, Time Left=13.55\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1828/3393 [14:28<12:39,  2.06batch/s, Batch Loss=0.0677, Avg Loss=0.2942, Time Left=13.54\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1829/3393 [14:28<13:03,  2.00batch/s, Batch Loss=0.0677, Avg Loss=0.2942, Time Left=13.54\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1829/3393 [14:28<13:03,  2.00batch/s, Batch Loss=0.2954, Avg Loss=0.2942, Time Left=13.53\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1830/3393 [14:28<12:48,  2.03batch/s, Batch Loss=0.2954, Avg Loss=0.2942, Time Left=13.53\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1830/3393 [14:29<12:48,  2.03batch/s, Batch Loss=0.0573, Avg Loss=0.2941, Time Left=13.52\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1831/3393 [14:29<12:58,  2.01batch/s, Batch Loss=0.0573, Avg Loss=0.2941, Time Left=13.52\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1831/3393 [14:29<12:58,  2.01batch/s, Batch Loss=0.1849, Avg Loss=0.2940, Time Left=13.52\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1832/3393 [14:29<12:53,  2.02batch/s, Batch Loss=0.1849, Avg Loss=0.2940, Time Left=13.52\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1832/3393 [14:30<12:53,  2.02batch/s, Batch Loss=0.5772, Avg Loss=0.2942, Time Left=13.51\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1833/3393 [14:30<12:46,  2.04batch/s, Batch Loss=0.5772, Avg Loss=0.2942, Time Left=13.51\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1833/3393 [14:30<12:46,  2.04batch/s, Batch Loss=0.1072, Avg Loss=0.2941, Time Left=13.50\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1834/3393 [14:30<12:29,  2.08batch/s, Batch Loss=0.1072, Avg Loss=0.2941, Time Left=13.50\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1834/3393 [14:31<12:29,  2.08batch/s, Batch Loss=0.0917, Avg Loss=0.2940, Time Left=13.49\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1835/3393 [14:31<13:24,  1.94batch/s, Batch Loss=0.0917, Avg Loss=0.2940, Time Left=13.49\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1835/3393 [14:31<13:24,  1.94batch/s, Batch Loss=0.1899, Avg Loss=0.2939, Time Left=13.48\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1836/3393 [14:31<13:26,  1.93batch/s, Batch Loss=0.1899, Avg Loss=0.2939, Time Left=13.48\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1836/3393 [14:32<13:26,  1.93batch/s, Batch Loss=0.2534, Avg Loss=0.2939, Time Left=13.48\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1837/3393 [14:32<13:45,  1.89batch/s, Batch Loss=0.2534, Avg Loss=0.2939, Time Left=13.48\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1837/3393 [14:33<13:45,  1.89batch/s, Batch Loss=0.1359, Avg Loss=0.2938, Time Left=13.47\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1838/3393 [14:33<13:21,  1.94batch/s, Batch Loss=0.1359, Avg Loss=0.2938, Time Left=13.47\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1838/3393 [14:33<13:21,  1.94batch/s, Batch Loss=0.1321, Avg Loss=0.2937, Time Left=13.46\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1839/3393 [14:33<13:50,  1.87batch/s, Batch Loss=0.1321, Avg Loss=0.2937, Time Left=13.46\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1839/3393 [14:34<13:50,  1.87batch/s, Batch Loss=0.1286, Avg Loss=0.2936, Time Left=13.45\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1840/3393 [14:34<14:07,  1.83batch/s, Batch Loss=0.1286, Avg Loss=0.2936, Time Left=13.45\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1840/3393 [14:34<14:07,  1.83batch/s, Batch Loss=0.0165, Avg Loss=0.2935, Time Left=13.45\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1841/3393 [14:34<13:33,  1.91batch/s, Batch Loss=0.0165, Avg Loss=0.2935, Time Left=13.45\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1841/3393 [14:35<13:33,  1.91batch/s, Batch Loss=0.0126, Avg Loss=0.2933, Time Left=13.44\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1842/3393 [14:35<13:26,  1.92batch/s, Batch Loss=0.0126, Avg Loss=0.2933, Time Left=13.44\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1842/3393 [14:35<13:26,  1.92batch/s, Batch Loss=0.2456, Avg Loss=0.2933, Time Left=13.43\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1843/3393 [14:35<12:53,  2.00batch/s, Batch Loss=0.2456, Avg Loss=0.2933, Time Left=13.43\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1843/3393 [14:36<12:53,  2.00batch/s, Batch Loss=0.1199, Avg Loss=0.2932, Time Left=13.42\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1844/3393 [14:36<12:54,  2.00batch/s, Batch Loss=0.1199, Avg Loss=0.2932, Time Left=13.42\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1844/3393 [14:36<12:54,  2.00batch/s, Batch Loss=0.0907, Avg Loss=0.2931, Time Left=13.41\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1845/3393 [14:36<13:00,  1.98batch/s, Batch Loss=0.0907, Avg Loss=0.2931, Time Left=13.41\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1845/3393 [14:37<13:00,  1.98batch/s, Batch Loss=0.0404, Avg Loss=0.2929, Time Left=13.40\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  54%|▌| 1846/3393 [14:37<12:52,  2.00batch/s, Batch Loss=0.0404, Avg Loss=0.2929, Time Left=13.40\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1846/3393 [14:37<12:52,  2.00batch/s, Batch Loss=0.1062, Avg Loss=0.2928, Time Left=13.40\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1847/3393 [14:37<13:06,  1.97batch/s, Batch Loss=0.1062, Avg Loss=0.2928, Time Left=13.40\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1847/3393 [14:38<13:06,  1.97batch/s, Batch Loss=0.4161, Avg Loss=0.2929, Time Left=13.39\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1848/3393 [14:38<12:55,  1.99batch/s, Batch Loss=0.4161, Avg Loss=0.2929, Time Left=13.39\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1848/3393 [14:38<12:55,  1.99batch/s, Batch Loss=0.1825, Avg Loss=0.2928, Time Left=13.38\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1849/3393 [14:38<13:01,  1.98batch/s, Batch Loss=0.1825, Avg Loss=0.2928, Time Left=13.38\u001b[A\n",
      "Epoch 1/3 - Training:  54%|▌| 1849/3393 [14:39<13:01,  1.98batch/s, Batch Loss=0.0575, Avg Loss=0.2927, Time Left=13.37\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1850/3393 [14:39<12:44,  2.02batch/s, Batch Loss=0.0575, Avg Loss=0.2927, Time Left=13.37\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1850/3393 [14:39<12:44,  2.02batch/s, Batch Loss=0.3010, Avg Loss=0.2927, Time Left=13.36\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1851/3393 [14:39<13:15,  1.94batch/s, Batch Loss=0.3010, Avg Loss=0.2927, Time Left=13.36\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1851/3393 [14:40<13:15,  1.94batch/s, Batch Loss=0.1815, Avg Loss=0.2926, Time Left=13.36\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1852/3393 [14:40<12:59,  1.98batch/s, Batch Loss=0.1815, Avg Loss=0.2926, Time Left=13.36\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1852/3393 [14:40<12:59,  1.98batch/s, Batch Loss=0.1881, Avg Loss=0.2926, Time Left=13.35\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1853/3393 [14:40<13:12,  1.94batch/s, Batch Loss=0.1881, Avg Loss=0.2926, Time Left=13.35\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1853/3393 [14:41<13:12,  1.94batch/s, Batch Loss=0.0365, Avg Loss=0.2924, Time Left=13.34\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1854/3393 [14:41<12:56,  1.98batch/s, Batch Loss=0.0365, Avg Loss=0.2924, Time Left=13.34\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1854/3393 [14:41<12:56,  1.98batch/s, Batch Loss=0.0609, Avg Loss=0.2923, Time Left=13.33\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1855/3393 [14:41<12:42,  2.02batch/s, Batch Loss=0.0609, Avg Loss=0.2923, Time Left=13.33\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1855/3393 [14:42<12:42,  2.02batch/s, Batch Loss=0.2258, Avg Loss=0.2923, Time Left=13.32\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1856/3393 [14:42<12:47,  2.00batch/s, Batch Loss=0.2258, Avg Loss=0.2923, Time Left=13.32\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1856/3393 [14:42<12:47,  2.00batch/s, Batch Loss=0.0826, Avg Loss=0.2921, Time Left=13.31\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1857/3393 [14:42<12:31,  2.04batch/s, Batch Loss=0.0826, Avg Loss=0.2921, Time Left=13.31\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1857/3393 [14:43<12:31,  2.04batch/s, Batch Loss=0.4787, Avg Loss=0.2922, Time Left=13.31\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1858/3393 [14:43<12:49,  1.99batch/s, Batch Loss=0.4787, Avg Loss=0.2922, Time Left=13.31\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1858/3393 [14:43<12:49,  1.99batch/s, Batch Loss=0.0611, Avg Loss=0.2921, Time Left=13.30\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1859/3393 [14:43<12:49,  1.99batch/s, Batch Loss=0.0611, Avg Loss=0.2921, Time Left=13.30\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1859/3393 [14:44<12:49,  1.99batch/s, Batch Loss=0.3073, Avg Loss=0.2921, Time Left=13.29\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1860/3393 [14:44<12:58,  1.97batch/s, Batch Loss=0.3073, Avg Loss=0.2921, Time Left=13.29\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1860/3393 [14:44<12:58,  1.97batch/s, Batch Loss=0.3781, Avg Loss=0.2922, Time Left=13.28\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1861/3393 [14:44<12:40,  2.01batch/s, Batch Loss=0.3781, Avg Loss=0.2922, Time Left=13.28\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1861/3393 [14:45<12:40,  2.01batch/s, Batch Loss=0.0782, Avg Loss=0.2921, Time Left=13.27\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1862/3393 [14:45<12:34,  2.03batch/s, Batch Loss=0.0782, Avg Loss=0.2921, Time Left=13.27\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1862/3393 [14:45<12:34,  2.03batch/s, Batch Loss=0.1360, Avg Loss=0.2920, Time Left=13.26\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1863/3393 [14:45<12:22,  2.06batch/s, Batch Loss=0.1360, Avg Loss=0.2920, Time Left=13.26\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1863/3393 [14:46<12:22,  2.06batch/s, Batch Loss=0.0615, Avg Loss=0.2918, Time Left=13.26\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1864/3393 [14:46<12:28,  2.04batch/s, Batch Loss=0.0615, Avg Loss=0.2918, Time Left=13.26\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1864/3393 [14:46<12:28,  2.04batch/s, Batch Loss=0.1688, Avg Loss=0.2918, Time Left=13.25\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1865/3393 [14:46<12:32,  2.03batch/s, Batch Loss=0.1688, Avg Loss=0.2918, Time Left=13.25\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1865/3393 [14:47<12:32,  2.03batch/s, Batch Loss=0.0419, Avg Loss=0.2916, Time Left=13.24\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1866/3393 [14:47<12:21,  2.06batch/s, Batch Loss=0.0419, Avg Loss=0.2916, Time Left=13.24\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1866/3393 [14:47<12:21,  2.06batch/s, Batch Loss=0.1115, Avg Loss=0.2915, Time Left=13.23\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1867/3393 [14:47<12:19,  2.06batch/s, Batch Loss=0.1115, Avg Loss=0.2915, Time Left=13.23\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1867/3393 [14:48<12:19,  2.06batch/s, Batch Loss=0.2709, Avg Loss=0.2915, Time Left=13.22\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1868/3393 [14:48<12:32,  2.03batch/s, Batch Loss=0.2709, Avg Loss=0.2915, Time Left=13.22\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1868/3393 [14:48<12:32,  2.03batch/s, Batch Loss=0.1348, Avg Loss=0.2914, Time Left=13.21\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1869/3393 [14:48<12:22,  2.05batch/s, Batch Loss=0.1348, Avg Loss=0.2914, Time Left=13.21\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1869/3393 [14:49<12:22,  2.05batch/s, Batch Loss=0.4625, Avg Loss=0.2915, Time Left=13.21\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1870/3393 [14:49<12:26,  2.04batch/s, Batch Loss=0.4625, Avg Loss=0.2915, Time Left=13.21\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1870/3393 [14:49<12:26,  2.04batch/s, Batch Loss=0.0814, Avg Loss=0.2914, Time Left=13.20\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1871/3393 [14:49<12:24,  2.04batch/s, Batch Loss=0.0814, Avg Loss=0.2914, Time Left=13.20\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1871/3393 [14:49<12:24,  2.04batch/s, Batch Loss=0.3094, Avg Loss=0.2914, Time Left=13.19\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1872/3393 [14:49<12:12,  2.08batch/s, Batch Loss=0.3094, Avg Loss=0.2914, Time Left=13.19\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1872/3393 [14:50<12:12,  2.08batch/s, Batch Loss=0.1898, Avg Loss=0.2914, Time Left=13.18\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1873/3393 [14:50<12:42,  1.99batch/s, Batch Loss=0.1898, Avg Loss=0.2914, Time Left=13.18\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1873/3393 [14:51<12:42,  1.99batch/s, Batch Loss=0.2134, Avg Loss=0.2913, Time Left=13.17\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1874/3393 [14:51<12:34,  2.01batch/s, Batch Loss=0.2134, Avg Loss=0.2913, Time Left=13.17\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1874/3393 [14:51<12:34,  2.01batch/s, Batch Loss=0.4808, Avg Loss=0.2914, Time Left=13.17\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1875/3393 [14:51<12:50,  1.97batch/s, Batch Loss=0.4808, Avg Loss=0.2914, Time Left=13.17\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1875/3393 [14:52<12:50,  1.97batch/s, Batch Loss=0.0632, Avg Loss=0.2913, Time Left=13.16\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1876/3393 [14:52<12:46,  1.98batch/s, Batch Loss=0.0632, Avg Loss=0.2913, Time Left=13.16\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1876/3393 [14:52<12:46,  1.98batch/s, Batch Loss=0.0754, Avg Loss=0.2912, Time Left=13.15\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1877/3393 [14:52<12:54,  1.96batch/s, Batch Loss=0.0754, Avg Loss=0.2912, Time Left=13.15\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1877/3393 [14:53<12:54,  1.96batch/s, Batch Loss=0.1292, Avg Loss=0.2911, Time Left=13.14\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1878/3393 [14:53<12:46,  1.98batch/s, Batch Loss=0.1292, Avg Loss=0.2911, Time Left=13.14\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1878/3393 [14:53<12:46,  1.98batch/s, Batch Loss=0.0770, Avg Loss=0.2910, Time Left=13.13\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  55%|▌| 1879/3393 [14:53<12:50,  1.97batch/s, Batch Loss=0.0770, Avg Loss=0.2910, Time Left=13.13\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1879/3393 [14:54<12:50,  1.97batch/s, Batch Loss=0.2051, Avg Loss=0.2909, Time Left=13.13\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1880/3393 [14:54<12:46,  1.97batch/s, Batch Loss=0.2051, Avg Loss=0.2909, Time Left=13.13\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1880/3393 [14:54<12:46,  1.97batch/s, Batch Loss=0.0995, Avg Loss=0.2908, Time Left=13.12\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1881/3393 [14:54<12:50,  1.96batch/s, Batch Loss=0.0995, Avg Loss=0.2908, Time Left=13.12\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1881/3393 [14:55<12:50,  1.96batch/s, Batch Loss=0.0743, Avg Loss=0.2907, Time Left=13.11\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1882/3393 [14:55<12:32,  2.01batch/s, Batch Loss=0.0743, Avg Loss=0.2907, Time Left=13.11\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1882/3393 [14:55<12:32,  2.01batch/s, Batch Loss=0.1634, Avg Loss=0.2906, Time Left=13.10\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1883/3393 [14:55<12:17,  2.05batch/s, Batch Loss=0.1634, Avg Loss=0.2906, Time Left=13.10\u001b[A\n",
      "Epoch 1/3 - Training:  55%|▌| 1883/3393 [14:56<12:17,  2.05batch/s, Batch Loss=0.1688, Avg Loss=0.2906, Time Left=13.09\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1884/3393 [14:56<12:35,  2.00batch/s, Batch Loss=0.1688, Avg Loss=0.2906, Time Left=13.09\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1884/3393 [14:56<12:35,  2.00batch/s, Batch Loss=0.1597, Avg Loss=0.2905, Time Left=13.08\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1885/3393 [14:56<12:29,  2.01batch/s, Batch Loss=0.1597, Avg Loss=0.2905, Time Left=13.08\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1885/3393 [14:57<12:29,  2.01batch/s, Batch Loss=0.0404, Avg Loss=0.2903, Time Left=13.08\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1886/3393 [14:57<12:41,  1.98batch/s, Batch Loss=0.0404, Avg Loss=0.2903, Time Left=13.08\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1886/3393 [14:57<12:41,  1.98batch/s, Batch Loss=0.0520, Avg Loss=0.2902, Time Left=13.07\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1887/3393 [14:57<12:35,  1.99batch/s, Batch Loss=0.0520, Avg Loss=0.2902, Time Left=13.07\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1887/3393 [14:58<12:35,  1.99batch/s, Batch Loss=0.0822, Avg Loss=0.2901, Time Left=13.06\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1888/3393 [14:58<12:41,  1.98batch/s, Batch Loss=0.0822, Avg Loss=0.2901, Time Left=13.06\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1888/3393 [14:58<12:41,  1.98batch/s, Batch Loss=0.0645, Avg Loss=0.2900, Time Left=13.05\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1889/3393 [14:58<12:31,  2.00batch/s, Batch Loss=0.0645, Avg Loss=0.2900, Time Left=13.05\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1889/3393 [14:59<12:31,  2.00batch/s, Batch Loss=0.2796, Avg Loss=0.2900, Time Left=13.04\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1890/3393 [14:59<12:30,  2.00batch/s, Batch Loss=0.2796, Avg Loss=0.2900, Time Left=13.04\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1890/3393 [14:59<12:30,  2.00batch/s, Batch Loss=0.1274, Avg Loss=0.2899, Time Left=13.03\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1891/3393 [14:59<12:23,  2.02batch/s, Batch Loss=0.1274, Avg Loss=0.2899, Time Left=13.03\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1891/3393 [15:00<12:23,  2.02batch/s, Batch Loss=0.0314, Avg Loss=0.2897, Time Left=13.03\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1892/3393 [15:00<12:21,  2.02batch/s, Batch Loss=0.0314, Avg Loss=0.2897, Time Left=13.03\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1892/3393 [15:00<12:21,  2.02batch/s, Batch Loss=0.0705, Avg Loss=0.2896, Time Left=13.02\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1893/3393 [15:00<12:27,  2.01batch/s, Batch Loss=0.0705, Avg Loss=0.2896, Time Left=13.02\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1893/3393 [15:01<12:27,  2.01batch/s, Batch Loss=0.0861, Avg Loss=0.2895, Time Left=13.01\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1894/3393 [15:01<12:20,  2.02batch/s, Batch Loss=0.0861, Avg Loss=0.2895, Time Left=13.01\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1894/3393 [15:01<12:20,  2.02batch/s, Batch Loss=0.3649, Avg Loss=0.2895, Time Left=13.00\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1895/3393 [15:01<12:22,  2.02batch/s, Batch Loss=0.3649, Avg Loss=0.2895, Time Left=13.00\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1895/3393 [15:02<12:22,  2.02batch/s, Batch Loss=0.1229, Avg Loss=0.2895, Time Left=12.99\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1896/3393 [15:02<12:17,  2.03batch/s, Batch Loss=0.1229, Avg Loss=0.2895, Time Left=12.99\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1896/3393 [15:02<12:17,  2.03batch/s, Batch Loss=0.0299, Avg Loss=0.2893, Time Left=12.98\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1897/3393 [15:02<12:05,  2.06batch/s, Batch Loss=0.0299, Avg Loss=0.2893, Time Left=12.98\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1897/3393 [15:02<12:05,  2.06batch/s, Batch Loss=0.0807, Avg Loss=0.2892, Time Left=12.98\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1898/3393 [15:03<12:19,  2.02batch/s, Batch Loss=0.0807, Avg Loss=0.2892, Time Left=12.98\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1898/3393 [15:03<12:19,  2.02batch/s, Batch Loss=0.0787, Avg Loss=0.2891, Time Left=12.97\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1899/3393 [15:03<11:59,  2.08batch/s, Batch Loss=0.0787, Avg Loss=0.2891, Time Left=12.97\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1899/3393 [15:03<11:59,  2.08batch/s, Batch Loss=0.3618, Avg Loss=0.2891, Time Left=12.96\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1900/3393 [15:03<11:59,  2.07batch/s, Batch Loss=0.3618, Avg Loss=0.2891, Time Left=12.96\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1900/3393 [15:04<11:59,  2.07batch/s, Batch Loss=0.1415, Avg Loss=0.2890, Time Left=12.95\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1901/3393 [15:04<12:21,  2.01batch/s, Batch Loss=0.1415, Avg Loss=0.2890, Time Left=12.95\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1901/3393 [15:04<12:21,  2.01batch/s, Batch Loss=0.1364, Avg Loss=0.2890, Time Left=12.94\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1902/3393 [15:04<12:15,  2.03batch/s, Batch Loss=0.1364, Avg Loss=0.2890, Time Left=12.94\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1902/3393 [15:05<12:15,  2.03batch/s, Batch Loss=0.3255, Avg Loss=0.2890, Time Left=12.94\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1903/3393 [15:05<12:18,  2.02batch/s, Batch Loss=0.3255, Avg Loss=0.2890, Time Left=12.94\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1903/3393 [15:05<12:18,  2.02batch/s, Batch Loss=0.2842, Avg Loss=0.2890, Time Left=12.93\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1904/3393 [15:05<11:58,  2.07batch/s, Batch Loss=0.2842, Avg Loss=0.2890, Time Left=12.93\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1904/3393 [15:06<11:58,  2.07batch/s, Batch Loss=0.0374, Avg Loss=0.2888, Time Left=12.92\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1905/3393 [15:06<11:58,  2.07batch/s, Batch Loss=0.0374, Avg Loss=0.2888, Time Left=12.92\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1905/3393 [15:06<11:58,  2.07batch/s, Batch Loss=0.4229, Avg Loss=0.2889, Time Left=12.91\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1906/3393 [15:06<12:05,  2.05batch/s, Batch Loss=0.4229, Avg Loss=0.2889, Time Left=12.91\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1906/3393 [15:07<12:05,  2.05batch/s, Batch Loss=0.1160, Avg Loss=0.2888, Time Left=12.90\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1907/3393 [15:07<11:48,  2.10batch/s, Batch Loss=0.1160, Avg Loss=0.2888, Time Left=12.90\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1907/3393 [15:07<11:48,  2.10batch/s, Batch Loss=0.1722, Avg Loss=0.2888, Time Left=12.89\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1908/3393 [15:07<11:50,  2.09batch/s, Batch Loss=0.1722, Avg Loss=0.2888, Time Left=12.89\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1908/3393 [15:08<11:50,  2.09batch/s, Batch Loss=0.0845, Avg Loss=0.2886, Time Left=12.88\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1909/3393 [15:08<12:06,  2.04batch/s, Batch Loss=0.0845, Avg Loss=0.2886, Time Left=12.88\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1909/3393 [15:08<12:06,  2.04batch/s, Batch Loss=0.0152, Avg Loss=0.2885, Time Left=12.88\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1910/3393 [15:08<11:49,  2.09batch/s, Batch Loss=0.0152, Avg Loss=0.2885, Time Left=12.88\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1910/3393 [15:09<11:49,  2.09batch/s, Batch Loss=0.1932, Avg Loss=0.2884, Time Left=12.87\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1911/3393 [15:09<11:51,  2.08batch/s, Batch Loss=0.1932, Avg Loss=0.2884, Time Left=12.87\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1911/3393 [15:09<11:51,  2.08batch/s, Batch Loss=0.1529, Avg Loss=0.2884, Time Left=12.86\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  56%|▌| 1912/3393 [15:09<12:06,  2.04batch/s, Batch Loss=0.1529, Avg Loss=0.2884, Time Left=12.86\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1912/3393 [15:10<12:06,  2.04batch/s, Batch Loss=0.2576, Avg Loss=0.2883, Time Left=12.85\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1913/3393 [15:10<12:03,  2.05batch/s, Batch Loss=0.2576, Avg Loss=0.2883, Time Left=12.85\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1913/3393 [15:10<12:03,  2.05batch/s, Batch Loss=0.1376, Avg Loss=0.2883, Time Left=12.84\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1914/3393 [15:10<12:14,  2.01batch/s, Batch Loss=0.1376, Avg Loss=0.2883, Time Left=12.84\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1914/3393 [15:11<12:14,  2.01batch/s, Batch Loss=0.2047, Avg Loss=0.2882, Time Left=12.83\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1915/3393 [15:11<12:09,  2.03batch/s, Batch Loss=0.2047, Avg Loss=0.2882, Time Left=12.83\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1915/3393 [15:11<12:09,  2.03batch/s, Batch Loss=0.1644, Avg Loss=0.2882, Time Left=12.83\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1916/3393 [15:11<11:57,  2.06batch/s, Batch Loss=0.1644, Avg Loss=0.2882, Time Left=12.83\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1916/3393 [15:12<11:57,  2.06batch/s, Batch Loss=0.2014, Avg Loss=0.2881, Time Left=12.82\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1917/3393 [15:12<12:02,  2.04batch/s, Batch Loss=0.2014, Avg Loss=0.2881, Time Left=12.82\u001b[A\n",
      "Epoch 1/3 - Training:  56%|▌| 1917/3393 [15:12<12:02,  2.04batch/s, Batch Loss=0.1529, Avg Loss=0.2880, Time Left=12.81\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1918/3393 [15:12<12:00,  2.05batch/s, Batch Loss=0.1529, Avg Loss=0.2880, Time Left=12.81\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1918/3393 [15:13<12:00,  2.05batch/s, Batch Loss=0.1422, Avg Loss=0.2880, Time Left=12.80\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1919/3393 [15:13<12:11,  2.01batch/s, Batch Loss=0.1422, Avg Loss=0.2880, Time Left=12.80\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1919/3393 [15:13<12:11,  2.01batch/s, Batch Loss=0.2404, Avg Loss=0.2879, Time Left=12.79\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1920/3393 [15:13<12:06,  2.03batch/s, Batch Loss=0.2404, Avg Loss=0.2879, Time Left=12.79\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1920/3393 [15:14<12:06,  2.03batch/s, Batch Loss=0.3933, Avg Loss=0.2880, Time Left=12.78\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1921/3393 [15:14<12:08,  2.02batch/s, Batch Loss=0.3933, Avg Loss=0.2880, Time Left=12.78\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1921/3393 [15:14<12:08,  2.02batch/s, Batch Loss=0.0578, Avg Loss=0.2879, Time Left=12.78\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1922/3393 [15:14<12:03,  2.03batch/s, Batch Loss=0.0578, Avg Loss=0.2879, Time Left=12.78\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1922/3393 [15:15<12:03,  2.03batch/s, Batch Loss=0.2114, Avg Loss=0.2878, Time Left=12.77\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1923/3393 [15:15<11:52,  2.06batch/s, Batch Loss=0.2114, Avg Loss=0.2878, Time Left=12.77\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1923/3393 [15:15<11:52,  2.06batch/s, Batch Loss=0.0570, Avg Loss=0.2877, Time Left=12.76\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1924/3393 [15:15<11:58,  2.05batch/s, Batch Loss=0.0570, Avg Loss=0.2877, Time Left=12.76\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1924/3393 [15:16<11:58,  2.05batch/s, Batch Loss=0.4257, Avg Loss=0.2878, Time Left=12.75\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1925/3393 [15:16<11:49,  2.07batch/s, Batch Loss=0.4257, Avg Loss=0.2878, Time Left=12.75\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1925/3393 [15:16<11:49,  2.07batch/s, Batch Loss=0.1141, Avg Loss=0.2877, Time Left=12.74\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1926/3393 [15:16<12:02,  2.03batch/s, Batch Loss=0.1141, Avg Loss=0.2877, Time Left=12.74\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1926/3393 [15:17<12:02,  2.03batch/s, Batch Loss=0.3663, Avg Loss=0.2877, Time Left=12.73\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1927/3393 [15:17<11:58,  2.04batch/s, Batch Loss=0.3663, Avg Loss=0.2877, Time Left=12.73\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1927/3393 [15:17<11:58,  2.04batch/s, Batch Loss=0.1834, Avg Loss=0.2877, Time Left=12.73\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1928/3393 [15:17<11:48,  2.07batch/s, Batch Loss=0.1834, Avg Loss=0.2877, Time Left=12.73\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1928/3393 [15:18<11:48,  2.07batch/s, Batch Loss=0.1541, Avg Loss=0.2876, Time Left=12.72\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1929/3393 [15:18<11:47,  2.07batch/s, Batch Loss=0.1541, Avg Loss=0.2876, Time Left=12.72\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1929/3393 [15:18<11:47,  2.07batch/s, Batch Loss=0.3522, Avg Loss=0.2876, Time Left=12.71\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1930/3393 [15:18<11:47,  2.07batch/s, Batch Loss=0.3522, Avg Loss=0.2876, Time Left=12.71\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1930/3393 [15:19<11:47,  2.07batch/s, Batch Loss=0.1661, Avg Loss=0.2876, Time Left=12.70\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1931/3393 [15:19<11:53,  2.05batch/s, Batch Loss=0.1661, Avg Loss=0.2876, Time Left=12.70\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1931/3393 [15:19<11:53,  2.05batch/s, Batch Loss=0.2191, Avg Loss=0.2875, Time Left=12.69\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1932/3393 [15:19<12:06,  2.01batch/s, Batch Loss=0.2191, Avg Loss=0.2875, Time Left=12.69\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1932/3393 [15:20<12:06,  2.01batch/s, Batch Loss=0.1844, Avg Loss=0.2875, Time Left=12.68\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1933/3393 [15:20<11:46,  2.07batch/s, Batch Loss=0.1844, Avg Loss=0.2875, Time Left=12.68\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1933/3393 [15:20<11:46,  2.07batch/s, Batch Loss=0.1097, Avg Loss=0.2874, Time Left=12.68\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1934/3393 [15:20<11:52,  2.05batch/s, Batch Loss=0.1097, Avg Loss=0.2874, Time Left=12.68\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1934/3393 [15:21<11:52,  2.05batch/s, Batch Loss=0.2182, Avg Loss=0.2873, Time Left=12.67\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1935/3393 [15:21<11:53,  2.04batch/s, Batch Loss=0.2182, Avg Loss=0.2873, Time Left=12.67\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1935/3393 [15:21<11:53,  2.04batch/s, Batch Loss=0.1480, Avg Loss=0.2873, Time Left=12.66\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1936/3393 [15:21<11:47,  2.06batch/s, Batch Loss=0.1480, Avg Loss=0.2873, Time Left=12.66\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1936/3393 [15:22<11:47,  2.06batch/s, Batch Loss=0.3149, Avg Loss=0.2873, Time Left=12.65\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1937/3393 [15:22<12:06,  2.00batch/s, Batch Loss=0.3149, Avg Loss=0.2873, Time Left=12.65\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1937/3393 [15:22<12:06,  2.00batch/s, Batch Loss=0.1200, Avg Loss=0.2872, Time Left=12.64\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1938/3393 [15:22<11:59,  2.02batch/s, Batch Loss=0.1200, Avg Loss=0.2872, Time Left=12.64\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1938/3393 [15:23<11:59,  2.02batch/s, Batch Loss=0.1384, Avg Loss=0.2871, Time Left=12.63\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1939/3393 [15:23<12:01,  2.01batch/s, Batch Loss=0.1384, Avg Loss=0.2871, Time Left=12.63\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1939/3393 [15:23<12:01,  2.01batch/s, Batch Loss=0.1466, Avg Loss=0.2870, Time Left=12.63\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1940/3393 [15:23<11:55,  2.03batch/s, Batch Loss=0.1466, Avg Loss=0.2870, Time Left=12.63\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1940/3393 [15:23<11:55,  2.03batch/s, Batch Loss=0.4326, Avg Loss=0.2871, Time Left=12.62\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1941/3393 [15:24<11:53,  2.03batch/s, Batch Loss=0.4326, Avg Loss=0.2871, Time Left=12.62\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1941/3393 [15:24<11:53,  2.03batch/s, Batch Loss=0.1038, Avg Loss=0.2870, Time Left=12.61\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1942/3393 [15:24<12:01,  2.01batch/s, Batch Loss=0.1038, Avg Loss=0.2870, Time Left=12.61\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1942/3393 [15:24<12:01,  2.01batch/s, Batch Loss=0.1437, Avg Loss=0.2869, Time Left=12.60\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1943/3393 [15:24<11:49,  2.05batch/s, Batch Loss=0.1437, Avg Loss=0.2869, Time Left=12.60\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1943/3393 [15:25<11:49,  2.05batch/s, Batch Loss=0.1830, Avg Loss=0.2869, Time Left=12.59\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1944/3393 [15:25<11:46,  2.05batch/s, Batch Loss=0.1830, Avg Loss=0.2869, Time Left=12.59\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1944/3393 [15:25<11:46,  2.05batch/s, Batch Loss=0.1112, Avg Loss=0.2868, Time Left=12.58\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  57%|▌| 1945/3393 [15:25<11:38,  2.07batch/s, Batch Loss=0.1112, Avg Loss=0.2868, Time Left=12.58\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1945/3393 [15:26<11:38,  2.07batch/s, Batch Loss=0.2970, Avg Loss=0.2868, Time Left=12.58\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1946/3393 [15:26<11:37,  2.08batch/s, Batch Loss=0.2970, Avg Loss=0.2868, Time Left=12.58\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1946/3393 [15:26<11:37,  2.08batch/s, Batch Loss=0.1867, Avg Loss=0.2867, Time Left=12.57\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1947/3393 [15:26<11:30,  2.10batch/s, Batch Loss=0.1867, Avg Loss=0.2867, Time Left=12.57\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1947/3393 [15:27<11:30,  2.10batch/s, Batch Loss=0.2223, Avg Loss=0.2867, Time Left=12.56\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1948/3393 [15:27<11:45,  2.05batch/s, Batch Loss=0.2223, Avg Loss=0.2867, Time Left=12.56\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1948/3393 [15:27<11:45,  2.05batch/s, Batch Loss=0.1675, Avg Loss=0.2866, Time Left=12.55\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1949/3393 [15:27<11:43,  2.05batch/s, Batch Loss=0.1675, Avg Loss=0.2866, Time Left=12.55\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1949/3393 [15:28<11:43,  2.05batch/s, Batch Loss=0.3879, Avg Loss=0.2867, Time Left=12.54\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1950/3393 [15:28<11:48,  2.04batch/s, Batch Loss=0.3879, Avg Loss=0.2867, Time Left=12.54\u001b[A\n",
      "Epoch 1/3 - Training:  57%|▌| 1950/3393 [15:28<11:48,  2.04batch/s, Batch Loss=0.2690, Avg Loss=0.2867, Time Left=12.53\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1951/3393 [15:28<11:52,  2.02batch/s, Batch Loss=0.2690, Avg Loss=0.2867, Time Left=12.53\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1951/3393 [15:29<11:52,  2.02batch/s, Batch Loss=0.1677, Avg Loss=0.2866, Time Left=12.53\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1952/3393 [15:29<11:40,  2.06batch/s, Batch Loss=0.1677, Avg Loss=0.2866, Time Left=12.53\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1952/3393 [15:29<11:40,  2.06batch/s, Batch Loss=0.0670, Avg Loss=0.2865, Time Left=12.52\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1953/3393 [15:29<11:32,  2.08batch/s, Batch Loss=0.0670, Avg Loss=0.2865, Time Left=12.52\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1953/3393 [15:30<11:32,  2.08batch/s, Batch Loss=0.1218, Avg Loss=0.2864, Time Left=12.51\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1954/3393 [15:30<11:19,  2.12batch/s, Batch Loss=0.1218, Avg Loss=0.2864, Time Left=12.51\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1954/3393 [15:30<11:19,  2.12batch/s, Batch Loss=0.1938, Avg Loss=0.2864, Time Left=12.50\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1955/3393 [15:30<11:29,  2.09batch/s, Batch Loss=0.1938, Avg Loss=0.2864, Time Left=12.50\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1955/3393 [15:31<11:29,  2.09batch/s, Batch Loss=0.0969, Avg Loss=0.2863, Time Left=12.49\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1956/3393 [15:31<11:24,  2.10batch/s, Batch Loss=0.0969, Avg Loss=0.2863, Time Left=12.49\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1956/3393 [15:31<11:24,  2.10batch/s, Batch Loss=0.1481, Avg Loss=0.2862, Time Left=12.48\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1957/3393 [15:31<11:20,  2.11batch/s, Batch Loss=0.1481, Avg Loss=0.2862, Time Left=12.48\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1957/3393 [15:32<11:20,  2.11batch/s, Batch Loss=0.1218, Avg Loss=0.2861, Time Left=12.47\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1958/3393 [15:32<11:23,  2.10batch/s, Batch Loss=0.1218, Avg Loss=0.2861, Time Left=12.47\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1958/3393 [15:32<11:23,  2.10batch/s, Batch Loss=0.1450, Avg Loss=0.2860, Time Left=12.47\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1959/3393 [15:32<11:25,  2.09batch/s, Batch Loss=0.1450, Avg Loss=0.2860, Time Left=12.47\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1959/3393 [15:33<11:25,  2.09batch/s, Batch Loss=0.1258, Avg Loss=0.2859, Time Left=12.46\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1960/3393 [15:33<11:24,  2.09batch/s, Batch Loss=0.1258, Avg Loss=0.2859, Time Left=12.46\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1960/3393 [15:33<11:24,  2.09batch/s, Batch Loss=0.0879, Avg Loss=0.2858, Time Left=12.45\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1961/3393 [15:33<11:30,  2.07batch/s, Batch Loss=0.0879, Avg Loss=0.2858, Time Left=12.45\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1961/3393 [15:34<11:30,  2.07batch/s, Batch Loss=0.2187, Avg Loss=0.2858, Time Left=12.44\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1962/3393 [15:34<11:30,  2.07batch/s, Batch Loss=0.2187, Avg Loss=0.2858, Time Left=12.44\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1962/3393 [15:34<11:30,  2.07batch/s, Batch Loss=0.0136, Avg Loss=0.2857, Time Left=12.43\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1963/3393 [15:34<11:23,  2.09batch/s, Batch Loss=0.0136, Avg Loss=0.2857, Time Left=12.43\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1963/3393 [15:35<11:23,  2.09batch/s, Batch Loss=0.0713, Avg Loss=0.2855, Time Left=12.42\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1964/3393 [15:35<11:25,  2.09batch/s, Batch Loss=0.0713, Avg Loss=0.2855, Time Left=12.42\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1964/3393 [15:35<11:25,  2.09batch/s, Batch Loss=0.3934, Avg Loss=0.2856, Time Left=12.41\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1965/3393 [15:35<11:33,  2.06batch/s, Batch Loss=0.3934, Avg Loss=0.2856, Time Left=12.41\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1965/3393 [15:36<11:33,  2.06batch/s, Batch Loss=0.1740, Avg Loss=0.2855, Time Left=12.41\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1966/3393 [15:36<11:32,  2.06batch/s, Batch Loss=0.1740, Avg Loss=0.2855, Time Left=12.41\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1966/3393 [15:36<11:32,  2.06batch/s, Batch Loss=0.0389, Avg Loss=0.2854, Time Left=12.40\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1967/3393 [15:36<11:51,  2.00batch/s, Batch Loss=0.0389, Avg Loss=0.2854, Time Left=12.40\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1967/3393 [15:37<11:51,  2.00batch/s, Batch Loss=0.3691, Avg Loss=0.2855, Time Left=12.39\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1968/3393 [15:37<11:49,  2.01batch/s, Batch Loss=0.3691, Avg Loss=0.2855, Time Left=12.39\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1968/3393 [15:37<11:49,  2.01batch/s, Batch Loss=0.0242, Avg Loss=0.2853, Time Left=12.38\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1969/3393 [15:37<11:59,  1.98batch/s, Batch Loss=0.0242, Avg Loss=0.2853, Time Left=12.38\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1969/3393 [15:38<11:59,  1.98batch/s, Batch Loss=0.1141, Avg Loss=0.2852, Time Left=12.37\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1970/3393 [15:38<11:50,  2.00batch/s, Batch Loss=0.1141, Avg Loss=0.2852, Time Left=12.37\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1970/3393 [15:38<11:50,  2.00batch/s, Batch Loss=0.1484, Avg Loss=0.2852, Time Left=12.37\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1971/3393 [15:38<11:43,  2.02batch/s, Batch Loss=0.1484, Avg Loss=0.2852, Time Left=12.37\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1971/3393 [15:39<11:43,  2.02batch/s, Batch Loss=0.2551, Avg Loss=0.2851, Time Left=12.36\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1972/3393 [15:39<11:39,  2.03batch/s, Batch Loss=0.2551, Avg Loss=0.2851, Time Left=12.36\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1972/3393 [15:39<11:39,  2.03batch/s, Batch Loss=0.2370, Avg Loss=0.2851, Time Left=12.35\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1973/3393 [15:39<11:41,  2.02batch/s, Batch Loss=0.2370, Avg Loss=0.2851, Time Left=12.35\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1973/3393 [15:40<11:41,  2.02batch/s, Batch Loss=0.0412, Avg Loss=0.2850, Time Left=12.34\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1974/3393 [15:40<11:42,  2.02batch/s, Batch Loss=0.0412, Avg Loss=0.2850, Time Left=12.34\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1974/3393 [15:40<11:42,  2.02batch/s, Batch Loss=0.1439, Avg Loss=0.2849, Time Left=12.33\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1975/3393 [15:40<11:45,  2.01batch/s, Batch Loss=0.1439, Avg Loss=0.2849, Time Left=12.33\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1975/3393 [15:41<11:45,  2.01batch/s, Batch Loss=0.1433, Avg Loss=0.2848, Time Left=12.32\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1976/3393 [15:41<11:32,  2.05batch/s, Batch Loss=0.1433, Avg Loss=0.2848, Time Left=12.32\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1976/3393 [15:41<11:32,  2.05batch/s, Batch Loss=0.0547, Avg Loss=0.2847, Time Left=12.32\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1977/3393 [15:41<11:33,  2.04batch/s, Batch Loss=0.0547, Avg Loss=0.2847, Time Left=12.32\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1977/3393 [15:41<11:33,  2.04batch/s, Batch Loss=0.0752, Avg Loss=0.2846, Time Left=12.31\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  58%|▌| 1978/3393 [15:41<11:26,  2.06batch/s, Batch Loss=0.0752, Avg Loss=0.2846, Time Left=12.31\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1978/3393 [15:42<11:26,  2.06batch/s, Batch Loss=0.2819, Avg Loss=0.2846, Time Left=12.30\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1979/3393 [15:42<11:45,  2.00batch/s, Batch Loss=0.2819, Avg Loss=0.2846, Time Left=12.30\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1979/3393 [15:43<11:45,  2.00batch/s, Batch Loss=0.1638, Avg Loss=0.2845, Time Left=12.29\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1980/3393 [15:43<11:39,  2.02batch/s, Batch Loss=0.1638, Avg Loss=0.2845, Time Left=12.29\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1980/3393 [15:43<11:39,  2.02batch/s, Batch Loss=0.3089, Avg Loss=0.2846, Time Left=12.28\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1981/3393 [15:43<11:27,  2.05batch/s, Batch Loss=0.3089, Avg Loss=0.2846, Time Left=12.28\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1981/3393 [15:43<11:27,  2.05batch/s, Batch Loss=0.0983, Avg Loss=0.2845, Time Left=12.27\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1982/3393 [15:43<11:25,  2.06batch/s, Batch Loss=0.0983, Avg Loss=0.2845, Time Left=12.27\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1982/3393 [15:44<11:25,  2.06batch/s, Batch Loss=0.0763, Avg Loss=0.2843, Time Left=12.27\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1983/3393 [15:44<11:10,  2.10batch/s, Batch Loss=0.0763, Avg Loss=0.2843, Time Left=12.27\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1983/3393 [15:44<11:10,  2.10batch/s, Batch Loss=0.1343, Avg Loss=0.2843, Time Left=12.26\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1984/3393 [15:44<11:13,  2.09batch/s, Batch Loss=0.1343, Avg Loss=0.2843, Time Left=12.26\u001b[A\n",
      "Epoch 1/3 - Training:  58%|▌| 1984/3393 [15:45<11:13,  2.09batch/s, Batch Loss=0.1020, Avg Loss=0.2842, Time Left=12.25\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1985/3393 [15:45<11:35,  2.02batch/s, Batch Loss=0.1020, Avg Loss=0.2842, Time Left=12.25\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1985/3393 [15:45<11:35,  2.02batch/s, Batch Loss=0.2228, Avg Loss=0.2841, Time Left=12.24\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1986/3393 [15:45<11:31,  2.04batch/s, Batch Loss=0.2228, Avg Loss=0.2841, Time Left=12.24\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1986/3393 [15:46<11:31,  2.04batch/s, Batch Loss=0.0800, Avg Loss=0.2840, Time Left=12.23\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1987/3393 [15:46<11:27,  2.05batch/s, Batch Loss=0.0800, Avg Loss=0.2840, Time Left=12.23\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1987/3393 [15:46<11:27,  2.05batch/s, Batch Loss=0.3510, Avg Loss=0.2841, Time Left=12.22\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1988/3393 [15:46<11:12,  2.09batch/s, Batch Loss=0.3510, Avg Loss=0.2841, Time Left=12.22\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1988/3393 [15:47<11:12,  2.09batch/s, Batch Loss=0.3194, Avg Loss=0.2841, Time Left=12.22\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1989/3393 [15:47<11:19,  2.07batch/s, Batch Loss=0.3194, Avg Loss=0.2841, Time Left=12.22\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1989/3393 [15:47<11:19,  2.07batch/s, Batch Loss=0.2470, Avg Loss=0.2841, Time Left=12.21\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1990/3393 [15:47<11:39,  2.01batch/s, Batch Loss=0.2470, Avg Loss=0.2841, Time Left=12.21\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1990/3393 [15:48<11:39,  2.01batch/s, Batch Loss=0.1359, Avg Loss=0.2840, Time Left=12.20\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1991/3393 [15:48<11:33,  2.02batch/s, Batch Loss=0.1359, Avg Loss=0.2840, Time Left=12.20\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1991/3393 [15:48<11:33,  2.02batch/s, Batch Loss=0.1658, Avg Loss=0.2839, Time Left=12.19\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1992/3393 [15:48<11:32,  2.02batch/s, Batch Loss=0.1658, Avg Loss=0.2839, Time Left=12.19\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1992/3393 [15:49<11:32,  2.02batch/s, Batch Loss=0.1424, Avg Loss=0.2839, Time Left=12.18\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1993/3393 [15:49<11:25,  2.04batch/s, Batch Loss=0.1424, Avg Loss=0.2839, Time Left=12.18\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1993/3393 [15:49<11:25,  2.04batch/s, Batch Loss=0.4927, Avg Loss=0.2840, Time Left=12.17\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1994/3393 [15:49<11:27,  2.04batch/s, Batch Loss=0.4927, Avg Loss=0.2840, Time Left=12.17\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1994/3393 [15:50<11:27,  2.04batch/s, Batch Loss=0.0681, Avg Loss=0.2839, Time Left=12.17\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1995/3393 [15:50<11:17,  2.06batch/s, Batch Loss=0.0681, Avg Loss=0.2839, Time Left=12.17\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1995/3393 [15:50<11:17,  2.06batch/s, Batch Loss=0.5025, Avg Loss=0.2840, Time Left=12.16\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1996/3393 [15:50<11:16,  2.07batch/s, Batch Loss=0.5025, Avg Loss=0.2840, Time Left=12.16\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1996/3393 [15:51<11:16,  2.07batch/s, Batch Loss=0.5158, Avg Loss=0.2841, Time Left=12.15\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1997/3393 [15:51<11:41,  1.99batch/s, Batch Loss=0.5158, Avg Loss=0.2841, Time Left=12.15\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1997/3393 [15:51<11:41,  1.99batch/s, Batch Loss=0.1032, Avg Loss=0.2840, Time Left=12.14\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1998/3393 [15:51<11:42,  1.99batch/s, Batch Loss=0.1032, Avg Loss=0.2840, Time Left=12.14\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1998/3393 [15:52<11:42,  1.99batch/s, Batch Loss=0.0512, Avg Loss=0.2839, Time Left=12.13\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1999/3393 [15:52<11:49,  1.96batch/s, Batch Loss=0.0512, Avg Loss=0.2839, Time Left=12.13\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 1999/3393 [15:52<11:49,  1.96batch/s, Batch Loss=0.5430, Avg Loss=0.2840, Time Left=12.13\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2000/3393 [15:52<11:35,  2.00batch/s, Batch Loss=0.5430, Avg Loss=0.2840, Time Left=12.13\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2000/3393 [15:53<11:35,  2.00batch/s, Batch Loss=0.1553, Avg Loss=0.2839, Time Left=12.12\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2001/3393 [15:53<11:36,  2.00batch/s, Batch Loss=0.1553, Avg Loss=0.2839, Time Left=12.12\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2001/3393 [15:53<11:36,  2.00batch/s, Batch Loss=0.1566, Avg Loss=0.2839, Time Left=12.11\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2002/3393 [15:53<11:29,  2.02batch/s, Batch Loss=0.1566, Avg Loss=0.2839, Time Left=12.11\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2002/3393 [15:54<11:29,  2.02batch/s, Batch Loss=0.3630, Avg Loss=0.2839, Time Left=12.10\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2003/3393 [15:54<11:17,  2.05batch/s, Batch Loss=0.3630, Avg Loss=0.2839, Time Left=12.10\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2003/3393 [15:54<11:17,  2.05batch/s, Batch Loss=0.1866, Avg Loss=0.2839, Time Left=12.09\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2004/3393 [15:54<11:34,  2.00batch/s, Batch Loss=0.1866, Avg Loss=0.2839, Time Left=12.09\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2004/3393 [15:55<11:34,  2.00batch/s, Batch Loss=0.2674, Avg Loss=0.2839, Time Left=12.08\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2005/3393 [15:55<11:21,  2.04batch/s, Batch Loss=0.2674, Avg Loss=0.2839, Time Left=12.08\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2005/3393 [15:55<11:21,  2.04batch/s, Batch Loss=0.3717, Avg Loss=0.2839, Time Left=12.07\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2006/3393 [15:55<11:11,  2.07batch/s, Batch Loss=0.3717, Avg Loss=0.2839, Time Left=12.07\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2006/3393 [15:56<11:11,  2.07batch/s, Batch Loss=0.4117, Avg Loss=0.2840, Time Left=12.07\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2007/3393 [15:56<11:16,  2.05batch/s, Batch Loss=0.4117, Avg Loss=0.2840, Time Left=12.07\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2007/3393 [15:56<11:16,  2.05batch/s, Batch Loss=0.4540, Avg Loss=0.2841, Time Left=12.06\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2008/3393 [15:56<11:15,  2.05batch/s, Batch Loss=0.4540, Avg Loss=0.2841, Time Left=12.06\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2008/3393 [15:57<11:15,  2.05batch/s, Batch Loss=0.1236, Avg Loss=0.2840, Time Left=12.05\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2009/3393 [15:57<11:31,  2.00batch/s, Batch Loss=0.1236, Avg Loss=0.2840, Time Left=12.05\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2009/3393 [15:57<11:31,  2.00batch/s, Batch Loss=0.1805, Avg Loss=0.2839, Time Left=12.04\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2010/3393 [15:57<11:25,  2.02batch/s, Batch Loss=0.1805, Avg Loss=0.2839, Time Left=12.04\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2010/3393 [15:58<11:25,  2.02batch/s, Batch Loss=0.1263, Avg Loss=0.2838, Time Left=12.03\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  59%|▌| 2011/3393 [15:58<11:48,  1.95batch/s, Batch Loss=0.1263, Avg Loss=0.2838, Time Left=12.03\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2011/3393 [15:58<11:48,  1.95batch/s, Batch Loss=0.1365, Avg Loss=0.2838, Time Left=12.03\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2012/3393 [15:58<11:43,  1.96batch/s, Batch Loss=0.1365, Avg Loss=0.2838, Time Left=12.03\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2012/3393 [15:59<11:43,  1.96batch/s, Batch Loss=0.2730, Avg Loss=0.2838, Time Left=12.02\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2013/3393 [15:59<11:44,  1.96batch/s, Batch Loss=0.2730, Avg Loss=0.2838, Time Left=12.02\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2013/3393 [15:59<11:44,  1.96batch/s, Batch Loss=0.2130, Avg Loss=0.2837, Time Left=12.01\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2014/3393 [15:59<11:34,  1.99batch/s, Batch Loss=0.2130, Avg Loss=0.2837, Time Left=12.01\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2014/3393 [16:00<11:34,  1.99batch/s, Batch Loss=0.1137, Avg Loss=0.2836, Time Left=12.00\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2015/3393 [16:00<11:58,  1.92batch/s, Batch Loss=0.1137, Avg Loss=0.2836, Time Left=12.00\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2015/3393 [16:00<11:58,  1.92batch/s, Batch Loss=0.1519, Avg Loss=0.2836, Time Left=11.99\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2016/3393 [16:00<11:36,  1.98batch/s, Batch Loss=0.1519, Avg Loss=0.2836, Time Left=11.99\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2016/3393 [16:01<11:36,  1.98batch/s, Batch Loss=0.0905, Avg Loss=0.2835, Time Left=11.99\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2017/3393 [16:01<11:48,  1.94batch/s, Batch Loss=0.0905, Avg Loss=0.2835, Time Left=11.99\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2017/3393 [16:01<11:48,  1.94batch/s, Batch Loss=0.2011, Avg Loss=0.2834, Time Left=11.98\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2018/3393 [16:01<11:27,  2.00batch/s, Batch Loss=0.2011, Avg Loss=0.2834, Time Left=11.98\u001b[A\n",
      "Epoch 1/3 - Training:  59%|▌| 2018/3393 [16:02<11:27,  2.00batch/s, Batch Loss=0.1784, Avg Loss=0.2834, Time Left=11.97\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2019/3393 [16:02<11:20,  2.02batch/s, Batch Loss=0.1784, Avg Loss=0.2834, Time Left=11.97\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2019/3393 [16:02<11:20,  2.02batch/s, Batch Loss=0.2084, Avg Loss=0.2833, Time Left=11.96\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2020/3393 [16:02<11:08,  2.05batch/s, Batch Loss=0.2084, Avg Loss=0.2833, Time Left=11.96\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2020/3393 [16:03<11:08,  2.05batch/s, Batch Loss=0.1724, Avg Loss=0.2833, Time Left=11.95\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2021/3393 [16:03<11:00,  2.08batch/s, Batch Loss=0.1724, Avg Loss=0.2833, Time Left=11.95\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2021/3393 [16:03<11:00,  2.08batch/s, Batch Loss=0.1643, Avg Loss=0.2832, Time Left=11.94\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2022/3393 [16:03<11:06,  2.06batch/s, Batch Loss=0.1643, Avg Loss=0.2832, Time Left=11.94\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2022/3393 [16:04<11:06,  2.06batch/s, Batch Loss=0.1490, Avg Loss=0.2831, Time Left=11.94\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2023/3393 [16:04<11:06,  2.06batch/s, Batch Loss=0.1490, Avg Loss=0.2831, Time Left=11.94\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2023/3393 [16:04<11:06,  2.06batch/s, Batch Loss=0.0930, Avg Loss=0.2830, Time Left=11.93\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2024/3393 [16:04<11:08,  2.05batch/s, Batch Loss=0.0930, Avg Loss=0.2830, Time Left=11.93\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2024/3393 [16:05<11:08,  2.05batch/s, Batch Loss=0.1352, Avg Loss=0.2830, Time Left=11.92\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2025/3393 [16:05<11:14,  2.03batch/s, Batch Loss=0.1352, Avg Loss=0.2830, Time Left=11.92\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2025/3393 [16:05<11:14,  2.03batch/s, Batch Loss=0.2113, Avg Loss=0.2829, Time Left=11.91\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2026/3393 [16:05<11:17,  2.02batch/s, Batch Loss=0.2113, Avg Loss=0.2829, Time Left=11.91\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2026/3393 [16:06<11:17,  2.02batch/s, Batch Loss=0.4443, Avg Loss=0.2830, Time Left=11.90\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2027/3393 [16:06<11:05,  2.05batch/s, Batch Loss=0.4443, Avg Loss=0.2830, Time Left=11.90\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2027/3393 [16:06<11:05,  2.05batch/s, Batch Loss=0.1625, Avg Loss=0.2830, Time Left=11.89\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2028/3393 [16:06<11:05,  2.05batch/s, Batch Loss=0.1625, Avg Loss=0.2830, Time Left=11.89\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2028/3393 [16:07<11:05,  2.05batch/s, Batch Loss=0.0077, Avg Loss=0.2828, Time Left=11.89\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2029/3393 [16:07<10:55,  2.08batch/s, Batch Loss=0.0077, Avg Loss=0.2828, Time Left=11.89\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2029/3393 [16:07<10:55,  2.08batch/s, Batch Loss=0.2224, Avg Loss=0.2828, Time Left=11.88\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2030/3393 [16:07<10:55,  2.08batch/s, Batch Loss=0.2224, Avg Loss=0.2828, Time Left=11.88\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2030/3393 [16:08<10:55,  2.08batch/s, Batch Loss=0.7223, Avg Loss=0.2830, Time Left=11.87\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2031/3393 [16:08<10:57,  2.07batch/s, Batch Loss=0.7223, Avg Loss=0.2830, Time Left=11.87\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2031/3393 [16:08<10:57,  2.07batch/s, Batch Loss=0.3772, Avg Loss=0.2831, Time Left=11.86\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2032/3393 [16:08<10:49,  2.10batch/s, Batch Loss=0.3772, Avg Loss=0.2831, Time Left=11.86\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2032/3393 [16:09<10:49,  2.10batch/s, Batch Loss=0.4558, Avg Loss=0.2831, Time Left=11.85\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2033/3393 [16:09<10:45,  2.11batch/s, Batch Loss=0.4558, Avg Loss=0.2831, Time Left=11.85\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2033/3393 [16:09<10:45,  2.11batch/s, Batch Loss=0.1646, Avg Loss=0.2831, Time Left=11.84\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2034/3393 [16:09<10:54,  2.08batch/s, Batch Loss=0.1646, Avg Loss=0.2831, Time Left=11.84\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2034/3393 [16:10<10:54,  2.08batch/s, Batch Loss=0.1023, Avg Loss=0.2830, Time Left=11.83\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2035/3393 [16:10<10:48,  2.09batch/s, Batch Loss=0.1023, Avg Loss=0.2830, Time Left=11.83\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2035/3393 [16:10<10:48,  2.09batch/s, Batch Loss=0.1213, Avg Loss=0.2829, Time Left=11.83\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2036/3393 [16:10<10:43,  2.11batch/s, Batch Loss=0.1213, Avg Loss=0.2829, Time Left=11.83\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2036/3393 [16:11<10:43,  2.11batch/s, Batch Loss=0.1198, Avg Loss=0.2828, Time Left=11.82\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2037/3393 [16:11<11:00,  2.05batch/s, Batch Loss=0.1198, Avg Loss=0.2828, Time Left=11.82\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2037/3393 [16:11<11:00,  2.05batch/s, Batch Loss=0.0788, Avg Loss=0.2827, Time Left=11.81\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2038/3393 [16:11<10:58,  2.06batch/s, Batch Loss=0.0788, Avg Loss=0.2827, Time Left=11.81\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2038/3393 [16:12<10:58,  2.06batch/s, Batch Loss=0.1025, Avg Loss=0.2826, Time Left=11.80\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2039/3393 [16:12<11:03,  2.04batch/s, Batch Loss=0.1025, Avg Loss=0.2826, Time Left=11.80\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2039/3393 [16:12<11:03,  2.04batch/s, Batch Loss=0.3399, Avg Loss=0.2827, Time Left=11.79\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2040/3393 [16:12<10:55,  2.07batch/s, Batch Loss=0.3399, Avg Loss=0.2827, Time Left=11.79\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2040/3393 [16:12<10:55,  2.07batch/s, Batch Loss=0.1185, Avg Loss=0.2826, Time Left=11.78\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2041/3393 [16:12<10:47,  2.09batch/s, Batch Loss=0.1185, Avg Loss=0.2826, Time Left=11.78\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2041/3393 [16:13<10:47,  2.09batch/s, Batch Loss=0.2712, Avg Loss=0.2826, Time Left=11.78\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2042/3393 [16:13<10:48,  2.08batch/s, Batch Loss=0.2712, Avg Loss=0.2826, Time Left=11.78\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2042/3393 [16:13<10:48,  2.08batch/s, Batch Loss=0.1485, Avg Loss=0.2825, Time Left=11.77\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2043/3393 [16:13<10:42,  2.10batch/s, Batch Loss=0.1485, Avg Loss=0.2825, Time Left=11.77\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2043/3393 [16:14<10:42,  2.10batch/s, Batch Loss=0.0191, Avg Loss=0.2824, Time Left=11.76\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  60%|▌| 2044/3393 [16:14<10:51,  2.07batch/s, Batch Loss=0.0191, Avg Loss=0.2824, Time Left=11.76\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2044/3393 [16:14<10:51,  2.07batch/s, Batch Loss=0.1779, Avg Loss=0.2823, Time Left=11.75\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2045/3393 [16:14<10:51,  2.07batch/s, Batch Loss=0.1779, Avg Loss=0.2823, Time Left=11.75\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2045/3393 [16:15<10:51,  2.07batch/s, Batch Loss=0.4458, Avg Loss=0.2824, Time Left=11.74\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2046/3393 [16:15<10:45,  2.09batch/s, Batch Loss=0.4458, Avg Loss=0.2824, Time Left=11.74\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2046/3393 [16:15<10:45,  2.09batch/s, Batch Loss=0.1742, Avg Loss=0.2823, Time Left=11.73\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2047/3393 [16:15<10:39,  2.11batch/s, Batch Loss=0.1742, Avg Loss=0.2823, Time Left=11.73\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2047/3393 [16:16<10:39,  2.11batch/s, Batch Loss=0.1503, Avg Loss=0.2823, Time Left=11.73\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2048/3393 [16:16<10:48,  2.07batch/s, Batch Loss=0.1503, Avg Loss=0.2823, Time Left=11.73\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2048/3393 [16:16<10:48,  2.07batch/s, Batch Loss=0.3492, Avg Loss=0.2823, Time Left=11.72\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2049/3393 [16:16<10:56,  2.05batch/s, Batch Loss=0.3492, Avg Loss=0.2823, Time Left=11.72\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2049/3393 [16:17<10:56,  2.05batch/s, Batch Loss=0.2139, Avg Loss=0.2823, Time Left=11.71\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2050/3393 [16:17<10:54,  2.05batch/s, Batch Loss=0.2139, Avg Loss=0.2823, Time Left=11.71\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2050/3393 [16:17<10:54,  2.05batch/s, Batch Loss=0.1238, Avg Loss=0.2822, Time Left=11.70\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2051/3393 [16:17<10:51,  2.06batch/s, Batch Loss=0.1238, Avg Loss=0.2822, Time Left=11.70\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2051/3393 [16:18<10:51,  2.06batch/s, Batch Loss=0.3794, Avg Loss=0.2822, Time Left=11.69\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2052/3393 [16:18<10:50,  2.06batch/s, Batch Loss=0.3794, Avg Loss=0.2822, Time Left=11.69\u001b[A\n",
      "Epoch 1/3 - Training:  60%|▌| 2052/3393 [16:18<10:50,  2.06batch/s, Batch Loss=0.1567, Avg Loss=0.2822, Time Left=11.68\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2053/3393 [16:18<11:02,  2.02batch/s, Batch Loss=0.1567, Avg Loss=0.2822, Time Left=11.68\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2053/3393 [16:19<11:02,  2.02batch/s, Batch Loss=0.1021, Avg Loss=0.2821, Time Left=11.68\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2054/3393 [16:19<10:57,  2.04batch/s, Batch Loss=0.1021, Avg Loss=0.2821, Time Left=11.68\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2054/3393 [16:19<10:57,  2.04batch/s, Batch Loss=0.0928, Avg Loss=0.2820, Time Left=11.67\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2055/3393 [16:19<10:54,  2.04batch/s, Batch Loss=0.0928, Avg Loss=0.2820, Time Left=11.67\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2055/3393 [16:20<10:54,  2.04batch/s, Batch Loss=0.2202, Avg Loss=0.2820, Time Left=11.66\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2056/3393 [16:20<10:52,  2.05batch/s, Batch Loss=0.2202, Avg Loss=0.2820, Time Left=11.66\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2056/3393 [16:20<10:52,  2.05batch/s, Batch Loss=0.2029, Avg Loss=0.2819, Time Left=11.65\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2057/3393 [16:20<10:44,  2.07batch/s, Batch Loss=0.2029, Avg Loss=0.2819, Time Left=11.65\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2057/3393 [16:21<10:44,  2.07batch/s, Batch Loss=0.2258, Avg Loss=0.2819, Time Left=11.64\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2058/3393 [16:21<10:43,  2.07batch/s, Batch Loss=0.2258, Avg Loss=0.2819, Time Left=11.64\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2058/3393 [16:21<10:43,  2.07batch/s, Batch Loss=0.3208, Avg Loss=0.2819, Time Left=11.63\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2059/3393 [16:21<10:43,  2.07batch/s, Batch Loss=0.3208, Avg Loss=0.2819, Time Left=11.63\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2059/3393 [16:22<10:43,  2.07batch/s, Batch Loss=0.2673, Avg Loss=0.2819, Time Left=11.62\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2060/3393 [16:22<10:37,  2.09batch/s, Batch Loss=0.2673, Avg Loss=0.2819, Time Left=11.62\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2060/3393 [16:22<10:37,  2.09batch/s, Batch Loss=0.2618, Avg Loss=0.2819, Time Left=11.62\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2061/3393 [16:22<11:00,  2.02batch/s, Batch Loss=0.2618, Avg Loss=0.2819, Time Left=11.62\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2061/3393 [16:23<11:00,  2.02batch/s, Batch Loss=0.1851, Avg Loss=0.2818, Time Left=11.61\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2062/3393 [16:23<10:52,  2.04batch/s, Batch Loss=0.1851, Avg Loss=0.2818, Time Left=11.61\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2062/3393 [16:23<10:52,  2.04batch/s, Batch Loss=0.1382, Avg Loss=0.2818, Time Left=11.60\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2063/3393 [16:23<10:50,  2.05batch/s, Batch Loss=0.1382, Avg Loss=0.2818, Time Left=11.60\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2063/3393 [16:24<10:50,  2.05batch/s, Batch Loss=0.3234, Avg Loss=0.2818, Time Left=11.59\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2064/3393 [16:24<10:44,  2.06batch/s, Batch Loss=0.3234, Avg Loss=0.2818, Time Left=11.59\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2064/3393 [16:24<10:44,  2.06batch/s, Batch Loss=0.2146, Avg Loss=0.2818, Time Left=11.58\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2065/3393 [16:24<10:41,  2.07batch/s, Batch Loss=0.2146, Avg Loss=0.2818, Time Left=11.58\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2065/3393 [16:25<10:41,  2.07batch/s, Batch Loss=0.2423, Avg Loss=0.2817, Time Left=11.57\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2066/3393 [16:25<10:38,  2.08batch/s, Batch Loss=0.2423, Avg Loss=0.2817, Time Left=11.57\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2066/3393 [16:25<10:38,  2.08batch/s, Batch Loss=0.1132, Avg Loss=0.2817, Time Left=11.57\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2067/3393 [16:25<11:20,  1.95batch/s, Batch Loss=0.1132, Avg Loss=0.2817, Time Left=11.57\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2067/3393 [16:26<11:20,  1.95batch/s, Batch Loss=0.0493, Avg Loss=0.2815, Time Left=11.56\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2068/3393 [16:26<11:32,  1.91batch/s, Batch Loss=0.0493, Avg Loss=0.2815, Time Left=11.56\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2068/3393 [16:26<11:32,  1.91batch/s, Batch Loss=0.1128, Avg Loss=0.2815, Time Left=11.55\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2069/3393 [16:26<11:46,  1.87batch/s, Batch Loss=0.1128, Avg Loss=0.2815, Time Left=11.55\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2069/3393 [16:27<11:46,  1.87batch/s, Batch Loss=0.2981, Avg Loss=0.2815, Time Left=11.54\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2070/3393 [16:27<11:44,  1.88batch/s, Batch Loss=0.2981, Avg Loss=0.2815, Time Left=11.54\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2070/3393 [16:27<11:44,  1.88batch/s, Batch Loss=0.0695, Avg Loss=0.2814, Time Left=11.54\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2071/3393 [16:27<11:49,  1.86batch/s, Batch Loss=0.0695, Avg Loss=0.2814, Time Left=11.54\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2071/3393 [16:28<11:49,  1.86batch/s, Batch Loss=0.1121, Avg Loss=0.2813, Time Left=11.53\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2072/3393 [16:28<11:27,  1.92batch/s, Batch Loss=0.1121, Avg Loss=0.2813, Time Left=11.53\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2072/3393 [16:28<11:27,  1.92batch/s, Batch Loss=0.1218, Avg Loss=0.2812, Time Left=11.52\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2073/3393 [16:28<11:39,  1.89batch/s, Batch Loss=0.1218, Avg Loss=0.2812, Time Left=11.52\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2073/3393 [16:29<11:39,  1.89batch/s, Batch Loss=0.1796, Avg Loss=0.2811, Time Left=11.51\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2074/3393 [16:29<11:25,  1.92batch/s, Batch Loss=0.1796, Avg Loss=0.2811, Time Left=11.51\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2074/3393 [16:29<11:25,  1.92batch/s, Batch Loss=0.1847, Avg Loss=0.2811, Time Left=11.50\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2075/3393 [16:29<11:16,  1.95batch/s, Batch Loss=0.1847, Avg Loss=0.2811, Time Left=11.50\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2075/3393 [16:30<11:16,  1.95batch/s, Batch Loss=0.1436, Avg Loss=0.2810, Time Left=11.50\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2076/3393 [16:30<11:08,  1.97batch/s, Batch Loss=0.1436, Avg Loss=0.2810, Time Left=11.50\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2076/3393 [16:30<11:08,  1.97batch/s, Batch Loss=0.0276, Avg Loss=0.2809, Time Left=11.49\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  61%|▌| 2077/3393 [16:30<11:04,  1.98batch/s, Batch Loss=0.0276, Avg Loss=0.2809, Time Left=11.49\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2077/3393 [16:31<11:04,  1.98batch/s, Batch Loss=0.0434, Avg Loss=0.2808, Time Left=11.48\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2078/3393 [16:31<11:03,  1.98batch/s, Batch Loss=0.0434, Avg Loss=0.2808, Time Left=11.48\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2078/3393 [16:31<11:03,  1.98batch/s, Batch Loss=0.2658, Avg Loss=0.2808, Time Left=11.47\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2079/3393 [16:31<11:27,  1.91batch/s, Batch Loss=0.2658, Avg Loss=0.2808, Time Left=11.47\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2079/3393 [16:32<11:27,  1.91batch/s, Batch Loss=0.0178, Avg Loss=0.2806, Time Left=11.47\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2080/3393 [16:32<11:48,  1.85batch/s, Batch Loss=0.0178, Avg Loss=0.2806, Time Left=11.47\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2080/3393 [16:32<11:48,  1.85batch/s, Batch Loss=0.0580, Avg Loss=0.2805, Time Left=11.46\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2081/3393 [16:32<11:29,  1.90batch/s, Batch Loss=0.0580, Avg Loss=0.2805, Time Left=11.46\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2081/3393 [16:33<11:29,  1.90batch/s, Batch Loss=0.0509, Avg Loss=0.2804, Time Left=11.45\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2082/3393 [16:33<11:47,  1.85batch/s, Batch Loss=0.0509, Avg Loss=0.2804, Time Left=11.45\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2082/3393 [16:34<11:47,  1.85batch/s, Batch Loss=0.1885, Avg Loss=0.2804, Time Left=11.44\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2083/3393 [16:34<11:56,  1.83batch/s, Batch Loss=0.1885, Avg Loss=0.2804, Time Left=11.44\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2083/3393 [16:34<11:56,  1.83batch/s, Batch Loss=0.1392, Avg Loss=0.2803, Time Left=11.43\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2084/3393 [16:34<11:56,  1.83batch/s, Batch Loss=0.1392, Avg Loss=0.2803, Time Left=11.43\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2084/3393 [16:35<11:56,  1.83batch/s, Batch Loss=0.3684, Avg Loss=0.2803, Time Left=11.43\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2085/3393 [16:35<11:48,  1.85batch/s, Batch Loss=0.3684, Avg Loss=0.2803, Time Left=11.43\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2085/3393 [16:35<11:48,  1.85batch/s, Batch Loss=0.1457, Avg Loss=0.2803, Time Left=11.42\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2086/3393 [16:35<11:22,  1.92batch/s, Batch Loss=0.1457, Avg Loss=0.2803, Time Left=11.42\u001b[A\n",
      "Epoch 1/3 - Training:  61%|▌| 2086/3393 [16:36<11:22,  1.92batch/s, Batch Loss=0.3203, Avg Loss=0.2803, Time Left=11.41\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2087/3393 [16:36<11:23,  1.91batch/s, Batch Loss=0.3203, Avg Loss=0.2803, Time Left=11.41\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2087/3393 [16:36<11:23,  1.91batch/s, Batch Loss=0.0754, Avg Loss=0.2802, Time Left=11.40\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2088/3393 [16:36<11:07,  1.95batch/s, Batch Loss=0.0754, Avg Loss=0.2802, Time Left=11.40\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2088/3393 [16:37<11:07,  1.95batch/s, Batch Loss=0.0766, Avg Loss=0.2801, Time Left=11.39\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2089/3393 [16:37<11:38,  1.87batch/s, Batch Loss=0.0766, Avg Loss=0.2801, Time Left=11.39\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2089/3393 [16:37<11:38,  1.87batch/s, Batch Loss=0.3113, Avg Loss=0.2801, Time Left=11.39\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2090/3393 [16:37<11:57,  1.82batch/s, Batch Loss=0.3113, Avg Loss=0.2801, Time Left=11.39\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2090/3393 [16:38<11:57,  1.82batch/s, Batch Loss=0.1213, Avg Loss=0.2800, Time Left=11.38\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2091/3393 [16:38<12:05,  1.79batch/s, Batch Loss=0.1213, Avg Loss=0.2800, Time Left=11.38\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2091/3393 [16:38<12:05,  1.79batch/s, Batch Loss=0.5278, Avg Loss=0.2802, Time Left=11.37\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2092/3393 [16:38<11:33,  1.88batch/s, Batch Loss=0.5278, Avg Loss=0.2802, Time Left=11.37\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2092/3393 [16:39<11:33,  1.88batch/s, Batch Loss=0.1832, Avg Loss=0.2801, Time Left=11.36\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2093/3393 [16:39<11:32,  1.88batch/s, Batch Loss=0.1832, Avg Loss=0.2801, Time Left=11.36\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2093/3393 [16:39<11:32,  1.88batch/s, Batch Loss=0.2561, Avg Loss=0.2801, Time Left=11.36\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2094/3393 [16:39<11:16,  1.92batch/s, Batch Loss=0.2561, Avg Loss=0.2801, Time Left=11.36\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2094/3393 [16:40<11:16,  1.92batch/s, Batch Loss=0.0945, Avg Loss=0.2800, Time Left=11.35\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2095/3393 [16:40<11:01,  1.96batch/s, Batch Loss=0.0945, Avg Loss=0.2800, Time Left=11.35\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2095/3393 [16:40<11:01,  1.96batch/s, Batch Loss=0.2630, Avg Loss=0.2800, Time Left=11.34\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2096/3393 [16:40<10:51,  1.99batch/s, Batch Loss=0.2630, Avg Loss=0.2800, Time Left=11.34\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2096/3393 [16:41<10:51,  1.99batch/s, Batch Loss=0.0347, Avg Loss=0.2799, Time Left=11.33\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2097/3393 [16:41<10:49,  2.00batch/s, Batch Loss=0.0347, Avg Loss=0.2799, Time Left=11.33\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2097/3393 [16:41<10:49,  2.00batch/s, Batch Loss=0.1555, Avg Loss=0.2798, Time Left=11.32\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2098/3393 [16:41<10:48,  2.00batch/s, Batch Loss=0.1555, Avg Loss=0.2798, Time Left=11.32\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2098/3393 [16:42<10:48,  2.00batch/s, Batch Loss=0.1202, Avg Loss=0.2797, Time Left=11.31\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2099/3393 [16:42<10:48,  2.00batch/s, Batch Loss=0.1202, Avg Loss=0.2797, Time Left=11.31\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2099/3393 [16:42<10:48,  2.00batch/s, Batch Loss=0.1503, Avg Loss=0.2797, Time Left=11.31\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2100/3393 [16:42<10:40,  2.02batch/s, Batch Loss=0.1503, Avg Loss=0.2797, Time Left=11.31\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2100/3393 [16:43<10:40,  2.02batch/s, Batch Loss=0.0356, Avg Loss=0.2795, Time Left=11.30\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2101/3393 [16:43<10:36,  2.03batch/s, Batch Loss=0.0356, Avg Loss=0.2795, Time Left=11.30\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2101/3393 [16:43<10:36,  2.03batch/s, Batch Loss=0.1126, Avg Loss=0.2795, Time Left=11.29\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2102/3393 [16:43<10:38,  2.02batch/s, Batch Loss=0.1126, Avg Loss=0.2795, Time Left=11.29\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2102/3393 [16:44<10:38,  2.02batch/s, Batch Loss=0.3935, Avg Loss=0.2795, Time Left=11.28\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2103/3393 [16:44<10:39,  2.02batch/s, Batch Loss=0.3935, Avg Loss=0.2795, Time Left=11.28\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2103/3393 [16:44<10:39,  2.02batch/s, Batch Loss=0.1866, Avg Loss=0.2795, Time Left=11.27\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2104/3393 [16:44<10:35,  2.03batch/s, Batch Loss=0.1866, Avg Loss=0.2795, Time Left=11.27\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2104/3393 [16:45<10:35,  2.03batch/s, Batch Loss=0.1762, Avg Loss=0.2794, Time Left=11.26\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2105/3393 [16:45<10:43,  2.00batch/s, Batch Loss=0.1762, Avg Loss=0.2794, Time Left=11.26\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2105/3393 [16:45<10:43,  2.00batch/s, Batch Loss=0.1918, Avg Loss=0.2794, Time Left=11.26\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2106/3393 [16:45<10:31,  2.04batch/s, Batch Loss=0.1918, Avg Loss=0.2794, Time Left=11.26\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2106/3393 [16:46<10:31,  2.04batch/s, Batch Loss=0.4347, Avg Loss=0.2795, Time Left=11.25\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2107/3393 [16:46<10:27,  2.05batch/s, Batch Loss=0.4347, Avg Loss=0.2795, Time Left=11.25\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2107/3393 [16:46<10:27,  2.05batch/s, Batch Loss=0.2908, Avg Loss=0.2795, Time Left=11.24\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2108/3393 [16:46<10:38,  2.01batch/s, Batch Loss=0.2908, Avg Loss=0.2795, Time Left=11.24\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2108/3393 [16:47<10:38,  2.01batch/s, Batch Loss=0.2654, Avg Loss=0.2795, Time Left=11.23\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2109/3393 [16:47<10:41,  2.00batch/s, Batch Loss=0.2654, Avg Loss=0.2795, Time Left=11.23\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2109/3393 [16:47<10:41,  2.00batch/s, Batch Loss=0.0237, Avg Loss=0.2793, Time Left=11.22\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  62%|▌| 2110/3393 [16:47<10:39,  2.01batch/s, Batch Loss=0.0237, Avg Loss=0.2793, Time Left=11.22\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2110/3393 [16:48<10:39,  2.01batch/s, Batch Loss=0.1635, Avg Loss=0.2793, Time Left=11.21\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2111/3393 [16:48<10:26,  2.05batch/s, Batch Loss=0.1635, Avg Loss=0.2793, Time Left=11.21\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2111/3393 [16:48<10:26,  2.05batch/s, Batch Loss=0.1063, Avg Loss=0.2792, Time Left=11.21\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2112/3393 [16:48<10:25,  2.05batch/s, Batch Loss=0.1063, Avg Loss=0.2792, Time Left=11.21\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2112/3393 [16:49<10:25,  2.05batch/s, Batch Loss=0.2509, Avg Loss=0.2792, Time Left=11.20\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2113/3393 [16:49<10:33,  2.02batch/s, Batch Loss=0.2509, Avg Loss=0.2792, Time Left=11.20\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2113/3393 [16:49<10:33,  2.02batch/s, Batch Loss=0.0904, Avg Loss=0.2791, Time Left=11.19\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2114/3393 [16:49<10:30,  2.03batch/s, Batch Loss=0.0904, Avg Loss=0.2791, Time Left=11.19\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2114/3393 [16:50<10:30,  2.03batch/s, Batch Loss=0.4450, Avg Loss=0.2792, Time Left=11.18\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2115/3393 [16:50<10:32,  2.02batch/s, Batch Loss=0.4450, Avg Loss=0.2792, Time Left=11.18\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2115/3393 [16:50<10:32,  2.02batch/s, Batch Loss=0.0723, Avg Loss=0.2791, Time Left=11.17\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2116/3393 [16:50<10:21,  2.05batch/s, Batch Loss=0.0723, Avg Loss=0.2791, Time Left=11.17\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2116/3393 [16:51<10:21,  2.05batch/s, Batch Loss=0.0892, Avg Loss=0.2790, Time Left=11.16\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2117/3393 [16:51<10:14,  2.08batch/s, Batch Loss=0.0892, Avg Loss=0.2790, Time Left=11.16\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2117/3393 [16:51<10:14,  2.08batch/s, Batch Loss=0.2350, Avg Loss=0.2789, Time Left=11.16\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2118/3393 [16:51<10:19,  2.06batch/s, Batch Loss=0.2350, Avg Loss=0.2789, Time Left=11.16\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2118/3393 [16:52<10:19,  2.06batch/s, Batch Loss=0.1244, Avg Loss=0.2789, Time Left=11.15\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2119/3393 [16:52<10:18,  2.06batch/s, Batch Loss=0.1244, Avg Loss=0.2789, Time Left=11.15\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2119/3393 [16:52<10:18,  2.06batch/s, Batch Loss=0.1742, Avg Loss=0.2788, Time Left=11.14\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2120/3393 [16:52<10:17,  2.06batch/s, Batch Loss=0.1742, Avg Loss=0.2788, Time Left=11.14\u001b[A\n",
      "Epoch 1/3 - Training:  62%|▌| 2120/3393 [16:53<10:17,  2.06batch/s, Batch Loss=0.0538, Avg Loss=0.2787, Time Left=11.13\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2121/3393 [16:53<10:16,  2.06batch/s, Batch Loss=0.0538, Avg Loss=0.2787, Time Left=11.13\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2121/3393 [16:53<10:16,  2.06batch/s, Batch Loss=0.1873, Avg Loss=0.2787, Time Left=11.12\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2122/3393 [16:53<10:16,  2.06batch/s, Batch Loss=0.1873, Avg Loss=0.2787, Time Left=11.12\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2122/3393 [16:54<10:16,  2.06batch/s, Batch Loss=0.2333, Avg Loss=0.2786, Time Left=11.11\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2123/3393 [16:54<10:32,  2.01batch/s, Batch Loss=0.2333, Avg Loss=0.2786, Time Left=11.11\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2123/3393 [16:54<10:32,  2.01batch/s, Batch Loss=0.1144, Avg Loss=0.2786, Time Left=11.11\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2124/3393 [16:54<10:36,  1.99batch/s, Batch Loss=0.1144, Avg Loss=0.2786, Time Left=11.11\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2124/3393 [16:55<10:36,  1.99batch/s, Batch Loss=0.0522, Avg Loss=0.2785, Time Left=11.10\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2125/3393 [16:55<10:38,  1.98batch/s, Batch Loss=0.0522, Avg Loss=0.2785, Time Left=11.10\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2125/3393 [16:55<10:38,  1.98batch/s, Batch Loss=0.1885, Avg Loss=0.2784, Time Left=11.09\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2126/3393 [16:55<10:37,  1.99batch/s, Batch Loss=0.1885, Avg Loss=0.2784, Time Left=11.09\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2126/3393 [16:56<10:37,  1.99batch/s, Batch Loss=0.0658, Avg Loss=0.2783, Time Left=11.08\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2127/3393 [16:56<10:47,  1.95batch/s, Batch Loss=0.0658, Avg Loss=0.2783, Time Left=11.08\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2127/3393 [16:56<10:47,  1.95batch/s, Batch Loss=0.0109, Avg Loss=0.2782, Time Left=11.07\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2128/3393 [16:56<10:37,  1.98batch/s, Batch Loss=0.0109, Avg Loss=0.2782, Time Left=11.07\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2128/3393 [16:57<10:37,  1.98batch/s, Batch Loss=0.1836, Avg Loss=0.2781, Time Left=11.07\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2129/3393 [16:57<10:35,  1.99batch/s, Batch Loss=0.1836, Avg Loss=0.2781, Time Left=11.07\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2129/3393 [16:57<10:35,  1.99batch/s, Batch Loss=0.1415, Avg Loss=0.2781, Time Left=11.06\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2130/3393 [16:57<10:28,  2.01batch/s, Batch Loss=0.1415, Avg Loss=0.2781, Time Left=11.06\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2130/3393 [16:58<10:28,  2.01batch/s, Batch Loss=0.1223, Avg Loss=0.2780, Time Left=11.05\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2131/3393 [16:58<10:22,  2.03batch/s, Batch Loss=0.1223, Avg Loss=0.2780, Time Left=11.05\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2131/3393 [16:58<10:22,  2.03batch/s, Batch Loss=0.1774, Avg Loss=0.2779, Time Left=11.04\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2132/3393 [16:58<10:23,  2.02batch/s, Batch Loss=0.1774, Avg Loss=0.2779, Time Left=11.04\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2132/3393 [16:59<10:23,  2.02batch/s, Batch Loss=0.0432, Avg Loss=0.2778, Time Left=11.03\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2133/3393 [16:59<10:07,  2.07batch/s, Batch Loss=0.0432, Avg Loss=0.2778, Time Left=11.03\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2133/3393 [16:59<10:07,  2.07batch/s, Batch Loss=0.1494, Avg Loss=0.2778, Time Left=11.02\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2134/3393 [16:59<10:22,  2.02batch/s, Batch Loss=0.1494, Avg Loss=0.2778, Time Left=11.02\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2134/3393 [17:00<10:22,  2.02batch/s, Batch Loss=0.0125, Avg Loss=0.2776, Time Left=11.02\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2135/3393 [17:00<10:15,  2.04batch/s, Batch Loss=0.0125, Avg Loss=0.2776, Time Left=11.02\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2135/3393 [17:00<10:15,  2.04batch/s, Batch Loss=0.0255, Avg Loss=0.2775, Time Left=11.01\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2136/3393 [17:00<10:06,  2.07batch/s, Batch Loss=0.0255, Avg Loss=0.2775, Time Left=11.01\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2136/3393 [17:01<10:06,  2.07batch/s, Batch Loss=0.2756, Avg Loss=0.2775, Time Left=11.00\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2137/3393 [17:01<10:06,  2.07batch/s, Batch Loss=0.2756, Avg Loss=0.2775, Time Left=11.00\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2137/3393 [17:01<10:06,  2.07batch/s, Batch Loss=0.0593, Avg Loss=0.2774, Time Left=10.99\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2138/3393 [17:01<10:06,  2.07batch/s, Batch Loss=0.0593, Avg Loss=0.2774, Time Left=10.99\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2138/3393 [17:02<10:06,  2.07batch/s, Batch Loss=0.5017, Avg Loss=0.2775, Time Left=10.98\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2139/3393 [17:02<10:06,  2.07batch/s, Batch Loss=0.5017, Avg Loss=0.2775, Time Left=10.98\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2139/3393 [17:02<10:06,  2.07batch/s, Batch Loss=0.2305, Avg Loss=0.2775, Time Left=10.97\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2140/3393 [17:02<10:18,  2.03batch/s, Batch Loss=0.2305, Avg Loss=0.2775, Time Left=10.97\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2140/3393 [17:03<10:18,  2.03batch/s, Batch Loss=0.3824, Avg Loss=0.2775, Time Left=10.97\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2141/3393 [17:03<10:14,  2.04batch/s, Batch Loss=0.3824, Avg Loss=0.2775, Time Left=10.97\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2141/3393 [17:03<10:14,  2.04batch/s, Batch Loss=0.0624, Avg Loss=0.2774, Time Left=10.96\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2142/3393 [17:03<10:28,  1.99batch/s, Batch Loss=0.0624, Avg Loss=0.2774, Time Left=10.96\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2142/3393 [17:04<10:28,  1.99batch/s, Batch Loss=0.0714, Avg Loss=0.2773, Time Left=10.95\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  63%|▋| 2143/3393 [17:04<10:28,  1.99batch/s, Batch Loss=0.0714, Avg Loss=0.2773, Time Left=10.95\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2143/3393 [17:04<10:28,  1.99batch/s, Batch Loss=0.0798, Avg Loss=0.2772, Time Left=10.94\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2144/3393 [17:04<10:38,  1.96batch/s, Batch Loss=0.0798, Avg Loss=0.2772, Time Left=10.94\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2144/3393 [17:05<10:38,  1.96batch/s, Batch Loss=0.1088, Avg Loss=0.2772, Time Left=10.93\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2145/3393 [17:05<10:28,  1.99batch/s, Batch Loss=0.1088, Avg Loss=0.2772, Time Left=10.93\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2145/3393 [17:05<10:28,  1.99batch/s, Batch Loss=0.0670, Avg Loss=0.2771, Time Left=10.93\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2146/3393 [17:05<10:20,  2.01batch/s, Batch Loss=0.0670, Avg Loss=0.2771, Time Left=10.93\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2146/3393 [17:06<10:20,  2.01batch/s, Batch Loss=0.0601, Avg Loss=0.2770, Time Left=10.92\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2147/3393 [17:06<10:09,  2.04batch/s, Batch Loss=0.0601, Avg Loss=0.2770, Time Left=10.92\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2147/3393 [17:06<10:09,  2.04batch/s, Batch Loss=0.1272, Avg Loss=0.2769, Time Left=10.91\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2148/3393 [17:06<10:12,  2.03batch/s, Batch Loss=0.1272, Avg Loss=0.2769, Time Left=10.91\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2148/3393 [17:07<10:12,  2.03batch/s, Batch Loss=0.1475, Avg Loss=0.2768, Time Left=10.90\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2149/3393 [17:07<10:26,  1.99batch/s, Batch Loss=0.1475, Avg Loss=0.2768, Time Left=10.90\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2149/3393 [17:07<10:26,  1.99batch/s, Batch Loss=0.0621, Avg Loss=0.2767, Time Left=10.89\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2150/3393 [17:07<10:13,  2.03batch/s, Batch Loss=0.0621, Avg Loss=0.2767, Time Left=10.89\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2150/3393 [17:08<10:13,  2.03batch/s, Batch Loss=0.0569, Avg Loss=0.2766, Time Left=10.88\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2151/3393 [17:08<10:09,  2.04batch/s, Batch Loss=0.0569, Avg Loss=0.2766, Time Left=10.88\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2151/3393 [17:08<10:09,  2.04batch/s, Batch Loss=0.1660, Avg Loss=0.2766, Time Left=10.88\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2152/3393 [17:08<10:06,  2.05batch/s, Batch Loss=0.1660, Avg Loss=0.2766, Time Left=10.88\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2152/3393 [17:09<10:06,  2.05batch/s, Batch Loss=0.1448, Avg Loss=0.2765, Time Left=10.87\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2153/3393 [17:09<09:58,  2.07batch/s, Batch Loss=0.1448, Avg Loss=0.2765, Time Left=10.87\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2153/3393 [17:09<09:58,  2.07batch/s, Batch Loss=0.2216, Avg Loss=0.2765, Time Left=10.86\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2154/3393 [17:09<09:57,  2.07batch/s, Batch Loss=0.2216, Avg Loss=0.2765, Time Left=10.86\u001b[A\n",
      "Epoch 1/3 - Training:  63%|▋| 2154/3393 [17:09<09:57,  2.07batch/s, Batch Loss=0.0320, Avg Loss=0.2763, Time Left=10.85\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2155/3393 [17:09<09:57,  2.07batch/s, Batch Loss=0.0320, Avg Loss=0.2763, Time Left=10.85\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2155/3393 [17:10<09:57,  2.07batch/s, Batch Loss=0.2789, Avg Loss=0.2763, Time Left=10.84\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2156/3393 [17:10<10:04,  2.05batch/s, Batch Loss=0.2789, Avg Loss=0.2763, Time Left=10.84\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2156/3393 [17:10<10:04,  2.05batch/s, Batch Loss=0.0532, Avg Loss=0.2762, Time Left=10.83\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2157/3393 [17:11<10:12,  2.02batch/s, Batch Loss=0.0532, Avg Loss=0.2762, Time Left=10.83\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2157/3393 [17:11<10:12,  2.02batch/s, Batch Loss=0.0527, Avg Loss=0.2761, Time Left=10.83\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2158/3393 [17:11<10:08,  2.03batch/s, Batch Loss=0.0527, Avg Loss=0.2761, Time Left=10.83\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2158/3393 [17:11<10:08,  2.03batch/s, Batch Loss=0.3393, Avg Loss=0.2762, Time Left=10.82\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2159/3393 [17:11<10:05,  2.04batch/s, Batch Loss=0.3393, Avg Loss=0.2762, Time Left=10.82\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2159/3393 [17:12<10:05,  2.04batch/s, Batch Loss=0.1125, Avg Loss=0.2761, Time Left=10.81\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2160/3393 [17:12<10:02,  2.05batch/s, Batch Loss=0.1125, Avg Loss=0.2761, Time Left=10.81\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2160/3393 [17:12<10:02,  2.05batch/s, Batch Loss=0.1510, Avg Loss=0.2760, Time Left=10.80\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2161/3393 [17:12<09:59,  2.06batch/s, Batch Loss=0.1510, Avg Loss=0.2760, Time Left=10.80\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2161/3393 [17:13<09:59,  2.06batch/s, Batch Loss=0.1391, Avg Loss=0.2760, Time Left=10.79\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2162/3393 [17:13<10:10,  2.02batch/s, Batch Loss=0.1391, Avg Loss=0.2760, Time Left=10.79\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2162/3393 [17:13<10:10,  2.02batch/s, Batch Loss=0.1892, Avg Loss=0.2759, Time Left=10.78\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2163/3393 [17:13<10:00,  2.05batch/s, Batch Loss=0.1892, Avg Loss=0.2759, Time Left=10.78\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2163/3393 [17:14<10:00,  2.05batch/s, Batch Loss=0.1041, Avg Loss=0.2758, Time Left=10.78\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2164/3393 [17:14<10:03,  2.04batch/s, Batch Loss=0.1041, Avg Loss=0.2758, Time Left=10.78\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2164/3393 [17:14<10:03,  2.04batch/s, Batch Loss=0.4121, Avg Loss=0.2759, Time Left=10.77\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2165/3393 [17:14<10:00,  2.04batch/s, Batch Loss=0.4121, Avg Loss=0.2759, Time Left=10.77\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2165/3393 [17:15<10:00,  2.04batch/s, Batch Loss=0.0900, Avg Loss=0.2758, Time Left=10.76\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2166/3393 [17:15<09:52,  2.07batch/s, Batch Loss=0.0900, Avg Loss=0.2758, Time Left=10.76\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2166/3393 [17:15<09:52,  2.07batch/s, Batch Loss=0.5016, Avg Loss=0.2759, Time Left=10.75\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2167/3393 [17:15<09:51,  2.07batch/s, Batch Loss=0.5016, Avg Loss=0.2759, Time Left=10.75\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2167/3393 [17:16<09:51,  2.07batch/s, Batch Loss=0.1092, Avg Loss=0.2758, Time Left=10.74\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2168/3393 [17:16<09:52,  2.07batch/s, Batch Loss=0.1092, Avg Loss=0.2758, Time Left=10.74\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2168/3393 [17:16<09:52,  2.07batch/s, Batch Loss=0.1815, Avg Loss=0.2758, Time Left=10.73\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2169/3393 [17:16<09:51,  2.07batch/s, Batch Loss=0.1815, Avg Loss=0.2758, Time Left=10.73\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2169/3393 [17:17<09:51,  2.07batch/s, Batch Loss=0.0435, Avg Loss=0.2757, Time Left=10.73\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2170/3393 [17:17<10:02,  2.03batch/s, Batch Loss=0.0435, Avg Loss=0.2757, Time Left=10.73\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2170/3393 [17:17<10:02,  2.03batch/s, Batch Loss=0.1163, Avg Loss=0.2756, Time Left=10.72\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2171/3393 [17:17<10:05,  2.02batch/s, Batch Loss=0.1163, Avg Loss=0.2756, Time Left=10.72\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2171/3393 [17:18<10:05,  2.02batch/s, Batch Loss=0.3070, Avg Loss=0.2756, Time Left=10.71\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2172/3393 [17:18<10:18,  1.97batch/s, Batch Loss=0.3070, Avg Loss=0.2756, Time Left=10.71\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2172/3393 [17:18<10:18,  1.97batch/s, Batch Loss=0.2040, Avg Loss=0.2756, Time Left=10.70\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2173/3393 [17:18<10:09,  2.00batch/s, Batch Loss=0.2040, Avg Loss=0.2756, Time Left=10.70\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2173/3393 [17:19<10:09,  2.00batch/s, Batch Loss=0.1004, Avg Loss=0.2755, Time Left=10.69\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2174/3393 [17:19<10:04,  2.02batch/s, Batch Loss=0.1004, Avg Loss=0.2755, Time Left=10.69\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2174/3393 [17:19<10:04,  2.02batch/s, Batch Loss=0.2040, Avg Loss=0.2755, Time Left=10.68\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2175/3393 [17:19<09:59,  2.03batch/s, Batch Loss=0.2040, Avg Loss=0.2755, Time Left=10.68\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2175/3393 [17:20<09:59,  2.03batch/s, Batch Loss=0.0947, Avg Loss=0.2754, Time Left=10.68\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  64%|▋| 2176/3393 [17:20<09:55,  2.04batch/s, Batch Loss=0.0947, Avg Loss=0.2754, Time Left=10.68\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2176/3393 [17:20<09:55,  2.04batch/s, Batch Loss=0.1119, Avg Loss=0.2753, Time Left=10.67\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2177/3393 [17:20<10:04,  2.01batch/s, Batch Loss=0.1119, Avg Loss=0.2753, Time Left=10.67\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2177/3393 [17:21<10:04,  2.01batch/s, Batch Loss=0.1428, Avg Loss=0.2752, Time Left=10.66\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2178/3393 [17:21<10:00,  2.02batch/s, Batch Loss=0.1428, Avg Loss=0.2752, Time Left=10.66\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2178/3393 [17:21<10:00,  2.02batch/s, Batch Loss=0.2474, Avg Loss=0.2752, Time Left=10.65\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2179/3393 [17:21<09:56,  2.04batch/s, Batch Loss=0.2474, Avg Loss=0.2752, Time Left=10.65\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2179/3393 [17:22<09:56,  2.04batch/s, Batch Loss=0.1128, Avg Loss=0.2752, Time Left=10.64\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2180/3393 [17:22<09:46,  2.07batch/s, Batch Loss=0.1128, Avg Loss=0.2752, Time Left=10.64\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2180/3393 [17:22<09:46,  2.07batch/s, Batch Loss=0.2915, Avg Loss=0.2752, Time Left=10.63\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2181/3393 [17:22<09:45,  2.07batch/s, Batch Loss=0.2915, Avg Loss=0.2752, Time Left=10.63\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2181/3393 [17:23<09:45,  2.07batch/s, Batch Loss=0.1955, Avg Loss=0.2751, Time Left=10.63\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2182/3393 [17:23<10:02,  2.01batch/s, Batch Loss=0.1955, Avg Loss=0.2751, Time Left=10.63\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2182/3393 [17:23<10:02,  2.01batch/s, Batch Loss=0.2905, Avg Loss=0.2751, Time Left=10.62\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2183/3393 [17:23<10:03,  2.01batch/s, Batch Loss=0.2905, Avg Loss=0.2751, Time Left=10.62\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2183/3393 [17:24<10:03,  2.01batch/s, Batch Loss=0.1913, Avg Loss=0.2751, Time Left=10.61\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2184/3393 [17:24<09:46,  2.06batch/s, Batch Loss=0.1913, Avg Loss=0.2751, Time Left=10.61\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2184/3393 [17:24<09:46,  2.06batch/s, Batch Loss=0.0633, Avg Loss=0.2750, Time Left=10.60\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2185/3393 [17:24<09:56,  2.03batch/s, Batch Loss=0.0633, Avg Loss=0.2750, Time Left=10.60\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2185/3393 [17:25<09:56,  2.03batch/s, Batch Loss=0.0747, Avg Loss=0.2749, Time Left=10.59\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2186/3393 [17:25<09:53,  2.04batch/s, Batch Loss=0.0747, Avg Loss=0.2749, Time Left=10.59\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2186/3393 [17:25<09:53,  2.04batch/s, Batch Loss=0.3006, Avg Loss=0.2749, Time Left=10.58\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2187/3393 [17:25<09:55,  2.02batch/s, Batch Loss=0.3006, Avg Loss=0.2749, Time Left=10.58\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2187/3393 [17:26<09:55,  2.02batch/s, Batch Loss=0.3676, Avg Loss=0.2750, Time Left=10.58\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2188/3393 [17:26<09:40,  2.08batch/s, Batch Loss=0.3676, Avg Loss=0.2750, Time Left=10.58\u001b[A\n",
      "Epoch 1/3 - Training:  64%|▋| 2188/3393 [17:26<09:40,  2.08batch/s, Batch Loss=0.0672, Avg Loss=0.2749, Time Left=10.57\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2189/3393 [17:26<09:39,  2.08batch/s, Batch Loss=0.0672, Avg Loss=0.2749, Time Left=10.57\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2189/3393 [17:27<09:39,  2.08batch/s, Batch Loss=0.3175, Avg Loss=0.2749, Time Left=10.56\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2190/3393 [17:27<09:51,  2.03batch/s, Batch Loss=0.3175, Avg Loss=0.2749, Time Left=10.56\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2190/3393 [17:27<09:51,  2.03batch/s, Batch Loss=0.1076, Avg Loss=0.2748, Time Left=10.55\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2191/3393 [17:27<09:54,  2.02batch/s, Batch Loss=0.1076, Avg Loss=0.2748, Time Left=10.55\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2191/3393 [17:28<09:54,  2.02batch/s, Batch Loss=0.1972, Avg Loss=0.2748, Time Left=10.54\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2192/3393 [17:28<10:01,  2.00batch/s, Batch Loss=0.1972, Avg Loss=0.2748, Time Left=10.54\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2192/3393 [17:28<10:01,  2.00batch/s, Batch Loss=0.1530, Avg Loss=0.2747, Time Left=10.53\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2193/3393 [17:28<09:55,  2.02batch/s, Batch Loss=0.1530, Avg Loss=0.2747, Time Left=10.53\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2193/3393 [17:29<09:55,  2.02batch/s, Batch Loss=0.1300, Avg Loss=0.2746, Time Left=10.53\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2194/3393 [17:29<09:56,  2.01batch/s, Batch Loss=0.1300, Avg Loss=0.2746, Time Left=10.53\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2194/3393 [17:29<09:56,  2.01batch/s, Batch Loss=0.2267, Avg Loss=0.2746, Time Left=10.52\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2195/3393 [17:29<09:45,  2.05batch/s, Batch Loss=0.2267, Avg Loss=0.2746, Time Left=10.52\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2195/3393 [17:30<09:45,  2.05batch/s, Batch Loss=0.1499, Avg Loss=0.2745, Time Left=10.51\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2196/3393 [17:30<09:37,  2.07batch/s, Batch Loss=0.1499, Avg Loss=0.2745, Time Left=10.51\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2196/3393 [17:30<09:37,  2.07batch/s, Batch Loss=0.0766, Avg Loss=0.2745, Time Left=10.50\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2197/3393 [17:30<09:59,  1.99batch/s, Batch Loss=0.0766, Avg Loss=0.2745, Time Left=10.50\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2197/3393 [17:31<09:59,  1.99batch/s, Batch Loss=0.3597, Avg Loss=0.2745, Time Left=10.49\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2198/3393 [17:31<09:53,  2.01batch/s, Batch Loss=0.3597, Avg Loss=0.2745, Time Left=10.49\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2198/3393 [17:31<09:53,  2.01batch/s, Batch Loss=0.1336, Avg Loss=0.2744, Time Left=10.49\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2199/3393 [17:31<10:05,  1.97batch/s, Batch Loss=0.1336, Avg Loss=0.2744, Time Left=10.49\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2199/3393 [17:32<10:05,  1.97batch/s, Batch Loss=0.3580, Avg Loss=0.2745, Time Left=10.48\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2200/3393 [17:32<09:57,  2.00batch/s, Batch Loss=0.3580, Avg Loss=0.2745, Time Left=10.48\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2200/3393 [17:32<09:57,  2.00batch/s, Batch Loss=0.1529, Avg Loss=0.2744, Time Left=10.47\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2201/3393 [17:32<09:45,  2.04batch/s, Batch Loss=0.1529, Avg Loss=0.2744, Time Left=10.47\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2201/3393 [17:33<09:45,  2.04batch/s, Batch Loss=0.1212, Avg Loss=0.2743, Time Left=10.46\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2202/3393 [17:33<09:48,  2.02batch/s, Batch Loss=0.1212, Avg Loss=0.2743, Time Left=10.46\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2202/3393 [17:33<09:48,  2.02batch/s, Batch Loss=0.1539, Avg Loss=0.2743, Time Left=10.45\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2203/3393 [17:33<09:44,  2.04batch/s, Batch Loss=0.1539, Avg Loss=0.2743, Time Left=10.45\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2203/3393 [17:34<09:44,  2.04batch/s, Batch Loss=0.1110, Avg Loss=0.2742, Time Left=10.44\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2204/3393 [17:34<09:52,  2.01batch/s, Batch Loss=0.1110, Avg Loss=0.2742, Time Left=10.44\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2204/3393 [17:34<09:52,  2.01batch/s, Batch Loss=0.0897, Avg Loss=0.2741, Time Left=10.44\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2205/3393 [17:34<09:41,  2.04batch/s, Batch Loss=0.0897, Avg Loss=0.2741, Time Left=10.44\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2205/3393 [17:35<09:41,  2.04batch/s, Batch Loss=0.2059, Avg Loss=0.2741, Time Left=10.43\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2206/3393 [17:35<09:39,  2.05batch/s, Batch Loss=0.2059, Avg Loss=0.2741, Time Left=10.43\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2206/3393 [17:35<09:39,  2.05batch/s, Batch Loss=0.1185, Avg Loss=0.2740, Time Left=10.42\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2207/3393 [17:35<09:47,  2.02batch/s, Batch Loss=0.1185, Avg Loss=0.2740, Time Left=10.42\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2207/3393 [17:36<09:47,  2.02batch/s, Batch Loss=0.1409, Avg Loss=0.2740, Time Left=10.41\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2208/3393 [17:36<09:43,  2.03batch/s, Batch Loss=0.1409, Avg Loss=0.2740, Time Left=10.41\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2208/3393 [17:36<09:43,  2.03batch/s, Batch Loss=0.1237, Avg Loss=0.2739, Time Left=10.40\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  65%|▋| 2209/3393 [17:36<09:46,  2.02batch/s, Batch Loss=0.1237, Avg Loss=0.2739, Time Left=10.40\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2209/3393 [17:37<09:46,  2.02batch/s, Batch Loss=0.3559, Avg Loss=0.2739, Time Left=10.39\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2210/3393 [17:37<09:36,  2.05batch/s, Batch Loss=0.3559, Avg Loss=0.2739, Time Left=10.39\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2210/3393 [17:37<09:36,  2.05batch/s, Batch Loss=0.0566, Avg Loss=0.2738, Time Left=10.39\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2211/3393 [17:37<09:39,  2.04batch/s, Batch Loss=0.0566, Avg Loss=0.2738, Time Left=10.39\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2211/3393 [17:38<09:39,  2.04batch/s, Batch Loss=0.0244, Avg Loss=0.2737, Time Left=10.38\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2212/3393 [17:38<09:36,  2.05batch/s, Batch Loss=0.0244, Avg Loss=0.2737, Time Left=10.38\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2212/3393 [17:38<09:36,  2.05batch/s, Batch Loss=0.0913, Avg Loss=0.2736, Time Left=10.37\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2213/3393 [17:38<09:29,  2.07batch/s, Batch Loss=0.0913, Avg Loss=0.2736, Time Left=10.37\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2213/3393 [17:38<09:29,  2.07batch/s, Batch Loss=0.0677, Avg Loss=0.2735, Time Left=10.36\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2214/3393 [17:39<09:28,  2.07batch/s, Batch Loss=0.0677, Avg Loss=0.2735, Time Left=10.36\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2214/3393 [17:39<09:28,  2.07batch/s, Batch Loss=0.0462, Avg Loss=0.2734, Time Left=10.35\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2215/3393 [17:39<09:29,  2.07batch/s, Batch Loss=0.0462, Avg Loss=0.2734, Time Left=10.35\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2215/3393 [17:39<09:29,  2.07batch/s, Batch Loss=0.3960, Avg Loss=0.2735, Time Left=10.34\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2216/3393 [17:39<09:23,  2.09batch/s, Batch Loss=0.3960, Avg Loss=0.2735, Time Left=10.34\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2216/3393 [17:40<09:23,  2.09batch/s, Batch Loss=0.0340, Avg Loss=0.2734, Time Left=10.34\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2217/3393 [17:40<09:46,  2.01batch/s, Batch Loss=0.0340, Avg Loss=0.2734, Time Left=10.34\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2217/3393 [17:40<09:46,  2.01batch/s, Batch Loss=0.1896, Avg Loss=0.2733, Time Left=10.33\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2218/3393 [17:40<09:41,  2.02batch/s, Batch Loss=0.1896, Avg Loss=0.2733, Time Left=10.33\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2218/3393 [17:41<09:41,  2.02batch/s, Batch Loss=0.3033, Avg Loss=0.2733, Time Left=10.32\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2219/3393 [17:41<09:42,  2.01batch/s, Batch Loss=0.3033, Avg Loss=0.2733, Time Left=10.32\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2219/3393 [17:41<09:42,  2.01batch/s, Batch Loss=0.2232, Avg Loss=0.2733, Time Left=10.31\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2220/3393 [17:41<09:32,  2.05batch/s, Batch Loss=0.2232, Avg Loss=0.2733, Time Left=10.31\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2220/3393 [17:42<09:32,  2.05batch/s, Batch Loss=0.0735, Avg Loss=0.2732, Time Left=10.30\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2221/3393 [17:42<09:30,  2.05batch/s, Batch Loss=0.0735, Avg Loss=0.2732, Time Left=10.30\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2221/3393 [17:42<09:30,  2.05batch/s, Batch Loss=0.6443, Avg Loss=0.2734, Time Left=10.29\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2222/3393 [17:42<09:39,  2.02batch/s, Batch Loss=0.6443, Avg Loss=0.2734, Time Left=10.29\u001b[A\n",
      "Epoch 1/3 - Training:  65%|▋| 2222/3393 [17:43<09:39,  2.02batch/s, Batch Loss=0.0632, Avg Loss=0.2733, Time Left=10.29\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2223/3393 [17:43<09:36,  2.03batch/s, Batch Loss=0.0632, Avg Loss=0.2733, Time Left=10.29\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2223/3393 [17:43<09:36,  2.03batch/s, Batch Loss=0.0514, Avg Loss=0.2732, Time Left=10.28\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2224/3393 [17:43<09:30,  2.05batch/s, Batch Loss=0.0514, Avg Loss=0.2732, Time Left=10.28\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2224/3393 [17:44<09:30,  2.05batch/s, Batch Loss=0.0282, Avg Loss=0.2731, Time Left=10.27\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2225/3393 [17:44<09:30,  2.05batch/s, Batch Loss=0.0282, Avg Loss=0.2731, Time Left=10.27\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2225/3393 [17:44<09:30,  2.05batch/s, Batch Loss=0.4670, Avg Loss=0.2732, Time Left=10.26\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2226/3393 [17:44<09:28,  2.05batch/s, Batch Loss=0.4670, Avg Loss=0.2732, Time Left=10.26\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2226/3393 [17:45<09:28,  2.05batch/s, Batch Loss=0.2098, Avg Loss=0.2731, Time Left=10.25\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2227/3393 [17:45<09:38,  2.02batch/s, Batch Loss=0.2098, Avg Loss=0.2731, Time Left=10.25\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2227/3393 [17:45<09:38,  2.02batch/s, Batch Loss=0.1579, Avg Loss=0.2731, Time Left=10.24\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2228/3393 [17:45<09:39,  2.01batch/s, Batch Loss=0.1579, Avg Loss=0.2731, Time Left=10.24\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2228/3393 [17:46<09:39,  2.01batch/s, Batch Loss=0.0564, Avg Loss=0.2730, Time Left=10.24\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2229/3393 [17:46<09:28,  2.05batch/s, Batch Loss=0.0564, Avg Loss=0.2730, Time Left=10.24\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2229/3393 [17:46<09:28,  2.05batch/s, Batch Loss=0.2634, Avg Loss=0.2730, Time Left=10.23\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2230/3393 [17:46<09:32,  2.03batch/s, Batch Loss=0.2634, Avg Loss=0.2730, Time Left=10.23\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2230/3393 [17:47<09:32,  2.03batch/s, Batch Loss=0.3822, Avg Loss=0.2730, Time Left=10.22\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2231/3393 [17:47<09:28,  2.04batch/s, Batch Loss=0.3822, Avg Loss=0.2730, Time Left=10.22\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2231/3393 [17:47<09:28,  2.04batch/s, Batch Loss=0.1004, Avg Loss=0.2729, Time Left=10.21\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2232/3393 [17:47<09:32,  2.03batch/s, Batch Loss=0.1004, Avg Loss=0.2729, Time Left=10.21\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2232/3393 [17:48<09:32,  2.03batch/s, Batch Loss=0.1686, Avg Loss=0.2729, Time Left=10.20\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2233/3393 [17:48<09:22,  2.06batch/s, Batch Loss=0.1686, Avg Loss=0.2729, Time Left=10.20\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2233/3393 [17:48<09:22,  2.06batch/s, Batch Loss=0.2355, Avg Loss=0.2729, Time Left=10.19\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2234/3393 [17:48<09:21,  2.06batch/s, Batch Loss=0.2355, Avg Loss=0.2729, Time Left=10.19\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2234/3393 [17:49<09:21,  2.06batch/s, Batch Loss=0.1641, Avg Loss=0.2728, Time Left=10.19\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2235/3393 [17:49<09:31,  2.03batch/s, Batch Loss=0.1641, Avg Loss=0.2728, Time Left=10.19\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2235/3393 [17:49<09:31,  2.03batch/s, Batch Loss=0.1246, Avg Loss=0.2728, Time Left=10.18\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2236/3393 [17:49<09:28,  2.03batch/s, Batch Loss=0.1246, Avg Loss=0.2728, Time Left=10.18\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2236/3393 [17:50<09:28,  2.03batch/s, Batch Loss=0.0971, Avg Loss=0.2727, Time Left=10.17\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2237/3393 [17:50<09:36,  2.01batch/s, Batch Loss=0.0971, Avg Loss=0.2727, Time Left=10.17\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2237/3393 [17:50<09:36,  2.01batch/s, Batch Loss=0.0437, Avg Loss=0.2726, Time Left=10.16\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2238/3393 [17:50<09:31,  2.02batch/s, Batch Loss=0.0437, Avg Loss=0.2726, Time Left=10.16\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2238/3393 [17:51<09:31,  2.02batch/s, Batch Loss=0.2589, Avg Loss=0.2726, Time Left=10.15\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2239/3393 [17:51<09:28,  2.03batch/s, Batch Loss=0.2589, Avg Loss=0.2726, Time Left=10.15\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2239/3393 [17:51<09:28,  2.03batch/s, Batch Loss=0.0506, Avg Loss=0.2725, Time Left=10.14\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2240/3393 [17:51<09:18,  2.07batch/s, Batch Loss=0.0506, Avg Loss=0.2725, Time Left=10.14\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2240/3393 [17:52<09:18,  2.07batch/s, Batch Loss=0.0845, Avg Loss=0.2724, Time Left=10.14\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2241/3393 [17:52<09:11,  2.09batch/s, Batch Loss=0.0845, Avg Loss=0.2724, Time Left=10.14\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2241/3393 [17:52<09:11,  2.09batch/s, Batch Loss=0.3040, Avg Loss=0.2724, Time Left=10.13\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  66%|▋| 2242/3393 [17:52<09:18,  2.06batch/s, Batch Loss=0.3040, Avg Loss=0.2724, Time Left=10.13\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2242/3393 [17:53<09:18,  2.06batch/s, Batch Loss=0.4077, Avg Loss=0.2725, Time Left=10.12\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2243/3393 [17:53<09:12,  2.08batch/s, Batch Loss=0.4077, Avg Loss=0.2725, Time Left=10.12\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2243/3393 [17:53<09:12,  2.08batch/s, Batch Loss=0.1845, Avg Loss=0.2724, Time Left=10.11\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2244/3393 [17:53<09:06,  2.10batch/s, Batch Loss=0.1845, Avg Loss=0.2724, Time Left=10.11\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2244/3393 [17:54<09:06,  2.10batch/s, Batch Loss=0.1281, Avg Loss=0.2724, Time Left=10.10\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2245/3393 [17:54<09:13,  2.07batch/s, Batch Loss=0.1281, Avg Loss=0.2724, Time Left=10.10\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2245/3393 [17:54<09:13,  2.07batch/s, Batch Loss=0.0515, Avg Loss=0.2722, Time Left=10.09\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2246/3393 [17:54<09:15,  2.06batch/s, Batch Loss=0.0515, Avg Loss=0.2722, Time Left=10.09\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2246/3393 [17:55<09:15,  2.06batch/s, Batch Loss=0.1791, Avg Loss=0.2722, Time Left=10.09\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2247/3393 [17:55<09:07,  2.09batch/s, Batch Loss=0.1791, Avg Loss=0.2722, Time Left=10.09\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2247/3393 [17:55<09:07,  2.09batch/s, Batch Loss=0.1460, Avg Loss=0.2721, Time Left=10.08\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2248/3393 [17:55<09:25,  2.02batch/s, Batch Loss=0.1460, Avg Loss=0.2721, Time Left=10.08\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2248/3393 [17:56<09:25,  2.02batch/s, Batch Loss=0.4063, Avg Loss=0.2722, Time Left=10.07\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2249/3393 [17:56<09:22,  2.04batch/s, Batch Loss=0.4063, Avg Loss=0.2722, Time Left=10.07\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2249/3393 [17:56<09:22,  2.04batch/s, Batch Loss=0.0660, Avg Loss=0.2721, Time Left=10.06\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2250/3393 [17:56<09:19,  2.04batch/s, Batch Loss=0.0660, Avg Loss=0.2721, Time Left=10.06\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2250/3393 [17:57<09:19,  2.04batch/s, Batch Loss=0.2194, Avg Loss=0.2721, Time Left=10.05\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2251/3393 [17:57<09:19,  2.04batch/s, Batch Loss=0.2194, Avg Loss=0.2721, Time Left=10.05\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2251/3393 [17:57<09:19,  2.04batch/s, Batch Loss=0.0954, Avg Loss=0.2720, Time Left=10.04\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2252/3393 [17:57<09:08,  2.08batch/s, Batch Loss=0.0954, Avg Loss=0.2720, Time Left=10.04\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2252/3393 [17:58<09:08,  2.08batch/s, Batch Loss=0.1358, Avg Loss=0.2719, Time Left=10.04\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2253/3393 [17:58<09:12,  2.06batch/s, Batch Loss=0.1358, Avg Loss=0.2719, Time Left=10.04\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2253/3393 [17:58<09:12,  2.06batch/s, Batch Loss=0.0307, Avg Loss=0.2718, Time Left=10.03\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2254/3393 [17:58<09:03,  2.10batch/s, Batch Loss=0.0307, Avg Loss=0.2718, Time Left=10.03\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2254/3393 [17:58<09:03,  2.10batch/s, Batch Loss=0.1072, Avg Loss=0.2718, Time Left=10.02\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2255/3393 [17:58<09:04,  2.09batch/s, Batch Loss=0.1072, Avg Loss=0.2718, Time Left=10.02\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2255/3393 [17:59<09:04,  2.09batch/s, Batch Loss=0.0787, Avg Loss=0.2717, Time Left=10.01\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2256/3393 [17:59<09:05,  2.08batch/s, Batch Loss=0.0787, Avg Loss=0.2717, Time Left=10.01\u001b[A\n",
      "Epoch 1/3 - Training:  66%|▋| 2256/3393 [17:59<09:05,  2.08batch/s, Batch Loss=0.1260, Avg Loss=0.2716, Time Left=10.00\u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2257/3393 [17:59<09:06,  2.08batch/s, Batch Loss=0.1260, Avg Loss=0.2716, Time Left=10.00\u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2257/3393 [18:00<09:06,  2.08batch/s, Batch Loss=0.2474, Avg Loss=0.2716, Time Left=9.99 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2258/3393 [18:00<09:14,  2.05batch/s, Batch Loss=0.2474, Avg Loss=0.2716, Time Left=9.99 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2258/3393 [18:00<09:14,  2.05batch/s, Batch Loss=0.0504, Avg Loss=0.2715, Time Left=9.99 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2259/3393 [18:00<09:21,  2.02batch/s, Batch Loss=0.0504, Avg Loss=0.2715, Time Left=9.99 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2259/3393 [18:01<09:21,  2.02batch/s, Batch Loss=0.4196, Avg Loss=0.2716, Time Left=9.98 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2260/3393 [18:01<09:11,  2.05batch/s, Batch Loss=0.4196, Avg Loss=0.2716, Time Left=9.98 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2260/3393 [18:01<09:11,  2.05batch/s, Batch Loss=0.0060, Avg Loss=0.2714, Time Left=9.97 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2261/3393 [18:01<09:04,  2.08batch/s, Batch Loss=0.0060, Avg Loss=0.2714, Time Left=9.97 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2261/3393 [18:02<09:04,  2.08batch/s, Batch Loss=0.0469, Avg Loss=0.2713, Time Left=9.96 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2262/3393 [18:02<09:10,  2.06batch/s, Batch Loss=0.0469, Avg Loss=0.2713, Time Left=9.96 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2262/3393 [18:02<09:10,  2.06batch/s, Batch Loss=0.0688, Avg Loss=0.2712, Time Left=9.95 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2263/3393 [18:02<08:58,  2.10batch/s, Batch Loss=0.0688, Avg Loss=0.2712, Time Left=9.95 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2263/3393 [18:03<08:58,  2.10batch/s, Batch Loss=0.3481, Avg Loss=0.2713, Time Left=9.94 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2264/3393 [18:03<08:59,  2.09batch/s, Batch Loss=0.3481, Avg Loss=0.2713, Time Left=9.94 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2264/3393 [18:03<08:59,  2.09batch/s, Batch Loss=0.4911, Avg Loss=0.2714, Time Left=9.94 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2265/3393 [18:03<09:11,  2.04batch/s, Batch Loss=0.4911, Avg Loss=0.2714, Time Left=9.94 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2265/3393 [18:04<09:11,  2.04batch/s, Batch Loss=0.0595, Avg Loss=0.2713, Time Left=9.93 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2266/3393 [18:04<09:09,  2.05batch/s, Batch Loss=0.0595, Avg Loss=0.2713, Time Left=9.93 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2266/3393 [18:04<09:09,  2.05batch/s, Batch Loss=0.1551, Avg Loss=0.2712, Time Left=9.92 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2267/3393 [18:04<09:18,  2.01batch/s, Batch Loss=0.1551, Avg Loss=0.2712, Time Left=9.92 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2267/3393 [18:05<09:18,  2.01batch/s, Batch Loss=0.0600, Avg Loss=0.2711, Time Left=9.91 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2268/3393 [18:05<09:14,  2.03batch/s, Batch Loss=0.0600, Avg Loss=0.2711, Time Left=9.91 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2268/3393 [18:05<09:14,  2.03batch/s, Batch Loss=0.1445, Avg Loss=0.2711, Time Left=9.90 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2269/3393 [18:05<09:05,  2.06batch/s, Batch Loss=0.1445, Avg Loss=0.2711, Time Left=9.90 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2269/3393 [18:06<09:05,  2.06batch/s, Batch Loss=0.4026, Avg Loss=0.2711, Time Left=9.89 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2270/3393 [18:06<09:10,  2.04batch/s, Batch Loss=0.4026, Avg Loss=0.2711, Time Left=9.89 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2270/3393 [18:06<09:10,  2.04batch/s, Batch Loss=0.1914, Avg Loss=0.2711, Time Left=9.89 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2271/3393 [18:06<09:12,  2.03batch/s, Batch Loss=0.1914, Avg Loss=0.2711, Time Left=9.89 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2271/3393 [18:07<09:12,  2.03batch/s, Batch Loss=0.1463, Avg Loss=0.2710, Time Left=9.88 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2272/3393 [18:07<09:09,  2.04batch/s, Batch Loss=0.1463, Avg Loss=0.2710, Time Left=9.88 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2272/3393 [18:07<09:09,  2.04batch/s, Batch Loss=0.0796, Avg Loss=0.2710, Time Left=9.87 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2273/3393 [18:07<09:01,  2.07batch/s, Batch Loss=0.0796, Avg Loss=0.2710, Time Left=9.87 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2273/3393 [18:08<09:01,  2.07batch/s, Batch Loss=0.0447, Avg Loss=0.2709, Time Left=9.86 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2274/3393 [18:08<09:02,  2.06batch/s, Batch Loss=0.0447, Avg Loss=0.2709, Time Left=9.86 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2274/3393 [18:08<09:02,  2.06batch/s, Batch Loss=0.2182, Avg Loss=0.2708, Time Left=9.85 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  67%|▋| 2275/3393 [18:08<09:10,  2.03batch/s, Batch Loss=0.2182, Avg Loss=0.2708, Time Left=9.85 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2275/3393 [18:09<09:10,  2.03batch/s, Batch Loss=0.1600, Avg Loss=0.2708, Time Left=9.84 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2276/3393 [18:09<09:12,  2.02batch/s, Batch Loss=0.1600, Avg Loss=0.2708, Time Left=9.84 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2276/3393 [18:09<09:12,  2.02batch/s, Batch Loss=0.1212, Avg Loss=0.2707, Time Left=9.84 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2277/3393 [18:09<09:14,  2.01batch/s, Batch Loss=0.1212, Avg Loss=0.2707, Time Left=9.84 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2277/3393 [18:10<09:14,  2.01batch/s, Batch Loss=0.2227, Avg Loss=0.2707, Time Left=9.83 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2278/3393 [18:10<09:15,  2.01batch/s, Batch Loss=0.2227, Avg Loss=0.2707, Time Left=9.83 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2278/3393 [18:10<09:15,  2.01batch/s, Batch Loss=0.0499, Avg Loss=0.2706, Time Left=9.82 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2279/3393 [18:10<09:04,  2.04batch/s, Batch Loss=0.0499, Avg Loss=0.2706, Time Left=9.82 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2279/3393 [18:11<09:04,  2.04batch/s, Batch Loss=0.3394, Avg Loss=0.2706, Time Left=9.81 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2280/3393 [18:11<08:57,  2.07batch/s, Batch Loss=0.3394, Avg Loss=0.2706, Time Left=9.81 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2280/3393 [18:11<08:57,  2.07batch/s, Batch Loss=0.1363, Avg Loss=0.2706, Time Left=9.80 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2281/3393 [18:11<08:51,  2.09batch/s, Batch Loss=0.1363, Avg Loss=0.2706, Time Left=9.80 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2281/3393 [18:12<08:51,  2.09batch/s, Batch Loss=0.1295, Avg Loss=0.2705, Time Left=9.79 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2282/3393 [18:12<08:52,  2.09batch/s, Batch Loss=0.1295, Avg Loss=0.2705, Time Left=9.79 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2282/3393 [18:12<08:52,  2.09batch/s, Batch Loss=0.0417, Avg Loss=0.2704, Time Left=9.79 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2283/3393 [18:12<08:58,  2.06batch/s, Batch Loss=0.0417, Avg Loss=0.2704, Time Left=9.79 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2283/3393 [18:13<08:58,  2.06batch/s, Batch Loss=0.0967, Avg Loss=0.2703, Time Left=9.78 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2284/3393 [18:13<08:47,  2.10batch/s, Batch Loss=0.0967, Avg Loss=0.2703, Time Left=9.78 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2284/3393 [18:13<08:47,  2.10batch/s, Batch Loss=0.0501, Avg Loss=0.2702, Time Left=9.77 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2285/3393 [18:13<08:59,  2.05batch/s, Batch Loss=0.0501, Avg Loss=0.2702, Time Left=9.77 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2285/3393 [18:14<08:59,  2.05batch/s, Batch Loss=0.2568, Avg Loss=0.2702, Time Left=9.76 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2286/3393 [18:14<08:53,  2.07batch/s, Batch Loss=0.2568, Avg Loss=0.2702, Time Left=9.76 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2286/3393 [18:14<08:53,  2.07batch/s, Batch Loss=0.2757, Avg Loss=0.2702, Time Left=9.75 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2287/3393 [18:14<08:47,  2.10batch/s, Batch Loss=0.2757, Avg Loss=0.2702, Time Left=9.75 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2287/3393 [18:15<08:47,  2.10batch/s, Batch Loss=0.1650, Avg Loss=0.2702, Time Left=9.74 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2288/3393 [18:15<08:59,  2.05batch/s, Batch Loss=0.1650, Avg Loss=0.2702, Time Left=9.74 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2288/3393 [18:15<08:59,  2.05batch/s, Batch Loss=0.0506, Avg Loss=0.2701, Time Left=9.74 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2289/3393 [18:15<08:53,  2.07batch/s, Batch Loss=0.0506, Avg Loss=0.2701, Time Left=9.74 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2289/3393 [18:16<08:53,  2.07batch/s, Batch Loss=0.0518, Avg Loss=0.2700, Time Left=9.73 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2290/3393 [18:16<08:52,  2.07batch/s, Batch Loss=0.0518, Avg Loss=0.2700, Time Left=9.73 \u001b[A\n",
      "Epoch 1/3 - Training:  67%|▋| 2290/3393 [18:16<08:52,  2.07batch/s, Batch Loss=0.0998, Avg Loss=0.2699, Time Left=9.72 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2291/3393 [18:16<08:57,  2.05batch/s, Batch Loss=0.0998, Avg Loss=0.2699, Time Left=9.72 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2291/3393 [18:16<08:57,  2.05batch/s, Batch Loss=0.0724, Avg Loss=0.2698, Time Left=9.71 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2292/3393 [18:16<08:54,  2.06batch/s, Batch Loss=0.0724, Avg Loss=0.2698, Time Left=9.71 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2292/3393 [18:17<08:54,  2.06batch/s, Batch Loss=0.3333, Avg Loss=0.2698, Time Left=9.70 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2293/3393 [18:17<08:54,  2.06batch/s, Batch Loss=0.3333, Avg Loss=0.2698, Time Left=9.70 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2293/3393 [18:17<08:54,  2.06batch/s, Batch Loss=0.0548, Avg Loss=0.2697, Time Left=9.69 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2294/3393 [18:17<08:53,  2.06batch/s, Batch Loss=0.0548, Avg Loss=0.2697, Time Left=9.69 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2294/3393 [18:18<08:53,  2.06batch/s, Batch Loss=0.0601, Avg Loss=0.2696, Time Left=9.69 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2295/3393 [18:18<08:52,  2.06batch/s, Batch Loss=0.0601, Avg Loss=0.2696, Time Left=9.69 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2295/3393 [18:18<08:52,  2.06batch/s, Batch Loss=0.2218, Avg Loss=0.2696, Time Left=9.68 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2296/3393 [18:18<08:51,  2.06batch/s, Batch Loss=0.2218, Avg Loss=0.2696, Time Left=9.68 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2296/3393 [18:19<08:51,  2.06batch/s, Batch Loss=0.0777, Avg Loss=0.2695, Time Left=9.67 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2297/3393 [18:19<08:50,  2.06batch/s, Batch Loss=0.0777, Avg Loss=0.2695, Time Left=9.67 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2297/3393 [18:19<08:50,  2.06batch/s, Batch Loss=0.0684, Avg Loss=0.2694, Time Left=9.66 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2298/3393 [18:19<08:45,  2.08batch/s, Batch Loss=0.0684, Avg Loss=0.2694, Time Left=9.66 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2298/3393 [18:20<08:45,  2.08batch/s, Batch Loss=0.0143, Avg Loss=0.2693, Time Left=9.65 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2299/3393 [18:20<09:00,  2.02batch/s, Batch Loss=0.0143, Avg Loss=0.2693, Time Left=9.65 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2299/3393 [18:20<09:00,  2.02batch/s, Batch Loss=0.0153, Avg Loss=0.2692, Time Left=9.64 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2300/3393 [18:20<08:57,  2.03batch/s, Batch Loss=0.0153, Avg Loss=0.2692, Time Left=9.64 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2300/3393 [18:21<08:57,  2.03batch/s, Batch Loss=0.0573, Avg Loss=0.2691, Time Left=9.64 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2301/3393 [18:21<08:49,  2.06batch/s, Batch Loss=0.0573, Avg Loss=0.2691, Time Left=9.64 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2301/3393 [18:21<08:49,  2.06batch/s, Batch Loss=0.3126, Avg Loss=0.2691, Time Left=9.63 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2302/3393 [18:21<08:48,  2.06batch/s, Batch Loss=0.3126, Avg Loss=0.2691, Time Left=9.63 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2302/3393 [18:22<08:48,  2.06batch/s, Batch Loss=0.0639, Avg Loss=0.2690, Time Left=9.62 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2303/3393 [18:22<08:47,  2.07batch/s, Batch Loss=0.0639, Avg Loss=0.2690, Time Left=9.62 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2303/3393 [18:22<08:47,  2.07batch/s, Batch Loss=0.0137, Avg Loss=0.2689, Time Left=9.61 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2304/3393 [18:22<08:52,  2.05batch/s, Batch Loss=0.0137, Avg Loss=0.2689, Time Left=9.61 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2304/3393 [18:23<08:52,  2.05batch/s, Batch Loss=0.1882, Avg Loss=0.2689, Time Left=9.60 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2305/3393 [18:23<08:50,  2.05batch/s, Batch Loss=0.1882, Avg Loss=0.2689, Time Left=9.60 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2305/3393 [18:23<08:50,  2.05batch/s, Batch Loss=0.2231, Avg Loss=0.2689, Time Left=9.59 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2306/3393 [18:23<08:38,  2.10batch/s, Batch Loss=0.2231, Avg Loss=0.2689, Time Left=9.59 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2306/3393 [18:24<08:38,  2.10batch/s, Batch Loss=0.0180, Avg Loss=0.2688, Time Left=9.59 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2307/3393 [18:24<08:54,  2.03batch/s, Batch Loss=0.0180, Avg Loss=0.2688, Time Left=9.59 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2307/3393 [18:24<08:54,  2.03batch/s, Batch Loss=0.1349, Avg Loss=0.2687, Time Left=9.58 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  68%|▋| 2308/3393 [18:24<08:52,  2.04batch/s, Batch Loss=0.1349, Avg Loss=0.2687, Time Left=9.58 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2308/3393 [18:25<08:52,  2.04batch/s, Batch Loss=0.3372, Avg Loss=0.2687, Time Left=9.57 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2309/3393 [18:25<08:49,  2.05batch/s, Batch Loss=0.3372, Avg Loss=0.2687, Time Left=9.57 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2309/3393 [18:25<08:49,  2.05batch/s, Batch Loss=0.1568, Avg Loss=0.2687, Time Left=9.56 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2310/3393 [18:25<08:47,  2.05batch/s, Batch Loss=0.1568, Avg Loss=0.2687, Time Left=9.56 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2310/3393 [18:26<08:47,  2.05batch/s, Batch Loss=0.2810, Avg Loss=0.2687, Time Left=9.55 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2311/3393 [18:26<08:40,  2.08batch/s, Batch Loss=0.2810, Avg Loss=0.2687, Time Left=9.55 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2311/3393 [18:26<08:40,  2.08batch/s, Batch Loss=0.2344, Avg Loss=0.2687, Time Left=9.54 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2312/3393 [18:26<08:43,  2.06batch/s, Batch Loss=0.2344, Avg Loss=0.2687, Time Left=9.54 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2312/3393 [18:27<08:43,  2.06batch/s, Batch Loss=0.2389, Avg Loss=0.2687, Time Left=9.54 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2313/3393 [18:27<08:40,  2.08batch/s, Batch Loss=0.2389, Avg Loss=0.2687, Time Left=9.54 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2313/3393 [18:27<08:40,  2.08batch/s, Batch Loss=0.0451, Avg Loss=0.2686, Time Left=9.53 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2314/3393 [18:27<08:35,  2.09batch/s, Batch Loss=0.0451, Avg Loss=0.2686, Time Left=9.53 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2314/3393 [18:28<08:35,  2.09batch/s, Batch Loss=0.1408, Avg Loss=0.2685, Time Left=9.52 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2315/3393 [18:28<08:51,  2.03batch/s, Batch Loss=0.1408, Avg Loss=0.2685, Time Left=9.52 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2315/3393 [18:28<08:51,  2.03batch/s, Batch Loss=0.0817, Avg Loss=0.2684, Time Left=9.51 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2316/3393 [18:28<08:59,  2.00batch/s, Batch Loss=0.0817, Avg Loss=0.2684, Time Left=9.51 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2316/3393 [18:29<08:59,  2.00batch/s, Batch Loss=0.1344, Avg Loss=0.2684, Time Left=9.50 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2317/3393 [18:29<09:03,  1.98batch/s, Batch Loss=0.1344, Avg Loss=0.2684, Time Left=9.50 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2317/3393 [18:29<09:03,  1.98batch/s, Batch Loss=0.2880, Avg Loss=0.2684, Time Left=9.50 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2318/3393 [18:29<09:01,  1.98batch/s, Batch Loss=0.2880, Avg Loss=0.2684, Time Left=9.50 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2318/3393 [18:30<09:01,  1.98batch/s, Batch Loss=0.0375, Avg Loss=0.2683, Time Left=9.49 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2319/3393 [18:30<08:55,  2.01batch/s, Batch Loss=0.0375, Avg Loss=0.2683, Time Left=9.49 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2319/3393 [18:30<08:55,  2.01batch/s, Batch Loss=0.4337, Avg Loss=0.2683, Time Left=9.48 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2320/3393 [18:30<08:55,  2.00batch/s, Batch Loss=0.4337, Avg Loss=0.2683, Time Left=9.48 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2320/3393 [18:31<08:55,  2.00batch/s, Batch Loss=0.3951, Avg Loss=0.2684, Time Left=9.47 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2321/3393 [18:31<08:49,  2.02batch/s, Batch Loss=0.3951, Avg Loss=0.2684, Time Left=9.47 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2321/3393 [18:31<08:49,  2.02batch/s, Batch Loss=0.1980, Avg Loss=0.2684, Time Left=9.46 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2322/3393 [18:31<08:51,  2.02batch/s, Batch Loss=0.1980, Avg Loss=0.2684, Time Left=9.46 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2322/3393 [18:32<08:51,  2.02batch/s, Batch Loss=0.0968, Avg Loss=0.2683, Time Left=9.45 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2323/3393 [18:32<08:41,  2.05batch/s, Batch Loss=0.0968, Avg Loss=0.2683, Time Left=9.45 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2323/3393 [18:32<08:41,  2.05batch/s, Batch Loss=0.0700, Avg Loss=0.2682, Time Left=9.45 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2324/3393 [18:32<08:39,  2.06batch/s, Batch Loss=0.0700, Avg Loss=0.2682, Time Left=9.45 \u001b[A\n",
      "Epoch 1/3 - Training:  68%|▋| 2324/3393 [18:33<08:39,  2.06batch/s, Batch Loss=0.0787, Avg Loss=0.2681, Time Left=9.44 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2325/3393 [18:33<08:38,  2.06batch/s, Batch Loss=0.0787, Avg Loss=0.2681, Time Left=9.44 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2325/3393 [18:33<08:38,  2.06batch/s, Batch Loss=0.0599, Avg Loss=0.2680, Time Left=9.43 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2326/3393 [18:33<08:44,  2.03batch/s, Batch Loss=0.0599, Avg Loss=0.2680, Time Left=9.43 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2326/3393 [18:34<08:44,  2.03batch/s, Batch Loss=0.1201, Avg Loss=0.2680, Time Left=9.42 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2327/3393 [18:34<08:49,  2.01batch/s, Batch Loss=0.1201, Avg Loss=0.2680, Time Left=9.42 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2327/3393 [18:34<08:49,  2.01batch/s, Batch Loss=0.0477, Avg Loss=0.2679, Time Left=9.41 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2328/3393 [18:34<08:40,  2.05batch/s, Batch Loss=0.0477, Avg Loss=0.2679, Time Left=9.41 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2328/3393 [18:35<08:40,  2.05batch/s, Batch Loss=0.0719, Avg Loss=0.2678, Time Left=9.40 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2329/3393 [18:35<08:38,  2.05batch/s, Batch Loss=0.0719, Avg Loss=0.2678, Time Left=9.40 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2329/3393 [18:35<08:38,  2.05batch/s, Batch Loss=0.1620, Avg Loss=0.2677, Time Left=9.40 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2330/3393 [18:35<08:41,  2.04batch/s, Batch Loss=0.1620, Avg Loss=0.2677, Time Left=9.40 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2330/3393 [18:36<08:41,  2.04batch/s, Batch Loss=0.3594, Avg Loss=0.2678, Time Left=9.39 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2331/3393 [18:36<08:39,  2.05batch/s, Batch Loss=0.3594, Avg Loss=0.2678, Time Left=9.39 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2331/3393 [18:36<08:39,  2.05batch/s, Batch Loss=0.0988, Avg Loss=0.2677, Time Left=9.38 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2332/3393 [18:36<08:47,  2.01batch/s, Batch Loss=0.0988, Avg Loss=0.2677, Time Left=9.38 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2332/3393 [18:37<08:47,  2.01batch/s, Batch Loss=0.3862, Avg Loss=0.2677, Time Left=9.37 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2333/3393 [18:37<08:43,  2.03batch/s, Batch Loss=0.3862, Avg Loss=0.2677, Time Left=9.37 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2333/3393 [18:37<08:43,  2.03batch/s, Batch Loss=0.2953, Avg Loss=0.2677, Time Left=9.36 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2334/3393 [18:37<08:44,  2.02batch/s, Batch Loss=0.2953, Avg Loss=0.2677, Time Left=9.36 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2334/3393 [18:38<08:44,  2.02batch/s, Batch Loss=0.1705, Avg Loss=0.2677, Time Left=9.35 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2335/3393 [18:38<08:40,  2.03batch/s, Batch Loss=0.1705, Avg Loss=0.2677, Time Left=9.35 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2335/3393 [18:38<08:40,  2.03batch/s, Batch Loss=0.0723, Avg Loss=0.2676, Time Left=9.35 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2336/3393 [18:38<08:37,  2.04batch/s, Batch Loss=0.0723, Avg Loss=0.2676, Time Left=9.35 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2336/3393 [18:39<08:37,  2.04batch/s, Batch Loss=0.0283, Avg Loss=0.2675, Time Left=9.34 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2337/3393 [18:39<08:40,  2.03batch/s, Batch Loss=0.0283, Avg Loss=0.2675, Time Left=9.34 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2337/3393 [18:39<08:40,  2.03batch/s, Batch Loss=0.1126, Avg Loss=0.2674, Time Left=9.33 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2338/3393 [18:39<08:31,  2.06batch/s, Batch Loss=0.1126, Avg Loss=0.2674, Time Left=9.33 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2338/3393 [18:40<08:31,  2.06batch/s, Batch Loss=0.1042, Avg Loss=0.2674, Time Left=9.32 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2339/3393 [18:40<08:35,  2.04batch/s, Batch Loss=0.1042, Avg Loss=0.2674, Time Left=9.32 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2339/3393 [18:40<08:35,  2.04batch/s, Batch Loss=0.1026, Avg Loss=0.2673, Time Left=9.31 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2340/3393 [18:40<08:33,  2.05batch/s, Batch Loss=0.1026, Avg Loss=0.2673, Time Left=9.31 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2340/3393 [18:40<08:33,  2.05batch/s, Batch Loss=0.0766, Avg Loss=0.2672, Time Left=9.30 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  69%|▋| 2341/3393 [18:40<08:27,  2.07batch/s, Batch Loss=0.0766, Avg Loss=0.2672, Time Left=9.30 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2341/3393 [18:41<08:27,  2.07batch/s, Batch Loss=0.1361, Avg Loss=0.2672, Time Left=9.30 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2342/3393 [18:41<08:26,  2.07batch/s, Batch Loss=0.1361, Avg Loss=0.2672, Time Left=9.30 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2342/3393 [18:41<08:26,  2.07batch/s, Batch Loss=0.1642, Avg Loss=0.2671, Time Left=9.29 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2343/3393 [18:41<08:21,  2.09batch/s, Batch Loss=0.1642, Avg Loss=0.2671, Time Left=9.29 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2343/3393 [18:42<08:21,  2.09batch/s, Batch Loss=0.1886, Avg Loss=0.2671, Time Left=9.28 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2344/3393 [18:42<08:22,  2.09batch/s, Batch Loss=0.1886, Avg Loss=0.2671, Time Left=9.28 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2344/3393 [18:42<08:22,  2.09batch/s, Batch Loss=0.5013, Avg Loss=0.2672, Time Left=9.27 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2345/3393 [18:42<08:24,  2.08batch/s, Batch Loss=0.5013, Avg Loss=0.2672, Time Left=9.27 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2345/3393 [18:43<08:24,  2.08batch/s, Batch Loss=0.2738, Avg Loss=0.2672, Time Left=9.26 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2346/3393 [18:43<08:27,  2.06batch/s, Batch Loss=0.2738, Avg Loss=0.2672, Time Left=9.26 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2346/3393 [18:43<08:27,  2.06batch/s, Batch Loss=0.2653, Avg Loss=0.2672, Time Left=9.25 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2347/3393 [18:43<08:23,  2.08batch/s, Batch Loss=0.2653, Avg Loss=0.2672, Time Left=9.25 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2347/3393 [18:44<08:23,  2.08batch/s, Batch Loss=0.1218, Avg Loss=0.2671, Time Left=9.25 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2348/3393 [18:44<08:28,  2.06batch/s, Batch Loss=0.1218, Avg Loss=0.2671, Time Left=9.25 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2348/3393 [18:44<08:28,  2.06batch/s, Batch Loss=0.1669, Avg Loss=0.2671, Time Left=9.24 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2349/3393 [18:44<08:22,  2.08batch/s, Batch Loss=0.1669, Avg Loss=0.2671, Time Left=9.24 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2349/3393 [18:45<08:22,  2.08batch/s, Batch Loss=0.2313, Avg Loss=0.2671, Time Left=9.23 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2350/3393 [18:45<08:17,  2.10batch/s, Batch Loss=0.2313, Avg Loss=0.2671, Time Left=9.23 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2350/3393 [18:45<08:17,  2.10batch/s, Batch Loss=0.2151, Avg Loss=0.2670, Time Left=9.22 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2351/3393 [18:45<08:29,  2.05batch/s, Batch Loss=0.2151, Avg Loss=0.2670, Time Left=9.22 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2351/3393 [18:46<08:29,  2.05batch/s, Batch Loss=0.1237, Avg Loss=0.2670, Time Left=9.21 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2352/3393 [18:46<08:33,  2.03batch/s, Batch Loss=0.1237, Avg Loss=0.2670, Time Left=9.21 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2352/3393 [18:46<08:33,  2.03batch/s, Batch Loss=0.2413, Avg Loss=0.2670, Time Left=9.20 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2353/3393 [18:46<08:28,  2.04batch/s, Batch Loss=0.2413, Avg Loss=0.2670, Time Left=9.20 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2353/3393 [18:47<08:28,  2.04batch/s, Batch Loss=0.2141, Avg Loss=0.2669, Time Left=9.20 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2354/3393 [18:47<08:27,  2.05batch/s, Batch Loss=0.2141, Avg Loss=0.2669, Time Left=9.20 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2354/3393 [18:47<08:27,  2.05batch/s, Batch Loss=0.1202, Avg Loss=0.2669, Time Left=9.19 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2355/3393 [18:47<08:19,  2.08batch/s, Batch Loss=0.1202, Avg Loss=0.2669, Time Left=9.19 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2355/3393 [18:48<08:19,  2.08batch/s, Batch Loss=0.1396, Avg Loss=0.2668, Time Left=9.18 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2356/3393 [18:48<08:34,  2.02batch/s, Batch Loss=0.1396, Avg Loss=0.2668, Time Left=9.18 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2356/3393 [18:48<08:34,  2.02batch/s, Batch Loss=0.3251, Avg Loss=0.2668, Time Left=9.17 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2357/3393 [18:48<08:30,  2.03batch/s, Batch Loss=0.3251, Avg Loss=0.2668, Time Left=9.17 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2357/3393 [18:49<08:30,  2.03batch/s, Batch Loss=0.0816, Avg Loss=0.2668, Time Left=9.16 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2358/3393 [18:49<08:37,  2.00batch/s, Batch Loss=0.0816, Avg Loss=0.2668, Time Left=9.16 \u001b[A\n",
      "Epoch 1/3 - Training:  69%|▋| 2358/3393 [18:49<08:37,  2.00batch/s, Batch Loss=0.1857, Avg Loss=0.2667, Time Left=9.15 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2359/3393 [18:49<08:27,  2.04batch/s, Batch Loss=0.1857, Avg Loss=0.2667, Time Left=9.15 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2359/3393 [18:50<08:27,  2.04batch/s, Batch Loss=0.1493, Avg Loss=0.2667, Time Left=9.15 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2360/3393 [18:50<08:29,  2.03batch/s, Batch Loss=0.1493, Avg Loss=0.2667, Time Left=9.15 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2360/3393 [18:50<08:29,  2.03batch/s, Batch Loss=0.0616, Avg Loss=0.2666, Time Left=9.14 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2361/3393 [18:50<08:35,  2.00batch/s, Batch Loss=0.0616, Avg Loss=0.2666, Time Left=9.14 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2361/3393 [18:51<08:35,  2.00batch/s, Batch Loss=0.1481, Avg Loss=0.2665, Time Left=9.13 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2362/3393 [18:51<08:30,  2.02batch/s, Batch Loss=0.1481, Avg Loss=0.2665, Time Left=9.13 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2362/3393 [18:51<08:30,  2.02batch/s, Batch Loss=0.0244, Avg Loss=0.2664, Time Left=9.12 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2363/3393 [18:51<08:37,  1.99batch/s, Batch Loss=0.0244, Avg Loss=0.2664, Time Left=9.12 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2363/3393 [18:52<08:37,  1.99batch/s, Batch Loss=0.1447, Avg Loss=0.2664, Time Left=9.11 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2364/3393 [18:52<08:36,  1.99batch/s, Batch Loss=0.1447, Avg Loss=0.2664, Time Left=9.11 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2364/3393 [18:52<08:36,  1.99batch/s, Batch Loss=0.3921, Avg Loss=0.2664, Time Left=9.11 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2365/3393 [18:52<08:39,  1.98batch/s, Batch Loss=0.3921, Avg Loss=0.2664, Time Left=9.11 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2365/3393 [18:53<08:39,  1.98batch/s, Batch Loss=0.3414, Avg Loss=0.2665, Time Left=9.10 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2366/3393 [18:53<08:38,  1.98batch/s, Batch Loss=0.3414, Avg Loss=0.2665, Time Left=9.10 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2366/3393 [18:53<08:38,  1.98batch/s, Batch Loss=0.0246, Avg Loss=0.2664, Time Left=9.09 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2367/3393 [18:53<08:45,  1.95batch/s, Batch Loss=0.0246, Avg Loss=0.2664, Time Left=9.09 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2367/3393 [18:54<08:45,  1.95batch/s, Batch Loss=0.2891, Avg Loss=0.2664, Time Left=9.08 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2368/3393 [18:54<08:34,  1.99batch/s, Batch Loss=0.2891, Avg Loss=0.2664, Time Left=9.08 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2368/3393 [18:54<08:34,  1.99batch/s, Batch Loss=0.1130, Avg Loss=0.2663, Time Left=9.07 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2369/3393 [18:54<08:41,  1.96batch/s, Batch Loss=0.1130, Avg Loss=0.2663, Time Left=9.07 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2369/3393 [18:55<08:41,  1.96batch/s, Batch Loss=0.2948, Avg Loss=0.2663, Time Left=9.06 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2370/3393 [18:55<08:33,  1.99batch/s, Batch Loss=0.2948, Avg Loss=0.2663, Time Left=9.06 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2370/3393 [18:55<08:33,  1.99batch/s, Batch Loss=0.1401, Avg Loss=0.2663, Time Left=9.06 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2371/3393 [18:55<08:27,  2.01batch/s, Batch Loss=0.1401, Avg Loss=0.2663, Time Left=9.06 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2371/3393 [18:56<08:27,  2.01batch/s, Batch Loss=0.1157, Avg Loss=0.2662, Time Left=9.05 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2372/3393 [18:56<08:22,  2.03batch/s, Batch Loss=0.1157, Avg Loss=0.2662, Time Left=9.05 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2372/3393 [18:56<08:22,  2.03batch/s, Batch Loss=0.0301, Avg Loss=0.2661, Time Left=9.04 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2373/3393 [18:56<08:14,  2.06batch/s, Batch Loss=0.0301, Avg Loss=0.2661, Time Left=9.04 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2373/3393 [18:57<08:14,  2.06batch/s, Batch Loss=0.0354, Avg Loss=0.2660, Time Left=9.03 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  70%|▋| 2374/3393 [18:57<08:13,  2.06batch/s, Batch Loss=0.0354, Avg Loss=0.2660, Time Left=9.03 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2374/3393 [18:57<08:13,  2.06batch/s, Batch Loss=0.1456, Avg Loss=0.2659, Time Left=9.02 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2375/3393 [18:57<08:13,  2.06batch/s, Batch Loss=0.1456, Avg Loss=0.2659, Time Left=9.02 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2375/3393 [18:58<08:13,  2.06batch/s, Batch Loss=0.1876, Avg Loss=0.2659, Time Left=9.01 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2376/3393 [18:58<08:07,  2.09batch/s, Batch Loss=0.1876, Avg Loss=0.2659, Time Left=9.01 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2376/3393 [18:58<08:07,  2.09batch/s, Batch Loss=0.1564, Avg Loss=0.2659, Time Left=9.01 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2377/3393 [18:58<08:08,  2.08batch/s, Batch Loss=0.1564, Avg Loss=0.2659, Time Left=9.01 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2377/3393 [18:59<08:08,  2.08batch/s, Batch Loss=0.1185, Avg Loss=0.2658, Time Left=9.00 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2378/3393 [18:59<08:13,  2.06batch/s, Batch Loss=0.1185, Avg Loss=0.2658, Time Left=9.00 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2378/3393 [18:59<08:13,  2.06batch/s, Batch Loss=0.1917, Avg Loss=0.2658, Time Left=8.99 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2379/3393 [18:59<08:12,  2.06batch/s, Batch Loss=0.1917, Avg Loss=0.2658, Time Left=8.99 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2379/3393 [19:00<08:12,  2.06batch/s, Batch Loss=0.1394, Avg Loss=0.2657, Time Left=8.98 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2380/3393 [19:00<08:16,  2.04batch/s, Batch Loss=0.1394, Avg Loss=0.2657, Time Left=8.98 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2380/3393 [19:00<08:16,  2.04batch/s, Batch Loss=0.3530, Avg Loss=0.2657, Time Left=8.97 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2381/3393 [19:00<08:09,  2.07batch/s, Batch Loss=0.3530, Avg Loss=0.2657, Time Left=8.97 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2381/3393 [19:01<08:09,  2.07batch/s, Batch Loss=0.0512, Avg Loss=0.2656, Time Left=8.96 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2382/3393 [19:01<08:08,  2.07batch/s, Batch Loss=0.0512, Avg Loss=0.2656, Time Left=8.96 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2382/3393 [19:01<08:08,  2.07batch/s, Batch Loss=0.0513, Avg Loss=0.2656, Time Left=8.96 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2383/3393 [19:01<08:17,  2.03batch/s, Batch Loss=0.0513, Avg Loss=0.2656, Time Left=8.96 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2383/3393 [19:02<08:17,  2.03batch/s, Batch Loss=0.2801, Avg Loss=0.2656, Time Left=8.95 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2384/3393 [19:02<08:14,  2.04batch/s, Batch Loss=0.2801, Avg Loss=0.2656, Time Left=8.95 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2384/3393 [19:02<08:14,  2.04batch/s, Batch Loss=0.1236, Avg Loss=0.2655, Time Left=8.94 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2385/3393 [19:02<08:22,  2.01batch/s, Batch Loss=0.1236, Avg Loss=0.2655, Time Left=8.94 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2385/3393 [19:03<08:22,  2.01batch/s, Batch Loss=0.3850, Avg Loss=0.2656, Time Left=8.93 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2386/3393 [19:03<08:22,  2.00batch/s, Batch Loss=0.3850, Avg Loss=0.2656, Time Left=8.93 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2386/3393 [19:03<08:22,  2.00batch/s, Batch Loss=0.0374, Avg Loss=0.2655, Time Left=8.92 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2387/3393 [19:03<08:17,  2.02batch/s, Batch Loss=0.0374, Avg Loss=0.2655, Time Left=8.92 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2387/3393 [19:04<08:17,  2.02batch/s, Batch Loss=0.2671, Avg Loss=0.2655, Time Left=8.92 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2388/3393 [19:04<08:08,  2.06batch/s, Batch Loss=0.2671, Avg Loss=0.2655, Time Left=8.92 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2388/3393 [19:04<08:08,  2.06batch/s, Batch Loss=0.0882, Avg Loss=0.2654, Time Left=8.91 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2389/3393 [19:04<08:02,  2.08batch/s, Batch Loss=0.0882, Avg Loss=0.2654, Time Left=8.91 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2389/3393 [19:05<08:02,  2.08batch/s, Batch Loss=0.0503, Avg Loss=0.2653, Time Left=8.90 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2390/3393 [19:05<08:08,  2.06batch/s, Batch Loss=0.0503, Avg Loss=0.2653, Time Left=8.90 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2390/3393 [19:05<08:08,  2.06batch/s, Batch Loss=0.1242, Avg Loss=0.2652, Time Left=8.89 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2391/3393 [19:05<08:06,  2.06batch/s, Batch Loss=0.1242, Avg Loss=0.2652, Time Left=8.89 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2391/3393 [19:05<08:06,  2.06batch/s, Batch Loss=0.1528, Avg Loss=0.2652, Time Left=8.88 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2392/3393 [19:06<08:05,  2.06batch/s, Batch Loss=0.1528, Avg Loss=0.2652, Time Left=8.88 \u001b[A\n",
      "Epoch 1/3 - Training:  70%|▋| 2392/3393 [19:06<08:05,  2.06batch/s, Batch Loss=0.1572, Avg Loss=0.2651, Time Left=8.87 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2393/3393 [19:06<08:09,  2.04batch/s, Batch Loss=0.1572, Avg Loss=0.2651, Time Left=8.87 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2393/3393 [19:06<08:09,  2.04batch/s, Batch Loss=0.2131, Avg Loss=0.2651, Time Left=8.87 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2394/3393 [19:06<08:05,  2.06batch/s, Batch Loss=0.2131, Avg Loss=0.2651, Time Left=8.87 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2394/3393 [19:07<08:05,  2.06batch/s, Batch Loss=0.4053, Avg Loss=0.2652, Time Left=8.86 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2395/3393 [19:07<08:04,  2.06batch/s, Batch Loss=0.4053, Avg Loss=0.2652, Time Left=8.86 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2395/3393 [19:07<08:04,  2.06batch/s, Batch Loss=0.0788, Avg Loss=0.2651, Time Left=8.85 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2396/3393 [19:07<08:04,  2.06batch/s, Batch Loss=0.0788, Avg Loss=0.2651, Time Left=8.85 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2396/3393 [19:08<08:04,  2.06batch/s, Batch Loss=0.1090, Avg Loss=0.2650, Time Left=8.84 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2397/3393 [19:08<07:59,  2.08batch/s, Batch Loss=0.1090, Avg Loss=0.2650, Time Left=8.84 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2397/3393 [19:08<07:59,  2.08batch/s, Batch Loss=0.1854, Avg Loss=0.2650, Time Left=8.83 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2398/3393 [19:08<07:58,  2.08batch/s, Batch Loss=0.1854, Avg Loss=0.2650, Time Left=8.83 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2398/3393 [19:09<07:58,  2.08batch/s, Batch Loss=0.0596, Avg Loss=0.2649, Time Left=8.82 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2399/3393 [19:09<07:59,  2.07batch/s, Batch Loss=0.0596, Avg Loss=0.2649, Time Left=8.82 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2399/3393 [19:09<07:59,  2.07batch/s, Batch Loss=0.1121, Avg Loss=0.2648, Time Left=8.82 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2400/3393 [19:09<07:59,  2.07batch/s, Batch Loss=0.1121, Avg Loss=0.2648, Time Left=8.82 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2400/3393 [19:10<07:59,  2.07batch/s, Batch Loss=0.1126, Avg Loss=0.2648, Time Left=8.81 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2401/3393 [19:10<08:12,  2.01batch/s, Batch Loss=0.1126, Avg Loss=0.2648, Time Left=8.81 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2401/3393 [19:10<08:12,  2.01batch/s, Batch Loss=0.1855, Avg Loss=0.2647, Time Left=8.80 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2402/3393 [19:10<08:07,  2.03batch/s, Batch Loss=0.1855, Avg Loss=0.2647, Time Left=8.80 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2402/3393 [19:11<08:07,  2.03batch/s, Batch Loss=0.1447, Avg Loss=0.2647, Time Left=8.79 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2403/3393 [19:11<08:06,  2.03batch/s, Batch Loss=0.1447, Avg Loss=0.2647, Time Left=8.79 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2403/3393 [19:11<08:06,  2.03batch/s, Batch Loss=0.0455, Avg Loss=0.2646, Time Left=8.78 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2404/3393 [19:11<08:03,  2.04batch/s, Batch Loss=0.0455, Avg Loss=0.2646, Time Left=8.78 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2404/3393 [19:12<08:03,  2.04batch/s, Batch Loss=0.1999, Avg Loss=0.2646, Time Left=8.77 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2405/3393 [19:12<08:06,  2.03batch/s, Batch Loss=0.1999, Avg Loss=0.2646, Time Left=8.77 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2405/3393 [19:12<08:06,  2.03batch/s, Batch Loss=0.2801, Avg Loss=0.2646, Time Left=8.77 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2406/3393 [19:12<08:03,  2.04batch/s, Batch Loss=0.2801, Avg Loss=0.2646, Time Left=8.77 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2406/3393 [19:13<08:03,  2.04batch/s, Batch Loss=0.0311, Avg Loss=0.2645, Time Left=8.76 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  71%|▋| 2407/3393 [19:13<07:56,  2.07batch/s, Batch Loss=0.0311, Avg Loss=0.2645, Time Left=8.76 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2407/3393 [19:13<07:56,  2.07batch/s, Batch Loss=0.1938, Avg Loss=0.2644, Time Left=8.75 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2408/3393 [19:13<07:55,  2.07batch/s, Batch Loss=0.1938, Avg Loss=0.2644, Time Left=8.75 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2408/3393 [19:14<07:55,  2.07batch/s, Batch Loss=0.2826, Avg Loss=0.2644, Time Left=8.74 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2409/3393 [19:14<08:13,  1.99batch/s, Batch Loss=0.2826, Avg Loss=0.2644, Time Left=8.74 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2409/3393 [19:14<08:13,  1.99batch/s, Batch Loss=0.4388, Avg Loss=0.2645, Time Left=8.73 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2410/3393 [19:14<08:07,  2.02batch/s, Batch Loss=0.4388, Avg Loss=0.2645, Time Left=8.73 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2410/3393 [19:15<08:07,  2.02batch/s, Batch Loss=0.5497, Avg Loss=0.2646, Time Left=8.72 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2411/3393 [19:15<08:05,  2.02batch/s, Batch Loss=0.5497, Avg Loss=0.2646, Time Left=8.72 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2411/3393 [19:15<08:05,  2.02batch/s, Batch Loss=0.3738, Avg Loss=0.2647, Time Left=8.72 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2412/3393 [19:15<08:06,  2.02batch/s, Batch Loss=0.3738, Avg Loss=0.2647, Time Left=8.72 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2412/3393 [19:16<08:06,  2.02batch/s, Batch Loss=0.0198, Avg Loss=0.2646, Time Left=8.71 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2413/3393 [19:16<07:57,  2.05batch/s, Batch Loss=0.0198, Avg Loss=0.2646, Time Left=8.71 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2413/3393 [19:16<07:57,  2.05batch/s, Batch Loss=0.0419, Avg Loss=0.2645, Time Left=8.70 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2414/3393 [19:16<07:51,  2.08batch/s, Batch Loss=0.0419, Avg Loss=0.2645, Time Left=8.70 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2414/3393 [19:17<07:51,  2.08batch/s, Batch Loss=0.1166, Avg Loss=0.2644, Time Left=8.69 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2415/3393 [19:17<07:46,  2.10batch/s, Batch Loss=0.1166, Avg Loss=0.2644, Time Left=8.69 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2415/3393 [19:17<07:46,  2.10batch/s, Batch Loss=0.0352, Avg Loss=0.2643, Time Left=8.68 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2416/3393 [19:17<07:47,  2.09batch/s, Batch Loss=0.0352, Avg Loss=0.2643, Time Left=8.68 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2416/3393 [19:18<07:47,  2.09batch/s, Batch Loss=0.1214, Avg Loss=0.2643, Time Left=8.67 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2417/3393 [19:18<07:57,  2.04batch/s, Batch Loss=0.1214, Avg Loss=0.2643, Time Left=8.67 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2417/3393 [19:18<07:57,  2.04batch/s, Batch Loss=0.1183, Avg Loss=0.2642, Time Left=8.67 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2418/3393 [19:18<08:01,  2.03batch/s, Batch Loss=0.1183, Avg Loss=0.2642, Time Left=8.67 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2418/3393 [19:19<08:01,  2.03batch/s, Batch Loss=0.1845, Avg Loss=0.2642, Time Left=8.66 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2419/3393 [19:19<08:02,  2.02batch/s, Batch Loss=0.1845, Avg Loss=0.2642, Time Left=8.66 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2419/3393 [19:19<08:02,  2.02batch/s, Batch Loss=0.5512, Avg Loss=0.2643, Time Left=8.65 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2420/3393 [19:19<07:54,  2.05batch/s, Batch Loss=0.5512, Avg Loss=0.2643, Time Left=8.65 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2420/3393 [19:20<07:54,  2.05batch/s, Batch Loss=0.1311, Avg Loss=0.2642, Time Left=8.64 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2421/3393 [19:20<07:56,  2.04batch/s, Batch Loss=0.1311, Avg Loss=0.2642, Time Left=8.64 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2421/3393 [19:20<07:56,  2.04batch/s, Batch Loss=0.1310, Avg Loss=0.2642, Time Left=8.63 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2422/3393 [19:20<07:49,  2.07batch/s, Batch Loss=0.1310, Avg Loss=0.2642, Time Left=8.63 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2422/3393 [19:21<07:49,  2.07batch/s, Batch Loss=0.0114, Avg Loss=0.2641, Time Left=8.62 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2423/3393 [19:21<07:44,  2.09batch/s, Batch Loss=0.0114, Avg Loss=0.2641, Time Left=8.62 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2423/3393 [19:21<07:44,  2.09batch/s, Batch Loss=0.2011, Avg Loss=0.2640, Time Left=8.62 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2424/3393 [19:21<07:49,  2.06batch/s, Batch Loss=0.2011, Avg Loss=0.2640, Time Left=8.62 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2424/3393 [19:22<07:49,  2.06batch/s, Batch Loss=0.0512, Avg Loss=0.2640, Time Left=8.61 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2425/3393 [19:22<07:52,  2.05batch/s, Batch Loss=0.0512, Avg Loss=0.2640, Time Left=8.61 \u001b[A\n",
      "Epoch 1/3 - Training:  71%|▋| 2425/3393 [19:22<07:52,  2.05batch/s, Batch Loss=0.1251, Avg Loss=0.2639, Time Left=8.60 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2426/3393 [19:22<07:48,  2.07batch/s, Batch Loss=0.1251, Avg Loss=0.2639, Time Left=8.60 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2426/3393 [19:23<07:48,  2.07batch/s, Batch Loss=0.1438, Avg Loss=0.2638, Time Left=8.59 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2427/3393 [19:23<07:56,  2.03batch/s, Batch Loss=0.1438, Avg Loss=0.2638, Time Left=8.59 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2427/3393 [19:23<07:56,  2.03batch/s, Batch Loss=0.1489, Avg Loss=0.2638, Time Left=8.58 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2428/3393 [19:23<07:53,  2.04batch/s, Batch Loss=0.1489, Avg Loss=0.2638, Time Left=8.58 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2428/3393 [19:24<07:53,  2.04batch/s, Batch Loss=0.0851, Avg Loss=0.2637, Time Left=8.57 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2429/3393 [19:24<07:51,  2.04batch/s, Batch Loss=0.0851, Avg Loss=0.2637, Time Left=8.57 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2429/3393 [19:24<07:51,  2.04batch/s, Batch Loss=0.2092, Avg Loss=0.2637, Time Left=8.57 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2430/3393 [19:24<07:53,  2.03batch/s, Batch Loss=0.2092, Avg Loss=0.2637, Time Left=8.57 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2430/3393 [19:25<07:53,  2.03batch/s, Batch Loss=0.2545, Avg Loss=0.2637, Time Left=8.56 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2431/3393 [19:25<07:51,  2.04batch/s, Batch Loss=0.2545, Avg Loss=0.2637, Time Left=8.56 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2431/3393 [19:25<07:51,  2.04batch/s, Batch Loss=0.0937, Avg Loss=0.2636, Time Left=8.55 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2432/3393 [19:25<07:53,  2.03batch/s, Batch Loss=0.0937, Avg Loss=0.2636, Time Left=8.55 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2432/3393 [19:26<07:53,  2.03batch/s, Batch Loss=0.2576, Avg Loss=0.2636, Time Left=8.54 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2433/3393 [19:26<07:55,  2.02batch/s, Batch Loss=0.2576, Avg Loss=0.2636, Time Left=8.54 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2433/3393 [19:26<07:55,  2.02batch/s, Batch Loss=0.1711, Avg Loss=0.2636, Time Left=8.53 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2434/3393 [19:26<07:55,  2.02batch/s, Batch Loss=0.1711, Avg Loss=0.2636, Time Left=8.53 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2434/3393 [19:27<07:55,  2.02batch/s, Batch Loss=0.2222, Avg Loss=0.2636, Time Left=8.53 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2435/3393 [19:27<07:47,  2.05batch/s, Batch Loss=0.2222, Avg Loss=0.2636, Time Left=8.53 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2435/3393 [19:27<07:47,  2.05batch/s, Batch Loss=0.1105, Avg Loss=0.2635, Time Left=8.52 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2436/3393 [19:27<07:41,  2.07batch/s, Batch Loss=0.1105, Avg Loss=0.2635, Time Left=8.52 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2436/3393 [19:28<07:41,  2.07batch/s, Batch Loss=0.1620, Avg Loss=0.2634, Time Left=8.51 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2437/3393 [19:28<07:54,  2.01batch/s, Batch Loss=0.1620, Avg Loss=0.2634, Time Left=8.51 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2437/3393 [19:28<07:54,  2.01batch/s, Batch Loss=0.1770, Avg Loss=0.2634, Time Left=8.50 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2438/3393 [19:28<07:51,  2.03batch/s, Batch Loss=0.1770, Avg Loss=0.2634, Time Left=8.50 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2438/3393 [19:29<07:51,  2.03batch/s, Batch Loss=0.2370, Avg Loss=0.2634, Time Left=8.49 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2439/3393 [19:29<08:01,  1.98batch/s, Batch Loss=0.2370, Avg Loss=0.2634, Time Left=8.49 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2439/3393 [19:29<08:01,  1.98batch/s, Batch Loss=0.1549, Avg Loss=0.2634, Time Left=8.48 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  72%|▋| 2440/3393 [19:29<07:55,  2.00batch/s, Batch Loss=0.1549, Avg Loss=0.2634, Time Left=8.48 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2440/3393 [19:30<07:55,  2.00batch/s, Batch Loss=0.0169, Avg Loss=0.2633, Time Left=8.48 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2441/3393 [19:30<08:04,  1.97batch/s, Batch Loss=0.0169, Avg Loss=0.2633, Time Left=8.48 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2441/3393 [19:30<08:04,  1.97batch/s, Batch Loss=0.2989, Avg Loss=0.2633, Time Left=8.47 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2442/3393 [19:30<07:52,  2.01batch/s, Batch Loss=0.2989, Avg Loss=0.2633, Time Left=8.47 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2442/3393 [19:30<07:52,  2.01batch/s, Batch Loss=0.1194, Avg Loss=0.2632, Time Left=8.46 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2443/3393 [19:30<07:48,  2.03batch/s, Batch Loss=0.1194, Avg Loss=0.2632, Time Left=8.46 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2443/3393 [19:31<07:48,  2.03batch/s, Batch Loss=0.0983, Avg Loss=0.2631, Time Left=8.45 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2444/3393 [19:31<07:53,  2.00batch/s, Batch Loss=0.0983, Avg Loss=0.2631, Time Left=8.45 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2444/3393 [19:31<07:53,  2.00batch/s, Batch Loss=0.2381, Avg Loss=0.2631, Time Left=8.44 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2445/3393 [19:32<07:49,  2.02batch/s, Batch Loss=0.2381, Avg Loss=0.2631, Time Left=8.44 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2445/3393 [19:32<07:49,  2.02batch/s, Batch Loss=0.8585, Avg Loss=0.2634, Time Left=8.44 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2446/3393 [19:32<07:55,  1.99batch/s, Batch Loss=0.8585, Avg Loss=0.2634, Time Left=8.44 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2446/3393 [19:32<07:55,  1.99batch/s, Batch Loss=0.0304, Avg Loss=0.2633, Time Left=8.43 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2447/3393 [19:32<07:45,  2.03batch/s, Batch Loss=0.0304, Avg Loss=0.2633, Time Left=8.43 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2447/3393 [19:33<07:45,  2.03batch/s, Batch Loss=0.6620, Avg Loss=0.2634, Time Left=8.42 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2448/3393 [19:33<07:42,  2.04batch/s, Batch Loss=0.6620, Avg Loss=0.2634, Time Left=8.42 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2448/3393 [19:33<07:42,  2.04batch/s, Batch Loss=0.0651, Avg Loss=0.2634, Time Left=8.41 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2449/3393 [19:33<07:44,  2.03batch/s, Batch Loss=0.0651, Avg Loss=0.2634, Time Left=8.41 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2449/3393 [19:34<07:44,  2.03batch/s, Batch Loss=0.3501, Avg Loss=0.2634, Time Left=8.40 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2450/3393 [19:34<07:41,  2.04batch/s, Batch Loss=0.3501, Avg Loss=0.2634, Time Left=8.40 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2450/3393 [19:34<07:41,  2.04batch/s, Batch Loss=0.2138, Avg Loss=0.2634, Time Left=8.39 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2451/3393 [19:34<07:39,  2.05batch/s, Batch Loss=0.2138, Avg Loss=0.2634, Time Left=8.39 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2451/3393 [19:35<07:39,  2.05batch/s, Batch Loss=0.5678, Avg Loss=0.2635, Time Left=8.39 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2452/3393 [19:35<07:37,  2.06batch/s, Batch Loss=0.5678, Avg Loss=0.2635, Time Left=8.39 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2452/3393 [19:35<07:37,  2.06batch/s, Batch Loss=0.0842, Avg Loss=0.2634, Time Left=8.38 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2453/3393 [19:35<07:36,  2.06batch/s, Batch Loss=0.0842, Avg Loss=0.2634, Time Left=8.38 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2453/3393 [19:36<07:36,  2.06batch/s, Batch Loss=0.0694, Avg Loss=0.2634, Time Left=8.37 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2454/3393 [19:36<07:35,  2.06batch/s, Batch Loss=0.0694, Avg Loss=0.2634, Time Left=8.37 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2454/3393 [19:36<07:35,  2.06batch/s, Batch Loss=0.0707, Avg Loss=0.2633, Time Left=8.36 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2455/3393 [19:36<07:34,  2.06batch/s, Batch Loss=0.0707, Avg Loss=0.2633, Time Left=8.36 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2455/3393 [19:37<07:34,  2.06batch/s, Batch Loss=0.2651, Avg Loss=0.2633, Time Left=8.35 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2456/3393 [19:37<07:29,  2.09batch/s, Batch Loss=0.2651, Avg Loss=0.2633, Time Left=8.35 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2456/3393 [19:37<07:29,  2.09batch/s, Batch Loss=0.1674, Avg Loss=0.2632, Time Left=8.34 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2457/3393 [19:37<07:38,  2.04batch/s, Batch Loss=0.1674, Avg Loss=0.2632, Time Left=8.34 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2457/3393 [19:38<07:38,  2.04batch/s, Batch Loss=0.3102, Avg Loss=0.2632, Time Left=8.34 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2458/3393 [19:38<07:42,  2.02batch/s, Batch Loss=0.3102, Avg Loss=0.2632, Time Left=8.34 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2458/3393 [19:38<07:42,  2.02batch/s, Batch Loss=0.2456, Avg Loss=0.2632, Time Left=8.33 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2459/3393 [19:38<07:42,  2.02batch/s, Batch Loss=0.2456, Avg Loss=0.2632, Time Left=8.33 \u001b[A\n",
      "Epoch 1/3 - Training:  72%|▋| 2459/3393 [19:39<07:42,  2.02batch/s, Batch Loss=0.2679, Avg Loss=0.2632, Time Left=8.32 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2460/3393 [19:39<07:35,  2.05batch/s, Batch Loss=0.2679, Avg Loss=0.2632, Time Left=8.32 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2460/3393 [19:39<07:35,  2.05batch/s, Batch Loss=0.1832, Avg Loss=0.2632, Time Left=8.31 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2461/3393 [19:39<07:33,  2.06batch/s, Batch Loss=0.1832, Avg Loss=0.2632, Time Left=8.31 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2461/3393 [19:40<07:33,  2.06batch/s, Batch Loss=0.2034, Avg Loss=0.2632, Time Left=8.30 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2462/3393 [19:40<07:36,  2.04batch/s, Batch Loss=0.2034, Avg Loss=0.2632, Time Left=8.30 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2462/3393 [19:40<07:36,  2.04batch/s, Batch Loss=0.1768, Avg Loss=0.2631, Time Left=8.29 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2463/3393 [19:40<07:25,  2.09batch/s, Batch Loss=0.1768, Avg Loss=0.2631, Time Left=8.29 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2463/3393 [19:41<07:25,  2.09batch/s, Batch Loss=0.1795, Avg Loss=0.2631, Time Left=8.29 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2464/3393 [19:41<07:30,  2.06batch/s, Batch Loss=0.1795, Avg Loss=0.2631, Time Left=8.29 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2464/3393 [19:41<07:30,  2.06batch/s, Batch Loss=0.0913, Avg Loss=0.2630, Time Left=8.28 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2465/3393 [19:41<07:31,  2.06batch/s, Batch Loss=0.0913, Avg Loss=0.2630, Time Left=8.28 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2465/3393 [19:42<07:31,  2.06batch/s, Batch Loss=0.0819, Avg Loss=0.2630, Time Left=8.27 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2466/3393 [19:42<07:33,  2.05batch/s, Batch Loss=0.0819, Avg Loss=0.2630, Time Left=8.27 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2466/3393 [19:42<07:33,  2.05batch/s, Batch Loss=0.3988, Avg Loss=0.2630, Time Left=8.26 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2467/3393 [19:42<07:31,  2.05batch/s, Batch Loss=0.3988, Avg Loss=0.2630, Time Left=8.26 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2467/3393 [19:43<07:31,  2.05batch/s, Batch Loss=0.2528, Avg Loss=0.2630, Time Left=8.25 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2468/3393 [19:43<07:29,  2.06batch/s, Batch Loss=0.2528, Avg Loss=0.2630, Time Left=8.25 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2468/3393 [19:43<07:29,  2.06batch/s, Batch Loss=0.0697, Avg Loss=0.2629, Time Left=8.24 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2469/3393 [19:43<07:28,  2.06batch/s, Batch Loss=0.0697, Avg Loss=0.2629, Time Left=8.24 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2469/3393 [19:44<07:28,  2.06batch/s, Batch Loss=0.1625, Avg Loss=0.2629, Time Left=8.24 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2470/3393 [19:44<07:36,  2.02batch/s, Batch Loss=0.1625, Avg Loss=0.2629, Time Left=8.24 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2470/3393 [19:44<07:36,  2.02batch/s, Batch Loss=0.1373, Avg Loss=0.2628, Time Left=8.23 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2471/3393 [19:44<07:37,  2.02batch/s, Batch Loss=0.1373, Avg Loss=0.2628, Time Left=8.23 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2471/3393 [19:45<07:37,  2.02batch/s, Batch Loss=0.1602, Avg Loss=0.2628, Time Left=8.22 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2472/3393 [19:45<07:37,  2.01batch/s, Batch Loss=0.1602, Avg Loss=0.2628, Time Left=8.22 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2472/3393 [19:45<07:37,  2.01batch/s, Batch Loss=0.1670, Avg Loss=0.2628, Time Left=8.21 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  73%|▋| 2473/3393 [19:45<07:34,  2.03batch/s, Batch Loss=0.1670, Avg Loss=0.2628, Time Left=8.21 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2473/3393 [19:46<07:34,  2.03batch/s, Batch Loss=0.0319, Avg Loss=0.2627, Time Left=8.20 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2474/3393 [19:46<07:26,  2.06batch/s, Batch Loss=0.0319, Avg Loss=0.2627, Time Left=8.20 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2474/3393 [19:46<07:26,  2.06batch/s, Batch Loss=0.1052, Avg Loss=0.2626, Time Left=8.19 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2475/3393 [19:46<07:25,  2.06batch/s, Batch Loss=0.1052, Avg Loss=0.2626, Time Left=8.19 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2475/3393 [19:47<07:25,  2.06batch/s, Batch Loss=0.2846, Avg Loss=0.2626, Time Left=8.19 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2476/3393 [19:47<07:20,  2.08batch/s, Batch Loss=0.2846, Avg Loss=0.2626, Time Left=8.19 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2476/3393 [19:47<07:20,  2.08batch/s, Batch Loss=0.0382, Avg Loss=0.2625, Time Left=8.18 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2477/3393 [19:47<07:25,  2.06batch/s, Batch Loss=0.0382, Avg Loss=0.2625, Time Left=8.18 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2477/3393 [19:48<07:25,  2.06batch/s, Batch Loss=0.3928, Avg Loss=0.2626, Time Left=8.17 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2478/3393 [19:48<07:23,  2.06batch/s, Batch Loss=0.3928, Avg Loss=0.2626, Time Left=8.17 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2478/3393 [19:48<07:23,  2.06batch/s, Batch Loss=0.1895, Avg Loss=0.2625, Time Left=8.16 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2479/3393 [19:48<07:23,  2.06batch/s, Batch Loss=0.1895, Avg Loss=0.2625, Time Left=8.16 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2479/3393 [19:49<07:23,  2.06batch/s, Batch Loss=0.0298, Avg Loss=0.2624, Time Left=8.15 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2480/3393 [19:49<07:35,  2.01batch/s, Batch Loss=0.0298, Avg Loss=0.2624, Time Left=8.15 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2480/3393 [19:49<07:35,  2.01batch/s, Batch Loss=0.0446, Avg Loss=0.2624, Time Left=8.14 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2481/3393 [19:49<07:26,  2.04batch/s, Batch Loss=0.0446, Avg Loss=0.2624, Time Left=8.14 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2481/3393 [19:50<07:26,  2.04batch/s, Batch Loss=0.3711, Avg Loss=0.2624, Time Left=8.14 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2482/3393 [19:50<07:24,  2.05batch/s, Batch Loss=0.3711, Avg Loss=0.2624, Time Left=8.14 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2482/3393 [19:50<07:24,  2.05batch/s, Batch Loss=0.1552, Avg Loss=0.2624, Time Left=8.13 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2483/3393 [19:50<07:22,  2.05batch/s, Batch Loss=0.1552, Avg Loss=0.2624, Time Left=8.13 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2483/3393 [19:51<07:22,  2.05batch/s, Batch Loss=0.1277, Avg Loss=0.2623, Time Left=8.12 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2484/3393 [19:51<07:17,  2.08batch/s, Batch Loss=0.1277, Avg Loss=0.2623, Time Left=8.12 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2484/3393 [19:51<07:17,  2.08batch/s, Batch Loss=0.0949, Avg Loss=0.2622, Time Left=8.11 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2485/3393 [19:51<07:17,  2.08batch/s, Batch Loss=0.0949, Avg Loss=0.2622, Time Left=8.11 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2485/3393 [19:51<07:17,  2.08batch/s, Batch Loss=0.1807, Avg Loss=0.2622, Time Left=8.10 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2486/3393 [19:51<07:17,  2.07batch/s, Batch Loss=0.1807, Avg Loss=0.2622, Time Left=8.10 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2486/3393 [19:52<07:17,  2.07batch/s, Batch Loss=0.0806, Avg Loss=0.2621, Time Left=8.09 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2487/3393 [19:52<07:20,  2.06batch/s, Batch Loss=0.0806, Avg Loss=0.2621, Time Left=8.09 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2487/3393 [19:52<07:20,  2.06batch/s, Batch Loss=0.2540, Avg Loss=0.2621, Time Left=8.09 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2488/3393 [19:52<07:24,  2.04batch/s, Batch Loss=0.2540, Avg Loss=0.2621, Time Left=8.09 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2488/3393 [19:53<07:24,  2.04batch/s, Batch Loss=0.0667, Avg Loss=0.2620, Time Left=8.08 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2489/3393 [19:53<07:27,  2.02batch/s, Batch Loss=0.0667, Avg Loss=0.2620, Time Left=8.08 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2489/3393 [19:53<07:27,  2.02batch/s, Batch Loss=0.0353, Avg Loss=0.2619, Time Left=8.07 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2490/3393 [19:53<07:19,  2.06batch/s, Batch Loss=0.0353, Avg Loss=0.2619, Time Left=8.07 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2490/3393 [19:54<07:19,  2.06batch/s, Batch Loss=0.2495, Avg Loss=0.2619, Time Left=8.06 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2491/3393 [19:54<07:17,  2.06batch/s, Batch Loss=0.2495, Avg Loss=0.2619, Time Left=8.06 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2491/3393 [19:54<07:17,  2.06batch/s, Batch Loss=0.0504, Avg Loss=0.2618, Time Left=8.05 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2492/3393 [19:54<07:13,  2.08batch/s, Batch Loss=0.0504, Avg Loss=0.2618, Time Left=8.05 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2492/3393 [19:55<07:13,  2.08batch/s, Batch Loss=0.3782, Avg Loss=0.2619, Time Left=8.05 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2493/3393 [19:55<07:12,  2.08batch/s, Batch Loss=0.3782, Avg Loss=0.2619, Time Left=8.05 \u001b[A\n",
      "Epoch 1/3 - Training:  73%|▋| 2493/3393 [19:55<07:12,  2.08batch/s, Batch Loss=0.3028, Avg Loss=0.2619, Time Left=8.04 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2494/3393 [19:55<07:13,  2.07batch/s, Batch Loss=0.3028, Avg Loss=0.2619, Time Left=8.04 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2494/3393 [19:56<07:13,  2.07batch/s, Batch Loss=0.3976, Avg Loss=0.2620, Time Left=8.03 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2495/3393 [19:56<07:12,  2.07batch/s, Batch Loss=0.3976, Avg Loss=0.2620, Time Left=8.03 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2495/3393 [19:56<07:12,  2.07batch/s, Batch Loss=0.1454, Avg Loss=0.2619, Time Left=8.02 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2496/3393 [19:56<07:21,  2.03batch/s, Batch Loss=0.1454, Avg Loss=0.2619, Time Left=8.02 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2496/3393 [19:57<07:21,  2.03batch/s, Batch Loss=0.2413, Avg Loss=0.2619, Time Left=8.01 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2497/3393 [19:57<07:25,  2.01batch/s, Batch Loss=0.2413, Avg Loss=0.2619, Time Left=8.01 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2497/3393 [19:57<07:25,  2.01batch/s, Batch Loss=0.2075, Avg Loss=0.2619, Time Left=8.00 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2498/3393 [19:57<07:28,  2.00batch/s, Batch Loss=0.2075, Avg Loss=0.2619, Time Left=8.00 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2498/3393 [19:58<07:28,  2.00batch/s, Batch Loss=0.0886, Avg Loss=0.2618, Time Left=8.00 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2499/3393 [19:58<07:35,  1.96batch/s, Batch Loss=0.0886, Avg Loss=0.2618, Time Left=8.00 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2499/3393 [19:58<07:35,  1.96batch/s, Batch Loss=0.1144, Avg Loss=0.2618, Time Left=7.99 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2500/3393 [19:58<07:37,  1.95batch/s, Batch Loss=0.1144, Avg Loss=0.2618, Time Left=7.99 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2500/3393 [19:59<07:37,  1.95batch/s, Batch Loss=0.0586, Avg Loss=0.2617, Time Left=7.98 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2501/3393 [19:59<07:29,  1.98batch/s, Batch Loss=0.0586, Avg Loss=0.2617, Time Left=7.98 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2501/3393 [19:59<07:29,  1.98batch/s, Batch Loss=0.1999, Avg Loss=0.2616, Time Left=7.97 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2502/3393 [19:59<07:32,  1.97batch/s, Batch Loss=0.1999, Avg Loss=0.2616, Time Left=7.97 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2502/3393 [20:00<07:32,  1.97batch/s, Batch Loss=0.2577, Avg Loss=0.2616, Time Left=7.96 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2503/3393 [20:00<07:24,  2.00batch/s, Batch Loss=0.2577, Avg Loss=0.2616, Time Left=7.96 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2503/3393 [20:00<07:24,  2.00batch/s, Batch Loss=0.0407, Avg Loss=0.2616, Time Left=7.95 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2504/3393 [20:00<07:21,  2.01batch/s, Batch Loss=0.0407, Avg Loss=0.2616, Time Left=7.95 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2504/3393 [20:01<07:21,  2.01batch/s, Batch Loss=0.0981, Avg Loss=0.2615, Time Left=7.95 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2505/3393 [20:01<07:20,  2.02batch/s, Batch Loss=0.0981, Avg Loss=0.2615, Time Left=7.95 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2505/3393 [20:01<07:20,  2.02batch/s, Batch Loss=0.1196, Avg Loss=0.2614, Time Left=7.94 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  74%|▋| 2506/3393 [20:01<07:13,  2.05batch/s, Batch Loss=0.1196, Avg Loss=0.2614, Time Left=7.94 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2506/3393 [20:02<07:13,  2.05batch/s, Batch Loss=0.3095, Avg Loss=0.2614, Time Left=7.93 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2507/3393 [20:02<07:11,  2.05batch/s, Batch Loss=0.3095, Avg Loss=0.2614, Time Left=7.93 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2507/3393 [20:02<07:11,  2.05batch/s, Batch Loss=0.1862, Avg Loss=0.2614, Time Left=7.92 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2508/3393 [20:02<07:10,  2.06batch/s, Batch Loss=0.1862, Avg Loss=0.2614, Time Left=7.92 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2508/3393 [20:03<07:10,  2.06batch/s, Batch Loss=0.0416, Avg Loss=0.2613, Time Left=7.91 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2509/3393 [20:03<07:00,  2.10batch/s, Batch Loss=0.0416, Avg Loss=0.2613, Time Left=7.91 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2509/3393 [20:03<07:00,  2.10batch/s, Batch Loss=0.2344, Avg Loss=0.2613, Time Left=7.90 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2510/3393 [20:03<07:07,  2.06batch/s, Batch Loss=0.2344, Avg Loss=0.2613, Time Left=7.90 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2510/3393 [20:04<07:07,  2.06batch/s, Batch Loss=0.2333, Avg Loss=0.2613, Time Left=7.90 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2511/3393 [20:04<07:05,  2.07batch/s, Batch Loss=0.2333, Avg Loss=0.2613, Time Left=7.90 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2511/3393 [20:04<07:05,  2.07batch/s, Batch Loss=0.0366, Avg Loss=0.2612, Time Left=7.89 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2512/3393 [20:04<06:57,  2.11batch/s, Batch Loss=0.0366, Avg Loss=0.2612, Time Left=7.89 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2512/3393 [20:05<06:57,  2.11batch/s, Batch Loss=0.2146, Avg Loss=0.2612, Time Left=7.88 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2513/3393 [20:05<07:03,  2.08batch/s, Batch Loss=0.2146, Avg Loss=0.2612, Time Left=7.88 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2513/3393 [20:05<07:03,  2.08batch/s, Batch Loss=0.1426, Avg Loss=0.2611, Time Left=7.87 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2514/3393 [20:05<07:03,  2.08batch/s, Batch Loss=0.1426, Avg Loss=0.2611, Time Left=7.87 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2514/3393 [20:06<07:03,  2.08batch/s, Batch Loss=0.2302, Avg Loss=0.2611, Time Left=7.86 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2515/3393 [20:06<07:03,  2.07batch/s, Batch Loss=0.2302, Avg Loss=0.2611, Time Left=7.86 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2515/3393 [20:06<07:03,  2.07batch/s, Batch Loss=0.2895, Avg Loss=0.2611, Time Left=7.86 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2516/3393 [20:06<07:11,  2.03batch/s, Batch Loss=0.2895, Avg Loss=0.2611, Time Left=7.86 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2516/3393 [20:07<07:11,  2.03batch/s, Batch Loss=0.0868, Avg Loss=0.2611, Time Left=7.85 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2517/3393 [20:07<07:13,  2.02batch/s, Batch Loss=0.0868, Avg Loss=0.2611, Time Left=7.85 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2517/3393 [20:07<07:13,  2.02batch/s, Batch Loss=0.1326, Avg Loss=0.2610, Time Left=7.84 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2518/3393 [20:07<07:22,  1.98batch/s, Batch Loss=0.1326, Avg Loss=0.2610, Time Left=7.84 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2518/3393 [20:08<07:22,  1.98batch/s, Batch Loss=0.1184, Avg Loss=0.2610, Time Left=7.83 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2519/3393 [20:08<07:17,  2.00batch/s, Batch Loss=0.1184, Avg Loss=0.2610, Time Left=7.83 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2519/3393 [20:08<07:17,  2.00batch/s, Batch Loss=0.0463, Avg Loss=0.2609, Time Left=7.82 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2520/3393 [20:08<07:24,  1.96batch/s, Batch Loss=0.0463, Avg Loss=0.2609, Time Left=7.82 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2520/3393 [20:09<07:24,  1.96batch/s, Batch Loss=0.3972, Avg Loss=0.2609, Time Left=7.81 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2521/3393 [20:09<07:18,  1.99batch/s, Batch Loss=0.3972, Avg Loss=0.2609, Time Left=7.81 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2521/3393 [20:09<07:18,  1.99batch/s, Batch Loss=0.1715, Avg Loss=0.2609, Time Left=7.81 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2522/3393 [20:09<07:13,  2.01batch/s, Batch Loss=0.1715, Avg Loss=0.2609, Time Left=7.81 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2522/3393 [20:10<07:13,  2.01batch/s, Batch Loss=0.0594, Avg Loss=0.2608, Time Left=7.80 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2523/3393 [20:10<07:04,  2.05batch/s, Batch Loss=0.0594, Avg Loss=0.2608, Time Left=7.80 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2523/3393 [20:10<07:04,  2.05batch/s, Batch Loss=0.0194, Avg Loss=0.2607, Time Left=7.79 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2524/3393 [20:10<06:58,  2.08batch/s, Batch Loss=0.0194, Avg Loss=0.2607, Time Left=7.79 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2524/3393 [20:11<06:58,  2.08batch/s, Batch Loss=0.1095, Avg Loss=0.2606, Time Left=7.78 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2525/3393 [20:11<07:02,  2.05batch/s, Batch Loss=0.1095, Avg Loss=0.2606, Time Left=7.78 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2525/3393 [20:11<07:02,  2.05batch/s, Batch Loss=0.0744, Avg Loss=0.2606, Time Left=7.77 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2526/3393 [20:11<06:57,  2.08batch/s, Batch Loss=0.0744, Avg Loss=0.2606, Time Left=7.77 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2526/3393 [20:12<06:57,  2.08batch/s, Batch Loss=0.1898, Avg Loss=0.2605, Time Left=7.76 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2527/3393 [20:12<06:53,  2.09batch/s, Batch Loss=0.1898, Avg Loss=0.2605, Time Left=7.76 \u001b[A\n",
      "Epoch 1/3 - Training:  74%|▋| 2527/3393 [20:12<06:53,  2.09batch/s, Batch Loss=0.0278, Avg Loss=0.2604, Time Left=7.76 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2528/3393 [20:12<06:58,  2.07batch/s, Batch Loss=0.0278, Avg Loss=0.2604, Time Left=7.76 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2528/3393 [20:13<06:58,  2.07batch/s, Batch Loss=0.1041, Avg Loss=0.2604, Time Left=7.75 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2529/3393 [20:13<06:56,  2.07batch/s, Batch Loss=0.1041, Avg Loss=0.2604, Time Left=7.75 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2529/3393 [20:13<06:56,  2.07batch/s, Batch Loss=0.3924, Avg Loss=0.2604, Time Left=7.74 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2530/3393 [20:13<06:57,  2.07batch/s, Batch Loss=0.3924, Avg Loss=0.2604, Time Left=7.74 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2530/3393 [20:14<06:57,  2.07batch/s, Batch Loss=0.1163, Avg Loss=0.2604, Time Left=7.73 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2531/3393 [20:14<07:05,  2.03batch/s, Batch Loss=0.1163, Avg Loss=0.2604, Time Left=7.73 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2531/3393 [20:14<07:05,  2.03batch/s, Batch Loss=0.1097, Avg Loss=0.2603, Time Left=7.72 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2532/3393 [20:14<07:07,  2.02batch/s, Batch Loss=0.1097, Avg Loss=0.2603, Time Left=7.72 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2532/3393 [20:15<07:07,  2.02batch/s, Batch Loss=0.1213, Avg Loss=0.2603, Time Left=7.71 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2533/3393 [20:15<07:03,  2.03batch/s, Batch Loss=0.1213, Avg Loss=0.2603, Time Left=7.71 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2533/3393 [20:15<07:03,  2.03batch/s, Batch Loss=0.0165, Avg Loss=0.2602, Time Left=7.71 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2534/3393 [20:15<07:01,  2.04batch/s, Batch Loss=0.0165, Avg Loss=0.2602, Time Left=7.71 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2534/3393 [20:16<07:01,  2.04batch/s, Batch Loss=0.0533, Avg Loss=0.2601, Time Left=7.70 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2535/3393 [20:16<06:58,  2.05batch/s, Batch Loss=0.0533, Avg Loss=0.2601, Time Left=7.70 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2535/3393 [20:16<06:58,  2.05batch/s, Batch Loss=0.2272, Avg Loss=0.2601, Time Left=7.69 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2536/3393 [20:16<07:05,  2.02batch/s, Batch Loss=0.2272, Avg Loss=0.2601, Time Left=7.69 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2536/3393 [20:17<07:05,  2.02batch/s, Batch Loss=0.0622, Avg Loss=0.2600, Time Left=7.68 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2537/3393 [20:17<06:57,  2.05batch/s, Batch Loss=0.0622, Avg Loss=0.2600, Time Left=7.68 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2537/3393 [20:17<06:57,  2.05batch/s, Batch Loss=0.1344, Avg Loss=0.2599, Time Left=7.67 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2538/3393 [20:17<06:55,  2.06batch/s, Batch Loss=0.1344, Avg Loss=0.2599, Time Left=7.67 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2538/3393 [20:17<06:55,  2.06batch/s, Batch Loss=0.2752, Avg Loss=0.2599, Time Left=7.66 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  75%|▋| 2539/3393 [20:17<06:54,  2.06batch/s, Batch Loss=0.2752, Avg Loss=0.2599, Time Left=7.66 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2539/3393 [20:18<06:54,  2.06batch/s, Batch Loss=0.0347, Avg Loss=0.2599, Time Left=7.66 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2540/3393 [20:18<06:50,  2.08batch/s, Batch Loss=0.0347, Avg Loss=0.2599, Time Left=7.66 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2540/3393 [20:18<06:50,  2.08batch/s, Batch Loss=0.5744, Avg Loss=0.2600, Time Left=7.65 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2541/3393 [20:18<06:51,  2.07batch/s, Batch Loss=0.5744, Avg Loss=0.2600, Time Left=7.65 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2541/3393 [20:19<06:51,  2.07batch/s, Batch Loss=0.0227, Avg Loss=0.2599, Time Left=7.64 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2542/3393 [20:19<06:49,  2.08batch/s, Batch Loss=0.0227, Avg Loss=0.2599, Time Left=7.64 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2542/3393 [20:19<06:49,  2.08batch/s, Batch Loss=0.0364, Avg Loss=0.2598, Time Left=7.63 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2543/3393 [20:19<06:41,  2.12batch/s, Batch Loss=0.0364, Avg Loss=0.2598, Time Left=7.63 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2543/3393 [20:20<06:41,  2.12batch/s, Batch Loss=0.1344, Avg Loss=0.2597, Time Left=7.62 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2544/3393 [20:20<06:43,  2.10batch/s, Batch Loss=0.1344, Avg Loss=0.2597, Time Left=7.62 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▋| 2544/3393 [20:20<06:43,  2.10batch/s, Batch Loss=0.0927, Avg Loss=0.2597, Time Left=7.61 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2545/3393 [20:20<06:54,  2.05batch/s, Batch Loss=0.0927, Avg Loss=0.2597, Time Left=7.61 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2545/3393 [20:21<06:54,  2.05batch/s, Batch Loss=0.1519, Avg Loss=0.2596, Time Left=7.61 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2546/3393 [20:21<06:51,  2.06batch/s, Batch Loss=0.1519, Avg Loss=0.2596, Time Left=7.61 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2546/3393 [20:21<06:51,  2.06batch/s, Batch Loss=0.2291, Avg Loss=0.2596, Time Left=7.60 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2547/3393 [20:21<06:58,  2.02batch/s, Batch Loss=0.2291, Avg Loss=0.2596, Time Left=7.60 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2547/3393 [20:22<06:58,  2.02batch/s, Batch Loss=0.0901, Avg Loss=0.2595, Time Left=7.59 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2548/3393 [20:22<06:55,  2.03batch/s, Batch Loss=0.0901, Avg Loss=0.2595, Time Left=7.59 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2548/3393 [20:22<06:55,  2.03batch/s, Batch Loss=0.2877, Avg Loss=0.2596, Time Left=7.58 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2549/3393 [20:22<06:49,  2.06batch/s, Batch Loss=0.2877, Avg Loss=0.2596, Time Left=7.58 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2549/3393 [20:23<06:49,  2.06batch/s, Batch Loss=0.2361, Avg Loss=0.2595, Time Left=7.57 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2550/3393 [20:23<06:52,  2.04batch/s, Batch Loss=0.2361, Avg Loss=0.2595, Time Left=7.57 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2550/3393 [20:23<06:52,  2.04batch/s, Batch Loss=0.1894, Avg Loss=0.2595, Time Left=7.57 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2551/3393 [20:23<06:50,  2.05batch/s, Batch Loss=0.1894, Avg Loss=0.2595, Time Left=7.57 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2551/3393 [20:24<06:50,  2.05batch/s, Batch Loss=0.0335, Avg Loss=0.2594, Time Left=7.56 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2552/3393 [20:24<06:57,  2.02batch/s, Batch Loss=0.0335, Avg Loss=0.2594, Time Left=7.56 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2552/3393 [20:24<06:57,  2.02batch/s, Batch Loss=0.1627, Avg Loss=0.2594, Time Left=7.55 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2553/3393 [20:24<06:50,  2.05batch/s, Batch Loss=0.1627, Avg Loss=0.2594, Time Left=7.55 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2553/3393 [20:25<06:50,  2.05batch/s, Batch Loss=0.1468, Avg Loss=0.2593, Time Left=7.54 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2554/3393 [20:25<06:52,  2.04batch/s, Batch Loss=0.1468, Avg Loss=0.2593, Time Left=7.54 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2554/3393 [20:25<06:52,  2.04batch/s, Batch Loss=0.1263, Avg Loss=0.2593, Time Left=7.53 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2555/3393 [20:25<06:45,  2.07batch/s, Batch Loss=0.1263, Avg Loss=0.2593, Time Left=7.53 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2555/3393 [20:26<06:45,  2.07batch/s, Batch Loss=0.0575, Avg Loss=0.2592, Time Left=7.52 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2556/3393 [20:26<06:41,  2.09batch/s, Batch Loss=0.0575, Avg Loss=0.2592, Time Left=7.52 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2556/3393 [20:26<06:41,  2.09batch/s, Batch Loss=0.1147, Avg Loss=0.2592, Time Left=7.52 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2557/3393 [20:26<06:41,  2.08batch/s, Batch Loss=0.1147, Avg Loss=0.2592, Time Left=7.52 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2557/3393 [20:27<06:41,  2.08batch/s, Batch Loss=0.0717, Avg Loss=0.2591, Time Left=7.51 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2558/3393 [20:27<06:45,  2.06batch/s, Batch Loss=0.0717, Avg Loss=0.2591, Time Left=7.51 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2558/3393 [20:27<06:45,  2.06batch/s, Batch Loss=0.2104, Avg Loss=0.2591, Time Left=7.50 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2559/3393 [20:27<06:45,  2.06batch/s, Batch Loss=0.2104, Avg Loss=0.2591, Time Left=7.50 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2559/3393 [20:28<06:45,  2.06batch/s, Batch Loss=0.1282, Avg Loss=0.2590, Time Left=7.49 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2560/3393 [20:28<06:51,  2.02batch/s, Batch Loss=0.1282, Avg Loss=0.2590, Time Left=7.49 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2560/3393 [20:28<06:51,  2.02batch/s, Batch Loss=0.0736, Avg Loss=0.2589, Time Left=7.48 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2561/3393 [20:28<06:49,  2.03batch/s, Batch Loss=0.0736, Avg Loss=0.2589, Time Left=7.48 \u001b[A\n",
      "Epoch 1/3 - Training:  75%|▊| 2561/3393 [20:29<06:49,  2.03batch/s, Batch Loss=0.2015, Avg Loss=0.2589, Time Left=7.47 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2562/3393 [20:29<06:58,  1.99batch/s, Batch Loss=0.2015, Avg Loss=0.2589, Time Left=7.47 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2562/3393 [20:29<06:58,  1.99batch/s, Batch Loss=0.1287, Avg Loss=0.2589, Time Left=7.47 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2563/3393 [20:29<06:46,  2.04batch/s, Batch Loss=0.1287, Avg Loss=0.2589, Time Left=7.47 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2563/3393 [20:30<06:46,  2.04batch/s, Batch Loss=0.3596, Avg Loss=0.2589, Time Left=7.46 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2564/3393 [20:30<06:39,  2.07batch/s, Batch Loss=0.3596, Avg Loss=0.2589, Time Left=7.46 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2564/3393 [20:30<06:39,  2.07batch/s, Batch Loss=0.0492, Avg Loss=0.2588, Time Left=7.45 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2565/3393 [20:30<06:50,  2.01batch/s, Batch Loss=0.0492, Avg Loss=0.2588, Time Left=7.45 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2565/3393 [20:31<06:50,  2.01batch/s, Batch Loss=0.3439, Avg Loss=0.2588, Time Left=7.44 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2566/3393 [20:31<06:48,  2.03batch/s, Batch Loss=0.3439, Avg Loss=0.2588, Time Left=7.44 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2566/3393 [20:31<06:48,  2.03batch/s, Batch Loss=0.4047, Avg Loss=0.2589, Time Left=7.43 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2567/3393 [20:31<06:56,  1.98batch/s, Batch Loss=0.4047, Avg Loss=0.2589, Time Left=7.43 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2567/3393 [20:32<06:56,  1.98batch/s, Batch Loss=0.1259, Avg Loss=0.2589, Time Left=7.43 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2568/3393 [20:32<06:52,  2.00batch/s, Batch Loss=0.1259, Avg Loss=0.2589, Time Left=7.43 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2568/3393 [20:32<06:52,  2.00batch/s, Batch Loss=0.1480, Avg Loss=0.2588, Time Left=7.42 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2569/3393 [20:32<06:47,  2.02batch/s, Batch Loss=0.1480, Avg Loss=0.2588, Time Left=7.42 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2569/3393 [20:33<06:47,  2.02batch/s, Batch Loss=0.1863, Avg Loss=0.2588, Time Left=7.41 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2570/3393 [20:33<06:40,  2.06batch/s, Batch Loss=0.1863, Avg Loss=0.2588, Time Left=7.41 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2570/3393 [20:33<06:40,  2.06batch/s, Batch Loss=0.2183, Avg Loss=0.2588, Time Left=7.40 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2571/3393 [20:33<06:39,  2.06batch/s, Batch Loss=0.2183, Avg Loss=0.2588, Time Left=7.40 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2571/3393 [20:34<06:39,  2.06batch/s, Batch Loss=0.1570, Avg Loss=0.2587, Time Left=7.39 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  76%|▊| 2572/3393 [20:34<06:50,  2.00batch/s, Batch Loss=0.1570, Avg Loss=0.2587, Time Left=7.39 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2572/3393 [20:34<06:50,  2.00batch/s, Batch Loss=0.2557, Avg Loss=0.2587, Time Left=7.38 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2573/3393 [20:34<06:45,  2.02batch/s, Batch Loss=0.2557, Avg Loss=0.2587, Time Left=7.38 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2573/3393 [20:35<06:45,  2.02batch/s, Batch Loss=0.2678, Avg Loss=0.2587, Time Left=7.38 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2574/3393 [20:35<06:43,  2.03batch/s, Batch Loss=0.2678, Avg Loss=0.2587, Time Left=7.38 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2574/3393 [20:35<06:43,  2.03batch/s, Batch Loss=0.2613, Avg Loss=0.2587, Time Left=7.37 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2575/3393 [20:35<06:40,  2.04batch/s, Batch Loss=0.2613, Avg Loss=0.2587, Time Left=7.37 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2575/3393 [20:36<06:40,  2.04batch/s, Batch Loss=0.0487, Avg Loss=0.2586, Time Left=7.36 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2576/3393 [20:36<06:38,  2.05batch/s, Batch Loss=0.0487, Avg Loss=0.2586, Time Left=7.36 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2576/3393 [20:36<06:38,  2.05batch/s, Batch Loss=0.0230, Avg Loss=0.2585, Time Left=7.35 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2577/3393 [20:36<06:44,  2.02batch/s, Batch Loss=0.0230, Avg Loss=0.2585, Time Left=7.35 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2577/3393 [20:37<06:44,  2.02batch/s, Batch Loss=0.1670, Avg Loss=0.2585, Time Left=7.34 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2578/3393 [20:37<06:41,  2.03batch/s, Batch Loss=0.1670, Avg Loss=0.2585, Time Left=7.34 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2578/3393 [20:37<06:41,  2.03batch/s, Batch Loss=0.1714, Avg Loss=0.2585, Time Left=7.33 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2579/3393 [20:37<06:38,  2.04batch/s, Batch Loss=0.1714, Avg Loss=0.2585, Time Left=7.33 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2579/3393 [20:38<06:38,  2.04batch/s, Batch Loss=0.1087, Avg Loss=0.2584, Time Left=7.33 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2580/3393 [20:38<06:40,  2.03batch/s, Batch Loss=0.1087, Avg Loss=0.2584, Time Left=7.33 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2580/3393 [20:38<06:40,  2.03batch/s, Batch Loss=0.2803, Avg Loss=0.2584, Time Left=7.32 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2581/3393 [20:38<06:38,  2.04batch/s, Batch Loss=0.2803, Avg Loss=0.2584, Time Left=7.32 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2581/3393 [20:39<06:38,  2.04batch/s, Batch Loss=0.0884, Avg Loss=0.2584, Time Left=7.31 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2582/3393 [20:39<06:43,  2.01batch/s, Batch Loss=0.0884, Avg Loss=0.2584, Time Left=7.31 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2582/3393 [20:39<06:43,  2.01batch/s, Batch Loss=0.2514, Avg Loss=0.2584, Time Left=7.30 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2583/3393 [20:39<06:44,  2.00batch/s, Batch Loss=0.2514, Avg Loss=0.2584, Time Left=7.30 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2583/3393 [20:40<06:44,  2.00batch/s, Batch Loss=0.2406, Avg Loss=0.2583, Time Left=7.29 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2584/3393 [20:40<06:48,  1.98batch/s, Batch Loss=0.2406, Avg Loss=0.2583, Time Left=7.29 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2584/3393 [20:40<06:48,  1.98batch/s, Batch Loss=0.0656, Avg Loss=0.2583, Time Left=7.28 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2585/3393 [20:40<06:42,  2.01batch/s, Batch Loss=0.0656, Avg Loss=0.2583, Time Left=7.28 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2585/3393 [20:41<06:42,  2.01batch/s, Batch Loss=0.0799, Avg Loss=0.2582, Time Left=7.28 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2586/3393 [20:41<06:38,  2.02batch/s, Batch Loss=0.0799, Avg Loss=0.2582, Time Left=7.28 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2586/3393 [20:41<06:38,  2.02batch/s, Batch Loss=0.2516, Avg Loss=0.2582, Time Left=7.27 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2587/3393 [20:41<06:35,  2.04batch/s, Batch Loss=0.2516, Avg Loss=0.2582, Time Left=7.27 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2587/3393 [20:42<06:35,  2.04batch/s, Batch Loss=0.0906, Avg Loss=0.2581, Time Left=7.26 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2588/3393 [20:42<06:39,  2.02batch/s, Batch Loss=0.0906, Avg Loss=0.2581, Time Left=7.26 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2588/3393 [20:42<06:39,  2.02batch/s, Batch Loss=0.5138, Avg Loss=0.2582, Time Left=7.25 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2589/3393 [20:42<06:40,  2.01batch/s, Batch Loss=0.5138, Avg Loss=0.2582, Time Left=7.25 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2589/3393 [20:42<06:40,  2.01batch/s, Batch Loss=0.0811, Avg Loss=0.2582, Time Left=7.24 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2590/3393 [20:42<06:26,  2.08batch/s, Batch Loss=0.0811, Avg Loss=0.2582, Time Left=7.24 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2590/3393 [20:43<06:26,  2.08batch/s, Batch Loss=0.2877, Avg Loss=0.2582, Time Left=7.24 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2591/3393 [20:43<06:30,  2.05batch/s, Batch Loss=0.2877, Avg Loss=0.2582, Time Left=7.24 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2591/3393 [20:44<06:30,  2.05batch/s, Batch Loss=0.0283, Avg Loss=0.2581, Time Left=7.23 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2592/3393 [20:44<06:39,  2.00batch/s, Batch Loss=0.0283, Avg Loss=0.2581, Time Left=7.23 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2592/3393 [20:44<06:39,  2.00batch/s, Batch Loss=0.0732, Avg Loss=0.2580, Time Left=7.22 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2593/3393 [20:44<06:32,  2.04batch/s, Batch Loss=0.0732, Avg Loss=0.2580, Time Left=7.22 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2593/3393 [20:45<06:32,  2.04batch/s, Batch Loss=0.0253, Avg Loss=0.2579, Time Left=7.21 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2594/3393 [20:45<06:37,  2.01batch/s, Batch Loss=0.0253, Avg Loss=0.2579, Time Left=7.21 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2594/3393 [20:45<06:37,  2.01batch/s, Batch Loss=0.0705, Avg Loss=0.2578, Time Left=7.20 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2595/3393 [20:45<06:34,  2.02batch/s, Batch Loss=0.0705, Avg Loss=0.2578, Time Left=7.20 \u001b[A\n",
      "Epoch 1/3 - Training:  76%|▊| 2595/3393 [20:46<06:34,  2.02batch/s, Batch Loss=0.0311, Avg Loss=0.2577, Time Left=7.19 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2596/3393 [20:46<06:42,  1.98batch/s, Batch Loss=0.0311, Avg Loss=0.2577, Time Left=7.19 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2596/3393 [20:46<06:42,  1.98batch/s, Batch Loss=0.1112, Avg Loss=0.2577, Time Left=7.19 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2597/3393 [20:46<06:30,  2.04batch/s, Batch Loss=0.1112, Avg Loss=0.2577, Time Left=7.19 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2597/3393 [20:46<06:30,  2.04batch/s, Batch Loss=0.1798, Avg Loss=0.2577, Time Left=7.18 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2598/3393 [20:46<06:28,  2.05batch/s, Batch Loss=0.1798, Avg Loss=0.2577, Time Left=7.18 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2598/3393 [20:47<06:28,  2.05batch/s, Batch Loss=0.1787, Avg Loss=0.2576, Time Left=7.17 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2599/3393 [20:47<06:29,  2.04batch/s, Batch Loss=0.1787, Avg Loss=0.2576, Time Left=7.17 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2599/3393 [20:47<06:29,  2.04batch/s, Batch Loss=0.5778, Avg Loss=0.2578, Time Left=7.16 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2600/3393 [20:47<06:20,  2.08batch/s, Batch Loss=0.5778, Avg Loss=0.2578, Time Left=7.16 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2600/3393 [20:48<06:20,  2.08batch/s, Batch Loss=0.1812, Avg Loss=0.2577, Time Left=7.15 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2601/3393 [20:48<06:16,  2.10batch/s, Batch Loss=0.1812, Avg Loss=0.2577, Time Left=7.15 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2601/3393 [20:48<06:16,  2.10batch/s, Batch Loss=0.0305, Avg Loss=0.2576, Time Left=7.14 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2602/3393 [20:48<06:29,  2.03batch/s, Batch Loss=0.0305, Avg Loss=0.2576, Time Left=7.14 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2602/3393 [20:49<06:29,  2.03batch/s, Batch Loss=0.0495, Avg Loss=0.2576, Time Left=7.14 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2603/3393 [20:49<06:23,  2.06batch/s, Batch Loss=0.0495, Avg Loss=0.2576, Time Left=7.14 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2603/3393 [20:49<06:23,  2.06batch/s, Batch Loss=0.2931, Avg Loss=0.2576, Time Left=7.13 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2604/3393 [20:49<06:22,  2.06batch/s, Batch Loss=0.2931, Avg Loss=0.2576, Time Left=7.13 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2604/3393 [20:50<06:22,  2.06batch/s, Batch Loss=0.1081, Avg Loss=0.2575, Time Left=7.12 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  77%|▊| 2605/3393 [20:50<06:21,  2.06batch/s, Batch Loss=0.1081, Avg Loss=0.2575, Time Left=7.12 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2605/3393 [20:50<06:21,  2.06batch/s, Batch Loss=0.1395, Avg Loss=0.2575, Time Left=7.11 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2606/3393 [20:50<06:21,  2.06batch/s, Batch Loss=0.1395, Avg Loss=0.2575, Time Left=7.11 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2606/3393 [20:51<06:21,  2.06batch/s, Batch Loss=0.4027, Avg Loss=0.2575, Time Left=7.10 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2607/3393 [20:51<06:31,  2.01batch/s, Batch Loss=0.4027, Avg Loss=0.2575, Time Left=7.10 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2607/3393 [20:51<06:31,  2.01batch/s, Batch Loss=0.2362, Avg Loss=0.2575, Time Left=7.09 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2608/3393 [20:51<06:28,  2.02batch/s, Batch Loss=0.2362, Avg Loss=0.2575, Time Left=7.09 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2608/3393 [20:52<06:28,  2.02batch/s, Batch Loss=0.0777, Avg Loss=0.2574, Time Left=7.09 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2609/3393 [20:52<06:21,  2.06batch/s, Batch Loss=0.0777, Avg Loss=0.2574, Time Left=7.09 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2609/3393 [20:52<06:21,  2.06batch/s, Batch Loss=0.0740, Avg Loss=0.2574, Time Left=7.08 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2610/3393 [20:52<06:24,  2.04batch/s, Batch Loss=0.0740, Avg Loss=0.2574, Time Left=7.08 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2610/3393 [20:53<06:24,  2.04batch/s, Batch Loss=0.1945, Avg Loss=0.2573, Time Left=7.07 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2611/3393 [20:53<06:22,  2.05batch/s, Batch Loss=0.1945, Avg Loss=0.2573, Time Left=7.07 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2611/3393 [20:53<06:22,  2.05batch/s, Batch Loss=0.0522, Avg Loss=0.2573, Time Left=7.06 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2612/3393 [20:53<06:27,  2.01batch/s, Batch Loss=0.0522, Avg Loss=0.2573, Time Left=7.06 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2612/3393 [20:54<06:27,  2.01batch/s, Batch Loss=0.1306, Avg Loss=0.2572, Time Left=7.05 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2613/3393 [20:54<06:28,  2.01batch/s, Batch Loss=0.1306, Avg Loss=0.2572, Time Left=7.05 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2613/3393 [20:54<06:28,  2.01batch/s, Batch Loss=0.0791, Avg Loss=0.2571, Time Left=7.05 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2614/3393 [20:54<06:32,  1.99batch/s, Batch Loss=0.0791, Avg Loss=0.2571, Time Left=7.05 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2614/3393 [20:55<06:32,  1.99batch/s, Batch Loss=0.3155, Avg Loss=0.2572, Time Left=7.04 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2615/3393 [20:55<06:31,  1.99batch/s, Batch Loss=0.3155, Avg Loss=0.2572, Time Left=7.04 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2615/3393 [20:55<06:31,  1.99batch/s, Batch Loss=0.1456, Avg Loss=0.2571, Time Left=7.03 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2616/3393 [20:55<06:33,  1.97batch/s, Batch Loss=0.1456, Avg Loss=0.2571, Time Left=7.03 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2616/3393 [20:56<06:33,  1.97batch/s, Batch Loss=0.1408, Avg Loss=0.2571, Time Left=7.02 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2617/3393 [20:56<06:20,  2.04batch/s, Batch Loss=0.1408, Avg Loss=0.2571, Time Left=7.02 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2617/3393 [20:56<06:20,  2.04batch/s, Batch Loss=0.2572, Avg Loss=0.2571, Time Left=7.01 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2618/3393 [20:56<06:18,  2.05batch/s, Batch Loss=0.2572, Avg Loss=0.2571, Time Left=7.01 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2618/3393 [20:57<06:18,  2.05batch/s, Batch Loss=0.1120, Avg Loss=0.2570, Time Left=7.00 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2619/3393 [20:57<06:27,  2.00batch/s, Batch Loss=0.1120, Avg Loss=0.2570, Time Left=7.00 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2619/3393 [20:57<06:27,  2.00batch/s, Batch Loss=0.0557, Avg Loss=0.2569, Time Left=7.00 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2620/3393 [20:57<06:23,  2.01batch/s, Batch Loss=0.0557, Avg Loss=0.2569, Time Left=7.00 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2620/3393 [20:58<06:23,  2.01batch/s, Batch Loss=0.0649, Avg Loss=0.2569, Time Left=6.99 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2621/3393 [20:58<06:27,  1.99batch/s, Batch Loss=0.0649, Avg Loss=0.2569, Time Left=6.99 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2621/3393 [20:58<06:27,  1.99batch/s, Batch Loss=0.0950, Avg Loss=0.2568, Time Left=6.98 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2622/3393 [20:58<06:16,  2.05batch/s, Batch Loss=0.0950, Avg Loss=0.2568, Time Left=6.98 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2622/3393 [20:59<06:16,  2.05batch/s, Batch Loss=0.0456, Avg Loss=0.2567, Time Left=6.97 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2623/3393 [20:59<06:14,  2.06batch/s, Batch Loss=0.0456, Avg Loss=0.2567, Time Left=6.97 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2623/3393 [20:59<06:14,  2.06batch/s, Batch Loss=0.0880, Avg Loss=0.2567, Time Left=6.96 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2624/3393 [20:59<06:13,  2.06batch/s, Batch Loss=0.0880, Avg Loss=0.2567, Time Left=6.96 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2624/3393 [21:00<06:13,  2.06batch/s, Batch Loss=0.1849, Avg Loss=0.2566, Time Left=6.95 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2625/3393 [21:00<06:12,  2.06batch/s, Batch Loss=0.1849, Avg Loss=0.2566, Time Left=6.95 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2625/3393 [21:00<06:12,  2.06batch/s, Batch Loss=0.0078, Avg Loss=0.2565, Time Left=6.95 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2626/3393 [21:00<06:11,  2.07batch/s, Batch Loss=0.0078, Avg Loss=0.2565, Time Left=6.95 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2626/3393 [21:01<06:11,  2.07batch/s, Batch Loss=0.0979, Avg Loss=0.2565, Time Left=6.94 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2627/3393 [21:01<06:13,  2.05batch/s, Batch Loss=0.0979, Avg Loss=0.2565, Time Left=6.94 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2627/3393 [21:01<06:13,  2.05batch/s, Batch Loss=0.2308, Avg Loss=0.2565, Time Left=6.93 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2628/3393 [21:01<06:09,  2.07batch/s, Batch Loss=0.2308, Avg Loss=0.2565, Time Left=6.93 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2628/3393 [21:02<06:09,  2.07batch/s, Batch Loss=0.0470, Avg Loss=0.2564, Time Left=6.92 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2629/3393 [21:02<06:20,  2.01batch/s, Batch Loss=0.0470, Avg Loss=0.2564, Time Left=6.92 \u001b[A\n",
      "Epoch 1/3 - Training:  77%|▊| 2629/3393 [21:02<06:20,  2.01batch/s, Batch Loss=0.0086, Avg Loss=0.2563, Time Left=6.91 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2630/3393 [21:02<06:20,  2.00batch/s, Batch Loss=0.0086, Avg Loss=0.2563, Time Left=6.91 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2630/3393 [21:03<06:20,  2.00batch/s, Batch Loss=0.0386, Avg Loss=0.2562, Time Left=6.90 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2631/3393 [21:03<06:13,  2.04batch/s, Batch Loss=0.0386, Avg Loss=0.2562, Time Left=6.90 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2631/3393 [21:03<06:13,  2.04batch/s, Batch Loss=0.0347, Avg Loss=0.2561, Time Left=6.90 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2632/3393 [21:03<06:11,  2.05batch/s, Batch Loss=0.0347, Avg Loss=0.2561, Time Left=6.90 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2632/3393 [21:04<06:11,  2.05batch/s, Batch Loss=0.0401, Avg Loss=0.2560, Time Left=6.89 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2633/3393 [21:04<06:10,  2.05batch/s, Batch Loss=0.0401, Avg Loss=0.2560, Time Left=6.89 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2633/3393 [21:04<06:10,  2.05batch/s, Batch Loss=0.1178, Avg Loss=0.2560, Time Left=6.88 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2634/3393 [21:04<06:15,  2.02batch/s, Batch Loss=0.1178, Avg Loss=0.2560, Time Left=6.88 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2634/3393 [21:05<06:15,  2.02batch/s, Batch Loss=0.5694, Avg Loss=0.2561, Time Left=6.87 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2635/3393 [21:05<06:13,  2.03batch/s, Batch Loss=0.5694, Avg Loss=0.2561, Time Left=6.87 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2635/3393 [21:05<06:13,  2.03batch/s, Batch Loss=0.2085, Avg Loss=0.2561, Time Left=6.86 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2636/3393 [21:05<06:07,  2.06batch/s, Batch Loss=0.2085, Avg Loss=0.2561, Time Left=6.86 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2636/3393 [21:06<06:07,  2.06batch/s, Batch Loss=0.1247, Avg Loss=0.2560, Time Left=6.86 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2637/3393 [21:06<06:13,  2.03batch/s, Batch Loss=0.1247, Avg Loss=0.2560, Time Left=6.86 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2637/3393 [21:06<06:13,  2.03batch/s, Batch Loss=0.3153, Avg Loss=0.2560, Time Left=6.85 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  78%|▊| 2638/3393 [21:06<06:11,  2.03batch/s, Batch Loss=0.3153, Avg Loss=0.2560, Time Left=6.85 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2638/3393 [21:07<06:11,  2.03batch/s, Batch Loss=0.3295, Avg Loss=0.2561, Time Left=6.84 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2639/3393 [21:07<06:12,  2.03batch/s, Batch Loss=0.3295, Avg Loss=0.2561, Time Left=6.84 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2639/3393 [21:07<06:12,  2.03batch/s, Batch Loss=0.0651, Avg Loss=0.2560, Time Left=6.83 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2640/3393 [21:07<06:13,  2.02batch/s, Batch Loss=0.0651, Avg Loss=0.2560, Time Left=6.83 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2640/3393 [21:08<06:13,  2.02batch/s, Batch Loss=0.2042, Avg Loss=0.2560, Time Left=6.82 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2641/3393 [21:08<06:06,  2.05batch/s, Batch Loss=0.2042, Avg Loss=0.2560, Time Left=6.82 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2641/3393 [21:08<06:06,  2.05batch/s, Batch Loss=0.0303, Avg Loss=0.2559, Time Left=6.81 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2642/3393 [21:08<06:08,  2.04batch/s, Batch Loss=0.0303, Avg Loss=0.2559, Time Left=6.81 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2642/3393 [21:09<06:08,  2.04batch/s, Batch Loss=0.1708, Avg Loss=0.2559, Time Left=6.81 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2643/3393 [21:09<06:06,  2.04batch/s, Batch Loss=0.1708, Avg Loss=0.2559, Time Left=6.81 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2643/3393 [21:09<06:06,  2.04batch/s, Batch Loss=0.0273, Avg Loss=0.2558, Time Left=6.80 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2644/3393 [21:09<06:12,  2.01batch/s, Batch Loss=0.0273, Avg Loss=0.2558, Time Left=6.80 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2644/3393 [21:10<06:12,  2.01batch/s, Batch Loss=0.1411, Avg Loss=0.2557, Time Left=6.79 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2645/3393 [21:10<06:09,  2.03batch/s, Batch Loss=0.1411, Avg Loss=0.2557, Time Left=6.79 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2645/3393 [21:10<06:09,  2.03batch/s, Batch Loss=0.1704, Avg Loss=0.2557, Time Left=6.78 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2646/3393 [21:10<06:06,  2.04batch/s, Batch Loss=0.1704, Avg Loss=0.2557, Time Left=6.78 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2646/3393 [21:11<06:06,  2.04batch/s, Batch Loss=0.3073, Avg Loss=0.2557, Time Left=6.77 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2647/3393 [21:11<06:04,  2.04batch/s, Batch Loss=0.3073, Avg Loss=0.2557, Time Left=6.77 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2647/3393 [21:11<06:04,  2.04batch/s, Batch Loss=0.1839, Avg Loss=0.2557, Time Left=6.76 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2648/3393 [21:11<05:58,  2.08batch/s, Batch Loss=0.1839, Avg Loss=0.2557, Time Left=6.76 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2648/3393 [21:11<05:58,  2.08batch/s, Batch Loss=0.0243, Avg Loss=0.2556, Time Left=6.76 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2649/3393 [21:11<06:02,  2.05batch/s, Batch Loss=0.0243, Avg Loss=0.2556, Time Left=6.76 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2649/3393 [21:12<06:02,  2.05batch/s, Batch Loss=0.0757, Avg Loss=0.2555, Time Left=6.75 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2650/3393 [21:12<06:00,  2.06batch/s, Batch Loss=0.0757, Avg Loss=0.2555, Time Left=6.75 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2650/3393 [21:12<06:00,  2.06batch/s, Batch Loss=0.0529, Avg Loss=0.2554, Time Left=6.74 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2651/3393 [21:12<05:53,  2.10batch/s, Batch Loss=0.0529, Avg Loss=0.2554, Time Left=6.74 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2651/3393 [21:13<05:53,  2.10batch/s, Batch Loss=0.2250, Avg Loss=0.2554, Time Left=6.73 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2652/3393 [21:13<05:57,  2.07batch/s, Batch Loss=0.2250, Avg Loss=0.2554, Time Left=6.73 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2652/3393 [21:13<05:57,  2.07batch/s, Batch Loss=0.2424, Avg Loss=0.2554, Time Left=6.72 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2653/3393 [21:13<05:54,  2.09batch/s, Batch Loss=0.2424, Avg Loss=0.2554, Time Left=6.72 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2653/3393 [21:14<05:54,  2.09batch/s, Batch Loss=0.3196, Avg Loss=0.2554, Time Left=6.71 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2654/3393 [21:14<05:47,  2.13batch/s, Batch Loss=0.3196, Avg Loss=0.2554, Time Left=6.71 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2654/3393 [21:14<05:47,  2.13batch/s, Batch Loss=0.0987, Avg Loss=0.2554, Time Left=6.71 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2655/3393 [21:14<05:49,  2.11batch/s, Batch Loss=0.0987, Avg Loss=0.2554, Time Left=6.71 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2655/3393 [21:15<05:49,  2.11batch/s, Batch Loss=0.4192, Avg Loss=0.2554, Time Left=6.70 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2656/3393 [21:15<05:58,  2.06batch/s, Batch Loss=0.4192, Avg Loss=0.2554, Time Left=6.70 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2656/3393 [21:15<05:58,  2.06batch/s, Batch Loss=0.2006, Avg Loss=0.2554, Time Left=6.69 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2657/3393 [21:15<05:57,  2.06batch/s, Batch Loss=0.2006, Avg Loss=0.2554, Time Left=6.69 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2657/3393 [21:16<05:57,  2.06batch/s, Batch Loss=0.2782, Avg Loss=0.2554, Time Left=6.68 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2658/3393 [21:16<06:03,  2.02batch/s, Batch Loss=0.2782, Avg Loss=0.2554, Time Left=6.68 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2658/3393 [21:16<06:03,  2.02batch/s, Batch Loss=0.0879, Avg Loss=0.2554, Time Left=6.67 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2659/3393 [21:16<06:00,  2.03batch/s, Batch Loss=0.0879, Avg Loss=0.2554, Time Left=6.67 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2659/3393 [21:17<06:00,  2.03batch/s, Batch Loss=0.2374, Avg Loss=0.2554, Time Left=6.66 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2660/3393 [21:17<05:55,  2.06batch/s, Batch Loss=0.2374, Avg Loss=0.2554, Time Left=6.66 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2660/3393 [21:17<05:55,  2.06batch/s, Batch Loss=0.1371, Avg Loss=0.2553, Time Left=6.66 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2661/3393 [21:17<05:54,  2.06batch/s, Batch Loss=0.1371, Avg Loss=0.2553, Time Left=6.66 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2661/3393 [21:18<05:54,  2.06batch/s, Batch Loss=0.1072, Avg Loss=0.2553, Time Left=6.65 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2662/3393 [21:18<05:50,  2.09batch/s, Batch Loss=0.1072, Avg Loss=0.2553, Time Left=6.65 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2662/3393 [21:18<05:50,  2.09batch/s, Batch Loss=0.0562, Avg Loss=0.2552, Time Left=6.64 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2663/3393 [21:18<05:50,  2.08batch/s, Batch Loss=0.0562, Avg Loss=0.2552, Time Left=6.64 \u001b[A\n",
      "Epoch 1/3 - Training:  78%|▊| 2663/3393 [21:19<05:50,  2.08batch/s, Batch Loss=0.1245, Avg Loss=0.2551, Time Left=6.63 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2664/3393 [21:19<05:50,  2.08batch/s, Batch Loss=0.1245, Avg Loss=0.2551, Time Left=6.63 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2664/3393 [21:19<05:50,  2.08batch/s, Batch Loss=0.2618, Avg Loss=0.2551, Time Left=6.62 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2665/3393 [21:19<05:54,  2.05batch/s, Batch Loss=0.2618, Avg Loss=0.2551, Time Left=6.62 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2665/3393 [21:20<05:54,  2.05batch/s, Batch Loss=0.3624, Avg Loss=0.2552, Time Left=6.62 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2666/3393 [21:20<06:00,  2.02batch/s, Batch Loss=0.3624, Avg Loss=0.2552, Time Left=6.62 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2666/3393 [21:20<06:00,  2.02batch/s, Batch Loss=0.1814, Avg Loss=0.2551, Time Left=6.61 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2667/3393 [21:20<05:57,  2.03batch/s, Batch Loss=0.1814, Avg Loss=0.2551, Time Left=6.61 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2667/3393 [21:21<05:57,  2.03batch/s, Batch Loss=0.0561, Avg Loss=0.2551, Time Left=6.60 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2668/3393 [21:21<05:55,  2.04batch/s, Batch Loss=0.0561, Avg Loss=0.2551, Time Left=6.60 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2668/3393 [21:21<05:55,  2.04batch/s, Batch Loss=0.1493, Avg Loss=0.2550, Time Left=6.59 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2669/3393 [21:21<05:52,  2.06batch/s, Batch Loss=0.1493, Avg Loss=0.2550, Time Left=6.59 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2669/3393 [21:22<05:52,  2.06batch/s, Batch Loss=0.2487, Avg Loss=0.2550, Time Left=6.58 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2670/3393 [21:22<05:45,  2.09batch/s, Batch Loss=0.2487, Avg Loss=0.2550, Time Left=6.58 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2670/3393 [21:22<05:45,  2.09batch/s, Batch Loss=0.0355, Avg Loss=0.2549, Time Left=6.57 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  79%|▊| 2671/3393 [21:22<05:49,  2.07batch/s, Batch Loss=0.0355, Avg Loss=0.2549, Time Left=6.57 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2671/3393 [21:23<05:49,  2.07batch/s, Batch Loss=0.1199, Avg Loss=0.2549, Time Left=6.57 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2672/3393 [21:23<05:49,  2.06batch/s, Batch Loss=0.1199, Avg Loss=0.2549, Time Left=6.57 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2672/3393 [21:23<05:49,  2.06batch/s, Batch Loss=0.1624, Avg Loss=0.2549, Time Left=6.56 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2673/3393 [21:23<05:48,  2.07batch/s, Batch Loss=0.1624, Avg Loss=0.2549, Time Left=6.56 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2673/3393 [21:24<05:48,  2.07batch/s, Batch Loss=0.0445, Avg Loss=0.2548, Time Left=6.55 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2674/3393 [21:24<06:00,  1.99batch/s, Batch Loss=0.0445, Avg Loss=0.2548, Time Left=6.55 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2674/3393 [21:24<06:00,  1.99batch/s, Batch Loss=0.0647, Avg Loss=0.2547, Time Left=6.54 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2675/3393 [21:24<05:54,  2.03batch/s, Batch Loss=0.0647, Avg Loss=0.2547, Time Left=6.54 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2675/3393 [21:25<05:54,  2.03batch/s, Batch Loss=0.0958, Avg Loss=0.2546, Time Left=6.53 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2676/3393 [21:25<06:01,  1.98batch/s, Batch Loss=0.0958, Avg Loss=0.2546, Time Left=6.53 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2676/3393 [21:25<06:01,  1.98batch/s, Batch Loss=0.1127, Avg Loss=0.2546, Time Left=6.52 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2677/3393 [21:25<05:57,  2.00batch/s, Batch Loss=0.1127, Avg Loss=0.2546, Time Left=6.52 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2677/3393 [21:26<05:57,  2.00batch/s, Batch Loss=0.1567, Avg Loss=0.2546, Time Left=6.52 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2678/3393 [21:26<05:50,  2.04batch/s, Batch Loss=0.1567, Avg Loss=0.2546, Time Left=6.52 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2678/3393 [21:26<05:50,  2.04batch/s, Batch Loss=0.0562, Avg Loss=0.2545, Time Left=6.51 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2679/3393 [21:26<05:48,  2.05batch/s, Batch Loss=0.0562, Avg Loss=0.2545, Time Left=6.51 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2679/3393 [21:27<05:48,  2.05batch/s, Batch Loss=0.0401, Avg Loss=0.2544, Time Left=6.50 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2680/3393 [21:27<05:47,  2.05batch/s, Batch Loss=0.0401, Avg Loss=0.2544, Time Left=6.50 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2680/3393 [21:27<05:47,  2.05batch/s, Batch Loss=0.3038, Avg Loss=0.2544, Time Left=6.49 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2681/3393 [21:27<05:55,  2.00batch/s, Batch Loss=0.3038, Avg Loss=0.2544, Time Left=6.49 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2681/3393 [21:28<05:55,  2.00batch/s, Batch Loss=0.3929, Avg Loss=0.2545, Time Left=6.48 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2682/3393 [21:28<05:53,  2.01batch/s, Batch Loss=0.3929, Avg Loss=0.2545, Time Left=6.48 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2682/3393 [21:28<05:53,  2.01batch/s, Batch Loss=0.2891, Avg Loss=0.2545, Time Left=6.47 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2683/3393 [21:28<05:49,  2.03batch/s, Batch Loss=0.2891, Avg Loss=0.2545, Time Left=6.47 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2683/3393 [21:29<05:49,  2.03batch/s, Batch Loss=0.2278, Avg Loss=0.2545, Time Left=6.47 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2684/3393 [21:29<05:43,  2.06batch/s, Batch Loss=0.2278, Avg Loss=0.2545, Time Left=6.47 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2684/3393 [21:29<05:43,  2.06batch/s, Batch Loss=0.4931, Avg Loss=0.2546, Time Left=6.46 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2685/3393 [21:29<05:39,  2.08batch/s, Batch Loss=0.4931, Avg Loss=0.2546, Time Left=6.46 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2685/3393 [21:29<05:39,  2.08batch/s, Batch Loss=0.3885, Avg Loss=0.2546, Time Left=6.45 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2686/3393 [21:29<05:39,  2.08batch/s, Batch Loss=0.3885, Avg Loss=0.2546, Time Left=6.45 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2686/3393 [21:30<05:39,  2.08batch/s, Batch Loss=0.0268, Avg Loss=0.2545, Time Left=6.44 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2687/3393 [21:30<05:43,  2.06batch/s, Batch Loss=0.0268, Avg Loss=0.2545, Time Left=6.44 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2687/3393 [21:30<05:43,  2.06batch/s, Batch Loss=0.0550, Avg Loss=0.2544, Time Left=6.43 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2688/3393 [21:30<05:42,  2.06batch/s, Batch Loss=0.0550, Avg Loss=0.2544, Time Left=6.43 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2688/3393 [21:31<05:42,  2.06batch/s, Batch Loss=0.2574, Avg Loss=0.2544, Time Left=6.43 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2689/3393 [21:31<05:48,  2.02batch/s, Batch Loss=0.2574, Avg Loss=0.2544, Time Left=6.43 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2689/3393 [21:31<05:48,  2.02batch/s, Batch Loss=0.2298, Avg Loss=0.2544, Time Left=6.42 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2690/3393 [21:31<05:45,  2.03batch/s, Batch Loss=0.2298, Avg Loss=0.2544, Time Left=6.42 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2690/3393 [21:32<05:45,  2.03batch/s, Batch Loss=0.0542, Avg Loss=0.2544, Time Left=6.41 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2691/3393 [21:32<05:43,  2.04batch/s, Batch Loss=0.0542, Avg Loss=0.2544, Time Left=6.41 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2691/3393 [21:32<05:43,  2.04batch/s, Batch Loss=0.1659, Avg Loss=0.2543, Time Left=6.40 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2692/3393 [21:32<05:38,  2.07batch/s, Batch Loss=0.1659, Avg Loss=0.2543, Time Left=6.40 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2692/3393 [21:33<05:38,  2.07batch/s, Batch Loss=0.1719, Avg Loss=0.2543, Time Left=6.39 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2693/3393 [21:33<05:38,  2.07batch/s, Batch Loss=0.1719, Avg Loss=0.2543, Time Left=6.39 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2693/3393 [21:33<05:38,  2.07batch/s, Batch Loss=0.0731, Avg Loss=0.2542, Time Left=6.38 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2694/3393 [21:33<05:50,  1.99batch/s, Batch Loss=0.0731, Avg Loss=0.2542, Time Left=6.38 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2694/3393 [21:34<05:50,  1.99batch/s, Batch Loss=0.0474, Avg Loss=0.2541, Time Left=6.38 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2695/3393 [21:34<05:47,  2.01batch/s, Batch Loss=0.0474, Avg Loss=0.2541, Time Left=6.38 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2695/3393 [21:34<05:47,  2.01batch/s, Batch Loss=0.1081, Avg Loss=0.2541, Time Left=6.37 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2696/3393 [21:34<05:40,  2.05batch/s, Batch Loss=0.1081, Avg Loss=0.2541, Time Left=6.37 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2696/3393 [21:35<05:40,  2.05batch/s, Batch Loss=0.1748, Avg Loss=0.2541, Time Left=6.36 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2697/3393 [21:35<05:39,  2.05batch/s, Batch Loss=0.1748, Avg Loss=0.2541, Time Left=6.36 \u001b[A\n",
      "Epoch 1/3 - Training:  79%|▊| 2697/3393 [21:35<05:39,  2.05batch/s, Batch Loss=0.2072, Avg Loss=0.2540, Time Left=6.35 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2698/3393 [21:35<05:41,  2.04batch/s, Batch Loss=0.2072, Avg Loss=0.2540, Time Left=6.35 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2698/3393 [21:36<05:41,  2.04batch/s, Batch Loss=0.0784, Avg Loss=0.2540, Time Left=6.34 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2699/3393 [21:36<05:45,  2.01batch/s, Batch Loss=0.0784, Avg Loss=0.2540, Time Left=6.34 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2699/3393 [21:36<05:45,  2.01batch/s, Batch Loss=0.0551, Avg Loss=0.2539, Time Left=6.33 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2700/3393 [21:36<05:42,  2.02batch/s, Batch Loss=0.0551, Avg Loss=0.2539, Time Left=6.33 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2700/3393 [21:37<05:42,  2.02batch/s, Batch Loss=0.0758, Avg Loss=0.2538, Time Left=6.33 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2701/3393 [21:37<05:39,  2.04batch/s, Batch Loss=0.0758, Avg Loss=0.2538, Time Left=6.33 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2701/3393 [21:37<05:39,  2.04batch/s, Batch Loss=0.0480, Avg Loss=0.2538, Time Left=6.32 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2702/3393 [21:37<05:38,  2.04batch/s, Batch Loss=0.0480, Avg Loss=0.2538, Time Left=6.32 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2702/3393 [21:38<05:38,  2.04batch/s, Batch Loss=0.0636, Avg Loss=0.2537, Time Left=6.31 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2703/3393 [21:38<05:29,  2.09batch/s, Batch Loss=0.0636, Avg Loss=0.2537, Time Left=6.31 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2703/3393 [21:38<05:29,  2.09batch/s, Batch Loss=0.3035, Avg Loss=0.2537, Time Left=6.30 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  80%|▊| 2704/3393 [21:38<05:30,  2.09batch/s, Batch Loss=0.3035, Avg Loss=0.2537, Time Left=6.30 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2704/3393 [21:39<05:30,  2.09batch/s, Batch Loss=0.2003, Avg Loss=0.2537, Time Left=6.29 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2705/3393 [21:39<05:36,  2.04batch/s, Batch Loss=0.2003, Avg Loss=0.2537, Time Left=6.29 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2705/3393 [21:39<05:36,  2.04batch/s, Batch Loss=0.0441, Avg Loss=0.2536, Time Left=6.28 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2706/3393 [21:39<05:34,  2.05batch/s, Batch Loss=0.0441, Avg Loss=0.2536, Time Left=6.28 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2706/3393 [21:40<05:34,  2.05batch/s, Batch Loss=0.2670, Avg Loss=0.2536, Time Left=6.28 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2707/3393 [21:40<05:40,  2.02batch/s, Batch Loss=0.2670, Avg Loss=0.2536, Time Left=6.28 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2707/3393 [21:40<05:40,  2.02batch/s, Batch Loss=0.0477, Avg Loss=0.2535, Time Left=6.27 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2708/3393 [21:40<05:35,  2.04batch/s, Batch Loss=0.0477, Avg Loss=0.2535, Time Left=6.27 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2708/3393 [21:41<05:35,  2.04batch/s, Batch Loss=0.0245, Avg Loss=0.2534, Time Left=6.26 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2709/3393 [21:41<05:33,  2.05batch/s, Batch Loss=0.0245, Avg Loss=0.2534, Time Left=6.26 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2709/3393 [21:41<05:33,  2.05batch/s, Batch Loss=0.0069, Avg Loss=0.2533, Time Left=6.25 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2710/3393 [21:41<05:35,  2.04batch/s, Batch Loss=0.0069, Avg Loss=0.2533, Time Left=6.25 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2710/3393 [21:42<05:35,  2.04batch/s, Batch Loss=0.2889, Avg Loss=0.2534, Time Left=6.24 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2711/3393 [21:42<05:33,  2.05batch/s, Batch Loss=0.2889, Avg Loss=0.2534, Time Left=6.24 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2711/3393 [21:42<05:33,  2.05batch/s, Batch Loss=0.0597, Avg Loss=0.2533, Time Left=6.24 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2712/3393 [21:42<05:38,  2.01batch/s, Batch Loss=0.0597, Avg Loss=0.2533, Time Left=6.24 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2712/3393 [21:43<05:38,  2.01batch/s, Batch Loss=0.0200, Avg Loss=0.2532, Time Left=6.23 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2713/3393 [21:43<05:35,  2.03batch/s, Batch Loss=0.0200, Avg Loss=0.2532, Time Left=6.23 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2713/3393 [21:43<05:35,  2.03batch/s, Batch Loss=0.1318, Avg Loss=0.2532, Time Left=6.22 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2714/3393 [21:43<05:36,  2.02batch/s, Batch Loss=0.1318, Avg Loss=0.2532, Time Left=6.22 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2714/3393 [21:44<05:36,  2.02batch/s, Batch Loss=0.0480, Avg Loss=0.2531, Time Left=6.21 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2715/3393 [21:44<05:33,  2.03batch/s, Batch Loss=0.0480, Avg Loss=0.2531, Time Left=6.21 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2715/3393 [21:44<05:33,  2.03batch/s, Batch Loss=0.0381, Avg Loss=0.2530, Time Left=6.20 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2716/3393 [21:44<05:28,  2.06batch/s, Batch Loss=0.0381, Avg Loss=0.2530, Time Left=6.20 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2716/3393 [21:45<05:28,  2.06batch/s, Batch Loss=0.2732, Avg Loss=0.2530, Time Left=6.19 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2717/3393 [21:45<05:34,  2.02batch/s, Batch Loss=0.2732, Avg Loss=0.2530, Time Left=6.19 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2717/3393 [21:45<05:34,  2.02batch/s, Batch Loss=0.3668, Avg Loss=0.2530, Time Left=6.19 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2718/3393 [21:45<05:34,  2.02batch/s, Batch Loss=0.3668, Avg Loss=0.2530, Time Left=6.19 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2718/3393 [21:46<05:34,  2.02batch/s, Batch Loss=0.0618, Avg Loss=0.2530, Time Left=6.18 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2719/3393 [21:46<05:38,  1.99batch/s, Batch Loss=0.0618, Avg Loss=0.2530, Time Left=6.18 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2719/3393 [21:46<05:38,  1.99batch/s, Batch Loss=0.0724, Avg Loss=0.2529, Time Left=6.17 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2720/3393 [21:46<05:34,  2.01batch/s, Batch Loss=0.0724, Avg Loss=0.2529, Time Left=6.17 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2720/3393 [21:47<05:34,  2.01batch/s, Batch Loss=0.2523, Avg Loss=0.2529, Time Left=6.16 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2721/3393 [21:47<05:34,  2.01batch/s, Batch Loss=0.2523, Avg Loss=0.2529, Time Left=6.16 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2721/3393 [21:47<05:34,  2.01batch/s, Batch Loss=0.1476, Avg Loss=0.2529, Time Left=6.15 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2722/3393 [21:47<05:31,  2.02batch/s, Batch Loss=0.1476, Avg Loss=0.2529, Time Left=6.15 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2722/3393 [21:48<05:31,  2.02batch/s, Batch Loss=0.0278, Avg Loss=0.2528, Time Left=6.14 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2723/3393 [21:48<05:29,  2.04batch/s, Batch Loss=0.0278, Avg Loss=0.2528, Time Left=6.14 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2723/3393 [21:48<05:29,  2.04batch/s, Batch Loss=0.3515, Avg Loss=0.2528, Time Left=6.14 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2724/3393 [21:48<05:33,  2.01batch/s, Batch Loss=0.3515, Avg Loss=0.2528, Time Left=6.14 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2724/3393 [21:49<05:33,  2.01batch/s, Batch Loss=0.1462, Avg Loss=0.2528, Time Left=6.13 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2725/3393 [21:49<05:30,  2.02batch/s, Batch Loss=0.1462, Avg Loss=0.2528, Time Left=6.13 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2725/3393 [21:49<05:30,  2.02batch/s, Batch Loss=0.3108, Avg Loss=0.2528, Time Left=6.12 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2726/3393 [21:49<05:37,  1.98batch/s, Batch Loss=0.3108, Avg Loss=0.2528, Time Left=6.12 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2726/3393 [21:50<05:37,  1.98batch/s, Batch Loss=0.0407, Avg Loss=0.2527, Time Left=6.11 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2727/3393 [21:50<05:32,  2.00batch/s, Batch Loss=0.0407, Avg Loss=0.2527, Time Left=6.11 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2727/3393 [21:50<05:32,  2.00batch/s, Batch Loss=0.2500, Avg Loss=0.2527, Time Left=6.10 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2728/3393 [21:50<05:35,  1.98batch/s, Batch Loss=0.2500, Avg Loss=0.2527, Time Left=6.10 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2728/3393 [21:51<05:35,  1.98batch/s, Batch Loss=0.1812, Avg Loss=0.2527, Time Left=6.10 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2729/3393 [21:51<05:27,  2.03batch/s, Batch Loss=0.1812, Avg Loss=0.2527, Time Left=6.10 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2729/3393 [21:51<05:27,  2.03batch/s, Batch Loss=0.0149, Avg Loss=0.2526, Time Left=6.09 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2730/3393 [21:51<05:25,  2.04batch/s, Batch Loss=0.0149, Avg Loss=0.2526, Time Left=6.09 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2730/3393 [21:52<05:25,  2.04batch/s, Batch Loss=0.1652, Avg Loss=0.2526, Time Left=6.08 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2731/3393 [21:52<05:32,  1.99batch/s, Batch Loss=0.1652, Avg Loss=0.2526, Time Left=6.08 \u001b[A\n",
      "Epoch 1/3 - Training:  80%|▊| 2731/3393 [21:52<05:32,  1.99batch/s, Batch Loss=0.0188, Avg Loss=0.2525, Time Left=6.07 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2732/3393 [21:52<05:28,  2.01batch/s, Batch Loss=0.0188, Avg Loss=0.2525, Time Left=6.07 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2732/3393 [21:53<05:28,  2.01batch/s, Batch Loss=0.1866, Avg Loss=0.2525, Time Left=6.06 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2733/3393 [21:53<05:29,  2.01batch/s, Batch Loss=0.1866, Avg Loss=0.2525, Time Left=6.06 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2733/3393 [21:53<05:29,  2.01batch/s, Batch Loss=0.0804, Avg Loss=0.2524, Time Left=6.05 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2734/3393 [21:53<05:29,  2.00batch/s, Batch Loss=0.0804, Avg Loss=0.2524, Time Left=6.05 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2734/3393 [21:54<05:29,  2.00batch/s, Batch Loss=0.1140, Avg Loss=0.2523, Time Left=6.05 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2735/3393 [21:54<05:31,  1.99batch/s, Batch Loss=0.1140, Avg Loss=0.2523, Time Left=6.05 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2735/3393 [21:54<05:31,  1.99batch/s, Batch Loss=0.7200, Avg Loss=0.2525, Time Left=6.04 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2736/3393 [21:54<05:23,  2.03batch/s, Batch Loss=0.7200, Avg Loss=0.2525, Time Left=6.04 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2736/3393 [21:55<05:23,  2.03batch/s, Batch Loss=0.2031, Avg Loss=0.2525, Time Left=6.03 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  81%|▊| 2737/3393 [21:55<05:21,  2.04batch/s, Batch Loss=0.2031, Avg Loss=0.2525, Time Left=6.03 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2737/3393 [21:55<05:21,  2.04batch/s, Batch Loss=0.4890, Avg Loss=0.2526, Time Left=6.02 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2738/3393 [21:55<05:26,  2.01batch/s, Batch Loss=0.4890, Avg Loss=0.2526, Time Left=6.02 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2738/3393 [21:56<05:26,  2.01batch/s, Batch Loss=0.0321, Avg Loss=0.2525, Time Left=6.01 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2739/3393 [21:56<05:23,  2.02batch/s, Batch Loss=0.0321, Avg Loss=0.2525, Time Left=6.01 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2739/3393 [21:56<05:23,  2.02batch/s, Batch Loss=0.3603, Avg Loss=0.2525, Time Left=6.01 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2740/3393 [21:56<05:29,  1.98batch/s, Batch Loss=0.3603, Avg Loss=0.2525, Time Left=6.01 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2740/3393 [21:57<05:29,  1.98batch/s, Batch Loss=0.1993, Avg Loss=0.2525, Time Left=6.00 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2741/3393 [21:57<05:23,  2.01batch/s, Batch Loss=0.1993, Avg Loss=0.2525, Time Left=6.00 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2741/3393 [21:57<05:23,  2.01batch/s, Batch Loss=0.0622, Avg Loss=0.2525, Time Left=5.99 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2742/3393 [21:57<05:25,  2.00batch/s, Batch Loss=0.0622, Avg Loss=0.2525, Time Left=5.99 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2742/3393 [21:58<05:25,  2.00batch/s, Batch Loss=0.1663, Avg Loss=0.2524, Time Left=5.98 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2743/3393 [21:58<05:27,  1.98batch/s, Batch Loss=0.1663, Avg Loss=0.2524, Time Left=5.98 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2743/3393 [21:58<05:27,  1.98batch/s, Batch Loss=0.2044, Avg Loss=0.2524, Time Left=5.97 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2744/3393 [21:58<05:24,  2.00batch/s, Batch Loss=0.2044, Avg Loss=0.2524, Time Left=5.97 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2744/3393 [21:59<05:24,  2.00batch/s, Batch Loss=0.1119, Avg Loss=0.2523, Time Left=5.96 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2745/3393 [21:59<05:18,  2.03batch/s, Batch Loss=0.1119, Avg Loss=0.2523, Time Left=5.96 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2745/3393 [21:59<05:18,  2.03batch/s, Batch Loss=0.2838, Avg Loss=0.2524, Time Left=5.96 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2746/3393 [21:59<05:17,  2.04batch/s, Batch Loss=0.2838, Avg Loss=0.2524, Time Left=5.96 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2746/3393 [22:00<05:17,  2.04batch/s, Batch Loss=0.0273, Avg Loss=0.2523, Time Left=5.95 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2747/3393 [22:00<05:28,  1.97batch/s, Batch Loss=0.0273, Avg Loss=0.2523, Time Left=5.95 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2747/3393 [22:00<05:28,  1.97batch/s, Batch Loss=0.0881, Avg Loss=0.2522, Time Left=5.94 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2748/3393 [22:00<05:26,  1.98batch/s, Batch Loss=0.0881, Avg Loss=0.2522, Time Left=5.94 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2748/3393 [22:01<05:26,  1.98batch/s, Batch Loss=0.0215, Avg Loss=0.2521, Time Left=5.93 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2749/3393 [22:01<05:26,  1.97batch/s, Batch Loss=0.0215, Avg Loss=0.2521, Time Left=5.93 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2749/3393 [22:01<05:26,  1.97batch/s, Batch Loss=0.3116, Avg Loss=0.2521, Time Left=5.92 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2750/3393 [22:01<05:24,  1.98batch/s, Batch Loss=0.3116, Avg Loss=0.2521, Time Left=5.92 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2750/3393 [22:02<05:24,  1.98batch/s, Batch Loss=0.2223, Avg Loss=0.2521, Time Left=5.91 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2751/3393 [22:02<05:22,  1.99batch/s, Batch Loss=0.2223, Avg Loss=0.2521, Time Left=5.91 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2751/3393 [22:02<05:22,  1.99batch/s, Batch Loss=0.1826, Avg Loss=0.2521, Time Left=5.91 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2752/3393 [22:02<05:19,  2.01batch/s, Batch Loss=0.1826, Avg Loss=0.2521, Time Left=5.91 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2752/3393 [22:03<05:19,  2.01batch/s, Batch Loss=0.0382, Avg Loss=0.2520, Time Left=5.90 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2753/3393 [22:03<05:19,  2.00batch/s, Batch Loss=0.0382, Avg Loss=0.2520, Time Left=5.90 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2753/3393 [22:03<05:19,  2.00batch/s, Batch Loss=0.1759, Avg Loss=0.2520, Time Left=5.89 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2754/3393 [22:03<05:23,  1.98batch/s, Batch Loss=0.1759, Avg Loss=0.2520, Time Left=5.89 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2754/3393 [22:04<05:23,  1.98batch/s, Batch Loss=0.1967, Avg Loss=0.2520, Time Left=5.88 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2755/3393 [22:04<05:18,  2.01batch/s, Batch Loss=0.1967, Avg Loss=0.2520, Time Left=5.88 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2755/3393 [22:04<05:18,  2.01batch/s, Batch Loss=0.1523, Avg Loss=0.2519, Time Left=5.87 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2756/3393 [22:04<05:16,  2.01batch/s, Batch Loss=0.1523, Avg Loss=0.2519, Time Left=5.87 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2756/3393 [22:05<05:16,  2.01batch/s, Batch Loss=0.0909, Avg Loss=0.2519, Time Left=5.87 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2757/3393 [22:05<05:13,  2.03batch/s, Batch Loss=0.0909, Avg Loss=0.2519, Time Left=5.87 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2757/3393 [22:05<05:13,  2.03batch/s, Batch Loss=0.1205, Avg Loss=0.2518, Time Left=5.86 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2758/3393 [22:05<05:23,  1.96batch/s, Batch Loss=0.1205, Avg Loss=0.2518, Time Left=5.86 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2758/3393 [22:06<05:23,  1.96batch/s, Batch Loss=0.0320, Avg Loss=0.2518, Time Left=5.85 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2759/3393 [22:06<05:18,  1.99batch/s, Batch Loss=0.0320, Avg Loss=0.2518, Time Left=5.85 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2759/3393 [22:06<05:18,  1.99batch/s, Batch Loss=0.0458, Avg Loss=0.2517, Time Left=5.84 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2760/3393 [22:06<05:29,  1.92batch/s, Batch Loss=0.0458, Avg Loss=0.2517, Time Left=5.84 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2760/3393 [22:07<05:29,  1.92batch/s, Batch Loss=0.4450, Avg Loss=0.2518, Time Left=5.83 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2761/3393 [22:07<05:37,  1.87batch/s, Batch Loss=0.4450, Avg Loss=0.2518, Time Left=5.83 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2761/3393 [22:07<05:37,  1.87batch/s, Batch Loss=0.1646, Avg Loss=0.2517, Time Left=5.83 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2762/3393 [22:07<05:37,  1.87batch/s, Batch Loss=0.1646, Avg Loss=0.2517, Time Left=5.83 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2762/3393 [22:08<05:37,  1.87batch/s, Batch Loss=0.1111, Avg Loss=0.2517, Time Left=5.82 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2763/3393 [22:08<05:36,  1.87batch/s, Batch Loss=0.1111, Avg Loss=0.2517, Time Left=5.82 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2763/3393 [22:08<05:36,  1.87batch/s, Batch Loss=0.1365, Avg Loss=0.2516, Time Left=5.81 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2764/3393 [22:08<05:33,  1.89batch/s, Batch Loss=0.1365, Avg Loss=0.2516, Time Left=5.81 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2764/3393 [22:09<05:33,  1.89batch/s, Batch Loss=0.5673, Avg Loss=0.2517, Time Left=5.80 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2765/3393 [22:09<05:30,  1.90batch/s, Batch Loss=0.5673, Avg Loss=0.2517, Time Left=5.80 \u001b[A\n",
      "Epoch 1/3 - Training:  81%|▊| 2765/3393 [22:10<05:30,  1.90batch/s, Batch Loss=0.0546, Avg Loss=0.2517, Time Left=5.79 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2766/3393 [22:10<05:37,  1.86batch/s, Batch Loss=0.0546, Avg Loss=0.2517, Time Left=5.79 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2766/3393 [22:10<05:37,  1.86batch/s, Batch Loss=0.0562, Avg Loss=0.2516, Time Left=5.78 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2767/3393 [22:10<05:24,  1.93batch/s, Batch Loss=0.0562, Avg Loss=0.2516, Time Left=5.78 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2767/3393 [22:10<05:24,  1.93batch/s, Batch Loss=0.1870, Avg Loss=0.2516, Time Left=5.78 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2768/3393 [22:10<05:19,  1.96batch/s, Batch Loss=0.1870, Avg Loss=0.2516, Time Left=5.78 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2768/3393 [22:11<05:19,  1.96batch/s, Batch Loss=0.1310, Avg Loss=0.2515, Time Left=5.77 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2769/3393 [22:11<05:11,  2.01batch/s, Batch Loss=0.1310, Avg Loss=0.2515, Time Left=5.77 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2769/3393 [22:11<05:11,  2.01batch/s, Batch Loss=0.0265, Avg Loss=0.2514, Time Left=5.76 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  82%|▊| 2770/3393 [22:11<05:07,  2.02batch/s, Batch Loss=0.0265, Avg Loss=0.2514, Time Left=5.76 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2770/3393 [22:12<05:07,  2.02batch/s, Batch Loss=0.0428, Avg Loss=0.2514, Time Left=5.75 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2771/3393 [22:12<05:08,  2.02batch/s, Batch Loss=0.0428, Avg Loss=0.2514, Time Left=5.75 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2771/3393 [22:12<05:08,  2.02batch/s, Batch Loss=0.1179, Avg Loss=0.2513, Time Left=5.74 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2772/3393 [22:12<05:02,  2.05batch/s, Batch Loss=0.1179, Avg Loss=0.2513, Time Left=5.74 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2772/3393 [22:13<05:02,  2.05batch/s, Batch Loss=0.1721, Avg Loss=0.2513, Time Left=5.73 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2773/3393 [22:13<04:58,  2.08batch/s, Batch Loss=0.1721, Avg Loss=0.2513, Time Left=5.73 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2773/3393 [22:13<04:58,  2.08batch/s, Batch Loss=0.1649, Avg Loss=0.2513, Time Left=5.73 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2774/3393 [22:13<05:06,  2.02batch/s, Batch Loss=0.1649, Avg Loss=0.2513, Time Left=5.73 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2774/3393 [22:14<05:06,  2.02batch/s, Batch Loss=0.0950, Avg Loss=0.2512, Time Left=5.72 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2775/3393 [22:14<05:04,  2.03batch/s, Batch Loss=0.0950, Avg Loss=0.2512, Time Left=5.72 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2775/3393 [22:14<05:04,  2.03batch/s, Batch Loss=0.1660, Avg Loss=0.2512, Time Left=5.71 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2776/3393 [22:14<05:11,  1.98batch/s, Batch Loss=0.1660, Avg Loss=0.2512, Time Left=5.71 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2776/3393 [22:15<05:11,  1.98batch/s, Batch Loss=0.1854, Avg Loss=0.2511, Time Left=5.70 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2777/3393 [22:15<05:10,  1.99batch/s, Batch Loss=0.1854, Avg Loss=0.2511, Time Left=5.70 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2777/3393 [22:15<05:10,  1.99batch/s, Batch Loss=0.0557, Avg Loss=0.2511, Time Left=5.69 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2778/3393 [22:15<05:06,  2.01batch/s, Batch Loss=0.0557, Avg Loss=0.2511, Time Left=5.69 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2778/3393 [22:16<05:06,  2.01batch/s, Batch Loss=0.0572, Avg Loss=0.2510, Time Left=5.69 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2779/3393 [22:16<05:03,  2.02batch/s, Batch Loss=0.0572, Avg Loss=0.2510, Time Left=5.69 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2779/3393 [22:16<05:03,  2.02batch/s, Batch Loss=0.0227, Avg Loss=0.2509, Time Left=5.68 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2780/3393 [22:16<04:57,  2.06batch/s, Batch Loss=0.0227, Avg Loss=0.2509, Time Left=5.68 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2780/3393 [22:17<04:57,  2.06batch/s, Batch Loss=0.1345, Avg Loss=0.2509, Time Left=5.67 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2781/3393 [22:17<05:05,  2.00batch/s, Batch Loss=0.1345, Avg Loss=0.2509, Time Left=5.67 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2781/3393 [22:17<05:05,  2.00batch/s, Batch Loss=0.0611, Avg Loss=0.2508, Time Left=5.66 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2782/3393 [22:17<04:59,  2.04batch/s, Batch Loss=0.0611, Avg Loss=0.2508, Time Left=5.66 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2782/3393 [22:18<04:59,  2.04batch/s, Batch Loss=0.0534, Avg Loss=0.2507, Time Left=5.65 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2783/3393 [22:18<04:58,  2.05batch/s, Batch Loss=0.0534, Avg Loss=0.2507, Time Left=5.65 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2783/3393 [22:18<04:58,  2.05batch/s, Batch Loss=0.1961, Avg Loss=0.2507, Time Left=5.64 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2784/3393 [22:18<04:53,  2.07batch/s, Batch Loss=0.1961, Avg Loss=0.2507, Time Left=5.64 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2784/3393 [22:19<04:53,  2.07batch/s, Batch Loss=0.2039, Avg Loss=0.2507, Time Left=5.64 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2785/3393 [22:19<04:53,  2.07batch/s, Batch Loss=0.2039, Avg Loss=0.2507, Time Left=5.64 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2785/3393 [22:19<04:53,  2.07batch/s, Batch Loss=0.0727, Avg Loss=0.2506, Time Left=5.63 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2786/3393 [22:19<04:49,  2.09batch/s, Batch Loss=0.0727, Avg Loss=0.2506, Time Left=5.63 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2786/3393 [22:20<04:49,  2.09batch/s, Batch Loss=0.1376, Avg Loss=0.2506, Time Left=5.62 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2787/3393 [22:20<04:53,  2.07batch/s, Batch Loss=0.1376, Avg Loss=0.2506, Time Left=5.62 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2787/3393 [22:20<04:53,  2.07batch/s, Batch Loss=0.2020, Avg Loss=0.2506, Time Left=5.61 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2788/3393 [22:20<04:53,  2.06batch/s, Batch Loss=0.2020, Avg Loss=0.2506, Time Left=5.61 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2788/3393 [22:21<04:53,  2.06batch/s, Batch Loss=0.1024, Avg Loss=0.2505, Time Left=5.60 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2789/3393 [22:21<04:55,  2.04batch/s, Batch Loss=0.1024, Avg Loss=0.2505, Time Left=5.60 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2789/3393 [22:21<04:55,  2.04batch/s, Batch Loss=0.1632, Avg Loss=0.2505, Time Left=5.59 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2790/3393 [22:21<04:53,  2.06batch/s, Batch Loss=0.1632, Avg Loss=0.2505, Time Left=5.59 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2790/3393 [22:22<04:53,  2.06batch/s, Batch Loss=0.1001, Avg Loss=0.2504, Time Left=5.59 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2791/3393 [22:22<04:50,  2.07batch/s, Batch Loss=0.1001, Avg Loss=0.2504, Time Left=5.59 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2791/3393 [22:22<04:50,  2.07batch/s, Batch Loss=0.0486, Avg Loss=0.2503, Time Left=5.58 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2792/3393 [22:22<04:52,  2.06batch/s, Batch Loss=0.0486, Avg Loss=0.2503, Time Left=5.58 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2792/3393 [22:23<04:52,  2.06batch/s, Batch Loss=0.0519, Avg Loss=0.2503, Time Left=5.57 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2793/3393 [22:23<04:48,  2.08batch/s, Batch Loss=0.0519, Avg Loss=0.2503, Time Left=5.57 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2793/3393 [22:23<04:48,  2.08batch/s, Batch Loss=0.0036, Avg Loss=0.2502, Time Left=5.56 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2794/3393 [22:23<04:48,  2.08batch/s, Batch Loss=0.0036, Avg Loss=0.2502, Time Left=5.56 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2794/3393 [22:24<04:48,  2.08batch/s, Batch Loss=0.1776, Avg Loss=0.2502, Time Left=5.55 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2795/3393 [22:24<04:53,  2.03batch/s, Batch Loss=0.1776, Avg Loss=0.2502, Time Left=5.55 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2795/3393 [22:24<04:53,  2.03batch/s, Batch Loss=0.1096, Avg Loss=0.2501, Time Left=5.54 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2796/3393 [22:24<04:52,  2.04batch/s, Batch Loss=0.1096, Avg Loss=0.2501, Time Left=5.54 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2796/3393 [22:25<04:52,  2.04batch/s, Batch Loss=0.2507, Avg Loss=0.2501, Time Left=5.54 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2797/3393 [22:25<04:53,  2.03batch/s, Batch Loss=0.2507, Avg Loss=0.2501, Time Left=5.54 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2797/3393 [22:25<04:53,  2.03batch/s, Batch Loss=0.0995, Avg Loss=0.2500, Time Left=5.53 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2798/3393 [22:25<04:51,  2.04batch/s, Batch Loss=0.0995, Avg Loss=0.2500, Time Left=5.53 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2798/3393 [22:26<04:51,  2.04batch/s, Batch Loss=0.0311, Avg Loss=0.2500, Time Left=5.52 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2799/3393 [22:26<04:50,  2.05batch/s, Batch Loss=0.0311, Avg Loss=0.2500, Time Left=5.52 \u001b[A\n",
      "Epoch 1/3 - Training:  82%|▊| 2799/3393 [22:26<04:50,  2.05batch/s, Batch Loss=0.5032, Avg Loss=0.2501, Time Left=5.51 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2800/3393 [22:26<04:48,  2.05batch/s, Batch Loss=0.5032, Avg Loss=0.2501, Time Left=5.51 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2800/3393 [22:27<04:48,  2.05batch/s, Batch Loss=0.3674, Avg Loss=0.2501, Time Left=5.50 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2801/3393 [22:27<04:45,  2.08batch/s, Batch Loss=0.3674, Avg Loss=0.2501, Time Left=5.50 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2801/3393 [22:27<04:45,  2.08batch/s, Batch Loss=0.1221, Avg Loss=0.2501, Time Left=5.50 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2802/3393 [22:27<04:44,  2.08batch/s, Batch Loss=0.1221, Avg Loss=0.2501, Time Left=5.50 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2802/3393 [22:28<04:44,  2.08batch/s, Batch Loss=0.0779, Avg Loss=0.2500, Time Left=5.49 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  83%|▊| 2803/3393 [22:28<04:44,  2.07batch/s, Batch Loss=0.0779, Avg Loss=0.2500, Time Left=5.49 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2803/3393 [22:28<04:44,  2.07batch/s, Batch Loss=0.1493, Avg Loss=0.2500, Time Left=5.48 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2804/3393 [22:28<04:41,  2.09batch/s, Batch Loss=0.1493, Avg Loss=0.2500, Time Left=5.48 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2804/3393 [22:28<04:41,  2.09batch/s, Batch Loss=0.0603, Avg Loss=0.2499, Time Left=5.47 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2805/3393 [22:28<04:41,  2.09batch/s, Batch Loss=0.0603, Avg Loss=0.2499, Time Left=5.47 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2805/3393 [22:29<04:41,  2.09batch/s, Batch Loss=0.0823, Avg Loss=0.2498, Time Left=5.46 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2806/3393 [22:29<04:44,  2.06batch/s, Batch Loss=0.0823, Avg Loss=0.2498, Time Left=5.46 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2806/3393 [22:29<04:44,  2.06batch/s, Batch Loss=0.0454, Avg Loss=0.2498, Time Left=5.45 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2807/3393 [22:29<04:45,  2.05batch/s, Batch Loss=0.0454, Avg Loss=0.2498, Time Left=5.45 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2807/3393 [22:30<04:45,  2.05batch/s, Batch Loss=0.0628, Avg Loss=0.2497, Time Left=5.45 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2808/3393 [22:30<04:48,  2.03batch/s, Batch Loss=0.0628, Avg Loss=0.2497, Time Left=5.45 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2808/3393 [22:30<04:48,  2.03batch/s, Batch Loss=0.0334, Avg Loss=0.2496, Time Left=5.44 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2809/3393 [22:30<04:49,  2.02batch/s, Batch Loss=0.0334, Avg Loss=0.2496, Time Left=5.44 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2809/3393 [22:31<04:49,  2.02batch/s, Batch Loss=0.0337, Avg Loss=0.2495, Time Left=5.43 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2810/3393 [22:31<04:44,  2.05batch/s, Batch Loss=0.0337, Avg Loss=0.2495, Time Left=5.43 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2810/3393 [22:31<04:44,  2.05batch/s, Batch Loss=0.5225, Avg Loss=0.2496, Time Left=5.42 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2811/3393 [22:31<04:40,  2.08batch/s, Batch Loss=0.5225, Avg Loss=0.2496, Time Left=5.42 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2811/3393 [22:32<04:40,  2.08batch/s, Batch Loss=0.1464, Avg Loss=0.2496, Time Left=5.41 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2812/3393 [22:32<04:39,  2.08batch/s, Batch Loss=0.1464, Avg Loss=0.2496, Time Left=5.41 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2812/3393 [22:32<04:39,  2.08batch/s, Batch Loss=0.0435, Avg Loss=0.2495, Time Left=5.40 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2813/3393 [22:32<04:39,  2.07batch/s, Batch Loss=0.0435, Avg Loss=0.2495, Time Left=5.40 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2813/3393 [22:33<04:39,  2.07batch/s, Batch Loss=0.0609, Avg Loss=0.2494, Time Left=5.40 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2814/3393 [22:33<04:42,  2.05batch/s, Batch Loss=0.0609, Avg Loss=0.2494, Time Left=5.40 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2814/3393 [22:33<04:42,  2.05batch/s, Batch Loss=0.3098, Avg Loss=0.2495, Time Left=5.39 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2815/3393 [22:33<04:38,  2.08batch/s, Batch Loss=0.3098, Avg Loss=0.2495, Time Left=5.39 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2815/3393 [22:34<04:38,  2.08batch/s, Batch Loss=0.1369, Avg Loss=0.2494, Time Left=5.38 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2816/3393 [22:34<04:46,  2.01batch/s, Batch Loss=0.1369, Avg Loss=0.2494, Time Left=5.38 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2816/3393 [22:34<04:46,  2.01batch/s, Batch Loss=0.1952, Avg Loss=0.2494, Time Left=5.37 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2817/3393 [22:34<04:41,  2.05batch/s, Batch Loss=0.1952, Avg Loss=0.2494, Time Left=5.37 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2817/3393 [22:35<04:41,  2.05batch/s, Batch Loss=0.4709, Avg Loss=0.2495, Time Left=5.36 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2818/3393 [22:35<04:42,  2.03batch/s, Batch Loss=0.4709, Avg Loss=0.2495, Time Left=5.36 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2818/3393 [22:35<04:42,  2.03batch/s, Batch Loss=0.0947, Avg Loss=0.2494, Time Left=5.35 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2819/3393 [22:35<04:38,  2.06batch/s, Batch Loss=0.0947, Avg Loss=0.2494, Time Left=5.35 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2819/3393 [22:36<04:38,  2.06batch/s, Batch Loss=0.2310, Avg Loss=0.2494, Time Left=5.35 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2820/3393 [22:36<04:40,  2.04batch/s, Batch Loss=0.2310, Avg Loss=0.2494, Time Left=5.35 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2820/3393 [22:36<04:40,  2.04batch/s, Batch Loss=0.1401, Avg Loss=0.2494, Time Left=5.34 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2821/3393 [22:36<04:44,  2.01batch/s, Batch Loss=0.1401, Avg Loss=0.2494, Time Left=5.34 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2821/3393 [22:37<04:44,  2.01batch/s, Batch Loss=0.1045, Avg Loss=0.2493, Time Left=5.33 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2822/3393 [22:37<04:39,  2.04batch/s, Batch Loss=0.1045, Avg Loss=0.2493, Time Left=5.33 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2822/3393 [22:37<04:39,  2.04batch/s, Batch Loss=0.1477, Avg Loss=0.2493, Time Left=5.32 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2823/3393 [22:37<04:40,  2.03batch/s, Batch Loss=0.1477, Avg Loss=0.2493, Time Left=5.32 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2823/3393 [22:38<04:40,  2.03batch/s, Batch Loss=0.1594, Avg Loss=0.2493, Time Left=5.31 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2824/3393 [22:38<04:38,  2.04batch/s, Batch Loss=0.1594, Avg Loss=0.2493, Time Left=5.31 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2824/3393 [22:38<04:38,  2.04batch/s, Batch Loss=0.1349, Avg Loss=0.2492, Time Left=5.31 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2825/3393 [22:38<04:37,  2.05batch/s, Batch Loss=0.1349, Avg Loss=0.2492, Time Left=5.31 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2825/3393 [22:39<04:37,  2.05batch/s, Batch Loss=0.2927, Avg Loss=0.2492, Time Left=5.30 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2826/3393 [22:39<04:41,  2.01batch/s, Batch Loss=0.2927, Avg Loss=0.2492, Time Left=5.30 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2826/3393 [22:39<04:41,  2.01batch/s, Batch Loss=0.1587, Avg Loss=0.2492, Time Left=5.29 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2827/3393 [22:39<04:39,  2.03batch/s, Batch Loss=0.1587, Avg Loss=0.2492, Time Left=5.29 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2827/3393 [22:40<04:39,  2.03batch/s, Batch Loss=0.0534, Avg Loss=0.2491, Time Left=5.28 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2828/3393 [22:40<04:36,  2.04batch/s, Batch Loss=0.0534, Avg Loss=0.2491, Time Left=5.28 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2828/3393 [22:40<04:36,  2.04batch/s, Batch Loss=0.2624, Avg Loss=0.2491, Time Left=5.27 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2829/3393 [22:40<04:35,  2.05batch/s, Batch Loss=0.2624, Avg Loss=0.2491, Time Left=5.27 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2829/3393 [22:41<04:35,  2.05batch/s, Batch Loss=0.2061, Avg Loss=0.2491, Time Left=5.26 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2830/3393 [22:41<04:34,  2.05batch/s, Batch Loss=0.2061, Avg Loss=0.2491, Time Left=5.26 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2830/3393 [22:41<04:34,  2.05batch/s, Batch Loss=0.2729, Avg Loss=0.2491, Time Left=5.26 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2831/3393 [22:41<04:38,  2.02batch/s, Batch Loss=0.2729, Avg Loss=0.2491, Time Left=5.26 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2831/3393 [22:42<04:38,  2.02batch/s, Batch Loss=0.1016, Avg Loss=0.2491, Time Left=5.25 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2832/3393 [22:42<04:38,  2.01batch/s, Batch Loss=0.1016, Avg Loss=0.2491, Time Left=5.25 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2832/3393 [22:42<04:38,  2.01batch/s, Batch Loss=0.1537, Avg Loss=0.2490, Time Left=5.24 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2833/3393 [22:42<04:38,  2.01batch/s, Batch Loss=0.1537, Avg Loss=0.2490, Time Left=5.24 \u001b[A\n",
      "Epoch 1/3 - Training:  83%|▊| 2833/3393 [22:43<04:38,  2.01batch/s, Batch Loss=0.2125, Avg Loss=0.2490, Time Left=5.23 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2834/3393 [22:43<04:33,  2.04batch/s, Batch Loss=0.2125, Avg Loss=0.2490, Time Left=5.23 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2834/3393 [22:43<04:33,  2.04batch/s, Batch Loss=0.1764, Avg Loss=0.2490, Time Left=5.22 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2835/3393 [22:43<04:34,  2.03batch/s, Batch Loss=0.1764, Avg Loss=0.2490, Time Left=5.22 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2835/3393 [22:44<04:34,  2.03batch/s, Batch Loss=0.0499, Avg Loss=0.2489, Time Left=5.21 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  84%|▊| 2836/3393 [22:44<04:35,  2.02batch/s, Batch Loss=0.0499, Avg Loss=0.2489, Time Left=5.21 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2836/3393 [22:44<04:35,  2.02batch/s, Batch Loss=0.1749, Avg Loss=0.2489, Time Left=5.21 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2837/3393 [22:44<04:35,  2.02batch/s, Batch Loss=0.1749, Avg Loss=0.2489, Time Left=5.21 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2837/3393 [22:45<04:35,  2.02batch/s, Batch Loss=0.1636, Avg Loss=0.2489, Time Left=5.20 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2838/3393 [22:45<04:38,  1.99batch/s, Batch Loss=0.1636, Avg Loss=0.2489, Time Left=5.20 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2838/3393 [22:45<04:38,  1.99batch/s, Batch Loss=0.2470, Avg Loss=0.2489, Time Left=5.19 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2839/3393 [22:45<04:35,  2.01batch/s, Batch Loss=0.2470, Avg Loss=0.2489, Time Left=5.19 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2839/3393 [22:46<04:35,  2.01batch/s, Batch Loss=0.1995, Avg Loss=0.2489, Time Left=5.18 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2840/3393 [22:46<04:35,  2.01batch/s, Batch Loss=0.1995, Avg Loss=0.2489, Time Left=5.18 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2840/3393 [22:46<04:35,  2.01batch/s, Batch Loss=0.1600, Avg Loss=0.2488, Time Left=5.17 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2841/3393 [22:46<04:31,  2.03batch/s, Batch Loss=0.1600, Avg Loss=0.2488, Time Left=5.17 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2841/3393 [22:47<04:31,  2.03batch/s, Batch Loss=0.1599, Avg Loss=0.2488, Time Left=5.17 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2842/3393 [22:47<04:23,  2.09batch/s, Batch Loss=0.1599, Avg Loss=0.2488, Time Left=5.17 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2842/3393 [22:47<04:23,  2.09batch/s, Batch Loss=0.1591, Avg Loss=0.2488, Time Left=5.16 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2843/3393 [22:47<04:34,  2.00batch/s, Batch Loss=0.1591, Avg Loss=0.2488, Time Left=5.16 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2843/3393 [22:48<04:34,  2.00batch/s, Batch Loss=0.2974, Avg Loss=0.2488, Time Left=5.15 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2844/3393 [22:48<04:30,  2.03batch/s, Batch Loss=0.2974, Avg Loss=0.2488, Time Left=5.15 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2844/3393 [22:48<04:30,  2.03batch/s, Batch Loss=0.2417, Avg Loss=0.2488, Time Left=5.14 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2845/3393 [22:48<04:32,  2.01batch/s, Batch Loss=0.2417, Avg Loss=0.2488, Time Left=5.14 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2845/3393 [22:49<04:32,  2.01batch/s, Batch Loss=0.1586, Avg Loss=0.2487, Time Left=5.13 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2846/3393 [22:49<04:26,  2.06batch/s, Batch Loss=0.1586, Avg Loss=0.2487, Time Left=5.13 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2846/3393 [22:49<04:26,  2.06batch/s, Batch Loss=0.0405, Avg Loss=0.2487, Time Left=5.12 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2847/3393 [22:49<04:27,  2.04batch/s, Batch Loss=0.0405, Avg Loss=0.2487, Time Left=5.12 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2847/3393 [22:50<04:27,  2.04batch/s, Batch Loss=0.0884, Avg Loss=0.2486, Time Left=5.12 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2848/3393 [22:50<04:33,  1.99batch/s, Batch Loss=0.0884, Avg Loss=0.2486, Time Left=5.12 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2848/3393 [22:50<04:33,  1.99batch/s, Batch Loss=0.2303, Avg Loss=0.2486, Time Left=5.11 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2849/3393 [22:50<04:30,  2.01batch/s, Batch Loss=0.2303, Avg Loss=0.2486, Time Left=5.11 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2849/3393 [22:51<04:30,  2.01batch/s, Batch Loss=0.1986, Avg Loss=0.2486, Time Left=5.10 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2850/3393 [22:51<04:33,  1.99batch/s, Batch Loss=0.1986, Avg Loss=0.2486, Time Left=5.10 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2850/3393 [22:51<04:33,  1.99batch/s, Batch Loss=0.0817, Avg Loss=0.2485, Time Left=5.09 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2851/3393 [22:51<04:29,  2.01batch/s, Batch Loss=0.0817, Avg Loss=0.2485, Time Left=5.09 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2851/3393 [22:52<04:29,  2.01batch/s, Batch Loss=0.0859, Avg Loss=0.2485, Time Left=5.08 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2852/3393 [22:52<04:26,  2.03batch/s, Batch Loss=0.0859, Avg Loss=0.2485, Time Left=5.08 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2852/3393 [22:52<04:26,  2.03batch/s, Batch Loss=0.0101, Avg Loss=0.2484, Time Left=5.07 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2853/3393 [22:52<04:24,  2.04batch/s, Batch Loss=0.0101, Avg Loss=0.2484, Time Left=5.07 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2853/3393 [22:53<04:24,  2.04batch/s, Batch Loss=0.2860, Avg Loss=0.2484, Time Left=5.07 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2854/3393 [22:53<04:24,  2.04batch/s, Batch Loss=0.2860, Avg Loss=0.2484, Time Left=5.07 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2854/3393 [22:53<04:24,  2.04batch/s, Batch Loss=0.1539, Avg Loss=0.2484, Time Left=5.06 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2855/3393 [22:53<04:26,  2.02batch/s, Batch Loss=0.1539, Avg Loss=0.2484, Time Left=5.06 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2855/3393 [22:54<04:26,  2.02batch/s, Batch Loss=0.2274, Avg Loss=0.2483, Time Left=5.05 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2856/3393 [22:54<04:27,  2.01batch/s, Batch Loss=0.2274, Avg Loss=0.2483, Time Left=5.05 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2856/3393 [22:54<04:27,  2.01batch/s, Batch Loss=0.3198, Avg Loss=0.2484, Time Left=5.04 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2857/3393 [22:54<04:32,  1.97batch/s, Batch Loss=0.3198, Avg Loss=0.2484, Time Left=5.04 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2857/3393 [22:55<04:32,  1.97batch/s, Batch Loss=0.1516, Avg Loss=0.2483, Time Left=5.03 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2858/3393 [22:55<04:30,  1.98batch/s, Batch Loss=0.1516, Avg Loss=0.2483, Time Left=5.03 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2858/3393 [22:55<04:30,  1.98batch/s, Batch Loss=0.2505, Avg Loss=0.2483, Time Left=5.03 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2859/3393 [22:55<04:31,  1.97batch/s, Batch Loss=0.2505, Avg Loss=0.2483, Time Left=5.03 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2859/3393 [22:56<04:31,  1.97batch/s, Batch Loss=0.1529, Avg Loss=0.2483, Time Left=5.02 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2860/3393 [22:56<04:27,  1.99batch/s, Batch Loss=0.1529, Avg Loss=0.2483, Time Left=5.02 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2860/3393 [22:56<04:27,  1.99batch/s, Batch Loss=0.0970, Avg Loss=0.2482, Time Left=5.01 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2861/3393 [22:56<04:24,  2.01batch/s, Batch Loss=0.0970, Avg Loss=0.2482, Time Left=5.01 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2861/3393 [22:57<04:24,  2.01batch/s, Batch Loss=0.0461, Avg Loss=0.2482, Time Left=5.00 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2862/3393 [22:57<04:20,  2.04batch/s, Batch Loss=0.0461, Avg Loss=0.2482, Time Left=5.00 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2862/3393 [22:57<04:20,  2.04batch/s, Batch Loss=0.4265, Avg Loss=0.2482, Time Left=4.99 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2863/3393 [22:57<04:20,  2.04batch/s, Batch Loss=0.4265, Avg Loss=0.2482, Time Left=4.99 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2863/3393 [22:58<04:20,  2.04batch/s, Batch Loss=0.1392, Avg Loss=0.2482, Time Left=4.98 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2864/3393 [22:58<04:26,  1.99batch/s, Batch Loss=0.1392, Avg Loss=0.2482, Time Left=4.98 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2864/3393 [22:58<04:26,  1.99batch/s, Batch Loss=0.0837, Avg Loss=0.2481, Time Left=4.98 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2865/3393 [22:58<04:25,  1.99batch/s, Batch Loss=0.0837, Avg Loss=0.2481, Time Left=4.98 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2865/3393 [22:59<04:25,  1.99batch/s, Batch Loss=0.3470, Avg Loss=0.2482, Time Left=4.97 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2866/3393 [22:59<04:27,  1.97batch/s, Batch Loss=0.3470, Avg Loss=0.2482, Time Left=4.97 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2866/3393 [22:59<04:27,  1.97batch/s, Batch Loss=0.0968, Avg Loss=0.2481, Time Left=4.96 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2867/3393 [22:59<04:25,  1.98batch/s, Batch Loss=0.0968, Avg Loss=0.2481, Time Left=4.96 \u001b[A\n",
      "Epoch 1/3 - Training:  84%|▊| 2867/3393 [23:00<04:25,  1.98batch/s, Batch Loss=0.1852, Avg Loss=0.2481, Time Left=4.95 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2868/3393 [23:00<04:26,  1.97batch/s, Batch Loss=0.1852, Avg Loss=0.2481, Time Left=4.95 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2868/3393 [23:00<04:26,  1.97batch/s, Batch Loss=0.0305, Avg Loss=0.2480, Time Left=4.94 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  85%|▊| 2869/3393 [23:00<04:25,  1.98batch/s, Batch Loss=0.0305, Avg Loss=0.2480, Time Left=4.94 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2869/3393 [23:01<04:25,  1.98batch/s, Batch Loss=0.3452, Avg Loss=0.2481, Time Left=4.94 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2870/3393 [23:01<04:26,  1.96batch/s, Batch Loss=0.3452, Avg Loss=0.2481, Time Left=4.94 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2870/3393 [23:01<04:26,  1.96batch/s, Batch Loss=0.0886, Avg Loss=0.2480, Time Left=4.93 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2871/3393 [23:01<04:24,  1.97batch/s, Batch Loss=0.0886, Avg Loss=0.2480, Time Left=4.93 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2871/3393 [23:02<04:24,  1.97batch/s, Batch Loss=0.1535, Avg Loss=0.2480, Time Left=4.92 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2872/3393 [23:02<04:20,  2.00batch/s, Batch Loss=0.1535, Avg Loss=0.2480, Time Left=4.92 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2872/3393 [23:02<04:20,  2.00batch/s, Batch Loss=0.2750, Avg Loss=0.2480, Time Left=4.91 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2873/3393 [23:02<04:17,  2.02batch/s, Batch Loss=0.2750, Avg Loss=0.2480, Time Left=4.91 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2873/3393 [23:03<04:17,  2.02batch/s, Batch Loss=0.0972, Avg Loss=0.2479, Time Left=4.90 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2874/3393 [23:03<04:11,  2.06batch/s, Batch Loss=0.0972, Avg Loss=0.2479, Time Left=4.90 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2874/3393 [23:03<04:11,  2.06batch/s, Batch Loss=0.2336, Avg Loss=0.2479, Time Left=4.89 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2875/3393 [23:03<04:09,  2.08batch/s, Batch Loss=0.2336, Avg Loss=0.2479, Time Left=4.89 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2875/3393 [23:04<04:09,  2.08batch/s, Batch Loss=0.1173, Avg Loss=0.2479, Time Left=4.89 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2876/3393 [23:04<04:14,  2.03batch/s, Batch Loss=0.1173, Avg Loss=0.2479, Time Left=4.89 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2876/3393 [23:04<04:14,  2.03batch/s, Batch Loss=0.2271, Avg Loss=0.2479, Time Left=4.88 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2877/3393 [23:04<04:09,  2.06batch/s, Batch Loss=0.2271, Avg Loss=0.2479, Time Left=4.88 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2877/3393 [23:05<04:09,  2.06batch/s, Batch Loss=0.2611, Avg Loss=0.2479, Time Left=4.87 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2878/3393 [23:05<04:14,  2.03batch/s, Batch Loss=0.2611, Avg Loss=0.2479, Time Left=4.87 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2878/3393 [23:05<04:14,  2.03batch/s, Batch Loss=0.1397, Avg Loss=0.2478, Time Left=4.86 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2879/3393 [23:05<04:12,  2.04batch/s, Batch Loss=0.1397, Avg Loss=0.2478, Time Left=4.86 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2879/3393 [23:06<04:12,  2.04batch/s, Batch Loss=0.1692, Avg Loss=0.2478, Time Left=4.85 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2880/3393 [23:06<04:08,  2.06batch/s, Batch Loss=0.1692, Avg Loss=0.2478, Time Left=4.85 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2880/3393 [23:06<04:08,  2.06batch/s, Batch Loss=0.2218, Avg Loss=0.2478, Time Left=4.84 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2881/3393 [23:06<04:12,  2.03batch/s, Batch Loss=0.2218, Avg Loss=0.2478, Time Left=4.84 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2881/3393 [23:07<04:12,  2.03batch/s, Batch Loss=0.0782, Avg Loss=0.2477, Time Left=4.84 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2882/3393 [23:07<04:10,  2.04batch/s, Batch Loss=0.0782, Avg Loss=0.2477, Time Left=4.84 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2882/3393 [23:07<04:10,  2.04batch/s, Batch Loss=0.0477, Avg Loss=0.2477, Time Left=4.83 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2883/3393 [23:07<04:13,  2.01batch/s, Batch Loss=0.0477, Avg Loss=0.2477, Time Left=4.83 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2883/3393 [23:08<04:13,  2.01batch/s, Batch Loss=0.1810, Avg Loss=0.2476, Time Left=4.82 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2884/3393 [23:08<04:11,  2.02batch/s, Batch Loss=0.1810, Avg Loss=0.2476, Time Left=4.82 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2884/3393 [23:08<04:11,  2.02batch/s, Batch Loss=0.0425, Avg Loss=0.2476, Time Left=4.81 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2885/3393 [23:08<04:08,  2.04batch/s, Batch Loss=0.0425, Avg Loss=0.2476, Time Left=4.81 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2885/3393 [23:08<04:08,  2.04batch/s, Batch Loss=0.0750, Avg Loss=0.2475, Time Left=4.80 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2886/3393 [23:08<04:06,  2.05batch/s, Batch Loss=0.0750, Avg Loss=0.2475, Time Left=4.80 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2886/3393 [23:09<04:06,  2.05batch/s, Batch Loss=0.2812, Avg Loss=0.2475, Time Left=4.79 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2887/3393 [23:09<04:07,  2.05batch/s, Batch Loss=0.2812, Avg Loss=0.2475, Time Left=4.79 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2887/3393 [23:10<04:07,  2.05batch/s, Batch Loss=0.2087, Avg Loss=0.2475, Time Left=4.79 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2888/3393 [23:10<04:10,  2.01batch/s, Batch Loss=0.2087, Avg Loss=0.2475, Time Left=4.79 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2888/3393 [23:10<04:10,  2.01batch/s, Batch Loss=0.0871, Avg Loss=0.2474, Time Left=4.78 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2889/3393 [23:10<04:08,  2.03batch/s, Batch Loss=0.0871, Avg Loss=0.2474, Time Left=4.78 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2889/3393 [23:10<04:08,  2.03batch/s, Batch Loss=0.0438, Avg Loss=0.2474, Time Left=4.77 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2890/3393 [23:10<04:04,  2.06batch/s, Batch Loss=0.0438, Avg Loss=0.2474, Time Left=4.77 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2890/3393 [23:11<04:04,  2.06batch/s, Batch Loss=0.1539, Avg Loss=0.2473, Time Left=4.76 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2891/3393 [23:11<04:08,  2.02batch/s, Batch Loss=0.1539, Avg Loss=0.2473, Time Left=4.76 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2891/3393 [23:11<04:08,  2.02batch/s, Batch Loss=0.0216, Avg Loss=0.2473, Time Left=4.75 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2892/3393 [23:11<04:06,  2.03batch/s, Batch Loss=0.0216, Avg Loss=0.2473, Time Left=4.75 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2892/3393 [23:12<04:06,  2.03batch/s, Batch Loss=0.0564, Avg Loss=0.2472, Time Left=4.75 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2893/3393 [23:12<04:07,  2.02batch/s, Batch Loss=0.0564, Avg Loss=0.2472, Time Left=4.75 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2893/3393 [23:12<04:07,  2.02batch/s, Batch Loss=0.0457, Avg Loss=0.2471, Time Left=4.74 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2894/3393 [23:12<04:07,  2.02batch/s, Batch Loss=0.0457, Avg Loss=0.2471, Time Left=4.74 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2894/3393 [23:13<04:07,  2.02batch/s, Batch Loss=0.0371, Avg Loss=0.2470, Time Left=4.73 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2895/3393 [23:13<04:02,  2.05batch/s, Batch Loss=0.0371, Avg Loss=0.2470, Time Left=4.73 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2895/3393 [23:13<04:02,  2.05batch/s, Batch Loss=0.0455, Avg Loss=0.2470, Time Left=4.72 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2896/3393 [23:13<04:01,  2.06batch/s, Batch Loss=0.0455, Avg Loss=0.2470, Time Left=4.72 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2896/3393 [23:14<04:01,  2.06batch/s, Batch Loss=0.3169, Avg Loss=0.2470, Time Left=4.71 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2897/3393 [23:14<04:03,  2.04batch/s, Batch Loss=0.3169, Avg Loss=0.2470, Time Left=4.71 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2897/3393 [23:14<04:03,  2.04batch/s, Batch Loss=0.0436, Avg Loss=0.2469, Time Left=4.70 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2898/3393 [23:14<04:04,  2.03batch/s, Batch Loss=0.0436, Avg Loss=0.2469, Time Left=4.70 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2898/3393 [23:15<04:04,  2.03batch/s, Batch Loss=0.0623, Avg Loss=0.2469, Time Left=4.70 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2899/3393 [23:15<04:05,  2.01batch/s, Batch Loss=0.0623, Avg Loss=0.2469, Time Left=4.70 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2899/3393 [23:15<04:05,  2.01batch/s, Batch Loss=0.1884, Avg Loss=0.2468, Time Left=4.69 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2900/3393 [23:15<04:02,  2.03batch/s, Batch Loss=0.1884, Avg Loss=0.2468, Time Left=4.69 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2900/3393 [23:16<04:02,  2.03batch/s, Batch Loss=0.1153, Avg Loss=0.2468, Time Left=4.68 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2901/3393 [23:16<03:58,  2.07batch/s, Batch Loss=0.1153, Avg Loss=0.2468, Time Left=4.68 \u001b[A\n",
      "Epoch 1/3 - Training:  85%|▊| 2901/3393 [23:16<03:58,  2.07batch/s, Batch Loss=0.1377, Avg Loss=0.2468, Time Left=4.67 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  86%|▊| 2902/3393 [23:16<03:55,  2.08batch/s, Batch Loss=0.1377, Avg Loss=0.2468, Time Left=4.67 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2902/3393 [23:17<03:55,  2.08batch/s, Batch Loss=0.1671, Avg Loss=0.2467, Time Left=4.66 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2903/3393 [23:17<03:55,  2.08batch/s, Batch Loss=0.1671, Avg Loss=0.2467, Time Left=4.66 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2903/3393 [23:17<03:55,  2.08batch/s, Batch Loss=0.2917, Avg Loss=0.2467, Time Left=4.65 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2904/3393 [23:17<03:55,  2.08batch/s, Batch Loss=0.2917, Avg Loss=0.2467, Time Left=4.65 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2904/3393 [23:18<03:55,  2.08batch/s, Batch Loss=0.1051, Avg Loss=0.2467, Time Left=4.65 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2905/3393 [23:18<03:50,  2.12batch/s, Batch Loss=0.1051, Avg Loss=0.2467, Time Left=4.65 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2905/3393 [23:18<03:50,  2.12batch/s, Batch Loss=0.0175, Avg Loss=0.2466, Time Left=4.64 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2906/3393 [23:18<03:51,  2.10batch/s, Batch Loss=0.0175, Avg Loss=0.2466, Time Left=4.64 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2906/3393 [23:19<03:51,  2.10batch/s, Batch Loss=0.1794, Avg Loss=0.2466, Time Left=4.63 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2907/3393 [23:19<03:54,  2.07batch/s, Batch Loss=0.1794, Avg Loss=0.2466, Time Left=4.63 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2907/3393 [23:19<03:54,  2.07batch/s, Batch Loss=0.1663, Avg Loss=0.2466, Time Left=4.62 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2908/3393 [23:19<03:54,  2.07batch/s, Batch Loss=0.1663, Avg Loss=0.2466, Time Left=4.62 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2908/3393 [23:20<03:54,  2.07batch/s, Batch Loss=0.0744, Avg Loss=0.2465, Time Left=4.61 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2909/3393 [23:20<03:51,  2.09batch/s, Batch Loss=0.0744, Avg Loss=0.2465, Time Left=4.61 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2909/3393 [23:20<03:51,  2.09batch/s, Batch Loss=0.0850, Avg Loss=0.2464, Time Left=4.60 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2910/3393 [23:20<03:49,  2.10batch/s, Batch Loss=0.0850, Avg Loss=0.2464, Time Left=4.60 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2910/3393 [23:21<03:49,  2.10batch/s, Batch Loss=0.0389, Avg Loss=0.2464, Time Left=4.60 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2911/3393 [23:21<03:52,  2.07batch/s, Batch Loss=0.0389, Avg Loss=0.2464, Time Left=4.60 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2911/3393 [23:21<03:52,  2.07batch/s, Batch Loss=0.0644, Avg Loss=0.2463, Time Left=4.59 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2912/3393 [23:21<03:52,  2.07batch/s, Batch Loss=0.0644, Avg Loss=0.2463, Time Left=4.59 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2912/3393 [23:22<03:52,  2.07batch/s, Batch Loss=0.0365, Avg Loss=0.2462, Time Left=4.58 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2913/3393 [23:22<03:49,  2.09batch/s, Batch Loss=0.0365, Avg Loss=0.2462, Time Left=4.58 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2913/3393 [23:22<03:49,  2.09batch/s, Batch Loss=0.3494, Avg Loss=0.2463, Time Left=4.57 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2914/3393 [23:22<03:49,  2.08batch/s, Batch Loss=0.3494, Avg Loss=0.2463, Time Left=4.57 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2914/3393 [23:23<03:49,  2.08batch/s, Batch Loss=0.0294, Avg Loss=0.2462, Time Left=4.56 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2915/3393 [23:23<03:51,  2.06batch/s, Batch Loss=0.0294, Avg Loss=0.2462, Time Left=4.56 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2915/3393 [23:23<03:51,  2.06batch/s, Batch Loss=0.0610, Avg Loss=0.2461, Time Left=4.55 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2916/3393 [23:23<03:51,  2.06batch/s, Batch Loss=0.0610, Avg Loss=0.2461, Time Left=4.55 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2916/3393 [23:24<03:51,  2.06batch/s, Batch Loss=0.0393, Avg Loss=0.2461, Time Left=4.55 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2917/3393 [23:24<03:53,  2.04batch/s, Batch Loss=0.0393, Avg Loss=0.2461, Time Left=4.55 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2917/3393 [23:24<03:53,  2.04batch/s, Batch Loss=0.0307, Avg Loss=0.2460, Time Left=4.54 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2918/3393 [23:24<03:51,  2.05batch/s, Batch Loss=0.0307, Avg Loss=0.2460, Time Left=4.54 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2918/3393 [23:25<03:51,  2.05batch/s, Batch Loss=0.2420, Avg Loss=0.2460, Time Left=4.53 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2919/3393 [23:25<03:48,  2.07batch/s, Batch Loss=0.2420, Avg Loss=0.2460, Time Left=4.53 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2919/3393 [23:25<03:48,  2.07batch/s, Batch Loss=0.0583, Avg Loss=0.2459, Time Left=4.52 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2920/3393 [23:25<03:50,  2.05batch/s, Batch Loss=0.0583, Avg Loss=0.2459, Time Left=4.52 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2920/3393 [23:26<03:50,  2.05batch/s, Batch Loss=0.0806, Avg Loss=0.2459, Time Left=4.51 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2921/3393 [23:26<03:49,  2.05batch/s, Batch Loss=0.0806, Avg Loss=0.2459, Time Left=4.51 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2921/3393 [23:26<03:49,  2.05batch/s, Batch Loss=0.0158, Avg Loss=0.2458, Time Left=4.51 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2922/3393 [23:26<03:48,  2.06batch/s, Batch Loss=0.0158, Avg Loss=0.2458, Time Left=4.51 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2922/3393 [23:27<03:48,  2.06batch/s, Batch Loss=0.6600, Avg Loss=0.2459, Time Left=4.50 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2923/3393 [23:27<03:54,  2.00batch/s, Batch Loss=0.6600, Avg Loss=0.2459, Time Left=4.50 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2923/3393 [23:27<03:54,  2.00batch/s, Batch Loss=0.1973, Avg Loss=0.2459, Time Left=4.49 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2924/3393 [23:27<03:52,  2.02batch/s, Batch Loss=0.1973, Avg Loss=0.2459, Time Left=4.49 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2924/3393 [23:28<03:52,  2.02batch/s, Batch Loss=0.4394, Avg Loss=0.2460, Time Left=4.48 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2925/3393 [23:28<03:54,  1.99batch/s, Batch Loss=0.4394, Avg Loss=0.2460, Time Left=4.48 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2925/3393 [23:28<03:54,  1.99batch/s, Batch Loss=0.0364, Avg Loss=0.2459, Time Left=4.47 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2926/3393 [23:28<03:54,  1.99batch/s, Batch Loss=0.0364, Avg Loss=0.2459, Time Left=4.47 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2926/3393 [23:29<03:54,  1.99batch/s, Batch Loss=0.0704, Avg Loss=0.2458, Time Left=4.46 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2927/3393 [23:29<03:55,  1.98batch/s, Batch Loss=0.0704, Avg Loss=0.2458, Time Left=4.46 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2927/3393 [23:29<03:55,  1.98batch/s, Batch Loss=0.4127, Avg Loss=0.2459, Time Left=4.46 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2928/3393 [23:29<03:54,  1.98batch/s, Batch Loss=0.4127, Avg Loss=0.2459, Time Left=4.46 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2928/3393 [23:30<03:54,  1.98batch/s, Batch Loss=0.0429, Avg Loss=0.2458, Time Left=4.45 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2929/3393 [23:30<03:48,  2.03batch/s, Batch Loss=0.0429, Avg Loss=0.2458, Time Left=4.45 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2929/3393 [23:30<03:48,  2.03batch/s, Batch Loss=0.1900, Avg Loss=0.2458, Time Left=4.44 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2930/3393 [23:30<03:49,  2.02batch/s, Batch Loss=0.1900, Avg Loss=0.2458, Time Left=4.44 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2930/3393 [23:30<03:49,  2.02batch/s, Batch Loss=0.3149, Avg Loss=0.2458, Time Left=4.43 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2931/3393 [23:30<03:47,  2.03batch/s, Batch Loss=0.3149, Avg Loss=0.2458, Time Left=4.43 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2931/3393 [23:31<03:47,  2.03batch/s, Batch Loss=0.0174, Avg Loss=0.2457, Time Left=4.42 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2932/3393 [23:31<03:52,  1.98batch/s, Batch Loss=0.0174, Avg Loss=0.2457, Time Left=4.42 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2932/3393 [23:32<03:52,  1.98batch/s, Batch Loss=0.0913, Avg Loss=0.2457, Time Left=4.42 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2933/3393 [23:32<03:49,  2.01batch/s, Batch Loss=0.0913, Avg Loss=0.2457, Time Left=4.42 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2933/3393 [23:32<03:49,  2.01batch/s, Batch Loss=0.0903, Avg Loss=0.2456, Time Left=4.41 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2934/3393 [23:32<03:51,  1.99batch/s, Batch Loss=0.0903, Avg Loss=0.2456, Time Left=4.41 \u001b[A\n",
      "Epoch 1/3 - Training:  86%|▊| 2934/3393 [23:33<03:51,  1.99batch/s, Batch Loss=0.0187, Avg Loss=0.2456, Time Left=4.40 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  87%|▊| 2935/3393 [23:33<03:48,  2.01batch/s, Batch Loss=0.0187, Avg Loss=0.2456, Time Left=4.40 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2935/3393 [23:33<03:48,  2.01batch/s, Batch Loss=0.0312, Avg Loss=0.2455, Time Left=4.39 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2936/3393 [23:33<03:43,  2.04batch/s, Batch Loss=0.0312, Avg Loss=0.2455, Time Left=4.39 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2936/3393 [23:33<03:43,  2.04batch/s, Batch Loss=0.0801, Avg Loss=0.2454, Time Left=4.38 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2937/3393 [23:33<03:46,  2.01batch/s, Batch Loss=0.0801, Avg Loss=0.2454, Time Left=4.38 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2937/3393 [23:34<03:46,  2.01batch/s, Batch Loss=0.0560, Avg Loss=0.2454, Time Left=4.37 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2938/3393 [23:34<03:46,  2.01batch/s, Batch Loss=0.0560, Avg Loss=0.2454, Time Left=4.37 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2938/3393 [23:34<03:46,  2.01batch/s, Batch Loss=0.0291, Avg Loss=0.2453, Time Left=4.37 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2939/3393 [23:34<03:46,  2.01batch/s, Batch Loss=0.0291, Avg Loss=0.2453, Time Left=4.37 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2939/3393 [23:35<03:46,  2.01batch/s, Batch Loss=0.3392, Avg Loss=0.2453, Time Left=4.36 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2940/3393 [23:35<03:43,  2.03batch/s, Batch Loss=0.3392, Avg Loss=0.2453, Time Left=4.36 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2940/3393 [23:35<03:43,  2.03batch/s, Batch Loss=0.0404, Avg Loss=0.2452, Time Left=4.35 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2941/3393 [23:35<03:42,  2.03batch/s, Batch Loss=0.0404, Avg Loss=0.2452, Time Left=4.35 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2941/3393 [23:36<03:42,  2.03batch/s, Batch Loss=0.1305, Avg Loss=0.2452, Time Left=4.34 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2942/3393 [23:36<03:40,  2.04batch/s, Batch Loss=0.1305, Avg Loss=0.2452, Time Left=4.34 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2942/3393 [23:36<03:40,  2.04batch/s, Batch Loss=0.2828, Avg Loss=0.2452, Time Left=4.33 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2943/3393 [23:36<03:39,  2.05batch/s, Batch Loss=0.2828, Avg Loss=0.2452, Time Left=4.33 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2943/3393 [23:37<03:39,  2.05batch/s, Batch Loss=0.0396, Avg Loss=0.2451, Time Left=4.32 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2944/3393 [23:37<03:44,  2.00batch/s, Batch Loss=0.0396, Avg Loss=0.2451, Time Left=4.32 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2944/3393 [23:37<03:44,  2.00batch/s, Batch Loss=0.2523, Avg Loss=0.2451, Time Left=4.32 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2945/3393 [23:37<03:42,  2.02batch/s, Batch Loss=0.2523, Avg Loss=0.2451, Time Left=4.32 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2945/3393 [23:38<03:42,  2.02batch/s, Batch Loss=0.0526, Avg Loss=0.2451, Time Left=4.31 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2946/3393 [23:38<03:46,  1.98batch/s, Batch Loss=0.0526, Avg Loss=0.2451, Time Left=4.31 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2946/3393 [23:38<03:46,  1.98batch/s, Batch Loss=0.0093, Avg Loss=0.2450, Time Left=4.30 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2947/3393 [23:38<03:43,  2.00batch/s, Batch Loss=0.0093, Avg Loss=0.2450, Time Left=4.30 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2947/3393 [23:39<03:43,  2.00batch/s, Batch Loss=0.0985, Avg Loss=0.2449, Time Left=4.29 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2948/3393 [23:39<03:38,  2.04batch/s, Batch Loss=0.0985, Avg Loss=0.2449, Time Left=4.29 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2948/3393 [23:39<03:38,  2.04batch/s, Batch Loss=0.0914, Avg Loss=0.2449, Time Left=4.28 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2949/3393 [23:39<03:37,  2.04batch/s, Batch Loss=0.0914, Avg Loss=0.2449, Time Left=4.28 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2949/3393 [23:40<03:37,  2.04batch/s, Batch Loss=0.1153, Avg Loss=0.2448, Time Left=4.28 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2950/3393 [23:40<03:37,  2.03batch/s, Batch Loss=0.1153, Avg Loss=0.2448, Time Left=4.28 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2950/3393 [23:40<03:37,  2.03batch/s, Batch Loss=0.2159, Avg Loss=0.2448, Time Left=4.27 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2951/3393 [23:40<03:40,  2.00batch/s, Batch Loss=0.2159, Avg Loss=0.2448, Time Left=4.27 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2951/3393 [23:41<03:40,  2.00batch/s, Batch Loss=0.1500, Avg Loss=0.2448, Time Left=4.26 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2952/3393 [23:41<03:36,  2.04batch/s, Batch Loss=0.1500, Avg Loss=0.2448, Time Left=4.26 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2952/3393 [23:41<03:36,  2.04batch/s, Batch Loss=0.2406, Avg Loss=0.2448, Time Left=4.25 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2953/3393 [23:41<03:34,  2.05batch/s, Batch Loss=0.2406, Avg Loss=0.2448, Time Left=4.25 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2953/3393 [23:42<03:34,  2.05batch/s, Batch Loss=0.4921, Avg Loss=0.2449, Time Left=4.24 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2954/3393 [23:42<03:37,  2.01batch/s, Batch Loss=0.4921, Avg Loss=0.2449, Time Left=4.24 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2954/3393 [23:42<03:37,  2.01batch/s, Batch Loss=0.0956, Avg Loss=0.2448, Time Left=4.23 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2955/3393 [23:42<03:35,  2.03batch/s, Batch Loss=0.0956, Avg Loss=0.2448, Time Left=4.23 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2955/3393 [23:43<03:35,  2.03batch/s, Batch Loss=0.0986, Avg Loss=0.2448, Time Left=4.23 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2956/3393 [23:43<03:38,  2.00batch/s, Batch Loss=0.0986, Avg Loss=0.2448, Time Left=4.23 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2956/3393 [23:43<03:38,  2.00batch/s, Batch Loss=0.0214, Avg Loss=0.2447, Time Left=4.22 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2957/3393 [23:43<03:36,  2.02batch/s, Batch Loss=0.0214, Avg Loss=0.2447, Time Left=4.22 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2957/3393 [23:44<03:36,  2.02batch/s, Batch Loss=0.1479, Avg Loss=0.2447, Time Left=4.21 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2958/3393 [23:44<03:38,  1.99batch/s, Batch Loss=0.1479, Avg Loss=0.2447, Time Left=4.21 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2958/3393 [23:44<03:38,  1.99batch/s, Batch Loss=0.1533, Avg Loss=0.2446, Time Left=4.20 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2959/3393 [23:44<03:37,  1.99batch/s, Batch Loss=0.1533, Avg Loss=0.2446, Time Left=4.20 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2959/3393 [23:45<03:37,  1.99batch/s, Batch Loss=0.1473, Avg Loss=0.2446, Time Left=4.19 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2960/3393 [23:45<03:32,  2.03batch/s, Batch Loss=0.1473, Avg Loss=0.2446, Time Left=4.19 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2960/3393 [23:45<03:32,  2.03batch/s, Batch Loss=0.1797, Avg Loss=0.2446, Time Left=4.18 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2961/3393 [23:45<03:31,  2.04batch/s, Batch Loss=0.1797, Avg Loss=0.2446, Time Left=4.18 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2961/3393 [23:46<03:31,  2.04batch/s, Batch Loss=0.0383, Avg Loss=0.2445, Time Left=4.18 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2962/3393 [23:46<03:29,  2.05batch/s, Batch Loss=0.0383, Avg Loss=0.2445, Time Left=4.18 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2962/3393 [23:46<03:29,  2.05batch/s, Batch Loss=0.0458, Avg Loss=0.2444, Time Left=4.17 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2963/3393 [23:46<03:37,  1.98batch/s, Batch Loss=0.0458, Avg Loss=0.2444, Time Left=4.17 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2963/3393 [23:47<03:37,  1.98batch/s, Batch Loss=0.1410, Avg Loss=0.2444, Time Left=4.16 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2964/3393 [23:47<03:34,  2.00batch/s, Batch Loss=0.1410, Avg Loss=0.2444, Time Left=4.16 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2964/3393 [23:47<03:34,  2.00batch/s, Batch Loss=0.0411, Avg Loss=0.2443, Time Left=4.15 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2965/3393 [23:47<03:35,  1.98batch/s, Batch Loss=0.0411, Avg Loss=0.2443, Time Left=4.15 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2965/3393 [23:48<03:35,  1.98batch/s, Batch Loss=0.1802, Avg Loss=0.2443, Time Left=4.14 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2966/3393 [23:48<03:35,  1.98batch/s, Batch Loss=0.1802, Avg Loss=0.2443, Time Left=4.14 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2966/3393 [23:48<03:35,  1.98batch/s, Batch Loss=0.0695, Avg Loss=0.2443, Time Left=4.14 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2967/3393 [23:48<03:29,  2.03batch/s, Batch Loss=0.0695, Avg Loss=0.2443, Time Left=4.14 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2967/3393 [23:49<03:29,  2.03batch/s, Batch Loss=0.0232, Avg Loss=0.2442, Time Left=4.13 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  87%|▊| 2968/3393 [23:49<03:32,  2.00batch/s, Batch Loss=0.0232, Avg Loss=0.2442, Time Left=4.13 \u001b[A\n",
      "Epoch 1/3 - Training:  87%|▊| 2968/3393 [23:49<03:32,  2.00batch/s, Batch Loss=0.2771, Avg Loss=0.2442, Time Left=4.12 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2969/3393 [23:49<03:27,  2.04batch/s, Batch Loss=0.2771, Avg Loss=0.2442, Time Left=4.12 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2969/3393 [23:50<03:27,  2.04batch/s, Batch Loss=0.3018, Avg Loss=0.2442, Time Left=4.11 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2970/3393 [23:50<03:32,  1.99batch/s, Batch Loss=0.3018, Avg Loss=0.2442, Time Left=4.11 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2970/3393 [23:50<03:32,  1.99batch/s, Batch Loss=0.0108, Avg Loss=0.2441, Time Left=4.10 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2971/3393 [23:50<03:32,  1.99batch/s, Batch Loss=0.0108, Avg Loss=0.2441, Time Left=4.10 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2971/3393 [23:51<03:32,  1.99batch/s, Batch Loss=0.0768, Avg Loss=0.2441, Time Left=4.09 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2972/3393 [23:51<03:31,  1.99batch/s, Batch Loss=0.0768, Avg Loss=0.2441, Time Left=4.09 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2972/3393 [23:51<03:31,  1.99batch/s, Batch Loss=0.1304, Avg Loss=0.2440, Time Left=4.09 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2973/3393 [23:51<03:26,  2.03batch/s, Batch Loss=0.1304, Avg Loss=0.2440, Time Left=4.09 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2973/3393 [23:52<03:26,  2.03batch/s, Batch Loss=0.1767, Avg Loss=0.2440, Time Left=4.08 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2974/3393 [23:52<03:31,  1.99batch/s, Batch Loss=0.1767, Avg Loss=0.2440, Time Left=4.08 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2974/3393 [23:52<03:31,  1.99batch/s, Batch Loss=0.1642, Avg Loss=0.2440, Time Left=4.07 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2975/3393 [23:52<03:28,  2.00batch/s, Batch Loss=0.1642, Avg Loss=0.2440, Time Left=4.07 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2975/3393 [23:53<03:28,  2.00batch/s, Batch Loss=0.0166, Avg Loss=0.2439, Time Left=4.06 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2976/3393 [23:53<03:25,  2.03batch/s, Batch Loss=0.0166, Avg Loss=0.2439, Time Left=4.06 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2976/3393 [23:53<03:25,  2.03batch/s, Batch Loss=0.0208, Avg Loss=0.2438, Time Left=4.05 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2977/3393 [23:53<03:27,  2.00batch/s, Batch Loss=0.0208, Avg Loss=0.2438, Time Left=4.05 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2977/3393 [23:54<03:27,  2.00batch/s, Batch Loss=0.3646, Avg Loss=0.2439, Time Left=4.04 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2978/3393 [23:54<03:29,  1.98batch/s, Batch Loss=0.3646, Avg Loss=0.2439, Time Left=4.04 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2978/3393 [23:54<03:29,  1.98batch/s, Batch Loss=0.3713, Avg Loss=0.2439, Time Left=4.04 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2979/3393 [23:54<03:28,  1.98batch/s, Batch Loss=0.3713, Avg Loss=0.2439, Time Left=4.04 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2979/3393 [23:55<03:28,  1.98batch/s, Batch Loss=0.1125, Avg Loss=0.2439, Time Left=4.03 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2980/3393 [23:55<03:25,  2.01batch/s, Batch Loss=0.1125, Avg Loss=0.2439, Time Left=4.03 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2980/3393 [23:55<03:25,  2.01batch/s, Batch Loss=0.0388, Avg Loss=0.2438, Time Left=4.02 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2981/3393 [23:55<03:27,  1.99batch/s, Batch Loss=0.0388, Avg Loss=0.2438, Time Left=4.02 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2981/3393 [23:56<03:27,  1.99batch/s, Batch Loss=0.3527, Avg Loss=0.2438, Time Left=4.01 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2982/3393 [23:56<03:22,  2.03batch/s, Batch Loss=0.3527, Avg Loss=0.2438, Time Left=4.01 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2982/3393 [23:56<03:22,  2.03batch/s, Batch Loss=0.1481, Avg Loss=0.2438, Time Left=4.00 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2983/3393 [23:56<03:23,  2.02batch/s, Batch Loss=0.1481, Avg Loss=0.2438, Time Left=4.00 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2983/3393 [23:57<03:23,  2.02batch/s, Batch Loss=0.0412, Avg Loss=0.2437, Time Left=4.00 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2984/3393 [23:57<03:22,  2.02batch/s, Batch Loss=0.0412, Avg Loss=0.2437, Time Left=4.00 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2984/3393 [23:57<03:22,  2.02batch/s, Batch Loss=0.0929, Avg Loss=0.2437, Time Left=3.99 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2985/3393 [23:57<03:21,  2.03batch/s, Batch Loss=0.0929, Avg Loss=0.2437, Time Left=3.99 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2985/3393 [23:58<03:21,  2.03batch/s, Batch Loss=0.0993, Avg Loss=0.2436, Time Left=3.98 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2986/3393 [23:58<03:29,  1.94batch/s, Batch Loss=0.0993, Avg Loss=0.2436, Time Left=3.98 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2986/3393 [23:58<03:29,  1.94batch/s, Batch Loss=0.1015, Avg Loss=0.2436, Time Left=3.97 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2987/3393 [23:58<03:25,  1.97batch/s, Batch Loss=0.1015, Avg Loss=0.2436, Time Left=3.97 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2987/3393 [23:59<03:25,  1.97batch/s, Batch Loss=0.0367, Avg Loss=0.2435, Time Left=3.96 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2988/3393 [23:59<03:24,  1.98batch/s, Batch Loss=0.0367, Avg Loss=0.2435, Time Left=3.96 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2988/3393 [23:59<03:24,  1.98batch/s, Batch Loss=0.1814, Avg Loss=0.2435, Time Left=3.95 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2989/3393 [23:59<03:19,  2.03batch/s, Batch Loss=0.1814, Avg Loss=0.2435, Time Left=3.95 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2989/3393 [24:00<03:19,  2.03batch/s, Batch Loss=0.5024, Avg Loss=0.2436, Time Left=3.95 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2990/3393 [24:00<03:25,  1.96batch/s, Batch Loss=0.5024, Avg Loss=0.2436, Time Left=3.95 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2990/3393 [24:00<03:25,  1.96batch/s, Batch Loss=0.0623, Avg Loss=0.2435, Time Left=3.94 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2991/3393 [24:00<03:22,  1.99batch/s, Batch Loss=0.0623, Avg Loss=0.2435, Time Left=3.94 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2991/3393 [24:01<03:22,  1.99batch/s, Batch Loss=0.1903, Avg Loss=0.2435, Time Left=3.93 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2992/3393 [24:01<03:17,  2.03batch/s, Batch Loss=0.1903, Avg Loss=0.2435, Time Left=3.93 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2992/3393 [24:01<03:17,  2.03batch/s, Batch Loss=0.3719, Avg Loss=0.2435, Time Left=3.92 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2993/3393 [24:01<03:14,  2.06batch/s, Batch Loss=0.3719, Avg Loss=0.2435, Time Left=3.92 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2993/3393 [24:02<03:14,  2.06batch/s, Batch Loss=0.0849, Avg Loss=0.2435, Time Left=3.91 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2994/3393 [24:02<03:11,  2.09batch/s, Batch Loss=0.0849, Avg Loss=0.2435, Time Left=3.91 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2994/3393 [24:02<03:11,  2.09batch/s, Batch Loss=0.0484, Avg Loss=0.2434, Time Left=3.90 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2995/3393 [24:02<03:11,  2.08batch/s, Batch Loss=0.0484, Avg Loss=0.2434, Time Left=3.90 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2995/3393 [24:03<03:11,  2.08batch/s, Batch Loss=0.4274, Avg Loss=0.2435, Time Left=3.90 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2996/3393 [24:03<03:13,  2.06batch/s, Batch Loss=0.4274, Avg Loss=0.2435, Time Left=3.90 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2996/3393 [24:03<03:13,  2.06batch/s, Batch Loss=0.2397, Avg Loss=0.2435, Time Left=3.89 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2997/3393 [24:03<03:12,  2.06batch/s, Batch Loss=0.2397, Avg Loss=0.2435, Time Left=3.89 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2997/3393 [24:04<03:12,  2.06batch/s, Batch Loss=0.2457, Avg Loss=0.2435, Time Left=3.88 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2998/3393 [24:04<03:15,  2.02batch/s, Batch Loss=0.2457, Avg Loss=0.2435, Time Left=3.88 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2998/3393 [24:04<03:15,  2.02batch/s, Batch Loss=0.1271, Avg Loss=0.2434, Time Left=3.87 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2999/3393 [24:04<03:13,  2.03batch/s, Batch Loss=0.1271, Avg Loss=0.2434, Time Left=3.87 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 2999/3393 [24:05<03:13,  2.03batch/s, Batch Loss=0.0273, Avg Loss=0.2434, Time Left=3.86 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 3000/3393 [24:05<03:12,  2.04batch/s, Batch Loss=0.0273, Avg Loss=0.2434, Time Left=3.86 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 3000/3393 [24:05<03:12,  2.04batch/s, Batch Loss=0.1132, Avg Loss=0.2433, Time Left=3.86 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  88%|▉| 3001/3393 [24:05<03:11,  2.05batch/s, Batch Loss=0.1132, Avg Loss=0.2433, Time Left=3.86 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 3001/3393 [24:06<03:11,  2.05batch/s, Batch Loss=0.1285, Avg Loss=0.2433, Time Left=3.85 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 3002/3393 [24:06<03:08,  2.07batch/s, Batch Loss=0.1285, Avg Loss=0.2433, Time Left=3.85 \u001b[A\n",
      "Epoch 1/3 - Training:  88%|▉| 3002/3393 [24:06<03:08,  2.07batch/s, Batch Loss=0.0455, Avg Loss=0.2432, Time Left=3.84 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3003/3393 [24:06<03:06,  2.10batch/s, Batch Loss=0.0455, Avg Loss=0.2432, Time Left=3.84 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3003/3393 [24:07<03:06,  2.10batch/s, Batch Loss=0.0979, Avg Loss=0.2432, Time Left=3.83 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3004/3393 [24:07<03:09,  2.05batch/s, Batch Loss=0.0979, Avg Loss=0.2432, Time Left=3.83 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3004/3393 [24:07<03:09,  2.05batch/s, Batch Loss=0.1088, Avg Loss=0.2431, Time Left=3.82 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3005/3393 [24:07<03:08,  2.05batch/s, Batch Loss=0.1088, Avg Loss=0.2431, Time Left=3.82 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3005/3393 [24:08<03:08,  2.05batch/s, Batch Loss=0.2485, Avg Loss=0.2431, Time Left=3.81 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3006/3393 [24:08<03:09,  2.04batch/s, Batch Loss=0.2485, Avg Loss=0.2431, Time Left=3.81 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3006/3393 [24:08<03:09,  2.04batch/s, Batch Loss=0.1088, Avg Loss=0.2431, Time Left=3.81 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3007/3393 [24:08<03:10,  2.02batch/s, Batch Loss=0.1088, Avg Loss=0.2431, Time Left=3.81 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3007/3393 [24:09<03:10,  2.02batch/s, Batch Loss=0.2309, Avg Loss=0.2431, Time Left=3.80 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3008/3393 [24:09<03:07,  2.06batch/s, Batch Loss=0.2309, Avg Loss=0.2431, Time Left=3.80 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3008/3393 [24:09<03:07,  2.06batch/s, Batch Loss=0.3065, Avg Loss=0.2431, Time Left=3.79 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3009/3393 [24:09<03:08,  2.04batch/s, Batch Loss=0.3065, Avg Loss=0.2431, Time Left=3.79 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3009/3393 [24:10<03:08,  2.04batch/s, Batch Loss=0.3025, Avg Loss=0.2431, Time Left=3.78 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3010/3393 [24:10<03:07,  2.05batch/s, Batch Loss=0.3025, Avg Loss=0.2431, Time Left=3.78 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3010/3393 [24:10<03:07,  2.05batch/s, Batch Loss=0.1826, Avg Loss=0.2431, Time Left=3.77 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3011/3393 [24:10<03:09,  2.02batch/s, Batch Loss=0.1826, Avg Loss=0.2431, Time Left=3.77 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3011/3393 [24:11<03:09,  2.02batch/s, Batch Loss=0.0379, Avg Loss=0.2430, Time Left=3.76 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3012/3393 [24:11<03:07,  2.03batch/s, Batch Loss=0.0379, Avg Loss=0.2430, Time Left=3.76 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3012/3393 [24:11<03:07,  2.03batch/s, Batch Loss=0.0749, Avg Loss=0.2430, Time Left=3.76 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3013/3393 [24:11<03:04,  2.06batch/s, Batch Loss=0.0749, Avg Loss=0.2430, Time Left=3.76 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3013/3393 [24:12<03:04,  2.06batch/s, Batch Loss=0.1177, Avg Loss=0.2429, Time Left=3.75 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3014/3393 [24:12<03:03,  2.06batch/s, Batch Loss=0.1177, Avg Loss=0.2429, Time Left=3.75 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3014/3393 [24:12<03:03,  2.06batch/s, Batch Loss=0.1468, Avg Loss=0.2429, Time Left=3.74 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3015/3393 [24:12<03:03,  2.06batch/s, Batch Loss=0.1468, Avg Loss=0.2429, Time Left=3.74 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3015/3393 [24:13<03:03,  2.06batch/s, Batch Loss=0.1166, Avg Loss=0.2429, Time Left=3.73 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3016/3393 [24:13<03:02,  2.06batch/s, Batch Loss=0.1166, Avg Loss=0.2429, Time Left=3.73 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3016/3393 [24:13<03:02,  2.06batch/s, Batch Loss=0.0694, Avg Loss=0.2428, Time Left=3.72 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3017/3393 [24:13<03:02,  2.07batch/s, Batch Loss=0.0694, Avg Loss=0.2428, Time Left=3.72 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3017/3393 [24:13<03:02,  2.07batch/s, Batch Loss=0.2336, Avg Loss=0.2428, Time Left=3.71 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3018/3393 [24:13<03:01,  2.06batch/s, Batch Loss=0.2336, Avg Loss=0.2428, Time Left=3.71 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3018/3393 [24:14<03:01,  2.06batch/s, Batch Loss=0.2783, Avg Loss=0.2428, Time Left=3.71 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3019/3393 [24:14<03:04,  2.03batch/s, Batch Loss=0.2783, Avg Loss=0.2428, Time Left=3.71 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3019/3393 [24:15<03:04,  2.03batch/s, Batch Loss=0.3109, Avg Loss=0.2428, Time Left=3.70 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3020/3393 [24:15<03:04,  2.02batch/s, Batch Loss=0.3109, Avg Loss=0.2428, Time Left=3.70 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3020/3393 [24:15<03:04,  2.02batch/s, Batch Loss=0.1331, Avg Loss=0.2428, Time Left=3.69 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3021/3393 [24:15<03:01,  2.05batch/s, Batch Loss=0.1331, Avg Loss=0.2428, Time Left=3.69 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3021/3393 [24:15<03:01,  2.05batch/s, Batch Loss=0.3815, Avg Loss=0.2428, Time Left=3.68 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3022/3393 [24:15<03:00,  2.05batch/s, Batch Loss=0.3815, Avg Loss=0.2428, Time Left=3.68 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3022/3393 [24:16<03:00,  2.05batch/s, Batch Loss=0.1523, Avg Loss=0.2428, Time Left=3.67 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3023/3393 [24:16<03:00,  2.05batch/s, Batch Loss=0.1523, Avg Loss=0.2428, Time Left=3.67 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3023/3393 [24:16<03:00,  2.05batch/s, Batch Loss=0.2334, Avg Loss=0.2428, Time Left=3.67 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3024/3393 [24:16<02:57,  2.08batch/s, Batch Loss=0.2334, Avg Loss=0.2428, Time Left=3.67 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3024/3393 [24:17<02:57,  2.08batch/s, Batch Loss=0.4294, Avg Loss=0.2429, Time Left=3.66 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3025/3393 [24:17<02:56,  2.08batch/s, Batch Loss=0.4294, Avg Loss=0.2429, Time Left=3.66 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3025/3393 [24:17<02:56,  2.08batch/s, Batch Loss=0.3463, Avg Loss=0.2429, Time Left=3.65 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3026/3393 [24:17<02:54,  2.10batch/s, Batch Loss=0.3463, Avg Loss=0.2429, Time Left=3.65 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3026/3393 [24:18<02:54,  2.10batch/s, Batch Loss=0.1028, Avg Loss=0.2429, Time Left=3.64 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3027/3393 [24:18<02:56,  2.07batch/s, Batch Loss=0.1028, Avg Loss=0.2429, Time Left=3.64 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3027/3393 [24:18<02:56,  2.07batch/s, Batch Loss=0.0325, Avg Loss=0.2428, Time Left=3.63 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3028/3393 [24:18<02:54,  2.09batch/s, Batch Loss=0.0325, Avg Loss=0.2428, Time Left=3.63 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3028/3393 [24:19<02:54,  2.09batch/s, Batch Loss=0.0338, Avg Loss=0.2427, Time Left=3.62 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3029/3393 [24:19<02:54,  2.08batch/s, Batch Loss=0.0338, Avg Loss=0.2427, Time Left=3.62 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3029/3393 [24:19<02:54,  2.08batch/s, Batch Loss=0.1686, Avg Loss=0.2427, Time Left=3.62 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3030/3393 [24:19<02:58,  2.04batch/s, Batch Loss=0.1686, Avg Loss=0.2427, Time Left=3.62 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3030/3393 [24:20<02:58,  2.04batch/s, Batch Loss=0.2644, Avg Loss=0.2427, Time Left=3.61 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3031/3393 [24:20<02:58,  2.03batch/s, Batch Loss=0.2644, Avg Loss=0.2427, Time Left=3.61 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3031/3393 [24:20<02:58,  2.03batch/s, Batch Loss=0.0754, Avg Loss=0.2426, Time Left=3.60 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3032/3393 [24:20<02:55,  2.06batch/s, Batch Loss=0.0754, Avg Loss=0.2426, Time Left=3.60 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3032/3393 [24:21<02:55,  2.06batch/s, Batch Loss=0.1348, Avg Loss=0.2426, Time Left=3.59 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3033/3393 [24:21<02:58,  2.02batch/s, Batch Loss=0.1348, Avg Loss=0.2426, Time Left=3.59 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3033/3393 [24:21<02:58,  2.02batch/s, Batch Loss=0.1371, Avg Loss=0.2426, Time Left=3.58 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  89%|▉| 3034/3393 [24:21<02:54,  2.05batch/s, Batch Loss=0.1371, Avg Loss=0.2426, Time Left=3.58 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3034/3393 [24:22<02:54,  2.05batch/s, Batch Loss=0.1943, Avg Loss=0.2425, Time Left=3.57 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3035/3393 [24:22<02:55,  2.04batch/s, Batch Loss=0.1943, Avg Loss=0.2425, Time Left=3.57 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3035/3393 [24:22<02:55,  2.04batch/s, Batch Loss=0.1378, Avg Loss=0.2425, Time Left=3.57 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3036/3393 [24:22<02:52,  2.07batch/s, Batch Loss=0.1378, Avg Loss=0.2425, Time Left=3.57 \u001b[A\n",
      "Epoch 1/3 - Training:  89%|▉| 3036/3393 [24:23<02:52,  2.07batch/s, Batch Loss=0.2013, Avg Loss=0.2425, Time Left=3.56 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3037/3393 [24:23<02:50,  2.09batch/s, Batch Loss=0.2013, Avg Loss=0.2425, Time Left=3.56 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3037/3393 [24:23<02:50,  2.09batch/s, Batch Loss=0.0816, Avg Loss=0.2424, Time Left=3.55 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3038/3393 [24:23<02:55,  2.02batch/s, Batch Loss=0.0816, Avg Loss=0.2424, Time Left=3.55 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3038/3393 [24:24<02:55,  2.02batch/s, Batch Loss=0.2556, Avg Loss=0.2424, Time Left=3.54 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3039/3393 [24:24<02:54,  2.03batch/s, Batch Loss=0.2556, Avg Loss=0.2424, Time Left=3.54 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3039/3393 [24:24<02:54,  2.03batch/s, Batch Loss=0.0677, Avg Loss=0.2424, Time Left=3.53 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3040/3393 [24:24<02:57,  1.98batch/s, Batch Loss=0.0677, Avg Loss=0.2424, Time Left=3.53 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3040/3393 [24:25<02:57,  1.98batch/s, Batch Loss=0.0387, Avg Loss=0.2423, Time Left=3.53 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3041/3393 [24:25<02:52,  2.05batch/s, Batch Loss=0.0387, Avg Loss=0.2423, Time Left=3.53 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3041/3393 [24:25<02:52,  2.05batch/s, Batch Loss=0.0832, Avg Loss=0.2423, Time Left=3.52 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3042/3393 [24:25<02:52,  2.03batch/s, Batch Loss=0.0832, Avg Loss=0.2423, Time Left=3.52 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3042/3393 [24:26<02:52,  2.03batch/s, Batch Loss=0.0313, Avg Loss=0.2422, Time Left=3.51 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3043/3393 [24:26<02:49,  2.06batch/s, Batch Loss=0.0313, Avg Loss=0.2422, Time Left=3.51 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3043/3393 [24:26<02:49,  2.06batch/s, Batch Loss=0.0483, Avg Loss=0.2421, Time Left=3.50 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3044/3393 [24:26<02:47,  2.09batch/s, Batch Loss=0.0483, Avg Loss=0.2421, Time Left=3.50 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3044/3393 [24:27<02:47,  2.09batch/s, Batch Loss=0.2189, Avg Loss=0.2421, Time Left=3.49 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3045/3393 [24:27<02:47,  2.08batch/s, Batch Loss=0.2189, Avg Loss=0.2421, Time Left=3.49 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3045/3393 [24:27<02:47,  2.08batch/s, Batch Loss=0.0163, Avg Loss=0.2420, Time Left=3.48 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3046/3393 [24:27<02:48,  2.06batch/s, Batch Loss=0.0163, Avg Loss=0.2420, Time Left=3.48 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3046/3393 [24:28<02:48,  2.06batch/s, Batch Loss=0.1272, Avg Loss=0.2420, Time Left=3.48 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3047/3393 [24:28<02:46,  2.08batch/s, Batch Loss=0.1272, Avg Loss=0.2420, Time Left=3.48 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3047/3393 [24:28<02:46,  2.08batch/s, Batch Loss=0.1173, Avg Loss=0.2420, Time Left=3.47 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3048/3393 [24:28<02:52,  2.00batch/s, Batch Loss=0.1173, Avg Loss=0.2420, Time Left=3.47 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3048/3393 [24:29<02:52,  2.00batch/s, Batch Loss=0.0485, Avg Loss=0.2419, Time Left=3.46 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3049/3393 [24:29<02:50,  2.01batch/s, Batch Loss=0.0485, Avg Loss=0.2419, Time Left=3.46 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3049/3393 [24:29<02:50,  2.01batch/s, Batch Loss=0.2286, Avg Loss=0.2419, Time Left=3.45 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3050/3393 [24:29<02:53,  1.97batch/s, Batch Loss=0.2286, Avg Loss=0.2419, Time Left=3.45 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3050/3393 [24:30<02:53,  1.97batch/s, Batch Loss=0.3989, Avg Loss=0.2419, Time Left=3.44 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3051/3393 [24:30<02:51,  2.00batch/s, Batch Loss=0.3989, Avg Loss=0.2419, Time Left=3.44 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3051/3393 [24:30<02:51,  2.00batch/s, Batch Loss=0.0444, Avg Loss=0.2419, Time Left=3.43 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3052/3393 [24:30<02:53,  1.96batch/s, Batch Loss=0.0444, Avg Loss=0.2419, Time Left=3.43 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3052/3393 [24:31<02:53,  1.96batch/s, Batch Loss=0.1570, Avg Loss=0.2419, Time Left=3.43 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3053/3393 [24:31<02:52,  1.97batch/s, Batch Loss=0.1570, Avg Loss=0.2419, Time Left=3.43 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3053/3393 [24:31<02:52,  1.97batch/s, Batch Loss=0.3072, Avg Loss=0.2419, Time Left=3.42 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3054/3393 [24:31<02:52,  1.96batch/s, Batch Loss=0.3072, Avg Loss=0.2419, Time Left=3.42 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3054/3393 [24:32<02:52,  1.96batch/s, Batch Loss=0.0144, Avg Loss=0.2418, Time Left=3.41 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3055/3393 [24:32<02:46,  2.03batch/s, Batch Loss=0.0144, Avg Loss=0.2418, Time Left=3.41 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3055/3393 [24:32<02:46,  2.03batch/s, Batch Loss=0.2658, Avg Loss=0.2418, Time Left=3.40 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3056/3393 [24:32<02:45,  2.04batch/s, Batch Loss=0.2658, Avg Loss=0.2418, Time Left=3.40 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3056/3393 [24:33<02:45,  2.04batch/s, Batch Loss=0.3208, Avg Loss=0.2418, Time Left=3.39 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3057/3393 [24:33<02:45,  2.03batch/s, Batch Loss=0.3208, Avg Loss=0.2418, Time Left=3.39 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3057/3393 [24:33<02:45,  2.03batch/s, Batch Loss=0.0136, Avg Loss=0.2418, Time Left=3.39 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3058/3393 [24:33<02:45,  2.02batch/s, Batch Loss=0.0136, Avg Loss=0.2418, Time Left=3.39 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3058/3393 [24:34<02:45,  2.02batch/s, Batch Loss=0.2142, Avg Loss=0.2417, Time Left=3.38 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3059/3393 [24:34<02:47,  1.99batch/s, Batch Loss=0.2142, Avg Loss=0.2417, Time Left=3.38 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3059/3393 [24:34<02:47,  1.99batch/s, Batch Loss=0.0750, Avg Loss=0.2417, Time Left=3.37 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3060/3393 [24:34<02:44,  2.02batch/s, Batch Loss=0.0750, Avg Loss=0.2417, Time Left=3.37 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3060/3393 [24:35<02:44,  2.02batch/s, Batch Loss=0.1199, Avg Loss=0.2417, Time Left=3.36 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3061/3393 [24:35<02:42,  2.05batch/s, Batch Loss=0.1199, Avg Loss=0.2417, Time Left=3.36 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3061/3393 [24:35<02:42,  2.05batch/s, Batch Loss=0.2516, Avg Loss=0.2417, Time Left=3.35 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3062/3393 [24:35<02:44,  2.02batch/s, Batch Loss=0.2516, Avg Loss=0.2417, Time Left=3.35 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3062/3393 [24:36<02:44,  2.02batch/s, Batch Loss=0.0545, Avg Loss=0.2416, Time Left=3.34 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3063/3393 [24:36<02:42,  2.03batch/s, Batch Loss=0.0545, Avg Loss=0.2416, Time Left=3.34 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3063/3393 [24:36<02:42,  2.03batch/s, Batch Loss=0.0838, Avg Loss=0.2415, Time Left=3.34 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3064/3393 [24:36<02:44,  2.00batch/s, Batch Loss=0.0838, Avg Loss=0.2415, Time Left=3.34 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3064/3393 [24:37<02:44,  2.00batch/s, Batch Loss=0.1288, Avg Loss=0.2415, Time Left=3.33 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3065/3393 [24:37<02:42,  2.02batch/s, Batch Loss=0.1288, Avg Loss=0.2415, Time Left=3.33 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3065/3393 [24:37<02:42,  2.02batch/s, Batch Loss=0.0891, Avg Loss=0.2415, Time Left=3.32 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3066/3393 [24:37<02:43,  1.99batch/s, Batch Loss=0.0891, Avg Loss=0.2415, Time Left=3.32 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3066/3393 [24:38<02:43,  1.99batch/s, Batch Loss=0.2017, Avg Loss=0.2414, Time Left=3.31 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  90%|▉| 3067/3393 [24:38<02:40,  2.03batch/s, Batch Loss=0.2017, Avg Loss=0.2414, Time Left=3.31 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3067/3393 [24:38<02:40,  2.03batch/s, Batch Loss=0.0783, Avg Loss=0.2414, Time Left=3.30 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3068/3393 [24:38<02:39,  2.04batch/s, Batch Loss=0.0783, Avg Loss=0.2414, Time Left=3.30 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3068/3393 [24:39<02:39,  2.04batch/s, Batch Loss=0.1752, Avg Loss=0.2414, Time Left=3.29 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3069/3393 [24:39<02:40,  2.01batch/s, Batch Loss=0.1752, Avg Loss=0.2414, Time Left=3.29 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3069/3393 [24:39<02:40,  2.01batch/s, Batch Loss=0.2195, Avg Loss=0.2414, Time Left=3.29 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3070/3393 [24:39<02:39,  2.03batch/s, Batch Loss=0.2195, Avg Loss=0.2414, Time Left=3.29 \u001b[A\n",
      "Epoch 1/3 - Training:  90%|▉| 3070/3393 [24:40<02:39,  2.03batch/s, Batch Loss=0.1315, Avg Loss=0.2413, Time Left=3.28 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3071/3393 [24:40<02:36,  2.06batch/s, Batch Loss=0.1315, Avg Loss=0.2413, Time Left=3.28 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3071/3393 [24:40<02:36,  2.06batch/s, Batch Loss=0.0675, Avg Loss=0.2413, Time Left=3.27 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3072/3393 [24:40<02:39,  2.02batch/s, Batch Loss=0.0675, Avg Loss=0.2413, Time Left=3.27 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3072/3393 [24:41<02:39,  2.02batch/s, Batch Loss=0.1617, Avg Loss=0.2412, Time Left=3.26 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3073/3393 [24:41<02:37,  2.04batch/s, Batch Loss=0.1617, Avg Loss=0.2412, Time Left=3.26 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3073/3393 [24:41<02:37,  2.04batch/s, Batch Loss=0.2613, Avg Loss=0.2412, Time Left=3.25 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3074/3393 [24:41<02:37,  2.03batch/s, Batch Loss=0.2613, Avg Loss=0.2412, Time Left=3.25 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3074/3393 [24:42<02:37,  2.03batch/s, Batch Loss=0.2401, Avg Loss=0.2412, Time Left=3.25 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3075/3393 [24:42<02:36,  2.04batch/s, Batch Loss=0.2401, Avg Loss=0.2412, Time Left=3.25 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3075/3393 [24:42<02:36,  2.04batch/s, Batch Loss=0.0931, Avg Loss=0.2412, Time Left=3.24 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3076/3393 [24:42<02:35,  2.04batch/s, Batch Loss=0.0931, Avg Loss=0.2412, Time Left=3.24 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3076/3393 [24:42<02:35,  2.04batch/s, Batch Loss=0.1673, Avg Loss=0.2412, Time Left=3.23 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3077/3393 [24:42<02:33,  2.05batch/s, Batch Loss=0.1673, Avg Loss=0.2412, Time Left=3.23 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3077/3393 [24:43<02:33,  2.05batch/s, Batch Loss=0.0121, Avg Loss=0.2411, Time Left=3.22 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3078/3393 [24:43<02:31,  2.08batch/s, Batch Loss=0.0121, Avg Loss=0.2411, Time Left=3.22 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3078/3393 [24:43<02:31,  2.08batch/s, Batch Loss=0.0580, Avg Loss=0.2410, Time Left=3.21 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3079/3393 [24:43<02:34,  2.03batch/s, Batch Loss=0.0580, Avg Loss=0.2410, Time Left=3.21 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3079/3393 [24:44<02:34,  2.03batch/s, Batch Loss=0.1426, Avg Loss=0.2410, Time Left=3.20 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3080/3393 [24:44<02:31,  2.06batch/s, Batch Loss=0.1426, Avg Loss=0.2410, Time Left=3.20 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3080/3393 [24:44<02:31,  2.06batch/s, Batch Loss=0.1221, Avg Loss=0.2410, Time Left=3.20 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3081/3393 [24:44<02:29,  2.08batch/s, Batch Loss=0.1221, Avg Loss=0.2410, Time Left=3.20 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3081/3393 [24:45<02:29,  2.08batch/s, Batch Loss=0.0482, Avg Loss=0.2409, Time Left=3.19 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3082/3393 [24:45<02:33,  2.02batch/s, Batch Loss=0.0482, Avg Loss=0.2409, Time Left=3.19 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3082/3393 [24:45<02:33,  2.02batch/s, Batch Loss=0.0796, Avg Loss=0.2408, Time Left=3.18 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3083/3393 [24:45<02:31,  2.05batch/s, Batch Loss=0.0796, Avg Loss=0.2408, Time Left=3.18 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3083/3393 [24:46<02:31,  2.05batch/s, Batch Loss=0.3427, Avg Loss=0.2409, Time Left=3.17 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3084/3393 [24:46<02:30,  2.06batch/s, Batch Loss=0.3427, Avg Loss=0.2409, Time Left=3.17 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3084/3393 [24:46<02:30,  2.06batch/s, Batch Loss=0.4923, Avg Loss=0.2410, Time Left=3.16 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3085/3393 [24:46<02:29,  2.06batch/s, Batch Loss=0.4923, Avg Loss=0.2410, Time Left=3.16 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3085/3393 [24:47<02:29,  2.06batch/s, Batch Loss=0.1193, Avg Loss=0.2409, Time Left=3.15 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3086/3393 [24:47<02:27,  2.08batch/s, Batch Loss=0.1193, Avg Loss=0.2409, Time Left=3.15 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3086/3393 [24:47<02:27,  2.08batch/s, Batch Loss=0.2692, Avg Loss=0.2409, Time Left=3.15 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3087/3393 [24:47<02:27,  2.08batch/s, Batch Loss=0.2692, Avg Loss=0.2409, Time Left=3.15 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3087/3393 [24:48<02:27,  2.08batch/s, Batch Loss=0.0095, Avg Loss=0.2408, Time Left=3.14 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3088/3393 [24:48<02:27,  2.07batch/s, Batch Loss=0.0095, Avg Loss=0.2408, Time Left=3.14 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3088/3393 [24:48<02:27,  2.07batch/s, Batch Loss=0.0493, Avg Loss=0.2408, Time Left=3.13 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3089/3393 [24:48<02:26,  2.07batch/s, Batch Loss=0.0493, Avg Loss=0.2408, Time Left=3.13 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3089/3393 [24:49<02:26,  2.07batch/s, Batch Loss=0.3476, Avg Loss=0.2408, Time Left=3.12 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3090/3393 [24:49<02:29,  2.03batch/s, Batch Loss=0.3476, Avg Loss=0.2408, Time Left=3.12 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3090/3393 [24:49<02:29,  2.03batch/s, Batch Loss=0.0942, Avg Loss=0.2408, Time Left=3.11 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3091/3393 [24:49<02:26,  2.06batch/s, Batch Loss=0.0942, Avg Loss=0.2408, Time Left=3.11 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3091/3393 [24:50<02:26,  2.06batch/s, Batch Loss=0.0145, Avg Loss=0.2407, Time Left=3.10 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3092/3393 [24:50<02:25,  2.06batch/s, Batch Loss=0.0145, Avg Loss=0.2407, Time Left=3.10 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3092/3393 [24:50<02:25,  2.06batch/s, Batch Loss=0.2250, Avg Loss=0.2407, Time Left=3.10 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3093/3393 [24:50<02:28,  2.02batch/s, Batch Loss=0.2250, Avg Loss=0.2407, Time Left=3.10 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3093/3393 [24:51<02:28,  2.02batch/s, Batch Loss=0.0344, Avg Loss=0.2406, Time Left=3.09 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3094/3393 [24:51<02:26,  2.03batch/s, Batch Loss=0.0344, Avg Loss=0.2406, Time Left=3.09 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3094/3393 [24:51<02:26,  2.03batch/s, Batch Loss=0.0551, Avg Loss=0.2406, Time Left=3.08 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3095/3393 [24:51<02:27,  2.02batch/s, Batch Loss=0.0551, Avg Loss=0.2406, Time Left=3.08 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3095/3393 [24:52<02:27,  2.02batch/s, Batch Loss=0.0404, Avg Loss=0.2405, Time Left=3.07 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3096/3393 [24:52<02:24,  2.06batch/s, Batch Loss=0.0404, Avg Loss=0.2405, Time Left=3.07 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3096/3393 [24:52<02:24,  2.06batch/s, Batch Loss=0.0196, Avg Loss=0.2404, Time Left=3.06 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3097/3393 [24:52<02:22,  2.08batch/s, Batch Loss=0.0196, Avg Loss=0.2404, Time Left=3.06 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3097/3393 [24:53<02:22,  2.08batch/s, Batch Loss=0.0568, Avg Loss=0.2404, Time Left=3.06 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3098/3393 [24:53<02:24,  2.04batch/s, Batch Loss=0.0568, Avg Loss=0.2404, Time Left=3.06 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3098/3393 [24:53<02:24,  2.04batch/s, Batch Loss=0.0509, Avg Loss=0.2403, Time Left=3.05 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3099/3393 [24:53<02:23,  2.05batch/s, Batch Loss=0.0509, Avg Loss=0.2403, Time Left=3.05 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3099/3393 [24:54<02:23,  2.05batch/s, Batch Loss=0.4099, Avg Loss=0.2404, Time Left=3.04 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  91%|▉| 3100/3393 [24:54<02:22,  2.05batch/s, Batch Loss=0.4099, Avg Loss=0.2404, Time Left=3.04 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3100/3393 [24:54<02:22,  2.05batch/s, Batch Loss=0.3419, Avg Loss=0.2404, Time Left=3.03 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3101/3393 [24:54<02:22,  2.06batch/s, Batch Loss=0.3419, Avg Loss=0.2404, Time Left=3.03 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3101/3393 [24:55<02:22,  2.06batch/s, Batch Loss=0.1081, Avg Loss=0.2403, Time Left=3.02 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3102/3393 [24:55<02:21,  2.06batch/s, Batch Loss=0.1081, Avg Loss=0.2403, Time Left=3.02 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3102/3393 [24:55<02:21,  2.06batch/s, Batch Loss=0.0385, Avg Loss=0.2403, Time Left=3.01 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3103/3393 [24:55<02:23,  2.02batch/s, Batch Loss=0.0385, Avg Loss=0.2403, Time Left=3.01 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3103/3393 [24:56<02:23,  2.02batch/s, Batch Loss=0.0128, Avg Loss=0.2402, Time Left=3.01 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3104/3393 [24:56<02:19,  2.07batch/s, Batch Loss=0.0128, Avg Loss=0.2402, Time Left=3.01 \u001b[A\n",
      "Epoch 1/3 - Training:  91%|▉| 3104/3393 [24:56<02:19,  2.07batch/s, Batch Loss=0.0381, Avg Loss=0.2401, Time Left=3.00 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3105/3393 [24:56<02:18,  2.07batch/s, Batch Loss=0.0381, Avg Loss=0.2401, Time Left=3.00 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3105/3393 [24:57<02:18,  2.07batch/s, Batch Loss=0.0064, Avg Loss=0.2401, Time Left=2.99 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3106/3393 [24:57<02:19,  2.06batch/s, Batch Loss=0.0064, Avg Loss=0.2401, Time Left=2.99 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3106/3393 [24:57<02:19,  2.06batch/s, Batch Loss=0.0900, Avg Loss=0.2400, Time Left=2.98 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3107/3393 [24:57<02:16,  2.09batch/s, Batch Loss=0.0900, Avg Loss=0.2400, Time Left=2.98 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3107/3393 [24:58<02:16,  2.09batch/s, Batch Loss=0.0191, Avg Loss=0.2399, Time Left=2.97 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3108/3393 [24:58<02:16,  2.09batch/s, Batch Loss=0.0191, Avg Loss=0.2399, Time Left=2.97 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3108/3393 [24:58<02:16,  2.09batch/s, Batch Loss=0.0216, Avg Loss=0.2399, Time Left=2.96 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3109/3393 [24:58<02:19,  2.04batch/s, Batch Loss=0.0216, Avg Loss=0.2399, Time Left=2.96 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3109/3393 [24:59<02:19,  2.04batch/s, Batch Loss=0.1376, Avg Loss=0.2398, Time Left=2.96 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3110/3393 [24:59<02:15,  2.09batch/s, Batch Loss=0.1376, Avg Loss=0.2398, Time Left=2.96 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3110/3393 [24:59<02:15,  2.09batch/s, Batch Loss=0.0645, Avg Loss=0.2398, Time Left=2.95 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3111/3393 [24:59<02:15,  2.08batch/s, Batch Loss=0.0645, Avg Loss=0.2398, Time Left=2.95 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3111/3393 [24:59<02:15,  2.08batch/s, Batch Loss=0.0175, Avg Loss=0.2397, Time Left=2.94 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3112/3393 [24:59<02:16,  2.06batch/s, Batch Loss=0.0175, Avg Loss=0.2397, Time Left=2.94 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3112/3393 [25:00<02:16,  2.06batch/s, Batch Loss=0.0384, Avg Loss=0.2396, Time Left=2.93 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3113/3393 [25:00<02:17,  2.04batch/s, Batch Loss=0.0384, Avg Loss=0.2396, Time Left=2.93 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3113/3393 [25:00<02:17,  2.04batch/s, Batch Loss=0.0471, Avg Loss=0.2396, Time Left=2.92 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3114/3393 [25:01<02:18,  2.01batch/s, Batch Loss=0.0471, Avg Loss=0.2396, Time Left=2.92 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3114/3393 [25:01<02:18,  2.01batch/s, Batch Loss=0.0290, Avg Loss=0.2395, Time Left=2.92 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3115/3393 [25:01<02:17,  2.02batch/s, Batch Loss=0.0290, Avg Loss=0.2395, Time Left=2.92 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3115/3393 [25:02<02:17,  2.02batch/s, Batch Loss=0.3917, Avg Loss=0.2395, Time Left=2.91 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3116/3393 [25:02<02:19,  1.98batch/s, Batch Loss=0.3917, Avg Loss=0.2395, Time Left=2.91 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3116/3393 [25:02<02:19,  1.98batch/s, Batch Loss=0.0176, Avg Loss=0.2395, Time Left=2.90 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3117/3393 [25:02<02:17,  2.00batch/s, Batch Loss=0.0176, Avg Loss=0.2395, Time Left=2.90 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3117/3393 [25:02<02:17,  2.00batch/s, Batch Loss=0.1807, Avg Loss=0.2395, Time Left=2.89 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3118/3393 [25:02<02:14,  2.04batch/s, Batch Loss=0.1807, Avg Loss=0.2395, Time Left=2.89 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3118/3393 [25:03<02:14,  2.04batch/s, Batch Loss=0.1482, Avg Loss=0.2394, Time Left=2.88 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3119/3393 [25:03<02:13,  2.05batch/s, Batch Loss=0.1482, Avg Loss=0.2394, Time Left=2.88 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3119/3393 [25:03<02:13,  2.05batch/s, Batch Loss=0.0836, Avg Loss=0.2394, Time Left=2.87 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3120/3393 [25:03<02:12,  2.05batch/s, Batch Loss=0.0836, Avg Loss=0.2394, Time Left=2.87 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3120/3393 [25:04<02:12,  2.05batch/s, Batch Loss=0.0480, Avg Loss=0.2393, Time Left=2.87 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3121/3393 [25:04<02:10,  2.08batch/s, Batch Loss=0.0480, Avg Loss=0.2393, Time Left=2.87 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3121/3393 [25:04<02:10,  2.08batch/s, Batch Loss=0.0347, Avg Loss=0.2392, Time Left=2.86 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3122/3393 [25:04<02:10,  2.07batch/s, Batch Loss=0.0347, Avg Loss=0.2392, Time Left=2.86 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3122/3393 [25:05<02:10,  2.07batch/s, Batch Loss=0.3594, Avg Loss=0.2393, Time Left=2.85 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3123/3393 [25:05<02:08,  2.10batch/s, Batch Loss=0.3594, Avg Loss=0.2393, Time Left=2.85 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3123/3393 [25:05<02:08,  2.10batch/s, Batch Loss=0.4172, Avg Loss=0.2393, Time Left=2.84 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3124/3393 [25:05<02:13,  2.01batch/s, Batch Loss=0.4172, Avg Loss=0.2393, Time Left=2.84 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3124/3393 [25:06<02:13,  2.01batch/s, Batch Loss=0.0056, Avg Loss=0.2393, Time Left=2.83 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3125/3393 [25:06<02:12,  2.02batch/s, Batch Loss=0.0056, Avg Loss=0.2393, Time Left=2.83 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3125/3393 [25:06<02:12,  2.02batch/s, Batch Loss=0.3565, Avg Loss=0.2393, Time Left=2.82 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3126/3393 [25:06<02:09,  2.05batch/s, Batch Loss=0.3565, Avg Loss=0.2393, Time Left=2.82 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3126/3393 [25:07<02:09,  2.05batch/s, Batch Loss=0.6680, Avg Loss=0.2394, Time Left=2.82 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3127/3393 [25:07<02:10,  2.04batch/s, Batch Loss=0.6680, Avg Loss=0.2394, Time Left=2.82 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3127/3393 [25:07<02:10,  2.04batch/s, Batch Loss=0.0569, Avg Loss=0.2394, Time Left=2.81 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3128/3393 [25:07<02:09,  2.05batch/s, Batch Loss=0.0569, Avg Loss=0.2394, Time Left=2.81 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3128/3393 [25:08<02:09,  2.05batch/s, Batch Loss=0.0802, Avg Loss=0.2393, Time Left=2.80 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3129/3393 [25:08<02:12,  2.00batch/s, Batch Loss=0.0802, Avg Loss=0.2393, Time Left=2.80 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3129/3393 [25:08<02:12,  2.00batch/s, Batch Loss=0.1339, Avg Loss=0.2393, Time Left=2.79 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3130/3393 [25:08<02:09,  2.03batch/s, Batch Loss=0.1339, Avg Loss=0.2393, Time Left=2.79 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3130/3393 [25:09<02:09,  2.03batch/s, Batch Loss=0.1839, Avg Loss=0.2393, Time Left=2.78 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3131/3393 [25:09<02:10,  2.01batch/s, Batch Loss=0.1839, Avg Loss=0.2393, Time Left=2.78 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3131/3393 [25:09<02:10,  2.01batch/s, Batch Loss=0.0325, Avg Loss=0.2392, Time Left=2.77 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3132/3393 [25:09<02:09,  2.02batch/s, Batch Loss=0.0325, Avg Loss=0.2392, Time Left=2.77 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3132/3393 [25:10<02:09,  2.02batch/s, Batch Loss=0.3364, Avg Loss=0.2392, Time Left=2.77 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  92%|▉| 3133/3393 [25:10<02:09,  2.01batch/s, Batch Loss=0.3364, Avg Loss=0.2392, Time Left=2.77 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3133/3393 [25:10<02:09,  2.01batch/s, Batch Loss=0.2244, Avg Loss=0.2392, Time Left=2.76 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3134/3393 [25:10<02:07,  2.03batch/s, Batch Loss=0.2244, Avg Loss=0.2392, Time Left=2.76 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3134/3393 [25:11<02:07,  2.03batch/s, Batch Loss=0.1500, Avg Loss=0.2392, Time Left=2.75 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3135/3393 [25:11<02:05,  2.06batch/s, Batch Loss=0.1500, Avg Loss=0.2392, Time Left=2.75 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3135/3393 [25:11<02:05,  2.06batch/s, Batch Loss=0.1577, Avg Loss=0.2392, Time Left=2.74 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3136/3393 [25:11<02:04,  2.06batch/s, Batch Loss=0.1577, Avg Loss=0.2392, Time Left=2.74 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3136/3393 [25:12<02:04,  2.06batch/s, Batch Loss=0.2105, Avg Loss=0.2392, Time Left=2.73 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3137/3393 [25:12<02:04,  2.06batch/s, Batch Loss=0.2105, Avg Loss=0.2392, Time Left=2.73 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3137/3393 [25:12<02:04,  2.06batch/s, Batch Loss=0.1059, Avg Loss=0.2391, Time Left=2.73 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3138/3393 [25:12<02:02,  2.09batch/s, Batch Loss=0.1059, Avg Loss=0.2391, Time Left=2.73 \u001b[A\n",
      "Epoch 1/3 - Training:  92%|▉| 3138/3393 [25:13<02:02,  2.09batch/s, Batch Loss=0.0728, Avg Loss=0.2391, Time Left=2.72 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3139/3393 [25:13<02:02,  2.08batch/s, Batch Loss=0.0728, Avg Loss=0.2391, Time Left=2.72 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3139/3393 [25:13<02:02,  2.08batch/s, Batch Loss=0.1261, Avg Loss=0.2390, Time Left=2.71 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3140/3393 [25:13<02:01,  2.08batch/s, Batch Loss=0.1261, Avg Loss=0.2390, Time Left=2.71 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3140/3393 [25:14<02:01,  2.08batch/s, Batch Loss=0.1215, Avg Loss=0.2390, Time Left=2.70 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3141/3393 [25:14<02:01,  2.08batch/s, Batch Loss=0.1215, Avg Loss=0.2390, Time Left=2.70 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3141/3393 [25:14<02:01,  2.08batch/s, Batch Loss=0.0832, Avg Loss=0.2390, Time Left=2.69 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3142/3393 [25:14<02:01,  2.07batch/s, Batch Loss=0.0832, Avg Loss=0.2390, Time Left=2.69 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3142/3393 [25:15<02:01,  2.07batch/s, Batch Loss=0.0995, Avg Loss=0.2389, Time Left=2.68 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3143/3393 [25:15<01:59,  2.09batch/s, Batch Loss=0.0995, Avg Loss=0.2389, Time Left=2.68 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3143/3393 [25:15<01:59,  2.09batch/s, Batch Loss=0.0854, Avg Loss=0.2389, Time Left=2.68 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3144/3393 [25:15<02:00,  2.06batch/s, Batch Loss=0.0854, Avg Loss=0.2389, Time Left=2.68 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3144/3393 [25:16<02:00,  2.06batch/s, Batch Loss=0.0827, Avg Loss=0.2388, Time Left=2.67 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3145/3393 [25:16<02:02,  2.02batch/s, Batch Loss=0.0827, Avg Loss=0.2388, Time Left=2.67 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3145/3393 [25:16<02:02,  2.02batch/s, Batch Loss=0.2718, Avg Loss=0.2388, Time Left=2.66 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3146/3393 [25:16<02:02,  2.02batch/s, Batch Loss=0.2718, Avg Loss=0.2388, Time Left=2.66 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3146/3393 [25:17<02:02,  2.02batch/s, Batch Loss=0.4856, Avg Loss=0.2389, Time Left=2.65 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3147/3393 [25:17<02:00,  2.05batch/s, Batch Loss=0.4856, Avg Loss=0.2389, Time Left=2.65 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3147/3393 [25:17<02:00,  2.05batch/s, Batch Loss=0.0582, Avg Loss=0.2388, Time Left=2.64 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3148/3393 [25:17<01:59,  2.05batch/s, Batch Loss=0.0582, Avg Loss=0.2388, Time Left=2.64 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3148/3393 [25:18<01:59,  2.05batch/s, Batch Loss=0.1697, Avg Loss=0.2388, Time Left=2.63 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3149/3393 [25:18<01:58,  2.06batch/s, Batch Loss=0.1697, Avg Loss=0.2388, Time Left=2.63 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3149/3393 [25:18<01:58,  2.06batch/s, Batch Loss=0.3338, Avg Loss=0.2388, Time Left=2.63 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3150/3393 [25:18<02:00,  2.02batch/s, Batch Loss=0.3338, Avg Loss=0.2388, Time Left=2.63 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3150/3393 [25:19<02:00,  2.02batch/s, Batch Loss=0.1506, Avg Loss=0.2388, Time Left=2.62 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3151/3393 [25:19<01:59,  2.03batch/s, Batch Loss=0.1506, Avg Loss=0.2388, Time Left=2.62 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3151/3393 [25:19<01:59,  2.03batch/s, Batch Loss=0.2222, Avg Loss=0.2388, Time Left=2.61 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3152/3393 [25:19<01:57,  2.04batch/s, Batch Loss=0.2222, Avg Loss=0.2388, Time Left=2.61 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3152/3393 [25:20<01:57,  2.04batch/s, Batch Loss=0.0674, Avg Loss=0.2388, Time Left=2.60 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3153/3393 [25:20<01:56,  2.05batch/s, Batch Loss=0.0674, Avg Loss=0.2388, Time Left=2.60 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3153/3393 [25:20<01:56,  2.05batch/s, Batch Loss=0.0540, Avg Loss=0.2387, Time Left=2.59 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3154/3393 [25:20<01:56,  2.06batch/s, Batch Loss=0.0540, Avg Loss=0.2387, Time Left=2.59 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3154/3393 [25:21<01:56,  2.06batch/s, Batch Loss=0.1167, Avg Loss=0.2387, Time Left=2.59 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3155/3393 [25:21<01:58,  2.00batch/s, Batch Loss=0.1167, Avg Loss=0.2387, Time Left=2.59 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3155/3393 [25:21<01:58,  2.00batch/s, Batch Loss=0.0626, Avg Loss=0.2386, Time Left=2.58 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3156/3393 [25:21<01:57,  2.02batch/s, Batch Loss=0.0626, Avg Loss=0.2386, Time Left=2.58 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3156/3393 [25:22<01:57,  2.02batch/s, Batch Loss=0.1203, Avg Loss=0.2386, Time Left=2.57 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3157/3393 [25:22<01:59,  1.97batch/s, Batch Loss=0.1203, Avg Loss=0.2386, Time Left=2.57 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3157/3393 [25:22<01:59,  1.97batch/s, Batch Loss=0.0487, Avg Loss=0.2385, Time Left=2.56 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3158/3393 [25:22<01:57,  2.00batch/s, Batch Loss=0.0487, Avg Loss=0.2385, Time Left=2.56 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3158/3393 [25:23<01:57,  2.00batch/s, Batch Loss=0.0874, Avg Loss=0.2385, Time Left=2.55 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3159/3393 [25:23<01:55,  2.02batch/s, Batch Loss=0.0874, Avg Loss=0.2385, Time Left=2.55 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3159/3393 [25:23<01:55,  2.02batch/s, Batch Loss=0.1197, Avg Loss=0.2384, Time Left=2.54 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3160/3393 [25:23<01:54,  2.03batch/s, Batch Loss=0.1197, Avg Loss=0.2384, Time Left=2.54 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3160/3393 [25:24<01:54,  2.03batch/s, Batch Loss=0.0459, Avg Loss=0.2383, Time Left=2.54 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3161/3393 [25:24<01:53,  2.04batch/s, Batch Loss=0.0459, Avg Loss=0.2383, Time Left=2.54 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3161/3393 [25:24<01:53,  2.04batch/s, Batch Loss=0.1500, Avg Loss=0.2383, Time Left=2.53 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3162/3393 [25:24<01:55,  1.99batch/s, Batch Loss=0.1500, Avg Loss=0.2383, Time Left=2.53 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3162/3393 [25:25<01:55,  1.99batch/s, Batch Loss=0.1471, Avg Loss=0.2383, Time Left=2.52 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3163/3393 [25:25<01:54,  2.01batch/s, Batch Loss=0.1471, Avg Loss=0.2383, Time Left=2.52 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3163/3393 [25:25<01:54,  2.01batch/s, Batch Loss=0.1246, Avg Loss=0.2383, Time Left=2.51 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3164/3393 [25:25<01:51,  2.05batch/s, Batch Loss=0.1246, Avg Loss=0.2383, Time Left=2.51 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3164/3393 [25:25<01:51,  2.05batch/s, Batch Loss=0.4212, Avg Loss=0.2383, Time Left=2.50 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3165/3393 [25:25<01:49,  2.07batch/s, Batch Loss=0.4212, Avg Loss=0.2383, Time Left=2.50 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3165/3393 [25:26<01:49,  2.07batch/s, Batch Loss=0.1924, Avg Loss=0.2383, Time Left=2.49 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  93%|▉| 3166/3393 [25:26<01:49,  2.07batch/s, Batch Loss=0.1924, Avg Loss=0.2383, Time Left=2.49 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3166/3393 [25:26<01:49,  2.07batch/s, Batch Loss=0.0477, Avg Loss=0.2382, Time Left=2.49 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3167/3393 [25:26<01:47,  2.09batch/s, Batch Loss=0.0477, Avg Loss=0.2382, Time Left=2.49 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3167/3393 [25:27<01:47,  2.09batch/s, Batch Loss=0.0591, Avg Loss=0.2382, Time Left=2.48 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3168/3393 [25:27<01:48,  2.06batch/s, Batch Loss=0.0591, Avg Loss=0.2382, Time Left=2.48 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3168/3393 [25:27<01:48,  2.06batch/s, Batch Loss=0.0862, Avg Loss=0.2381, Time Left=2.47 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3169/3393 [25:27<01:48,  2.06batch/s, Batch Loss=0.0862, Avg Loss=0.2381, Time Left=2.47 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3169/3393 [25:28<01:48,  2.06batch/s, Batch Loss=0.2611, Avg Loss=0.2381, Time Left=2.46 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3170/3393 [25:28<01:51,  2.01batch/s, Batch Loss=0.2611, Avg Loss=0.2381, Time Left=2.46 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3170/3393 [25:28<01:51,  2.01batch/s, Batch Loss=0.0540, Avg Loss=0.2381, Time Left=2.45 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3171/3393 [25:28<01:49,  2.03batch/s, Batch Loss=0.0540, Avg Loss=0.2381, Time Left=2.45 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3171/3393 [25:29<01:49,  2.03batch/s, Batch Loss=0.1149, Avg Loss=0.2380, Time Left=2.45 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3172/3393 [25:29<01:47,  2.05batch/s, Batch Loss=0.1149, Avg Loss=0.2380, Time Left=2.45 \u001b[A\n",
      "Epoch 1/3 - Training:  93%|▉| 3172/3393 [25:29<01:47,  2.05batch/s, Batch Loss=0.1650, Avg Loss=0.2380, Time Left=2.44 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3173/3393 [25:29<01:46,  2.06batch/s, Batch Loss=0.1650, Avg Loss=0.2380, Time Left=2.44 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3173/3393 [25:30<01:46,  2.06batch/s, Batch Loss=0.2173, Avg Loss=0.2380, Time Left=2.43 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3174/3393 [25:30<01:46,  2.05batch/s, Batch Loss=0.2173, Avg Loss=0.2380, Time Left=2.43 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3174/3393 [25:30<01:46,  2.05batch/s, Batch Loss=0.2002, Avg Loss=0.2380, Time Left=2.42 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3175/3393 [25:30<01:46,  2.05batch/s, Batch Loss=0.2002, Avg Loss=0.2380, Time Left=2.42 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3175/3393 [25:31<01:46,  2.05batch/s, Batch Loss=0.1582, Avg Loss=0.2380, Time Left=2.41 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3176/3393 [25:31<01:45,  2.06batch/s, Batch Loss=0.1582, Avg Loss=0.2380, Time Left=2.41 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3176/3393 [25:31<01:45,  2.06batch/s, Batch Loss=0.0921, Avg Loss=0.2379, Time Left=2.40 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3177/3393 [25:31<01:44,  2.06batch/s, Batch Loss=0.0921, Avg Loss=0.2379, Time Left=2.40 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3177/3393 [25:32<01:44,  2.06batch/s, Batch Loss=0.0430, Avg Loss=0.2379, Time Left=2.40 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3178/3393 [25:32<01:44,  2.06batch/s, Batch Loss=0.0430, Avg Loss=0.2379, Time Left=2.40 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3178/3393 [25:32<01:44,  2.06batch/s, Batch Loss=0.1486, Avg Loss=0.2378, Time Left=2.39 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3179/3393 [25:32<01:44,  2.04batch/s, Batch Loss=0.1486, Avg Loss=0.2378, Time Left=2.39 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3179/3393 [25:33<01:44,  2.04batch/s, Batch Loss=0.0930, Avg Loss=0.2378, Time Left=2.38 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3180/3393 [25:33<01:43,  2.07batch/s, Batch Loss=0.0930, Avg Loss=0.2378, Time Left=2.38 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3180/3393 [25:33<01:43,  2.07batch/s, Batch Loss=0.0274, Avg Loss=0.2377, Time Left=2.37 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3181/3393 [25:33<01:42,  2.07batch/s, Batch Loss=0.0274, Avg Loss=0.2377, Time Left=2.37 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3181/3393 [25:34<01:42,  2.07batch/s, Batch Loss=0.2511, Avg Loss=0.2377, Time Left=2.36 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3182/3393 [25:34<01:41,  2.09batch/s, Batch Loss=0.2511, Avg Loss=0.2377, Time Left=2.36 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3182/3393 [25:34<01:41,  2.09batch/s, Batch Loss=0.0193, Avg Loss=0.2377, Time Left=2.35 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3183/3393 [25:34<01:41,  2.06batch/s, Batch Loss=0.0193, Avg Loss=0.2377, Time Left=2.35 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3183/3393 [25:35<01:41,  2.06batch/s, Batch Loss=0.5837, Avg Loss=0.2378, Time Left=2.35 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3184/3393 [25:35<01:41,  2.06batch/s, Batch Loss=0.5837, Avg Loss=0.2378, Time Left=2.35 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3184/3393 [25:35<01:41,  2.06batch/s, Batch Loss=0.0955, Avg Loss=0.2377, Time Left=2.34 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3185/3393 [25:35<01:41,  2.05batch/s, Batch Loss=0.0955, Avg Loss=0.2377, Time Left=2.34 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3185/3393 [25:36<01:41,  2.05batch/s, Batch Loss=0.0323, Avg Loss=0.2377, Time Left=2.33 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3186/3393 [25:36<01:41,  2.03batch/s, Batch Loss=0.0323, Avg Loss=0.2377, Time Left=2.33 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3186/3393 [25:36<01:41,  2.03batch/s, Batch Loss=0.1090, Avg Loss=0.2376, Time Left=2.32 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3187/3393 [25:36<01:39,  2.07batch/s, Batch Loss=0.1090, Avg Loss=0.2376, Time Left=2.32 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3187/3393 [25:37<01:39,  2.07batch/s, Batch Loss=0.0280, Avg Loss=0.2375, Time Left=2.31 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3188/3393 [25:37<01:38,  2.08batch/s, Batch Loss=0.0280, Avg Loss=0.2375, Time Left=2.31 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3188/3393 [25:37<01:38,  2.08batch/s, Batch Loss=0.2196, Avg Loss=0.2375, Time Left=2.31 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3189/3393 [25:37<01:41,  2.02batch/s, Batch Loss=0.2196, Avg Loss=0.2375, Time Left=2.31 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3189/3393 [25:38<01:41,  2.02batch/s, Batch Loss=0.1139, Avg Loss=0.2375, Time Left=2.30 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3190/3393 [25:38<01:39,  2.03batch/s, Batch Loss=0.1139, Avg Loss=0.2375, Time Left=2.30 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3190/3393 [25:38<01:39,  2.03batch/s, Batch Loss=0.0202, Avg Loss=0.2374, Time Left=2.29 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3191/3393 [25:38<01:39,  2.02batch/s, Batch Loss=0.0202, Avg Loss=0.2374, Time Left=2.29 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3191/3393 [25:39<01:39,  2.02batch/s, Batch Loss=0.0154, Avg Loss=0.2374, Time Left=2.28 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3192/3393 [25:39<01:39,  2.01batch/s, Batch Loss=0.0154, Avg Loss=0.2374, Time Left=2.28 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3192/3393 [25:39<01:39,  2.01batch/s, Batch Loss=0.0362, Avg Loss=0.2373, Time Left=2.27 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3193/3393 [25:39<01:38,  2.03batch/s, Batch Loss=0.0362, Avg Loss=0.2373, Time Left=2.27 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3193/3393 [25:40<01:38,  2.03batch/s, Batch Loss=0.1381, Avg Loss=0.2373, Time Left=2.26 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3194/3393 [25:40<01:36,  2.06batch/s, Batch Loss=0.1381, Avg Loss=0.2373, Time Left=2.26 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3194/3393 [25:40<01:36,  2.06batch/s, Batch Loss=0.1772, Avg Loss=0.2372, Time Left=2.26 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3195/3393 [25:40<01:36,  2.04batch/s, Batch Loss=0.1772, Avg Loss=0.2372, Time Left=2.26 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3195/3393 [25:41<01:36,  2.04batch/s, Batch Loss=0.1039, Avg Loss=0.2372, Time Left=2.25 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3196/3393 [25:41<01:38,  2.00batch/s, Batch Loss=0.1039, Avg Loss=0.2372, Time Left=2.25 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3196/3393 [25:41<01:38,  2.00batch/s, Batch Loss=0.3273, Avg Loss=0.2372, Time Left=2.24 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3197/3393 [25:41<01:35,  2.05batch/s, Batch Loss=0.3273, Avg Loss=0.2372, Time Left=2.24 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3197/3393 [25:42<01:35,  2.05batch/s, Batch Loss=0.0579, Avg Loss=0.2372, Time Left=2.23 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3198/3393 [25:42<01:34,  2.06batch/s, Batch Loss=0.0579, Avg Loss=0.2372, Time Left=2.23 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3198/3393 [25:42<01:34,  2.06batch/s, Batch Loss=0.0183, Avg Loss=0.2371, Time Left=2.22 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  94%|▉| 3199/3393 [25:42<01:35,  2.04batch/s, Batch Loss=0.0183, Avg Loss=0.2371, Time Left=2.22 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3199/3393 [25:43<01:35,  2.04batch/s, Batch Loss=0.0543, Avg Loss=0.2370, Time Left=2.21 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3200/3393 [25:43<01:32,  2.09batch/s, Batch Loss=0.0543, Avg Loss=0.2370, Time Left=2.21 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3200/3393 [25:43<01:32,  2.09batch/s, Batch Loss=0.0521, Avg Loss=0.2370, Time Left=2.21 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3201/3393 [25:43<01:32,  2.08batch/s, Batch Loss=0.0521, Avg Loss=0.2370, Time Left=2.21 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3201/3393 [25:44<01:32,  2.08batch/s, Batch Loss=0.0213, Avg Loss=0.2369, Time Left=2.20 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3202/3393 [25:44<01:32,  2.06batch/s, Batch Loss=0.0213, Avg Loss=0.2369, Time Left=2.20 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3202/3393 [25:44<01:32,  2.06batch/s, Batch Loss=0.2157, Avg Loss=0.2369, Time Left=2.19 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3203/3393 [25:44<01:32,  2.06batch/s, Batch Loss=0.2157, Avg Loss=0.2369, Time Left=2.19 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3203/3393 [25:45<01:32,  2.06batch/s, Batch Loss=0.2457, Avg Loss=0.2369, Time Left=2.18 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3204/3393 [25:45<01:34,  2.00batch/s, Batch Loss=0.2457, Avg Loss=0.2369, Time Left=2.18 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3204/3393 [25:45<01:34,  2.00batch/s, Batch Loss=0.0355, Avg Loss=0.2368, Time Left=2.17 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3205/3393 [25:45<01:31,  2.06batch/s, Batch Loss=0.0355, Avg Loss=0.2368, Time Left=2.17 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3205/3393 [25:45<01:31,  2.06batch/s, Batch Loss=0.2512, Avg Loss=0.2368, Time Left=2.17 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3206/3393 [25:45<01:29,  2.08batch/s, Batch Loss=0.2512, Avg Loss=0.2368, Time Left=2.17 \u001b[A\n",
      "Epoch 1/3 - Training:  94%|▉| 3206/3393 [25:46<01:29,  2.08batch/s, Batch Loss=0.3382, Avg Loss=0.2369, Time Left=2.16 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3207/3393 [25:46<01:32,  2.02batch/s, Batch Loss=0.3382, Avg Loss=0.2369, Time Left=2.16 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3207/3393 [25:46<01:32,  2.02batch/s, Batch Loss=0.1731, Avg Loss=0.2369, Time Left=2.15 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3208/3393 [25:46<01:31,  2.03batch/s, Batch Loss=0.1731, Avg Loss=0.2369, Time Left=2.15 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3208/3393 [25:47<01:31,  2.03batch/s, Batch Loss=0.0125, Avg Loss=0.2368, Time Left=2.14 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3209/3393 [25:47<01:31,  2.00batch/s, Batch Loss=0.0125, Avg Loss=0.2368, Time Left=2.14 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3209/3393 [25:47<01:31,  2.00batch/s, Batch Loss=0.0926, Avg Loss=0.2367, Time Left=2.13 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3210/3393 [25:47<01:29,  2.04batch/s, Batch Loss=0.0926, Avg Loss=0.2367, Time Left=2.13 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3210/3393 [25:48<01:29,  2.04batch/s, Batch Loss=0.0829, Avg Loss=0.2367, Time Left=2.12 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3211/3393 [25:48<01:29,  2.03batch/s, Batch Loss=0.0829, Avg Loss=0.2367, Time Left=2.12 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3211/3393 [25:48<01:29,  2.03batch/s, Batch Loss=0.0692, Avg Loss=0.2366, Time Left=2.12 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3212/3393 [25:48<01:27,  2.06batch/s, Batch Loss=0.0692, Avg Loss=0.2366, Time Left=2.12 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3212/3393 [25:49<01:27,  2.06batch/s, Batch Loss=0.0179, Avg Loss=0.2366, Time Left=2.11 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3213/3393 [25:49<01:26,  2.08batch/s, Batch Loss=0.0179, Avg Loss=0.2366, Time Left=2.11 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3213/3393 [25:49<01:26,  2.08batch/s, Batch Loss=0.1505, Avg Loss=0.2365, Time Left=2.10 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3214/3393 [25:49<01:26,  2.08batch/s, Batch Loss=0.1505, Avg Loss=0.2365, Time Left=2.10 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3214/3393 [25:50<01:26,  2.08batch/s, Batch Loss=0.0353, Avg Loss=0.2365, Time Left=2.09 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3215/3393 [25:50<01:25,  2.08batch/s, Batch Loss=0.0353, Avg Loss=0.2365, Time Left=2.09 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3215/3393 [25:50<01:25,  2.08batch/s, Batch Loss=0.0308, Avg Loss=0.2364, Time Left=2.08 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3216/3393 [25:50<01:24,  2.09batch/s, Batch Loss=0.0308, Avg Loss=0.2364, Time Left=2.08 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3216/3393 [25:51<01:24,  2.09batch/s, Batch Loss=0.2041, Avg Loss=0.2364, Time Left=2.07 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3217/3393 [25:51<01:24,  2.09batch/s, Batch Loss=0.2041, Avg Loss=0.2364, Time Left=2.07 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3217/3393 [25:51<01:24,  2.09batch/s, Batch Loss=0.0488, Avg Loss=0.2363, Time Left=2.07 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3218/3393 [25:51<01:24,  2.08batch/s, Batch Loss=0.0488, Avg Loss=0.2363, Time Left=2.07 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3218/3393 [25:52<01:24,  2.08batch/s, Batch Loss=0.0259, Avg Loss=0.2363, Time Left=2.06 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3219/3393 [25:52<01:22,  2.12batch/s, Batch Loss=0.0259, Avg Loss=0.2363, Time Left=2.06 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3219/3393 [25:52<01:22,  2.12batch/s, Batch Loss=0.1075, Avg Loss=0.2362, Time Left=2.05 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3220/3393 [25:52<01:22,  2.11batch/s, Batch Loss=0.1075, Avg Loss=0.2362, Time Left=2.05 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3220/3393 [25:53<01:22,  2.11batch/s, Batch Loss=0.1948, Avg Loss=0.2362, Time Left=2.04 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3221/3393 [25:53<01:23,  2.05batch/s, Batch Loss=0.1948, Avg Loss=0.2362, Time Left=2.04 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3221/3393 [25:53<01:23,  2.05batch/s, Batch Loss=0.1199, Avg Loss=0.2362, Time Left=2.03 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3222/3393 [25:53<01:23,  2.05batch/s, Batch Loss=0.1199, Avg Loss=0.2362, Time Left=2.03 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3222/3393 [25:54<01:23,  2.05batch/s, Batch Loss=0.1393, Avg Loss=0.2362, Time Left=2.02 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3223/3393 [25:54<01:24,  2.02batch/s, Batch Loss=0.1393, Avg Loss=0.2362, Time Left=2.02 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3223/3393 [25:54<01:24,  2.02batch/s, Batch Loss=0.0365, Avg Loss=0.2361, Time Left=2.02 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3224/3393 [25:54<01:23,  2.03batch/s, Batch Loss=0.0365, Avg Loss=0.2361, Time Left=2.02 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3224/3393 [25:55<01:23,  2.03batch/s, Batch Loss=0.2745, Avg Loss=0.2361, Time Left=2.01 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3225/3393 [25:55<01:21,  2.06batch/s, Batch Loss=0.2745, Avg Loss=0.2361, Time Left=2.01 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3225/3393 [25:55<01:21,  2.06batch/s, Batch Loss=0.1094, Avg Loss=0.2361, Time Left=2.00 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3226/3393 [25:55<01:20,  2.06batch/s, Batch Loss=0.1094, Avg Loss=0.2361, Time Left=2.00 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3226/3393 [25:56<01:20,  2.06batch/s, Batch Loss=0.8342, Avg Loss=0.2363, Time Left=1.99 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3227/3393 [25:56<01:19,  2.10batch/s, Batch Loss=0.8342, Avg Loss=0.2363, Time Left=1.99 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3227/3393 [25:56<01:19,  2.10batch/s, Batch Loss=0.0595, Avg Loss=0.2362, Time Left=1.98 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3228/3393 [25:56<01:19,  2.08batch/s, Batch Loss=0.0595, Avg Loss=0.2362, Time Left=1.98 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3228/3393 [25:57<01:19,  2.08batch/s, Batch Loss=0.1401, Avg Loss=0.2362, Time Left=1.98 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3229/3393 [25:57<01:19,  2.06batch/s, Batch Loss=0.1401, Avg Loss=0.2362, Time Left=1.98 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3229/3393 [25:57<01:19,  2.06batch/s, Batch Loss=0.0359, Avg Loss=0.2361, Time Left=1.97 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3230/3393 [25:57<01:18,  2.08batch/s, Batch Loss=0.0359, Avg Loss=0.2361, Time Left=1.97 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3230/3393 [25:58<01:18,  2.08batch/s, Batch Loss=0.0786, Avg Loss=0.2361, Time Left=1.96 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3231/3393 [25:58<01:18,  2.06batch/s, Batch Loss=0.0786, Avg Loss=0.2361, Time Left=1.96 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3231/3393 [25:58<01:18,  2.06batch/s, Batch Loss=0.0796, Avg Loss=0.2360, Time Left=1.95 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  95%|▉| 3232/3393 [25:58<01:18,  2.06batch/s, Batch Loss=0.0796, Avg Loss=0.2360, Time Left=1.95 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3232/3393 [25:59<01:18,  2.06batch/s, Batch Loss=0.0895, Avg Loss=0.2360, Time Left=1.94 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3233/3393 [25:59<01:18,  2.03batch/s, Batch Loss=0.0895, Avg Loss=0.2360, Time Left=1.94 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3233/3393 [25:59<01:18,  2.03batch/s, Batch Loss=0.1216, Avg Loss=0.2359, Time Left=1.93 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3234/3393 [25:59<01:18,  2.03batch/s, Batch Loss=0.1216, Avg Loss=0.2359, Time Left=1.93 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3234/3393 [26:00<01:18,  2.03batch/s, Batch Loss=0.0382, Avg Loss=0.2359, Time Left=1.93 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3235/3393 [26:00<01:17,  2.04batch/s, Batch Loss=0.0382, Avg Loss=0.2359, Time Left=1.93 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3235/3393 [26:00<01:17,  2.04batch/s, Batch Loss=0.0422, Avg Loss=0.2358, Time Left=1.92 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3236/3393 [26:00<01:18,  2.00batch/s, Batch Loss=0.0422, Avg Loss=0.2358, Time Left=1.92 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3236/3393 [26:01<01:18,  2.00batch/s, Batch Loss=0.0233, Avg Loss=0.2357, Time Left=1.91 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3237/3393 [26:01<01:16,  2.04batch/s, Batch Loss=0.0233, Avg Loss=0.2357, Time Left=1.91 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3237/3393 [26:01<01:16,  2.04batch/s, Batch Loss=0.1247, Avg Loss=0.2357, Time Left=1.90 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3238/3393 [26:01<01:14,  2.07batch/s, Batch Loss=0.1247, Avg Loss=0.2357, Time Left=1.90 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3238/3393 [26:02<01:14,  2.07batch/s, Batch Loss=0.1031, Avg Loss=0.2357, Time Left=1.89 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3239/3393 [26:02<01:16,  2.01batch/s, Batch Loss=0.1031, Avg Loss=0.2357, Time Left=1.89 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3239/3393 [26:02<01:16,  2.01batch/s, Batch Loss=0.2409, Avg Loss=0.2357, Time Left=1.88 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3240/3393 [26:02<01:15,  2.02batch/s, Batch Loss=0.2409, Avg Loss=0.2357, Time Left=1.88 \u001b[A\n",
      "Epoch 1/3 - Training:  95%|▉| 3240/3393 [26:03<01:15,  2.02batch/s, Batch Loss=0.1043, Avg Loss=0.2356, Time Left=1.88 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3241/3393 [26:03<01:16,  2.00batch/s, Batch Loss=0.1043, Avg Loss=0.2356, Time Left=1.88 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3241/3393 [26:03<01:16,  2.00batch/s, Batch Loss=0.0717, Avg Loss=0.2356, Time Left=1.87 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3242/3393 [26:03<01:13,  2.06batch/s, Batch Loss=0.0717, Avg Loss=0.2356, Time Left=1.87 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3242/3393 [26:04<01:13,  2.06batch/s, Batch Loss=0.0497, Avg Loss=0.2355, Time Left=1.86 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3243/3393 [26:04<01:13,  2.04batch/s, Batch Loss=0.0497, Avg Loss=0.2355, Time Left=1.86 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3243/3393 [26:04<01:13,  2.04batch/s, Batch Loss=0.0126, Avg Loss=0.2354, Time Left=1.85 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3244/3393 [26:04<01:12,  2.05batch/s, Batch Loss=0.0126, Avg Loss=0.2354, Time Left=1.85 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3244/3393 [26:04<01:12,  2.05batch/s, Batch Loss=0.2164, Avg Loss=0.2354, Time Left=1.84 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3245/3393 [26:04<01:11,  2.07batch/s, Batch Loss=0.2164, Avg Loss=0.2354, Time Left=1.84 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3245/3393 [26:05<01:11,  2.07batch/s, Batch Loss=0.1327, Avg Loss=0.2354, Time Left=1.84 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3246/3393 [26:05<01:10,  2.07batch/s, Batch Loss=0.1327, Avg Loss=0.2354, Time Left=1.84 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3246/3393 [26:05<01:10,  2.07batch/s, Batch Loss=0.0125, Avg Loss=0.2353, Time Left=1.83 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3247/3393 [26:05<01:10,  2.07batch/s, Batch Loss=0.0125, Avg Loss=0.2353, Time Left=1.83 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3247/3393 [26:06<01:10,  2.07batch/s, Batch Loss=0.0972, Avg Loss=0.2353, Time Left=1.82 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3248/3393 [26:06<01:10,  2.05batch/s, Batch Loss=0.0972, Avg Loss=0.2353, Time Left=1.82 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3248/3393 [26:06<01:10,  2.05batch/s, Batch Loss=0.0374, Avg Loss=0.2352, Time Left=1.81 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3249/3393 [26:06<01:11,  2.02batch/s, Batch Loss=0.0374, Avg Loss=0.2352, Time Left=1.81 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3249/3393 [26:07<01:11,  2.02batch/s, Batch Loss=0.3727, Avg Loss=0.2353, Time Left=1.80 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3250/3393 [26:07<01:10,  2.03batch/s, Batch Loss=0.3727, Avg Loss=0.2353, Time Left=1.80 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3250/3393 [26:07<01:10,  2.03batch/s, Batch Loss=0.0462, Avg Loss=0.2352, Time Left=1.79 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3251/3393 [26:07<01:09,  2.04batch/s, Batch Loss=0.0462, Avg Loss=0.2352, Time Left=1.79 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3251/3393 [26:08<01:09,  2.04batch/s, Batch Loss=0.1054, Avg Loss=0.2352, Time Left=1.79 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3252/3393 [26:08<01:08,  2.07batch/s, Batch Loss=0.1054, Avg Loss=0.2352, Time Left=1.79 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3252/3393 [26:08<01:08,  2.07batch/s, Batch Loss=0.0731, Avg Loss=0.2351, Time Left=1.78 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3253/3393 [26:08<01:07,  2.09batch/s, Batch Loss=0.0731, Avg Loss=0.2351, Time Left=1.78 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3253/3393 [26:09<01:07,  2.09batch/s, Batch Loss=0.5422, Avg Loss=0.2352, Time Left=1.77 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3254/3393 [26:09<01:07,  2.07batch/s, Batch Loss=0.5422, Avg Loss=0.2352, Time Left=1.77 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3254/3393 [26:09<01:07,  2.07batch/s, Batch Loss=0.0118, Avg Loss=0.2351, Time Left=1.76 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3255/3393 [26:09<01:06,  2.08batch/s, Batch Loss=0.0118, Avg Loss=0.2351, Time Left=1.76 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3255/3393 [26:10<01:06,  2.08batch/s, Batch Loss=0.0941, Avg Loss=0.2351, Time Left=1.75 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3256/3393 [26:10<01:06,  2.06batch/s, Batch Loss=0.0941, Avg Loss=0.2351, Time Left=1.75 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3256/3393 [26:10<01:06,  2.06batch/s, Batch Loss=0.0219, Avg Loss=0.2350, Time Left=1.74 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3257/3393 [26:10<01:07,  2.02batch/s, Batch Loss=0.0219, Avg Loss=0.2350, Time Left=1.74 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3257/3393 [26:11<01:07,  2.02batch/s, Batch Loss=0.2546, Avg Loss=0.2350, Time Left=1.74 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3258/3393 [26:11<01:07,  2.00batch/s, Batch Loss=0.2546, Avg Loss=0.2350, Time Left=1.74 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3258/3393 [26:11<01:07,  2.00batch/s, Batch Loss=0.0856, Avg Loss=0.2350, Time Left=1.73 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3259/3393 [26:11<01:07,  1.99batch/s, Batch Loss=0.0856, Avg Loss=0.2350, Time Left=1.73 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3259/3393 [26:12<01:07,  1.99batch/s, Batch Loss=0.0751, Avg Loss=0.2349, Time Left=1.72 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3260/3393 [26:12<01:06,  1.99batch/s, Batch Loss=0.0751, Avg Loss=0.2349, Time Left=1.72 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3260/3393 [26:12<01:06,  1.99batch/s, Batch Loss=0.2016, Avg Loss=0.2349, Time Left=1.71 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3261/3393 [26:12<01:06,  1.98batch/s, Batch Loss=0.2016, Avg Loss=0.2349, Time Left=1.71 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3261/3393 [26:13<01:06,  1.98batch/s, Batch Loss=0.2636, Avg Loss=0.2349, Time Left=1.70 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3262/3393 [26:13<01:04,  2.02batch/s, Batch Loss=0.2636, Avg Loss=0.2349, Time Left=1.70 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3262/3393 [26:13<01:04,  2.02batch/s, Batch Loss=0.1361, Avg Loss=0.2349, Time Left=1.70 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3263/3393 [26:13<01:03,  2.06batch/s, Batch Loss=0.1361, Avg Loss=0.2349, Time Left=1.70 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3263/3393 [26:14<01:03,  2.06batch/s, Batch Loss=0.1072, Avg Loss=0.2349, Time Left=1.69 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3264/3393 [26:14<01:03,  2.02batch/s, Batch Loss=0.1072, Avg Loss=0.2349, Time Left=1.69 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3264/3393 [26:14<01:03,  2.02batch/s, Batch Loss=0.0953, Avg Loss=0.2348, Time Left=1.68 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  96%|▉| 3265/3393 [26:14<01:02,  2.05batch/s, Batch Loss=0.0953, Avg Loss=0.2348, Time Left=1.68 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3265/3393 [26:15<01:02,  2.05batch/s, Batch Loss=0.0474, Avg Loss=0.2348, Time Left=1.67 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3266/3393 [26:15<01:02,  2.05batch/s, Batch Loss=0.0474, Avg Loss=0.2348, Time Left=1.67 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3266/3393 [26:15<01:02,  2.05batch/s, Batch Loss=0.2093, Avg Loss=0.2348, Time Left=1.66 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3267/3393 [26:15<01:01,  2.04batch/s, Batch Loss=0.2093, Avg Loss=0.2348, Time Left=1.66 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3267/3393 [26:16<01:01,  2.04batch/s, Batch Loss=0.0400, Avg Loss=0.2347, Time Left=1.65 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3268/3393 [26:16<01:00,  2.05batch/s, Batch Loss=0.0400, Avg Loss=0.2347, Time Left=1.65 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3268/3393 [26:16<01:00,  2.05batch/s, Batch Loss=0.1377, Avg Loss=0.2347, Time Left=1.65 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3269/3393 [26:16<01:01,  2.01batch/s, Batch Loss=0.1377, Avg Loss=0.2347, Time Left=1.65 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3269/3393 [26:17<01:01,  2.01batch/s, Batch Loss=0.0547, Avg Loss=0.2346, Time Left=1.64 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3270/3393 [26:17<01:00,  2.03batch/s, Batch Loss=0.0547, Avg Loss=0.2346, Time Left=1.64 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3270/3393 [26:17<01:00,  2.03batch/s, Batch Loss=0.1484, Avg Loss=0.2346, Time Left=1.63 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3271/3393 [26:17<00:59,  2.04batch/s, Batch Loss=0.1484, Avg Loss=0.2346, Time Left=1.63 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3271/3393 [26:18<00:59,  2.04batch/s, Batch Loss=0.0435, Avg Loss=0.2345, Time Left=1.62 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3272/3393 [26:18<00:59,  2.05batch/s, Batch Loss=0.0435, Avg Loss=0.2345, Time Left=1.62 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3272/3393 [26:18<00:59,  2.05batch/s, Batch Loss=0.1331, Avg Loss=0.2345, Time Left=1.61 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3273/3393 [26:18<00:57,  2.07batch/s, Batch Loss=0.1331, Avg Loss=0.2345, Time Left=1.61 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3273/3393 [26:19<00:57,  2.07batch/s, Batch Loss=0.0264, Avg Loss=0.2344, Time Left=1.60 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3274/3393 [26:19<00:57,  2.07batch/s, Batch Loss=0.0264, Avg Loss=0.2344, Time Left=1.60 \u001b[A\n",
      "Epoch 1/3 - Training:  96%|▉| 3274/3393 [26:19<00:57,  2.07batch/s, Batch Loss=0.0770, Avg Loss=0.2344, Time Left=1.60 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3275/3393 [26:19<00:56,  2.07batch/s, Batch Loss=0.0770, Avg Loss=0.2344, Time Left=1.60 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3275/3393 [26:20<00:56,  2.07batch/s, Batch Loss=0.1226, Avg Loss=0.2343, Time Left=1.59 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3276/3393 [26:20<00:56,  2.07batch/s, Batch Loss=0.1226, Avg Loss=0.2343, Time Left=1.59 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3276/3393 [26:20<00:56,  2.07batch/s, Batch Loss=0.0758, Avg Loss=0.2343, Time Left=1.58 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3277/3393 [26:20<00:56,  2.05batch/s, Batch Loss=0.0758, Avg Loss=0.2343, Time Left=1.58 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3277/3393 [26:21<00:56,  2.05batch/s, Batch Loss=0.0561, Avg Loss=0.2342, Time Left=1.57 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3278/3393 [26:21<00:55,  2.07batch/s, Batch Loss=0.0561, Avg Loss=0.2342, Time Left=1.57 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3278/3393 [26:21<00:55,  2.07batch/s, Batch Loss=0.0963, Avg Loss=0.2342, Time Left=1.56 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3279/3393 [26:21<00:54,  2.07batch/s, Batch Loss=0.0963, Avg Loss=0.2342, Time Left=1.56 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3279/3393 [26:22<00:54,  2.07batch/s, Batch Loss=0.3403, Avg Loss=0.2342, Time Left=1.56 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3280/3393 [26:22<00:55,  2.05batch/s, Batch Loss=0.3403, Avg Loss=0.2342, Time Left=1.56 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3280/3393 [26:22<00:55,  2.05batch/s, Batch Loss=0.4332, Avg Loss=0.2343, Time Left=1.55 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3281/3393 [26:22<00:53,  2.07batch/s, Batch Loss=0.4332, Avg Loss=0.2343, Time Left=1.55 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3281/3393 [26:23<00:53,  2.07batch/s, Batch Loss=0.1505, Avg Loss=0.2343, Time Left=1.54 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3282/3393 [26:23<00:53,  2.07batch/s, Batch Loss=0.1505, Avg Loss=0.2343, Time Left=1.54 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3282/3393 [26:23<00:53,  2.07batch/s, Batch Loss=0.0536, Avg Loss=0.2342, Time Left=1.53 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3283/3393 [26:23<00:54,  2.03batch/s, Batch Loss=0.0536, Avg Loss=0.2342, Time Left=1.53 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3283/3393 [26:24<00:54,  2.03batch/s, Batch Loss=0.0705, Avg Loss=0.2341, Time Left=1.52 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3284/3393 [26:24<00:53,  2.04batch/s, Batch Loss=0.0705, Avg Loss=0.2341, Time Left=1.52 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3284/3393 [26:24<00:53,  2.04batch/s, Batch Loss=0.0581, Avg Loss=0.2341, Time Left=1.51 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3285/3393 [26:24<00:53,  2.01batch/s, Batch Loss=0.0581, Avg Loss=0.2341, Time Left=1.51 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3285/3393 [26:25<00:53,  2.01batch/s, Batch Loss=0.4021, Avg Loss=0.2341, Time Left=1.51 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3286/3393 [26:25<00:52,  2.02batch/s, Batch Loss=0.4021, Avg Loss=0.2341, Time Left=1.51 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3286/3393 [26:25<00:52,  2.02batch/s, Batch Loss=0.0582, Avg Loss=0.2341, Time Left=1.50 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3287/3393 [26:25<00:53,  1.98batch/s, Batch Loss=0.0582, Avg Loss=0.2341, Time Left=1.50 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3287/3393 [26:26<00:53,  1.98batch/s, Batch Loss=0.0514, Avg Loss=0.2340, Time Left=1.49 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3288/3393 [26:26<00:52,  2.00batch/s, Batch Loss=0.0514, Avg Loss=0.2340, Time Left=1.49 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3288/3393 [26:26<00:52,  2.00batch/s, Batch Loss=0.0467, Avg Loss=0.2340, Time Left=1.48 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3289/3393 [26:26<00:53,  1.95batch/s, Batch Loss=0.0467, Avg Loss=0.2340, Time Left=1.48 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3289/3393 [26:27<00:53,  1.95batch/s, Batch Loss=0.1503, Avg Loss=0.2339, Time Left=1.47 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3290/3393 [26:27<00:50,  2.03batch/s, Batch Loss=0.1503, Avg Loss=0.2339, Time Left=1.47 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3290/3393 [26:27<00:50,  2.03batch/s, Batch Loss=0.0379, Avg Loss=0.2339, Time Left=1.46 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3291/3393 [26:27<00:49,  2.04batch/s, Batch Loss=0.0379, Avg Loss=0.2339, Time Left=1.46 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3291/3393 [26:28<00:49,  2.04batch/s, Batch Loss=0.1156, Avg Loss=0.2339, Time Left=1.46 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3292/3393 [26:28<00:49,  2.03batch/s, Batch Loss=0.1156, Avg Loss=0.2339, Time Left=1.46 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3292/3393 [26:28<00:49,  2.03batch/s, Batch Loss=0.0731, Avg Loss=0.2338, Time Left=1.45 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3293/3393 [26:28<00:48,  2.04batch/s, Batch Loss=0.0731, Avg Loss=0.2338, Time Left=1.45 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3293/3393 [26:28<00:48,  2.04batch/s, Batch Loss=0.1280, Avg Loss=0.2338, Time Left=1.44 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3294/3393 [26:28<00:48,  2.05batch/s, Batch Loss=0.1280, Avg Loss=0.2338, Time Left=1.44 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3294/3393 [26:29<00:48,  2.05batch/s, Batch Loss=0.1289, Avg Loss=0.2337, Time Left=1.43 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3295/3393 [26:29<00:48,  2.04batch/s, Batch Loss=0.1289, Avg Loss=0.2337, Time Left=1.43 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3295/3393 [26:29<00:48,  2.04batch/s, Batch Loss=0.1012, Avg Loss=0.2337, Time Left=1.42 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3296/3393 [26:29<00:47,  2.04batch/s, Batch Loss=0.1012, Avg Loss=0.2337, Time Left=1.42 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3296/3393 [26:30<00:47,  2.04batch/s, Batch Loss=0.4532, Avg Loss=0.2338, Time Left=1.42 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3297/3393 [26:30<00:47,  2.01batch/s, Batch Loss=0.4532, Avg Loss=0.2338, Time Left=1.42 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3297/3393 [26:30<00:47,  2.01batch/s, Batch Loss=0.0967, Avg Loss=0.2337, Time Left=1.41 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  97%|▉| 3298/3393 [26:30<00:46,  2.02batch/s, Batch Loss=0.0967, Avg Loss=0.2337, Time Left=1.41 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3298/3393 [26:31<00:46,  2.02batch/s, Batch Loss=0.0460, Avg Loss=0.2337, Time Left=1.40 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3299/3393 [26:31<00:46,  2.02batch/s, Batch Loss=0.0460, Avg Loss=0.2337, Time Left=1.40 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3299/3393 [26:31<00:46,  2.02batch/s, Batch Loss=0.1356, Avg Loss=0.2336, Time Left=1.39 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3300/3393 [26:31<00:45,  2.03batch/s, Batch Loss=0.1356, Avg Loss=0.2336, Time Left=1.39 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3300/3393 [26:32<00:45,  2.03batch/s, Batch Loss=0.3295, Avg Loss=0.2337, Time Left=1.38 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3301/3393 [26:32<00:44,  2.06batch/s, Batch Loss=0.3295, Avg Loss=0.2337, Time Left=1.38 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3301/3393 [26:32<00:44,  2.06batch/s, Batch Loss=0.0343, Avg Loss=0.2336, Time Left=1.37 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3302/3393 [26:32<00:45,  2.00batch/s, Batch Loss=0.0343, Avg Loss=0.2336, Time Left=1.37 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3302/3393 [26:33<00:45,  2.00batch/s, Batch Loss=0.0753, Avg Loss=0.2336, Time Left=1.37 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3303/3393 [26:33<00:44,  2.04batch/s, Batch Loss=0.0753, Avg Loss=0.2336, Time Left=1.37 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3303/3393 [26:33<00:44,  2.04batch/s, Batch Loss=0.2657, Avg Loss=0.2336, Time Left=1.36 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3304/3393 [26:33<00:43,  2.05batch/s, Batch Loss=0.2657, Avg Loss=0.2336, Time Left=1.36 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3304/3393 [26:34<00:43,  2.05batch/s, Batch Loss=0.0635, Avg Loss=0.2335, Time Left=1.35 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3305/3393 [26:34<00:42,  2.07batch/s, Batch Loss=0.0635, Avg Loss=0.2335, Time Left=1.35 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3305/3393 [26:34<00:42,  2.07batch/s, Batch Loss=0.5508, Avg Loss=0.2336, Time Left=1.34 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3306/3393 [26:34<00:41,  2.10batch/s, Batch Loss=0.5508, Avg Loss=0.2336, Time Left=1.34 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3306/3393 [26:35<00:41,  2.10batch/s, Batch Loss=0.2455, Avg Loss=0.2336, Time Left=1.33 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3307/3393 [26:35<00:41,  2.06batch/s, Batch Loss=0.2455, Avg Loss=0.2336, Time Left=1.33 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3307/3393 [26:35<00:41,  2.06batch/s, Batch Loss=0.0471, Avg Loss=0.2336, Time Left=1.32 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3308/3393 [26:35<00:41,  2.07batch/s, Batch Loss=0.0471, Avg Loss=0.2336, Time Left=1.32 \u001b[A\n",
      "Epoch 1/3 - Training:  97%|▉| 3308/3393 [26:36<00:41,  2.07batch/s, Batch Loss=0.0930, Avg Loss=0.2335, Time Left=1.32 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3309/3393 [26:36<00:40,  2.09batch/s, Batch Loss=0.0930, Avg Loss=0.2335, Time Left=1.32 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3309/3393 [26:36<00:40,  2.09batch/s, Batch Loss=0.4059, Avg Loss=0.2336, Time Left=1.31 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3310/3393 [26:36<00:39,  2.08batch/s, Batch Loss=0.4059, Avg Loss=0.2336, Time Left=1.31 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3310/3393 [26:37<00:39,  2.08batch/s, Batch Loss=0.1406, Avg Loss=0.2335, Time Left=1.30 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3311/3393 [26:37<00:39,  2.10batch/s, Batch Loss=0.1406, Avg Loss=0.2335, Time Left=1.30 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3311/3393 [26:37<00:39,  2.10batch/s, Batch Loss=0.1039, Avg Loss=0.2335, Time Left=1.29 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3312/3393 [26:37<00:38,  2.09batch/s, Batch Loss=0.1039, Avg Loss=0.2335, Time Left=1.29 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3312/3393 [26:38<00:38,  2.09batch/s, Batch Loss=0.1169, Avg Loss=0.2335, Time Left=1.28 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3313/3393 [26:38<00:38,  2.10batch/s, Batch Loss=0.1169, Avg Loss=0.2335, Time Left=1.28 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3313/3393 [26:38<00:38,  2.10batch/s, Batch Loss=0.1513, Avg Loss=0.2334, Time Left=1.28 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3314/3393 [26:38<00:38,  2.06batch/s, Batch Loss=0.1513, Avg Loss=0.2334, Time Left=1.28 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3314/3393 [26:39<00:38,  2.06batch/s, Batch Loss=0.0971, Avg Loss=0.2334, Time Left=1.27 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3315/3393 [26:39<00:37,  2.08batch/s, Batch Loss=0.0971, Avg Loss=0.2334, Time Left=1.27 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3315/3393 [26:39<00:37,  2.08batch/s, Batch Loss=0.1096, Avg Loss=0.2334, Time Left=1.26 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3316/3393 [26:39<00:37,  2.03batch/s, Batch Loss=0.1096, Avg Loss=0.2334, Time Left=1.26 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3316/3393 [26:40<00:37,  2.03batch/s, Batch Loss=0.0315, Avg Loss=0.2333, Time Left=1.25 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3317/3393 [26:40<00:37,  2.04batch/s, Batch Loss=0.0315, Avg Loss=0.2333, Time Left=1.25 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3317/3393 [26:40<00:37,  2.04batch/s, Batch Loss=0.1124, Avg Loss=0.2333, Time Left=1.24 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3318/3393 [26:40<00:36,  2.05batch/s, Batch Loss=0.1124, Avg Loss=0.2333, Time Left=1.24 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3318/3393 [26:41<00:36,  2.05batch/s, Batch Loss=0.0828, Avg Loss=0.2332, Time Left=1.23 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3319/3393 [26:41<00:35,  2.07batch/s, Batch Loss=0.0828, Avg Loss=0.2332, Time Left=1.23 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3319/3393 [26:41<00:35,  2.07batch/s, Batch Loss=0.1717, Avg Loss=0.2332, Time Left=1.23 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3320/3393 [26:41<00:34,  2.09batch/s, Batch Loss=0.1717, Avg Loss=0.2332, Time Left=1.23 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3320/3393 [26:42<00:34,  2.09batch/s, Batch Loss=0.0644, Avg Loss=0.2331, Time Left=1.22 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3321/3393 [26:42<00:34,  2.09batch/s, Batch Loss=0.0644, Avg Loss=0.2331, Time Left=1.22 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3321/3393 [26:42<00:34,  2.09batch/s, Batch Loss=0.0460, Avg Loss=0.2331, Time Left=1.21 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3322/3393 [26:42<00:34,  2.06batch/s, Batch Loss=0.0460, Avg Loss=0.2331, Time Left=1.21 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3322/3393 [26:43<00:34,  2.06batch/s, Batch Loss=0.1842, Avg Loss=0.2331, Time Left=1.20 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3323/3393 [26:43<00:34,  2.04batch/s, Batch Loss=0.1842, Avg Loss=0.2331, Time Left=1.20 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3323/3393 [26:43<00:34,  2.04batch/s, Batch Loss=0.1286, Avg Loss=0.2330, Time Left=1.19 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3324/3393 [26:43<00:34,  2.01batch/s, Batch Loss=0.1286, Avg Loss=0.2330, Time Left=1.19 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3324/3393 [26:44<00:34,  2.01batch/s, Batch Loss=0.0674, Avg Loss=0.2330, Time Left=1.18 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3325/3393 [26:44<00:33,  2.02batch/s, Batch Loss=0.0674, Avg Loss=0.2330, Time Left=1.18 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3325/3393 [26:44<00:33,  2.02batch/s, Batch Loss=0.1296, Avg Loss=0.2329, Time Left=1.18 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3326/3393 [26:44<00:33,  1.98batch/s, Batch Loss=0.1296, Avg Loss=0.2329, Time Left=1.18 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3326/3393 [26:45<00:33,  1.98batch/s, Batch Loss=0.0288, Avg Loss=0.2329, Time Left=1.17 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3327/3393 [26:45<00:32,  2.00batch/s, Batch Loss=0.0288, Avg Loss=0.2329, Time Left=1.17 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3327/3393 [26:45<00:32,  2.00batch/s, Batch Loss=0.3382, Avg Loss=0.2329, Time Left=1.16 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3328/3393 [26:45<00:31,  2.04batch/s, Batch Loss=0.3382, Avg Loss=0.2329, Time Left=1.16 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3328/3393 [26:46<00:31,  2.04batch/s, Batch Loss=0.3354, Avg Loss=0.2329, Time Left=1.15 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3329/3393 [26:46<00:31,  2.03batch/s, Batch Loss=0.3354, Avg Loss=0.2329, Time Left=1.15 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3329/3393 [26:46<00:31,  2.03batch/s, Batch Loss=0.1179, Avg Loss=0.2329, Time Left=1.14 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3330/3393 [26:46<00:30,  2.04batch/s, Batch Loss=0.1179, Avg Loss=0.2329, Time Left=1.14 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3330/3393 [26:47<00:30,  2.04batch/s, Batch Loss=0.3372, Avg Loss=0.2329, Time Left=1.14 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  98%|▉| 3331/3393 [26:47<00:30,  2.01batch/s, Batch Loss=0.3372, Avg Loss=0.2329, Time Left=1.14 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3331/3393 [26:47<00:30,  2.01batch/s, Batch Loss=0.0135, Avg Loss=0.2329, Time Left=1.13 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3332/3393 [26:47<00:30,  2.00batch/s, Batch Loss=0.0135, Avg Loss=0.2329, Time Left=1.13 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3332/3393 [26:48<00:30,  2.00batch/s, Batch Loss=0.2798, Avg Loss=0.2329, Time Left=1.12 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3333/3393 [26:48<00:29,  2.04batch/s, Batch Loss=0.2798, Avg Loss=0.2329, Time Left=1.12 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3333/3393 [26:48<00:29,  2.04batch/s, Batch Loss=0.0346, Avg Loss=0.2328, Time Left=1.11 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3334/3393 [26:48<00:28,  2.05batch/s, Batch Loss=0.0346, Avg Loss=0.2328, Time Left=1.11 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3334/3393 [26:48<00:28,  2.05batch/s, Batch Loss=0.0351, Avg Loss=0.2328, Time Left=1.10 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3335/3393 [26:48<00:27,  2.08batch/s, Batch Loss=0.0351, Avg Loss=0.2328, Time Left=1.10 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3335/3393 [26:49<00:27,  2.08batch/s, Batch Loss=0.1366, Avg Loss=0.2327, Time Left=1.09 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3336/3393 [26:49<00:27,  2.08batch/s, Batch Loss=0.1366, Avg Loss=0.2327, Time Left=1.09 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3336/3393 [26:49<00:27,  2.08batch/s, Batch Loss=0.0503, Avg Loss=0.2327, Time Left=1.09 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3337/3393 [26:49<00:27,  2.07batch/s, Batch Loss=0.0503, Avg Loss=0.2327, Time Left=1.09 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3337/3393 [26:50<00:27,  2.07batch/s, Batch Loss=0.0777, Avg Loss=0.2326, Time Left=1.08 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3338/3393 [26:50<00:26,  2.09batch/s, Batch Loss=0.0777, Avg Loss=0.2326, Time Left=1.08 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3338/3393 [26:50<00:26,  2.09batch/s, Batch Loss=0.1882, Avg Loss=0.2326, Time Left=1.07 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3339/3393 [26:50<00:26,  2.06batch/s, Batch Loss=0.1882, Avg Loss=0.2326, Time Left=1.07 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3339/3393 [26:51<00:26,  2.06batch/s, Batch Loss=0.1104, Avg Loss=0.2326, Time Left=1.06 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3340/3393 [26:51<00:25,  2.06batch/s, Batch Loss=0.1104, Avg Loss=0.2326, Time Left=1.06 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3340/3393 [26:51<00:25,  2.06batch/s, Batch Loss=0.0474, Avg Loss=0.2325, Time Left=1.05 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3341/3393 [26:51<00:25,  2.06batch/s, Batch Loss=0.0474, Avg Loss=0.2325, Time Left=1.05 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3341/3393 [26:52<00:25,  2.06batch/s, Batch Loss=0.0686, Avg Loss=0.2325, Time Left=1.04 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3342/3393 [26:52<00:25,  2.03batch/s, Batch Loss=0.0686, Avg Loss=0.2325, Time Left=1.04 \u001b[A\n",
      "Epoch 1/3 - Training:  98%|▉| 3342/3393 [26:52<00:25,  2.03batch/s, Batch Loss=0.1917, Avg Loss=0.2325, Time Left=1.04 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3343/3393 [26:52<00:24,  2.04batch/s, Batch Loss=0.1917, Avg Loss=0.2325, Time Left=1.04 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3343/3393 [26:53<00:24,  2.04batch/s, Batch Loss=0.1272, Avg Loss=0.2324, Time Left=1.03 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3344/3393 [26:53<00:23,  2.06batch/s, Batch Loss=0.1272, Avg Loss=0.2324, Time Left=1.03 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3344/3393 [26:53<00:23,  2.06batch/s, Batch Loss=0.0407, Avg Loss=0.2324, Time Left=1.02 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3345/3393 [26:53<00:23,  2.07batch/s, Batch Loss=0.0407, Avg Loss=0.2324, Time Left=1.02 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3345/3393 [26:54<00:23,  2.07batch/s, Batch Loss=0.1402, Avg Loss=0.2323, Time Left=1.01 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3346/3393 [26:54<00:22,  2.11batch/s, Batch Loss=0.1402, Avg Loss=0.2323, Time Left=1.01 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3346/3393 [26:54<00:22,  2.11batch/s, Batch Loss=0.0572, Avg Loss=0.2323, Time Left=1.00 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3347/3393 [26:54<00:21,  2.10batch/s, Batch Loss=0.0572, Avg Loss=0.2323, Time Left=1.00 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3347/3393 [26:55<00:21,  2.10batch/s, Batch Loss=0.0435, Avg Loss=0.2322, Time Left=1.00 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3348/3393 [26:55<00:21,  2.05batch/s, Batch Loss=0.0435, Avg Loss=0.2322, Time Left=1.00 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3348/3393 [26:55<00:21,  2.05batch/s, Batch Loss=0.0283, Avg Loss=0.2322, Time Left=0.99 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3349/3393 [26:55<00:21,  2.07batch/s, Batch Loss=0.0283, Avg Loss=0.2322, Time Left=0.99 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3349/3393 [26:56<00:21,  2.07batch/s, Batch Loss=0.2210, Avg Loss=0.2322, Time Left=0.98 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3350/3393 [26:56<00:21,  2.01batch/s, Batch Loss=0.2210, Avg Loss=0.2322, Time Left=0.98 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3350/3393 [26:56<00:21,  2.01batch/s, Batch Loss=0.0152, Avg Loss=0.2321, Time Left=0.97 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3351/3393 [26:56<00:20,  2.02batch/s, Batch Loss=0.0152, Avg Loss=0.2321, Time Left=0.97 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3351/3393 [26:57<00:20,  2.02batch/s, Batch Loss=0.2542, Avg Loss=0.2321, Time Left=0.96 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3352/3393 [26:57<00:20,  1.99batch/s, Batch Loss=0.2542, Avg Loss=0.2321, Time Left=0.96 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3352/3393 [26:57<00:20,  1.99batch/s, Batch Loss=0.0452, Avg Loss=0.2321, Time Left=0.95 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3353/3393 [26:57<00:19,  2.04batch/s, Batch Loss=0.0452, Avg Loss=0.2321, Time Left=0.95 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3353/3393 [26:58<00:19,  2.04batch/s, Batch Loss=0.2517, Avg Loss=0.2321, Time Left=0.95 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3354/3393 [26:58<00:19,  2.05batch/s, Batch Loss=0.2517, Avg Loss=0.2321, Time Left=0.95 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3354/3393 [26:58<00:19,  2.05batch/s, Batch Loss=0.2512, Avg Loss=0.2321, Time Left=0.94 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3355/3393 [26:58<00:18,  2.04batch/s, Batch Loss=0.2512, Avg Loss=0.2321, Time Left=0.94 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3355/3393 [26:59<00:18,  2.04batch/s, Batch Loss=0.0533, Avg Loss=0.2320, Time Left=0.93 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3356/3393 [26:59<00:17,  2.08batch/s, Batch Loss=0.0533, Avg Loss=0.2320, Time Left=0.93 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3356/3393 [26:59<00:17,  2.08batch/s, Batch Loss=0.1050, Avg Loss=0.2320, Time Left=0.92 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3357/3393 [26:59<00:17,  2.07batch/s, Batch Loss=0.1050, Avg Loss=0.2320, Time Left=0.92 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3357/3393 [27:00<00:17,  2.07batch/s, Batch Loss=0.2795, Avg Loss=0.2320, Time Left=0.91 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3358/3393 [27:00<00:17,  2.03batch/s, Batch Loss=0.2795, Avg Loss=0.2320, Time Left=0.91 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3358/3393 [27:00<00:17,  2.03batch/s, Batch Loss=0.1001, Avg Loss=0.2319, Time Left=0.91 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3359/3393 [27:00<00:16,  2.03batch/s, Batch Loss=0.1001, Avg Loss=0.2319, Time Left=0.91 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3359/3393 [27:01<00:16,  2.03batch/s, Batch Loss=0.1446, Avg Loss=0.2319, Time Left=0.90 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3360/3393 [27:01<00:16,  2.02batch/s, Batch Loss=0.1446, Avg Loss=0.2319, Time Left=0.90 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3360/3393 [27:01<00:16,  2.02batch/s, Batch Loss=0.3032, Avg Loss=0.2319, Time Left=0.89 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3361/3393 [27:01<00:15,  2.01batch/s, Batch Loss=0.3032, Avg Loss=0.2319, Time Left=0.89 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3361/3393 [27:02<00:15,  2.01batch/s, Batch Loss=0.0613, Avg Loss=0.2319, Time Left=0.88 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3362/3393 [27:02<00:15,  2.01batch/s, Batch Loss=0.0613, Avg Loss=0.2319, Time Left=0.88 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3362/3393 [27:02<00:15,  2.01batch/s, Batch Loss=0.0436, Avg Loss=0.2318, Time Left=0.87 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3363/3393 [27:02<00:14,  2.02batch/s, Batch Loss=0.0436, Avg Loss=0.2318, Time Left=0.87 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3363/3393 [27:03<00:14,  2.02batch/s, Batch Loss=0.0192, Avg Loss=0.2318, Time Left=0.86 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:  99%|▉| 3364/3393 [27:03<00:14,  2.06batch/s, Batch Loss=0.0192, Avg Loss=0.2318, Time Left=0.86 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3364/3393 [27:03<00:14,  2.06batch/s, Batch Loss=0.0258, Avg Loss=0.2317, Time Left=0.86 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3365/3393 [27:03<00:13,  2.04batch/s, Batch Loss=0.0258, Avg Loss=0.2317, Time Left=0.86 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3365/3393 [27:04<00:13,  2.04batch/s, Batch Loss=0.0195, Avg Loss=0.2316, Time Left=0.85 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3366/3393 [27:04<00:13,  2.07batch/s, Batch Loss=0.0195, Avg Loss=0.2316, Time Left=0.85 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3366/3393 [27:04<00:13,  2.07batch/s, Batch Loss=0.2318, Avg Loss=0.2316, Time Left=0.84 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3367/3393 [27:04<00:12,  2.09batch/s, Batch Loss=0.2318, Avg Loss=0.2316, Time Left=0.84 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3367/3393 [27:05<00:12,  2.09batch/s, Batch Loss=0.0219, Avg Loss=0.2316, Time Left=0.83 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3368/3393 [27:05<00:12,  2.04batch/s, Batch Loss=0.0219, Avg Loss=0.2316, Time Left=0.83 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3368/3393 [27:05<00:12,  2.04batch/s, Batch Loss=0.0282, Avg Loss=0.2315, Time Left=0.82 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3369/3393 [27:05<00:11,  2.08batch/s, Batch Loss=0.0282, Avg Loss=0.2315, Time Left=0.82 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3369/3393 [27:06<00:11,  2.08batch/s, Batch Loss=0.1428, Avg Loss=0.2315, Time Left=0.81 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3370/3393 [27:06<00:10,  2.10batch/s, Batch Loss=0.1428, Avg Loss=0.2315, Time Left=0.81 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3370/3393 [27:06<00:10,  2.10batch/s, Batch Loss=0.1526, Avg Loss=0.2315, Time Left=0.81 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3371/3393 [27:06<00:10,  2.08batch/s, Batch Loss=0.1526, Avg Loss=0.2315, Time Left=0.81 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3371/3393 [27:06<00:10,  2.08batch/s, Batch Loss=0.1495, Avg Loss=0.2314, Time Left=0.80 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3372/3393 [27:06<00:09,  2.11batch/s, Batch Loss=0.1495, Avg Loss=0.2314, Time Left=0.80 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3372/3393 [27:07<00:09,  2.11batch/s, Batch Loss=0.0327, Avg Loss=0.2314, Time Left=0.79 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3373/3393 [27:07<00:09,  2.10batch/s, Batch Loss=0.0327, Avg Loss=0.2314, Time Left=0.79 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3373/3393 [27:07<00:09,  2.10batch/s, Batch Loss=0.0234, Avg Loss=0.2313, Time Left=0.78 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3374/3393 [27:07<00:09,  2.07batch/s, Batch Loss=0.0234, Avg Loss=0.2313, Time Left=0.78 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3374/3393 [27:08<00:09,  2.07batch/s, Batch Loss=0.0257, Avg Loss=0.2313, Time Left=0.77 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3375/3393 [27:08<00:08,  2.07batch/s, Batch Loss=0.0257, Avg Loss=0.2313, Time Left=0.77 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3375/3393 [27:08<00:08,  2.07batch/s, Batch Loss=0.0430, Avg Loss=0.2312, Time Left=0.77 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3376/3393 [27:08<00:08,  2.07batch/s, Batch Loss=0.0430, Avg Loss=0.2312, Time Left=0.77 \u001b[A\n",
      "Epoch 1/3 - Training:  99%|▉| 3376/3393 [27:09<00:08,  2.07batch/s, Batch Loss=0.0241, Avg Loss=0.2311, Time Left=0.76 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3377/3393 [27:09<00:07,  2.07batch/s, Batch Loss=0.0241, Avg Loss=0.2311, Time Left=0.76 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3377/3393 [27:09<00:07,  2.07batch/s, Batch Loss=0.0223, Avg Loss=0.2311, Time Left=0.75 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3378/3393 [27:09<00:07,  2.07batch/s, Batch Loss=0.0223, Avg Loss=0.2311, Time Left=0.75 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3378/3393 [27:10<00:07,  2.07batch/s, Batch Loss=0.0991, Avg Loss=0.2310, Time Left=0.74 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3379/3393 [27:10<00:06,  2.01batch/s, Batch Loss=0.0991, Avg Loss=0.2310, Time Left=0.74 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3379/3393 [27:10<00:06,  2.01batch/s, Batch Loss=0.0819, Avg Loss=0.2310, Time Left=0.73 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3380/3393 [27:10<00:06,  2.06batch/s, Batch Loss=0.0819, Avg Loss=0.2310, Time Left=0.73 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3380/3393 [27:11<00:06,  2.06batch/s, Batch Loss=0.1608, Avg Loss=0.2310, Time Left=0.72 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3381/3393 [27:11<00:05,  2.06batch/s, Batch Loss=0.1608, Avg Loss=0.2310, Time Left=0.72 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3381/3393 [27:11<00:05,  2.06batch/s, Batch Loss=0.0216, Avg Loss=0.2309, Time Left=0.72 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3382/3393 [27:11<00:05,  2.05batch/s, Batch Loss=0.0216, Avg Loss=0.2309, Time Left=0.72 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3382/3393 [27:12<00:05,  2.05batch/s, Batch Loss=0.0470, Avg Loss=0.2308, Time Left=0.71 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3383/3393 [27:12<00:04,  2.05batch/s, Batch Loss=0.0470, Avg Loss=0.2308, Time Left=0.71 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3383/3393 [27:12<00:04,  2.05batch/s, Batch Loss=0.0348, Avg Loss=0.2308, Time Left=0.70 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3384/3393 [27:12<00:04,  2.04batch/s, Batch Loss=0.0348, Avg Loss=0.2308, Time Left=0.70 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3384/3393 [27:13<00:04,  2.04batch/s, Batch Loss=0.1385, Avg Loss=0.2308, Time Left=0.69 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3385/3393 [27:13<00:03,  2.09batch/s, Batch Loss=0.1385, Avg Loss=0.2308, Time Left=0.69 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3385/3393 [27:13<00:03,  2.09batch/s, Batch Loss=0.1464, Avg Loss=0.2307, Time Left=0.68 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3386/3393 [27:13<00:03,  2.10batch/s, Batch Loss=0.1464, Avg Loss=0.2307, Time Left=0.68 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3386/3393 [27:14<00:03,  2.10batch/s, Batch Loss=0.0232, Avg Loss=0.2307, Time Left=0.67 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3387/3393 [27:14<00:02,  2.09batch/s, Batch Loss=0.0232, Avg Loss=0.2307, Time Left=0.67 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3387/3393 [27:14<00:02,  2.09batch/s, Batch Loss=0.0180, Avg Loss=0.2306, Time Left=0.67 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3388/3393 [27:14<00:02,  2.09batch/s, Batch Loss=0.0180, Avg Loss=0.2306, Time Left=0.67 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3388/3393 [27:15<00:02,  2.09batch/s, Batch Loss=0.1805, Avg Loss=0.2306, Time Left=0.66 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3389/3393 [27:15<00:01,  2.10batch/s, Batch Loss=0.1805, Avg Loss=0.2306, Time Left=0.66 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3389/3393 [27:15<00:01,  2.10batch/s, Batch Loss=0.0085, Avg Loss=0.2305, Time Left=0.65 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3390/3393 [27:15<00:01,  2.09batch/s, Batch Loss=0.0085, Avg Loss=0.2305, Time Left=0.65 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3390/3393 [27:16<00:01,  2.09batch/s, Batch Loss=0.4587, Avg Loss=0.2306, Time Left=0.64 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3391/3393 [27:16<00:00,  2.08batch/s, Batch Loss=0.4587, Avg Loss=0.2306, Time Left=0.64 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3391/3393 [27:16<00:00,  2.08batch/s, Batch Loss=0.0332, Avg Loss=0.2305, Time Left=0.63 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3392/3393 [27:16<00:00,  2.06batch/s, Batch Loss=0.0332, Avg Loss=0.2305, Time Left=0.63 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|▉| 3392/3393 [27:17<00:00,  2.06batch/s, Batch Loss=0.0262, Avg Loss=0.2305, Time Left=0.63 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|█| 3393/3393 [27:17<00:00,  2.02batch/s, Batch Loss=0.0262, Avg Loss=0.2305, Time Left=0.63 \u001b[A\n",
      "Epoch 1/3 - Training: 100%|█| 3393/3393 [27:17<00:00,  2.02batch/s, Batch Loss=0.1799, Avg Loss=0.2305, Time Left=0.62 \u001b[A\n",
      "Epoch 1/3 - Training: 3394batch [27:17,  2.03batch/s, Batch Loss=0.1799, Avg Loss=0.2305, Time Left=0.62 min]          \u001b[A\n",
      "Epoch 1/3 - Training: 3394batch [27:18,  2.03batch/s, Batch Loss=0.4093, Avg Loss=0.2305, Time Left=0.61 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3395batch [27:18,  2.04batch/s, Batch Loss=0.4093, Avg Loss=0.2305, Time Left=0.61 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3395batch [27:18,  2.04batch/s, Batch Loss=0.0175, Avg Loss=0.2304, Time Left=0.60 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3396batch [27:18,  2.05batch/s, Batch Loss=0.0175, Avg Loss=0.2304, Time Left=0.60 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3396batch [27:19,  2.05batch/s, Batch Loss=0.0235, Avg Loss=0.2304, Time Left=0.59 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3397batch [27:19,  2.10batch/s, Batch Loss=0.0235, Avg Loss=0.2304, Time Left=0.59 min]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training: 3397batch [27:19,  2.10batch/s, Batch Loss=0.1394, Avg Loss=0.2304, Time Left=0.58 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3398batch [27:19,  2.09batch/s, Batch Loss=0.1394, Avg Loss=0.2304, Time Left=0.58 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3398batch [27:20,  2.09batch/s, Batch Loss=0.2626, Avg Loss=0.2304, Time Left=0.58 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3399batch [27:20,  2.04batch/s, Batch Loss=0.2626, Avg Loss=0.2304, Time Left=0.58 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3399batch [27:20,  2.04batch/s, Batch Loss=0.3020, Avg Loss=0.2304, Time Left=0.57 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3400batch [27:20,  2.05batch/s, Batch Loss=0.3020, Avg Loss=0.2304, Time Left=0.57 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3400batch [27:21,  2.05batch/s, Batch Loss=0.0360, Avg Loss=0.2303, Time Left=0.56 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3401batch [27:21,  2.04batch/s, Batch Loss=0.0360, Avg Loss=0.2303, Time Left=0.56 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3401batch [27:21,  2.04batch/s, Batch Loss=0.1552, Avg Loss=0.2303, Time Left=0.55 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3402batch [27:21,  2.02batch/s, Batch Loss=0.1552, Avg Loss=0.2303, Time Left=0.55 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3402batch [27:22,  2.02batch/s, Batch Loss=0.4058, Avg Loss=0.2304, Time Left=0.54 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3403batch [27:22,  2.06batch/s, Batch Loss=0.4058, Avg Loss=0.2304, Time Left=0.54 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3403batch [27:22,  2.06batch/s, Batch Loss=0.4894, Avg Loss=0.2304, Time Left=0.53 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3404batch [27:22,  2.06batch/s, Batch Loss=0.4894, Avg Loss=0.2304, Time Left=0.53 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3404batch [27:22,  2.06batch/s, Batch Loss=0.0166, Avg Loss=0.2304, Time Left=0.53 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3405batch [27:22,  2.10batch/s, Batch Loss=0.0166, Avg Loss=0.2304, Time Left=0.53 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3405batch [27:23,  2.10batch/s, Batch Loss=0.0071, Avg Loss=0.2303, Time Left=0.52 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3406batch [27:23,  2.06batch/s, Batch Loss=0.0071, Avg Loss=0.2303, Time Left=0.52 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3406batch [27:23,  2.06batch/s, Batch Loss=0.1374, Avg Loss=0.2303, Time Left=0.51 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3407batch [27:23,  2.07batch/s, Batch Loss=0.1374, Avg Loss=0.2303, Time Left=0.51 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3407batch [27:24,  2.07batch/s, Batch Loss=0.2530, Avg Loss=0.2303, Time Left=0.50 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3408batch [27:24,  2.07batch/s, Batch Loss=0.2530, Avg Loss=0.2303, Time Left=0.50 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3408batch [27:24,  2.07batch/s, Batch Loss=0.1064, Avg Loss=0.2302, Time Left=0.49 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3409batch [27:24,  2.01batch/s, Batch Loss=0.1064, Avg Loss=0.2302, Time Left=0.49 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3409batch [27:25,  2.01batch/s, Batch Loss=0.5222, Avg Loss=0.2303, Time Left=0.49 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3410batch [27:25,  2.03batch/s, Batch Loss=0.5222, Avg Loss=0.2303, Time Left=0.49 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3410batch [27:25,  2.03batch/s, Batch Loss=0.0117, Avg Loss=0.2303, Time Left=0.48 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3411batch [27:25,  1.98batch/s, Batch Loss=0.0117, Avg Loss=0.2303, Time Left=0.48 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3411batch [27:26,  1.98batch/s, Batch Loss=0.1245, Avg Loss=0.2302, Time Left=0.47 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3412batch [27:26,  1.98batch/s, Batch Loss=0.1245, Avg Loss=0.2302, Time Left=0.47 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3412batch [27:27,  1.98batch/s, Batch Loss=0.2661, Avg Loss=0.2302, Time Left=0.46 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3413batch [27:27,  1.95batch/s, Batch Loss=0.2661, Avg Loss=0.2302, Time Left=0.46 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3413batch [27:27,  1.95batch/s, Batch Loss=0.0810, Avg Loss=0.2302, Time Left=0.45 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3414batch [27:27,  1.98batch/s, Batch Loss=0.0810, Avg Loss=0.2302, Time Left=0.45 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3414batch [27:28,  1.98batch/s, Batch Loss=0.2557, Avg Loss=0.2302, Time Left=0.44 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3415batch [27:28,  1.95batch/s, Batch Loss=0.2557, Avg Loss=0.2302, Time Left=0.44 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3415batch [27:28,  1.95batch/s, Batch Loss=0.0593, Avg Loss=0.2302, Time Left=0.44 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3416batch [27:28,  1.97batch/s, Batch Loss=0.0593, Avg Loss=0.2302, Time Left=0.44 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3416batch [27:29,  1.97batch/s, Batch Loss=0.4086, Avg Loss=0.2302, Time Left=0.43 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3417batch [27:29,  2.01batch/s, Batch Loss=0.4086, Avg Loss=0.2302, Time Left=0.43 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3417batch [27:29,  2.01batch/s, Batch Loss=0.0528, Avg Loss=0.2302, Time Left=0.42 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3418batch [27:29,  2.05batch/s, Batch Loss=0.0528, Avg Loss=0.2302, Time Left=0.42 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3418batch [27:29,  2.05batch/s, Batch Loss=0.2630, Avg Loss=0.2302, Time Left=0.41 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3419batch [27:29,  2.07batch/s, Batch Loss=0.2630, Avg Loss=0.2302, Time Left=0.41 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3419batch [27:30,  2.07batch/s, Batch Loss=0.2873, Avg Loss=0.2302, Time Left=0.40 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3420batch [27:30,  2.07batch/s, Batch Loss=0.2873, Avg Loss=0.2302, Time Left=0.40 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3420batch [27:30,  2.07batch/s, Batch Loss=0.1228, Avg Loss=0.2302, Time Left=0.39 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3421batch [27:30,  2.07batch/s, Batch Loss=0.1228, Avg Loss=0.2302, Time Left=0.39 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3421batch [27:31,  2.07batch/s, Batch Loss=0.2724, Avg Loss=0.2302, Time Left=0.39 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3422batch [27:31,  2.09batch/s, Batch Loss=0.2724, Avg Loss=0.2302, Time Left=0.39 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3422batch [27:31,  2.09batch/s, Batch Loss=0.2712, Avg Loss=0.2302, Time Left=0.38 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3423batch [27:31,  2.11batch/s, Batch Loss=0.2712, Avg Loss=0.2302, Time Left=0.38 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3423batch [27:32,  2.11batch/s, Batch Loss=0.1858, Avg Loss=0.2302, Time Left=0.37 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3424batch [27:32,  2.04batch/s, Batch Loss=0.1858, Avg Loss=0.2302, Time Left=0.37 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3424batch [27:32,  2.04batch/s, Batch Loss=0.3202, Avg Loss=0.2302, Time Left=0.36 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3425batch [27:32,  2.06batch/s, Batch Loss=0.3202, Avg Loss=0.2302, Time Left=0.36 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3425batch [27:33,  2.06batch/s, Batch Loss=0.1222, Avg Loss=0.2302, Time Left=0.35 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3426batch [27:33,  2.02batch/s, Batch Loss=0.1222, Avg Loss=0.2302, Time Left=0.35 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3426batch [27:33,  2.02batch/s, Batch Loss=0.3123, Avg Loss=0.2302, Time Left=0.35 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3427batch [27:33,  2.03batch/s, Batch Loss=0.3123, Avg Loss=0.2302, Time Left=0.35 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3427batch [27:34,  2.03batch/s, Batch Loss=0.3528, Avg Loss=0.2302, Time Left=0.34 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3428batch [27:34,  2.04batch/s, Batch Loss=0.3528, Avg Loss=0.2302, Time Left=0.34 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3428batch [27:34,  2.04batch/s, Batch Loss=0.2116, Avg Loss=0.2302, Time Left=0.33 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3429batch [27:34,  2.05batch/s, Batch Loss=0.2116, Avg Loss=0.2302, Time Left=0.33 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3429batch [27:35,  2.05batch/s, Batch Loss=0.1733, Avg Loss=0.2302, Time Left=0.32 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3430batch [27:35,  2.06batch/s, Batch Loss=0.1733, Avg Loss=0.2302, Time Left=0.32 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3430batch [27:35,  2.06batch/s, Batch Loss=0.2732, Avg Loss=0.2302, Time Left=0.31 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3431batch [27:35,  2.02batch/s, Batch Loss=0.2732, Avg Loss=0.2302, Time Left=0.31 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3431batch [27:36,  2.02batch/s, Batch Loss=0.1832, Avg Loss=0.2302, Time Left=0.30 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3432batch [27:36,  2.07batch/s, Batch Loss=0.1832, Avg Loss=0.2302, Time Left=0.30 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3432batch [27:36,  2.07batch/s, Batch Loss=0.3286, Avg Loss=0.2302, Time Left=0.30 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3433batch [27:36,  2.07batch/s, Batch Loss=0.3286, Avg Loss=0.2302, Time Left=0.30 min]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training: 3433batch [27:37,  2.07batch/s, Batch Loss=0.1761, Avg Loss=0.2302, Time Left=0.29 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3434batch [27:37,  2.07batch/s, Batch Loss=0.1761, Avg Loss=0.2302, Time Left=0.29 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3434batch [27:37,  2.07batch/s, Batch Loss=0.2078, Avg Loss=0.2302, Time Left=0.28 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3435batch [27:37,  2.07batch/s, Batch Loss=0.2078, Avg Loss=0.2302, Time Left=0.28 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3435batch [27:38,  2.07batch/s, Batch Loss=0.0949, Avg Loss=0.2302, Time Left=0.27 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3436batch [27:38,  2.07batch/s, Batch Loss=0.0949, Avg Loss=0.2302, Time Left=0.27 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3436batch [27:38,  2.07batch/s, Batch Loss=0.0974, Avg Loss=0.2301, Time Left=0.26 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3437batch [27:38,  2.03batch/s, Batch Loss=0.0974, Avg Loss=0.2301, Time Left=0.26 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3437batch [27:39,  2.03batch/s, Batch Loss=0.1278, Avg Loss=0.2301, Time Left=0.25 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3438batch [27:39,  2.04batch/s, Batch Loss=0.1278, Avg Loss=0.2301, Time Left=0.25 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3438batch [27:39,  2.04batch/s, Batch Loss=0.1046, Avg Loss=0.2301, Time Left=0.25 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3439batch [27:39,  2.03batch/s, Batch Loss=0.1046, Avg Loss=0.2301, Time Left=0.25 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3439batch [27:40,  2.03batch/s, Batch Loss=0.3000, Avg Loss=0.2301, Time Left=0.24 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3440batch [27:40,  2.06batch/s, Batch Loss=0.3000, Avg Loss=0.2301, Time Left=0.24 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3440batch [27:40,  2.06batch/s, Batch Loss=0.1396, Avg Loss=0.2301, Time Left=0.23 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3441batch [27:40,  2.08batch/s, Batch Loss=0.1396, Avg Loss=0.2301, Time Left=0.23 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3441batch [27:41,  2.08batch/s, Batch Loss=0.1871, Avg Loss=0.2300, Time Left=0.22 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3442batch [27:41,  2.04batch/s, Batch Loss=0.1871, Avg Loss=0.2300, Time Left=0.22 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3442batch [27:41,  2.04batch/s, Batch Loss=0.1698, Avg Loss=0.2300, Time Left=0.21 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3443batch [27:41,  2.07batch/s, Batch Loss=0.1698, Avg Loss=0.2300, Time Left=0.21 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3443batch [27:42,  2.07batch/s, Batch Loss=0.1985, Avg Loss=0.2300, Time Left=0.21 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3444batch [27:42,  2.05batch/s, Batch Loss=0.1985, Avg Loss=0.2300, Time Left=0.21 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3444batch [27:42,  2.05batch/s, Batch Loss=0.1691, Avg Loss=0.2300, Time Left=0.20 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3445batch [27:42,  2.05batch/s, Batch Loss=0.1691, Avg Loss=0.2300, Time Left=0.20 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3445batch [27:43,  2.05batch/s, Batch Loss=0.1411, Avg Loss=0.2300, Time Left=0.19 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3446batch [27:43,  2.05batch/s, Batch Loss=0.1411, Avg Loss=0.2300, Time Left=0.19 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3446batch [27:43,  2.05batch/s, Batch Loss=0.1414, Avg Loss=0.2299, Time Left=0.18 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3447batch [27:43,  2.02batch/s, Batch Loss=0.1414, Avg Loss=0.2299, Time Left=0.18 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3447batch [27:44,  2.02batch/s, Batch Loss=0.0725, Avg Loss=0.2299, Time Left=0.17 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3448batch [27:44,  2.01batch/s, Batch Loss=0.0725, Avg Loss=0.2299, Time Left=0.17 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3448batch [27:44,  2.01batch/s, Batch Loss=0.0905, Avg Loss=0.2299, Time Left=0.16 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3449batch [27:44,  1.99batch/s, Batch Loss=0.0905, Avg Loss=0.2299, Time Left=0.16 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3449batch [27:45,  1.99batch/s, Batch Loss=0.1263, Avg Loss=0.2298, Time Left=0.16 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3450batch [27:45,  2.01batch/s, Batch Loss=0.1263, Avg Loss=0.2298, Time Left=0.16 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3450batch [27:45,  2.01batch/s, Batch Loss=0.0050, Avg Loss=0.2298, Time Left=0.15 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3451batch [27:45,  2.03batch/s, Batch Loss=0.0050, Avg Loss=0.2298, Time Left=0.15 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3451batch [27:46,  2.03batch/s, Batch Loss=0.0650, Avg Loss=0.2297, Time Left=0.14 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3452batch [27:46,  2.06batch/s, Batch Loss=0.0650, Avg Loss=0.2297, Time Left=0.14 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3452batch [27:46,  2.06batch/s, Batch Loss=0.2279, Avg Loss=0.2297, Time Left=0.13 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3453batch [27:46,  2.04batch/s, Batch Loss=0.2279, Avg Loss=0.2297, Time Left=0.13 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3453batch [27:47,  2.04batch/s, Batch Loss=0.1604, Avg Loss=0.2297, Time Left=0.12 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3454batch [27:47,  2.07batch/s, Batch Loss=0.1604, Avg Loss=0.2297, Time Left=0.12 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3454batch [27:47,  2.07batch/s, Batch Loss=0.1865, Avg Loss=0.2297, Time Left=0.12 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3455batch [27:47,  2.09batch/s, Batch Loss=0.1865, Avg Loss=0.2297, Time Left=0.12 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3455batch [27:47,  2.09batch/s, Batch Loss=0.0656, Avg Loss=0.2296, Time Left=0.11 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3456batch [27:47,  2.11batch/s, Batch Loss=0.0656, Avg Loss=0.2296, Time Left=0.11 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3456batch [27:48,  2.11batch/s, Batch Loss=0.3270, Avg Loss=0.2297, Time Left=0.10 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3457batch [27:48,  2.09batch/s, Batch Loss=0.3270, Avg Loss=0.2297, Time Left=0.10 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3457batch [27:48,  2.09batch/s, Batch Loss=0.1235, Avg Loss=0.2296, Time Left=0.09 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3458batch [27:48,  2.09batch/s, Batch Loss=0.1235, Avg Loss=0.2296, Time Left=0.09 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3458batch [27:49,  2.09batch/s, Batch Loss=0.0813, Avg Loss=0.2296, Time Left=0.08 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3459batch [27:49,  2.08batch/s, Batch Loss=0.0813, Avg Loss=0.2296, Time Left=0.08 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3459batch [27:49,  2.08batch/s, Batch Loss=0.1071, Avg Loss=0.2295, Time Left=0.07 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3460batch [27:49,  2.08batch/s, Batch Loss=0.1071, Avg Loss=0.2295, Time Left=0.07 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3460batch [27:50,  2.08batch/s, Batch Loss=0.2498, Avg Loss=0.2295, Time Left=0.07 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3461batch [27:50,  2.10batch/s, Batch Loss=0.2498, Avg Loss=0.2295, Time Left=0.07 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3461batch [27:50,  2.10batch/s, Batch Loss=0.4792, Avg Loss=0.2296, Time Left=0.06 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3462batch [27:50,  2.09batch/s, Batch Loss=0.4792, Avg Loss=0.2296, Time Left=0.06 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3462batch [27:51,  2.09batch/s, Batch Loss=0.0925, Avg Loss=0.2296, Time Left=0.05 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3463batch [27:51,  2.00batch/s, Batch Loss=0.0925, Avg Loss=0.2296, Time Left=0.05 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3463batch [27:51,  2.00batch/s, Batch Loss=0.3429, Avg Loss=0.2296, Time Left=0.04 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3464batch [27:51,  2.06batch/s, Batch Loss=0.3429, Avg Loss=0.2296, Time Left=0.04 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3464batch [27:52,  2.06batch/s, Batch Loss=0.1081, Avg Loss=0.2296, Time Left=0.03 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3465batch [27:52,  2.04batch/s, Batch Loss=0.1081, Avg Loss=0.2296, Time Left=0.03 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3465batch [27:52,  2.04batch/s, Batch Loss=0.0864, Avg Loss=0.2295, Time Left=0.02 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3466batch [27:52,  2.05batch/s, Batch Loss=0.0864, Avg Loss=0.2295, Time Left=0.02 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3466batch [27:53,  2.05batch/s, Batch Loss=0.1974, Avg Loss=0.2295, Time Left=0.02 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3467batch [27:53,  2.05batch/s, Batch Loss=0.1974, Avg Loss=0.2295, Time Left=0.02 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3467batch [27:53,  2.05batch/s, Batch Loss=0.1295, Avg Loss=0.2295, Time Left=0.01 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3468batch [27:53,  2.04batch/s, Batch Loss=0.1295, Avg Loss=0.2295, Time Left=0.01 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3468batch [27:54,  2.04batch/s, Batch Loss=0.0036, Avg Loss=0.2294, Time Left=0.00 min]\u001b[A\n",
      "Epoch 1/3 - Training: 3469batch [27:54,  2.45batch/s, Batch Loss=0.0036, Avg Loss=0.2294, Time Left=0.00 min]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \u001b[A\n",
      "Epoch 1/3 - Evaluating:   0%|                                                               | 0/849 [00:00<?, ?batch/s]\u001b[A\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 Results:\n",
      "Train Loss: 0.2294\n",
      "Validation Loss: 0.1239, Accuracy: 0.9559\n",
      "\n",
      "Starting Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3 - Training:   0%|                                                                | 0/3393 [00:00<?, ?batch/s]\u001b[AC:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14248\\3382488418.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "\n",
      "Epoch 2/3 - Training:   0%|       | 0/3393 [00:00<?, ?batch/s, Batch Loss=0.0764, Avg Loss=0.0764, Time Left=29.75 min]\u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 100/3393 [00:00<00:17, 190.04batch/s, Batch Loss=0.0764, Avg Loss=0.0764, Time Left=29.75\u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 100/3393 [00:13<00:17, 190.04batch/s, Batch Loss=0.0764, Avg Loss=0.0764, Time Left=29.75\u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 100/3393 [00:13<00:17, 190.04batch/s, Batch Loss=0.0254, Avg Loss=0.1215, Time Left=26.59\u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 101/3393 [00:13<10:30,  5.22batch/s, Batch Loss=0.0254, Avg Loss=0.1215, Time Left=26.59 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 101/3393 [00:14<10:30,  5.22batch/s, Batch Loss=0.1509, Avg Loss=0.1225, Time Left=26.65 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 102/3393 [00:14<10:51,  5.05batch/s, Batch Loss=0.1509, Avg Loss=0.1225, Time Left=26.65 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 102/3393 [00:14<10:51,  5.05batch/s, Batch Loss=0.0702, Avg Loss=0.1208, Time Left=26.66 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 103/3393 [00:14<11:17,  4.86batch/s, Batch Loss=0.0702, Avg Loss=0.1208, Time Left=26.66 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 103/3393 [00:15<11:17,  4.86batch/s, Batch Loss=0.2554, Avg Loss=0.1250, Time Left=26.72 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 104/3393 [00:15<11:55,  4.60batch/s, Batch Loss=0.2554, Avg Loss=0.1250, Time Left=26.72 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 104/3393 [00:15<11:55,  4.60batch/s, Batch Loss=0.1346, Avg Loss=0.1253, Time Left=26.70 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 105/3393 [00:15<12:35,  4.35batch/s, Batch Loss=0.1346, Avg Loss=0.1253, Time Left=26.70 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 105/3393 [00:16<12:35,  4.35batch/s, Batch Loss=0.4172, Avg Loss=0.1339, Time Left=26.73 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 106/3393 [00:16<13:35,  4.03batch/s, Batch Loss=0.4172, Avg Loss=0.1339, Time Left=26.73 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 106/3393 [00:16<13:35,  4.03batch/s, Batch Loss=0.3677, Avg Loss=0.1406, Time Left=26.71 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 107/3393 [00:16<14:43,  3.72batch/s, Batch Loss=0.3677, Avg Loss=0.1406, Time Left=26.71 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 107/3393 [00:17<14:43,  3.72batch/s, Batch Loss=0.0640, Avg Loss=0.1385, Time Left=26.68 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 108/3393 [00:17<15:50,  3.46batch/s, Batch Loss=0.0640, Avg Loss=0.1385, Time Left=26.68 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 108/3393 [00:17<15:50,  3.46batch/s, Batch Loss=0.0757, Avg Loss=0.1368, Time Left=26.68 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 109/3393 [00:17<17:18,  3.16batch/s, Batch Loss=0.0757, Avg Loss=0.1368, Time Left=26.68 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 109/3393 [00:18<17:18,  3.16batch/s, Batch Loss=0.1862, Avg Loss=0.1381, Time Left=26.71 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 110/3393 [00:18<18:57,  2.89batch/s, Batch Loss=0.1862, Avg Loss=0.1381, Time Left=26.71 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 110/3393 [00:18<18:57,  2.89batch/s, Batch Loss=0.1333, Avg Loss=0.1379, Time Left=26.73 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 111/3393 [00:18<20:33,  2.66batch/s, Batch Loss=0.1333, Avg Loss=0.1379, Time Left=26.73 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 111/3393 [00:19<20:33,  2.66batch/s, Batch Loss=0.1364, Avg Loss=0.1379, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 112/3393 [00:19<22:10,  2.47batch/s, Batch Loss=0.1364, Avg Loss=0.1379, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 112/3393 [00:19<22:10,  2.47batch/s, Batch Loss=0.2983, Avg Loss=0.1418, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 113/3393 [00:19<23:24,  2.34batch/s, Batch Loss=0.2983, Avg Loss=0.1418, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 113/3393 [00:20<23:24,  2.34batch/s, Batch Loss=0.0402, Avg Loss=0.1394, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 114/3393 [00:20<24:35,  2.22batch/s, Batch Loss=0.0402, Avg Loss=0.1394, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 114/3393 [00:20<24:35,  2.22batch/s, Batch Loss=0.0971, Avg Loss=0.1384, Time Left=26.82 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 115/3393 [00:20<24:53,  2.20batch/s, Batch Loss=0.0971, Avg Loss=0.1384, Time Left=26.82 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 115/3393 [00:21<24:53,  2.20batch/s, Batch Loss=0.0933, Avg Loss=0.1374, Time Left=26.85 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 116/3393 [00:21<25:46,  2.12batch/s, Batch Loss=0.0933, Avg Loss=0.1374, Time Left=26.85 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 116/3393 [00:21<25:46,  2.12batch/s, Batch Loss=0.0396, Avg Loss=0.1352, Time Left=26.83 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 117/3393 [00:21<25:42,  2.12batch/s, Batch Loss=0.0396, Avg Loss=0.1352, Time Left=26.83 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 117/3393 [00:22<25:42,  2.12batch/s, Batch Loss=0.1421, Avg Loss=0.1354, Time Left=26.83 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 118/3393 [00:22<25:55,  2.11batch/s, Batch Loss=0.1421, Avg Loss=0.1354, Time Left=26.83 \u001b[A\n",
      "Epoch 2/3 - Training:   3%| | 118/3393 [00:22<25:55,  2.11batch/s, Batch Loss=0.1120, Avg Loss=0.1349, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 119/3393 [00:22<26:17,  2.08batch/s, Batch Loss=0.1120, Avg Loss=0.1349, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 119/3393 [00:23<26:17,  2.08batch/s, Batch Loss=0.0398, Avg Loss=0.1329, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 120/3393 [00:23<26:18,  2.07batch/s, Batch Loss=0.0398, Avg Loss=0.1329, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 120/3393 [00:23<26:18,  2.07batch/s, Batch Loss=0.3057, Avg Loss=0.1364, Time Left=26.83 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 121/3393 [00:23<26:20,  2.07batch/s, Batch Loss=0.3057, Avg Loss=0.1364, Time Left=26.83 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 121/3393 [00:24<26:20,  2.07batch/s, Batch Loss=0.2702, Avg Loss=0.1391, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 122/3393 [00:24<26:34,  2.05batch/s, Batch Loss=0.2702, Avg Loss=0.1391, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 122/3393 [00:24<26:34,  2.05batch/s, Batch Loss=0.0978, Avg Loss=0.1383, Time Left=26.80 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 123/3393 [00:24<26:00,  2.10batch/s, Batch Loss=0.0978, Avg Loss=0.1383, Time Left=26.80 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 123/3393 [00:25<26:00,  2.10batch/s, Batch Loss=0.0553, Avg Loss=0.1367, Time Left=26.80 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 124/3393 [00:25<26:05,  2.09batch/s, Batch Loss=0.0553, Avg Loss=0.1367, Time Left=26.80 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 124/3393 [00:25<26:05,  2.09batch/s, Batch Loss=0.1208, Avg Loss=0.1364, Time Left=26.79 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 125/3393 [00:25<26:10,  2.08batch/s, Batch Loss=0.1208, Avg Loss=0.1364, Time Left=26.79 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 125/3393 [00:25<26:10,  2.08batch/s, Batch Loss=0.0418, Avg Loss=0.1346, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 126/3393 [00:25<25:56,  2.10batch/s, Batch Loss=0.0418, Avg Loss=0.1346, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 126/3393 [00:26<25:56,  2.10batch/s, Batch Loss=0.1397, Avg Loss=0.1347, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 127/3393 [00:26<26:17,  2.07batch/s, Batch Loss=0.1397, Avg Loss=0.1347, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 127/3393 [00:26<26:17,  2.07batch/s, Batch Loss=0.0763, Avg Loss=0.1337, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 128/3393 [00:26<26:18,  2.07batch/s, Batch Loss=0.0763, Avg Loss=0.1337, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 128/3393 [00:27<26:18,  2.07batch/s, Batch Loss=0.0442, Avg Loss=0.1321, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 129/3393 [00:27<26:35,  2.05batch/s, Batch Loss=0.0442, Avg Loss=0.1321, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 129/3393 [00:27<26:35,  2.05batch/s, Batch Loss=0.0492, Avg Loss=0.1307, Time Left=26.81 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:   4%| | 130/3393 [00:27<27:01,  2.01batch/s, Batch Loss=0.0492, Avg Loss=0.1307, Time Left=26.81 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 130/3393 [00:28<27:01,  2.01batch/s, Batch Loss=0.0400, Avg Loss=0.1291, Time Left=26.82 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 131/3393 [00:28<27:02,  2.01batch/s, Batch Loss=0.0400, Avg Loss=0.1291, Time Left=26.82 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 131/3393 [00:28<27:02,  2.01batch/s, Batch Loss=0.0785, Avg Loss=0.1283, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 132/3393 [00:28<27:20,  1.99batch/s, Batch Loss=0.0785, Avg Loss=0.1283, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 132/3393 [00:29<27:20,  1.99batch/s, Batch Loss=0.0329, Avg Loss=0.1267, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 133/3393 [00:29<27:04,  2.01batch/s, Batch Loss=0.0329, Avg Loss=0.1267, Time Left=26.84 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 133/3393 [00:29<27:04,  2.01batch/s, Batch Loss=0.1514, Avg Loss=0.1271, Time Left=26.83 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 134/3393 [00:29<27:04,  2.01batch/s, Batch Loss=0.1514, Avg Loss=0.1271, Time Left=26.83 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 134/3393 [00:30<27:04,  2.01batch/s, Batch Loss=0.0645, Avg Loss=0.1261, Time Left=26.82 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 135/3393 [00:30<26:34,  2.04batch/s, Batch Loss=0.0645, Avg Loss=0.1261, Time Left=26.82 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 135/3393 [00:30<26:34,  2.04batch/s, Batch Loss=0.1001, Avg Loss=0.1257, Time Left=26.81 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 136/3393 [00:30<26:29,  2.05batch/s, Batch Loss=0.1001, Avg Loss=0.1257, Time Left=26.81 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 136/3393 [00:31<26:29,  2.05batch/s, Batch Loss=0.0223, Avg Loss=0.1241, Time Left=26.82 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 137/3393 [00:31<26:54,  2.02batch/s, Batch Loss=0.0223, Avg Loss=0.1241, Time Left=26.82 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 137/3393 [00:31<26:54,  2.02batch/s, Batch Loss=0.2530, Avg Loss=0.1261, Time Left=26.79 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 138/3393 [00:31<26:13,  2.07batch/s, Batch Loss=0.2530, Avg Loss=0.1261, Time Left=26.79 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 138/3393 [00:32<26:13,  2.07batch/s, Batch Loss=0.0028, Avg Loss=0.1243, Time Left=26.79 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 139/3393 [00:32<26:18,  2.06batch/s, Batch Loss=0.0028, Avg Loss=0.1243, Time Left=26.79 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 139/3393 [00:32<26:18,  2.06batch/s, Batch Loss=0.1342, Avg Loss=0.1244, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 140/3393 [00:32<26:24,  2.05batch/s, Batch Loss=0.1342, Avg Loss=0.1244, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 140/3393 [00:33<26:24,  2.05batch/s, Batch Loss=0.2937, Avg Loss=0.1269, Time Left=26.79 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 141/3393 [00:33<26:20,  2.06batch/s, Batch Loss=0.2937, Avg Loss=0.1269, Time Left=26.79 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 141/3393 [00:33<26:20,  2.06batch/s, Batch Loss=0.0086, Avg Loss=0.1252, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 142/3393 [00:33<26:20,  2.06batch/s, Batch Loss=0.0086, Avg Loss=0.1252, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 142/3393 [00:34<26:20,  2.06batch/s, Batch Loss=0.0085, Avg Loss=0.1235, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 143/3393 [00:34<26:17,  2.06batch/s, Batch Loss=0.0085, Avg Loss=0.1235, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 143/3393 [00:34<26:17,  2.06batch/s, Batch Loss=0.1051, Avg Loss=0.1233, Time Left=26.76 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 144/3393 [00:34<26:13,  2.07batch/s, Batch Loss=0.1051, Avg Loss=0.1233, Time Left=26.76 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 144/3393 [00:35<26:13,  2.07batch/s, Batch Loss=0.0089, Avg Loss=0.1217, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 145/3393 [00:35<26:44,  2.02batch/s, Batch Loss=0.0089, Avg Loss=0.1217, Time Left=26.78 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 145/3393 [00:35<26:44,  2.02batch/s, Batch Loss=0.0092, Avg Loss=0.1202, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 146/3393 [00:35<26:50,  2.02batch/s, Batch Loss=0.0092, Avg Loss=0.1202, Time Left=26.77 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 146/3393 [00:36<26:50,  2.02batch/s, Batch Loss=0.3230, Avg Loss=0.1229, Time Left=26.76 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 147/3393 [00:36<26:22,  2.05batch/s, Batch Loss=0.3230, Avg Loss=0.1229, Time Left=26.76 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 147/3393 [00:36<26:22,  2.05batch/s, Batch Loss=0.2261, Avg Loss=0.1242, Time Left=26.76 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 148/3393 [00:36<26:18,  2.06batch/s, Batch Loss=0.2261, Avg Loss=0.1242, Time Left=26.76 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 148/3393 [00:37<26:18,  2.06batch/s, Batch Loss=0.3241, Avg Loss=0.1268, Time Left=26.75 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 149/3393 [00:37<26:16,  2.06batch/s, Batch Loss=0.3241, Avg Loss=0.1268, Time Left=26.75 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 149/3393 [00:37<26:16,  2.06batch/s, Batch Loss=0.5021, Avg Loss=0.1316, Time Left=26.76 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 150/3393 [00:37<26:43,  2.02batch/s, Batch Loss=0.5021, Avg Loss=0.1316, Time Left=26.76 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 150/3393 [00:38<26:43,  2.02batch/s, Batch Loss=0.0872, Avg Loss=0.1311, Time Left=26.74 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 151/3393 [00:38<26:20,  2.05batch/s, Batch Loss=0.0872, Avg Loss=0.1311, Time Left=26.74 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 151/3393 [00:38<26:20,  2.05batch/s, Batch Loss=0.0670, Avg Loss=0.1303, Time Left=26.74 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 152/3393 [00:38<26:14,  2.06batch/s, Batch Loss=0.0670, Avg Loss=0.1303, Time Left=26.74 \u001b[A\n",
      "Epoch 2/3 - Training:   4%| | 152/3393 [00:39<26:14,  2.06batch/s, Batch Loss=0.2582, Avg Loss=0.1319, Time Left=26.75 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 153/3393 [00:39<26:41,  2.02batch/s, Batch Loss=0.2582, Avg Loss=0.1319, Time Left=26.75 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 153/3393 [00:39<26:41,  2.02batch/s, Batch Loss=0.1121, Avg Loss=0.1316, Time Left=26.74 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 154/3393 [00:39<26:34,  2.03batch/s, Batch Loss=0.1121, Avg Loss=0.1316, Time Left=26.74 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 154/3393 [00:40<26:34,  2.03batch/s, Batch Loss=0.0417, Avg Loss=0.1305, Time Left=26.75 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 155/3393 [00:40<26:54,  2.01batch/s, Batch Loss=0.0417, Avg Loss=0.1305, Time Left=26.75 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 155/3393 [00:40<26:54,  2.01batch/s, Batch Loss=0.0664, Avg Loss=0.1298, Time Left=26.75 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 156/3393 [00:40<26:43,  2.02batch/s, Batch Loss=0.0664, Avg Loss=0.1298, Time Left=26.75 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 156/3393 [00:41<26:43,  2.02batch/s, Batch Loss=0.1032, Avg Loss=0.1295, Time Left=26.74 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 157/3393 [00:41<26:39,  2.02batch/s, Batch Loss=0.1032, Avg Loss=0.1295, Time Left=26.74 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 157/3393 [00:41<26:39,  2.02batch/s, Batch Loss=0.0636, Avg Loss=0.1287, Time Left=26.72 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 158/3393 [00:41<26:05,  2.07batch/s, Batch Loss=0.0636, Avg Loss=0.1287, Time Left=26.72 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 158/3393 [00:42<26:05,  2.07batch/s, Batch Loss=0.0669, Avg Loss=0.1280, Time Left=26.71 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 159/3393 [00:42<26:03,  2.07batch/s, Batch Loss=0.0669, Avg Loss=0.1280, Time Left=26.71 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 159/3393 [00:42<26:03,  2.07batch/s, Batch Loss=0.0946, Avg Loss=0.1276, Time Left=26.70 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 160/3393 [00:42<26:02,  2.07batch/s, Batch Loss=0.0946, Avg Loss=0.1276, Time Left=26.70 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 160/3393 [00:43<26:02,  2.07batch/s, Batch Loss=0.2492, Avg Loss=0.1290, Time Left=26.68 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 161/3393 [00:43<25:46,  2.09batch/s, Batch Loss=0.2492, Avg Loss=0.1290, Time Left=26.68 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 161/3393 [00:43<25:46,  2.09batch/s, Batch Loss=0.3543, Avg Loss=0.1315, Time Left=26.66 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 162/3393 [00:43<25:35,  2.10batch/s, Batch Loss=0.3543, Avg Loss=0.1315, Time Left=26.66 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 162/3393 [00:44<25:35,  2.10batch/s, Batch Loss=0.2567, Avg Loss=0.1329, Time Left=26.65 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:   5%| | 163/3393 [00:44<25:42,  2.09batch/s, Batch Loss=0.2567, Avg Loss=0.1329, Time Left=26.65 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 163/3393 [00:44<25:42,  2.09batch/s, Batch Loss=0.0617, Avg Loss=0.1321, Time Left=26.64 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 164/3393 [00:44<25:47,  2.09batch/s, Batch Loss=0.0617, Avg Loss=0.1321, Time Left=26.64 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 164/3393 [00:45<25:47,  2.09batch/s, Batch Loss=0.3221, Avg Loss=0.1341, Time Left=26.63 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 165/3393 [00:45<25:47,  2.09batch/s, Batch Loss=0.3221, Avg Loss=0.1341, Time Left=26.63 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 165/3393 [00:45<25:47,  2.09batch/s, Batch Loss=0.1231, Avg Loss=0.1340, Time Left=26.62 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 166/3393 [00:45<25:54,  2.08batch/s, Batch Loss=0.1231, Avg Loss=0.1340, Time Left=26.62 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 166/3393 [00:46<25:54,  2.08batch/s, Batch Loss=0.0394, Avg Loss=0.1330, Time Left=26.62 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 167/3393 [00:46<25:55,  2.07batch/s, Batch Loss=0.0394, Avg Loss=0.1330, Time Left=26.62 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 167/3393 [00:46<25:55,  2.07batch/s, Batch Loss=0.1446, Avg Loss=0.1331, Time Left=26.61 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 168/3393 [00:46<25:57,  2.07batch/s, Batch Loss=0.1446, Avg Loss=0.1331, Time Left=26.61 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 168/3393 [00:47<25:57,  2.07batch/s, Batch Loss=0.0763, Avg Loss=0.1325, Time Left=26.62 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 169/3393 [00:47<26:43,  2.01batch/s, Batch Loss=0.0763, Avg Loss=0.1325, Time Left=26.62 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 169/3393 [00:47<26:43,  2.01batch/s, Batch Loss=0.0534, Avg Loss=0.1317, Time Left=26.60 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 170/3393 [00:47<26:01,  2.06batch/s, Batch Loss=0.0534, Avg Loss=0.1317, Time Left=26.60 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 170/3393 [00:47<26:01,  2.06batch/s, Batch Loss=0.0745, Avg Loss=0.1312, Time Left=26.60 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 171/3393 [00:47<25:59,  2.07batch/s, Batch Loss=0.0745, Avg Loss=0.1312, Time Left=26.60 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 171/3393 [00:48<25:59,  2.07batch/s, Batch Loss=0.0768, Avg Loss=0.1306, Time Left=26.59 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 172/3393 [00:48<26:10,  2.05batch/s, Batch Loss=0.0768, Avg Loss=0.1306, Time Left=26.59 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 172/3393 [00:48<26:10,  2.05batch/s, Batch Loss=0.1415, Avg Loss=0.1307, Time Left=26.58 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 173/3393 [00:48<25:54,  2.07batch/s, Batch Loss=0.1415, Avg Loss=0.1307, Time Left=26.58 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 173/3393 [00:49<25:54,  2.07batch/s, Batch Loss=0.0212, Avg Loss=0.1296, Time Left=26.56 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 174/3393 [00:49<25:39,  2.09batch/s, Batch Loss=0.0212, Avg Loss=0.1296, Time Left=26.56 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 174/3393 [00:49<25:39,  2.09batch/s, Batch Loss=0.1717, Avg Loss=0.1301, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 175/3393 [00:49<25:43,  2.08batch/s, Batch Loss=0.1717, Avg Loss=0.1301, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 175/3393 [00:50<25:43,  2.08batch/s, Batch Loss=0.1920, Avg Loss=0.1307, Time Left=26.54 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 176/3393 [00:50<25:31,  2.10batch/s, Batch Loss=0.1920, Avg Loss=0.1307, Time Left=26.54 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 176/3393 [00:50<25:31,  2.10batch/s, Batch Loss=0.0311, Avg Loss=0.1297, Time Left=26.54 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 177/3393 [00:50<25:52,  2.07batch/s, Batch Loss=0.0311, Avg Loss=0.1297, Time Left=26.54 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 177/3393 [00:51<25:52,  2.07batch/s, Batch Loss=0.0311, Avg Loss=0.1288, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 178/3393 [00:51<25:55,  2.07batch/s, Batch Loss=0.0311, Avg Loss=0.1288, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 178/3393 [00:51<25:55,  2.07batch/s, Batch Loss=0.3384, Avg Loss=0.1307, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 179/3393 [00:51<26:09,  2.05batch/s, Batch Loss=0.3384, Avg Loss=0.1307, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 179/3393 [00:52<26:09,  2.05batch/s, Batch Loss=0.0207, Avg Loss=0.1297, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 180/3393 [00:52<27:06,  1.98batch/s, Batch Loss=0.0207, Avg Loss=0.1297, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 180/3393 [00:52<27:06,  1.98batch/s, Batch Loss=0.1815, Avg Loss=0.1302, Time Left=26.54 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 181/3393 [00:52<26:46,  2.00batch/s, Batch Loss=0.1815, Avg Loss=0.1302, Time Left=26.54 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 181/3393 [00:53<26:46,  2.00batch/s, Batch Loss=0.1257, Avg Loss=0.1301, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 182/3393 [00:53<26:59,  1.98batch/s, Batch Loss=0.1257, Avg Loss=0.1301, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 182/3393 [00:53<26:59,  1.98batch/s, Batch Loss=0.1504, Avg Loss=0.1303, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 183/3393 [00:53<26:56,  1.99batch/s, Batch Loss=0.1504, Avg Loss=0.1303, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 183/3393 [00:54<26:56,  1.99batch/s, Batch Loss=0.0489, Avg Loss=0.1296, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 184/3393 [00:54<26:53,  1.99batch/s, Batch Loss=0.0489, Avg Loss=0.1296, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 184/3393 [00:54<26:53,  1.99batch/s, Batch Loss=0.0906, Avg Loss=0.1293, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 185/3393 [00:54<26:52,  1.99batch/s, Batch Loss=0.0906, Avg Loss=0.1293, Time Left=26.55 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 185/3393 [00:55<26:52,  1.99batch/s, Batch Loss=0.1669, Avg Loss=0.1296, Time Left=26.54 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 186/3393 [00:55<26:32,  2.01batch/s, Batch Loss=0.1669, Avg Loss=0.1296, Time Left=26.54 \u001b[A\n",
      "Epoch 2/3 - Training:   5%| | 186/3393 [00:55<26:32,  2.01batch/s, Batch Loss=0.1403, Avg Loss=0.1297, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 187/3393 [00:55<26:20,  2.03batch/s, Batch Loss=0.1403, Avg Loss=0.1297, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 187/3393 [00:56<26:20,  2.03batch/s, Batch Loss=0.0393, Avg Loss=0.1289, Time Left=26.52 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 188/3393 [00:56<26:19,  2.03batch/s, Batch Loss=0.0393, Avg Loss=0.1289, Time Left=26.52 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 188/3393 [00:56<26:19,  2.03batch/s, Batch Loss=0.1959, Avg Loss=0.1295, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 189/3393 [00:56<26:32,  2.01batch/s, Batch Loss=0.1959, Avg Loss=0.1295, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 189/3393 [00:57<26:32,  2.01batch/s, Batch Loss=0.1387, Avg Loss=0.1296, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 190/3393 [00:57<26:36,  2.01batch/s, Batch Loss=0.1387, Avg Loss=0.1296, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 190/3393 [00:57<26:36,  2.01batch/s, Batch Loss=0.0752, Avg Loss=0.1291, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 191/3393 [00:57<26:51,  1.99batch/s, Batch Loss=0.0752, Avg Loss=0.1291, Time Left=26.53 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 191/3393 [00:58<26:51,  1.99batch/s, Batch Loss=0.3430, Avg Loss=0.1309, Time Left=26.52 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 192/3393 [00:58<26:33,  2.01batch/s, Batch Loss=0.3430, Avg Loss=0.1309, Time Left=26.52 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 192/3393 [00:58<26:33,  2.01batch/s, Batch Loss=0.0613, Avg Loss=0.1303, Time Left=26.51 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 193/3393 [00:58<26:06,  2.04batch/s, Batch Loss=0.0613, Avg Loss=0.1303, Time Left=26.51 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 193/3393 [00:59<26:06,  2.04batch/s, Batch Loss=0.1099, Avg Loss=0.1301, Time Left=26.50 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 194/3393 [00:59<26:13,  2.03batch/s, Batch Loss=0.1099, Avg Loss=0.1301, Time Left=26.50 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 194/3393 [00:59<26:13,  2.03batch/s, Batch Loss=0.1195, Avg Loss=0.1301, Time Left=26.50 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 195/3393 [00:59<26:05,  2.04batch/s, Batch Loss=0.1195, Avg Loss=0.1301, Time Left=26.50 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 195/3393 [01:00<26:05,  2.04batch/s, Batch Loss=0.2643, Avg Loss=0.1311, Time Left=26.50 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:   6%| | 196/3393 [01:00<26:45,  1.99batch/s, Batch Loss=0.2643, Avg Loss=0.1311, Time Left=26.50 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 196/3393 [01:00<26:45,  1.99batch/s, Batch Loss=0.0823, Avg Loss=0.1307, Time Left=26.49 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 197/3393 [01:00<26:14,  2.03batch/s, Batch Loss=0.0823, Avg Loss=0.1307, Time Left=26.49 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 197/3393 [01:01<26:14,  2.03batch/s, Batch Loss=0.0777, Avg Loss=0.1303, Time Left=26.48 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 198/3393 [01:01<26:05,  2.04batch/s, Batch Loss=0.0777, Avg Loss=0.1303, Time Left=26.48 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 198/3393 [01:01<26:05,  2.04batch/s, Batch Loss=0.0695, Avg Loss=0.1298, Time Left=26.47 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 199/3393 [01:01<25:58,  2.05batch/s, Batch Loss=0.0695, Avg Loss=0.1298, Time Left=26.47 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 199/3393 [01:02<25:58,  2.05batch/s, Batch Loss=0.0713, Avg Loss=0.1294, Time Left=26.47 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 200/3393 [01:02<25:55,  2.05batch/s, Batch Loss=0.0713, Avg Loss=0.1294, Time Left=26.47 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 200/3393 [01:02<25:55,  2.05batch/s, Batch Loss=0.0099, Avg Loss=0.1285, Time Left=26.46 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 201/3393 [01:02<26:05,  2.04batch/s, Batch Loss=0.0099, Avg Loss=0.1285, Time Left=26.46 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 201/3393 [01:03<26:05,  2.04batch/s, Batch Loss=0.0057, Avg Loss=0.1275, Time Left=26.45 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 202/3393 [01:03<26:08,  2.03batch/s, Batch Loss=0.0057, Avg Loss=0.1275, Time Left=26.45 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 202/3393 [01:03<26:08,  2.03batch/s, Batch Loss=0.0221, Avg Loss=0.1267, Time Left=26.45 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 203/3393 [01:03<25:52,  2.05batch/s, Batch Loss=0.0221, Avg Loss=0.1267, Time Left=26.45 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 203/3393 [01:04<25:52,  2.05batch/s, Batch Loss=0.1907, Avg Loss=0.1272, Time Left=26.46 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 204/3393 [01:04<26:34,  2.00batch/s, Batch Loss=0.1907, Avg Loss=0.1272, Time Left=26.46 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 204/3393 [01:04<26:34,  2.00batch/s, Batch Loss=0.1628, Avg Loss=0.1275, Time Left=26.44 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 205/3393 [01:04<26:19,  2.02batch/s, Batch Loss=0.1628, Avg Loss=0.1275, Time Left=26.44 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 205/3393 [01:05<26:19,  2.02batch/s, Batch Loss=0.0046, Avg Loss=0.1265, Time Left=26.44 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 206/3393 [01:05<26:22,  2.01batch/s, Batch Loss=0.0046, Avg Loss=0.1265, Time Left=26.44 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 206/3393 [01:05<26:22,  2.01batch/s, Batch Loss=0.1126, Avg Loss=0.1264, Time Left=26.44 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 207/3393 [01:05<26:27,  2.01batch/s, Batch Loss=0.1126, Avg Loss=0.1264, Time Left=26.44 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 207/3393 [01:06<26:27,  2.01batch/s, Batch Loss=0.2686, Avg Loss=0.1275, Time Left=26.43 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 208/3393 [01:06<26:12,  2.03batch/s, Batch Loss=0.2686, Avg Loss=0.1275, Time Left=26.43 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 208/3393 [01:06<26:12,  2.03batch/s, Batch Loss=0.0267, Avg Loss=0.1268, Time Left=26.41 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 209/3393 [01:06<25:32,  2.08batch/s, Batch Loss=0.0267, Avg Loss=0.1268, Time Left=26.41 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 209/3393 [01:07<25:32,  2.08batch/s, Batch Loss=0.0185, Avg Loss=0.1260, Time Left=26.39 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 210/3393 [01:07<25:18,  2.10batch/s, Batch Loss=0.0185, Avg Loss=0.1260, Time Left=26.39 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 210/3393 [01:07<25:18,  2.10batch/s, Batch Loss=0.0317, Avg Loss=0.1253, Time Left=26.38 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 211/3393 [01:07<25:23,  2.09batch/s, Batch Loss=0.0317, Avg Loss=0.1253, Time Left=26.38 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 211/3393 [01:08<25:23,  2.09batch/s, Batch Loss=0.0206, Avg Loss=0.1245, Time Left=26.38 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 212/3393 [01:08<25:42,  2.06batch/s, Batch Loss=0.0206, Avg Loss=0.1245, Time Left=26.38 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 212/3393 [01:08<25:42,  2.06batch/s, Batch Loss=0.0441, Avg Loss=0.1240, Time Left=26.36 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 213/3393 [01:08<25:25,  2.08batch/s, Batch Loss=0.0441, Avg Loss=0.1240, Time Left=26.36 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 213/3393 [01:09<25:25,  2.08batch/s, Batch Loss=0.0373, Avg Loss=0.1234, Time Left=26.36 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 214/3393 [01:09<25:28,  2.08batch/s, Batch Loss=0.0373, Avg Loss=0.1234, Time Left=26.36 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 214/3393 [01:09<25:28,  2.08batch/s, Batch Loss=0.0133, Avg Loss=0.1226, Time Left=26.34 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 215/3393 [01:09<25:16,  2.10batch/s, Batch Loss=0.0133, Avg Loss=0.1226, Time Left=26.34 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 215/3393 [01:09<25:16,  2.10batch/s, Batch Loss=0.3981, Avg Loss=0.1245, Time Left=26.32 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 216/3393 [01:09<24:51,  2.13batch/s, Batch Loss=0.3981, Avg Loss=0.1245, Time Left=26.32 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 216/3393 [01:10<24:51,  2.13batch/s, Batch Loss=0.0608, Avg Loss=0.1241, Time Left=26.32 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 217/3393 [01:10<25:17,  2.09batch/s, Batch Loss=0.0608, Avg Loss=0.1241, Time Left=26.32 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 217/3393 [01:10<25:17,  2.09batch/s, Batch Loss=0.0539, Avg Loss=0.1236, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 218/3393 [01:10<25:37,  2.07batch/s, Batch Loss=0.0539, Avg Loss=0.1236, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 218/3393 [01:11<25:37,  2.07batch/s, Batch Loss=0.0021, Avg Loss=0.1228, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 219/3393 [01:11<25:56,  2.04batch/s, Batch Loss=0.0021, Avg Loss=0.1228, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 219/3393 [01:12<25:56,  2.04batch/s, Batch Loss=0.1218, Avg Loss=0.1228, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 220/3393 [01:12<26:16,  2.01batch/s, Batch Loss=0.1218, Avg Loss=0.1228, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   6%| | 220/3393 [01:12<26:16,  2.01batch/s, Batch Loss=0.0604, Avg Loss=0.1223, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 221/3393 [01:12<26:06,  2.02batch/s, Batch Loss=0.0604, Avg Loss=0.1223, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 221/3393 [01:13<26:06,  2.02batch/s, Batch Loss=0.0527, Avg Loss=0.1219, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 222/3393 [01:13<26:40,  1.98batch/s, Batch Loss=0.0527, Avg Loss=0.1219, Time Left=26.31 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 222/3393 [01:13<26:40,  1.98batch/s, Batch Loss=0.0509, Avg Loss=0.1214, Time Left=26.30 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 223/3393 [01:13<26:22,  2.00batch/s, Batch Loss=0.0509, Avg Loss=0.1214, Time Left=26.30 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 223/3393 [01:13<26:22,  2.00batch/s, Batch Loss=0.1721, Avg Loss=0.1217, Time Left=26.29 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 224/3393 [01:13<26:02,  2.03batch/s, Batch Loss=0.1721, Avg Loss=0.1217, Time Left=26.29 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 224/3393 [01:14<26:02,  2.03batch/s, Batch Loss=0.0119, Avg Loss=0.1210, Time Left=26.28 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 225/3393 [01:14<25:43,  2.05batch/s, Batch Loss=0.0119, Avg Loss=0.1210, Time Left=26.28 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 225/3393 [01:14<25:43,  2.05batch/s, Batch Loss=0.1144, Avg Loss=0.1210, Time Left=26.26 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 226/3393 [01:14<25:08,  2.10batch/s, Batch Loss=0.1144, Avg Loss=0.1210, Time Left=26.26 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 226/3393 [01:15<25:08,  2.10batch/s, Batch Loss=0.1689, Avg Loss=0.1213, Time Left=26.26 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 227/3393 [01:15<25:29,  2.07batch/s, Batch Loss=0.1689, Avg Loss=0.1213, Time Left=26.26 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 227/3393 [01:15<25:29,  2.07batch/s, Batch Loss=0.0110, Avg Loss=0.1206, Time Left=26.25 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 228/3393 [01:15<25:30,  2.07batch/s, Batch Loss=0.0110, Avg Loss=0.1206, Time Left=26.25 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 228/3393 [01:16<25:30,  2.07batch/s, Batch Loss=0.3139, Avg Loss=0.1218, Time Left=26.24 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:   7%| | 229/3393 [01:16<25:44,  2.05batch/s, Batch Loss=0.3139, Avg Loss=0.1218, Time Left=26.24 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 229/3393 [01:16<25:44,  2.05batch/s, Batch Loss=0.0849, Avg Loss=0.1216, Time Left=26.25 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 230/3393 [01:16<26:09,  2.02batch/s, Batch Loss=0.0849, Avg Loss=0.1216, Time Left=26.25 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 230/3393 [01:17<26:09,  2.02batch/s, Batch Loss=0.5479, Avg Loss=0.1243, Time Left=26.24 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 231/3393 [01:17<26:14,  2.01batch/s, Batch Loss=0.5479, Avg Loss=0.1243, Time Left=26.24 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 231/3393 [01:17<26:14,  2.01batch/s, Batch Loss=0.0229, Avg Loss=0.1236, Time Left=26.25 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 232/3393 [01:17<26:43,  1.97batch/s, Batch Loss=0.0229, Avg Loss=0.1236, Time Left=26.25 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 232/3393 [01:18<26:43,  1.97batch/s, Batch Loss=0.3341, Avg Loss=0.1249, Time Left=26.24 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 233/3393 [01:18<26:30,  1.99batch/s, Batch Loss=0.3341, Avg Loss=0.1249, Time Left=26.24 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 233/3393 [01:18<26:30,  1.99batch/s, Batch Loss=0.2535, Avg Loss=0.1257, Time Left=26.23 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 234/3393 [01:18<26:18,  2.00batch/s, Batch Loss=0.2535, Avg Loss=0.1257, Time Left=26.23 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 234/3393 [01:19<26:18,  2.00batch/s, Batch Loss=0.1495, Avg Loss=0.1259, Time Left=26.22 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 235/3393 [01:19<26:08,  2.01batch/s, Batch Loss=0.1495, Avg Loss=0.1259, Time Left=26.22 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 235/3393 [01:19<26:08,  2.01batch/s, Batch Loss=0.3843, Avg Loss=0.1274, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 236/3393 [01:19<25:24,  2.07batch/s, Batch Loss=0.3843, Avg Loss=0.1274, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 236/3393 [01:20<25:24,  2.07batch/s, Batch Loss=0.0627, Avg Loss=0.1271, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 237/3393 [01:20<26:22,  1.99batch/s, Batch Loss=0.0627, Avg Loss=0.1271, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 237/3393 [01:20<26:22,  1.99batch/s, Batch Loss=0.0871, Avg Loss=0.1268, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 238/3393 [01:20<26:08,  2.01batch/s, Batch Loss=0.0871, Avg Loss=0.1268, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 238/3393 [01:21<26:08,  2.01batch/s, Batch Loss=0.2085, Avg Loss=0.1273, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 239/3393 [01:21<26:24,  1.99batch/s, Batch Loss=0.2085, Avg Loss=0.1273, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 239/3393 [01:21<26:24,  1.99batch/s, Batch Loss=0.0667, Avg Loss=0.1269, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 240/3393 [01:21<26:25,  1.99batch/s, Batch Loss=0.0667, Avg Loss=0.1269, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 240/3393 [01:22<26:25,  1.99batch/s, Batch Loss=0.1849, Avg Loss=0.1273, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 241/3393 [01:22<26:36,  1.97batch/s, Batch Loss=0.1849, Avg Loss=0.1273, Time Left=26.21 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 241/3393 [01:22<26:36,  1.97batch/s, Batch Loss=0.1816, Avg Loss=0.1276, Time Left=26.20 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 242/3393 [01:22<26:16,  2.00batch/s, Batch Loss=0.1816, Avg Loss=0.1276, Time Left=26.20 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 242/3393 [01:23<26:16,  2.00batch/s, Batch Loss=0.1270, Avg Loss=0.1276, Time Left=26.20 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 243/3393 [01:23<26:14,  2.00batch/s, Batch Loss=0.1270, Avg Loss=0.1276, Time Left=26.20 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 243/3393 [01:23<26:14,  2.00batch/s, Batch Loss=0.1044, Avg Loss=0.1275, Time Left=26.18 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 244/3393 [01:23<25:59,  2.02batch/s, Batch Loss=0.1044, Avg Loss=0.1275, Time Left=26.18 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 244/3393 [01:24<25:59,  2.02batch/s, Batch Loss=0.1400, Avg Loss=0.1275, Time Left=26.18 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 245/3393 [01:24<25:47,  2.03batch/s, Batch Loss=0.1400, Avg Loss=0.1275, Time Left=26.18 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 245/3393 [01:24<25:47,  2.03batch/s, Batch Loss=0.2249, Avg Loss=0.1281, Time Left=26.18 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 246/3393 [01:24<26:11,  2.00batch/s, Batch Loss=0.2249, Avg Loss=0.1281, Time Left=26.18 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 246/3393 [01:25<26:11,  2.00batch/s, Batch Loss=0.1289, Avg Loss=0.1281, Time Left=26.16 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 247/3393 [01:25<25:42,  2.04batch/s, Batch Loss=0.1289, Avg Loss=0.1281, Time Left=26.16 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 247/3393 [01:25<25:42,  2.04batch/s, Batch Loss=0.1717, Avg Loss=0.1283, Time Left=26.16 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 248/3393 [01:25<25:50,  2.03batch/s, Batch Loss=0.1717, Avg Loss=0.1283, Time Left=26.16 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 248/3393 [01:26<25:50,  2.03batch/s, Batch Loss=0.0473, Avg Loss=0.1279, Time Left=26.15 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 249/3393 [01:26<25:40,  2.04batch/s, Batch Loss=0.0473, Avg Loss=0.1279, Time Left=26.15 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 249/3393 [01:26<25:40,  2.04batch/s, Batch Loss=0.0885, Avg Loss=0.1277, Time Left=26.14 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 250/3393 [01:26<25:33,  2.05batch/s, Batch Loss=0.0885, Avg Loss=0.1277, Time Left=26.14 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 250/3393 [01:27<25:33,  2.05batch/s, Batch Loss=0.1622, Avg Loss=0.1279, Time Left=26.14 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 251/3393 [01:27<25:58,  2.02batch/s, Batch Loss=0.1622, Avg Loss=0.1279, Time Left=26.14 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 251/3393 [01:27<25:58,  2.02batch/s, Batch Loss=0.4008, Avg Loss=0.1294, Time Left=26.13 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 252/3393 [01:27<25:48,  2.03batch/s, Batch Loss=0.4008, Avg Loss=0.1294, Time Left=26.13 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 252/3393 [01:28<25:48,  2.03batch/s, Batch Loss=0.4308, Avg Loss=0.1310, Time Left=26.12 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 253/3393 [01:28<25:25,  2.06batch/s, Batch Loss=0.4308, Avg Loss=0.1310, Time Left=26.12 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 253/3393 [01:28<25:25,  2.06batch/s, Batch Loss=0.0863, Avg Loss=0.1308, Time Left=26.11 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 254/3393 [01:28<25:22,  2.06batch/s, Batch Loss=0.0863, Avg Loss=0.1308, Time Left=26.11 \u001b[A\n",
      "Epoch 2/3 - Training:   7%| | 254/3393 [01:29<25:22,  2.06batch/s, Batch Loss=0.0751, Avg Loss=0.1305, Time Left=26.10 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 255/3393 [01:29<25:20,  2.06batch/s, Batch Loss=0.0751, Avg Loss=0.1305, Time Left=26.10 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 255/3393 [01:29<25:20,  2.06batch/s, Batch Loss=0.1713, Avg Loss=0.1307, Time Left=26.09 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 256/3393 [01:29<25:16,  2.07batch/s, Batch Loss=0.1713, Avg Loss=0.1307, Time Left=26.09 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 256/3393 [01:30<25:16,  2.07batch/s, Batch Loss=0.2123, Avg Loss=0.1312, Time Left=26.08 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 257/3393 [01:30<25:03,  2.09batch/s, Batch Loss=0.2123, Avg Loss=0.1312, Time Left=26.08 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 257/3393 [01:30<25:03,  2.09batch/s, Batch Loss=0.1461, Avg Loss=0.1312, Time Left=26.06 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 258/3393 [01:30<24:51,  2.10batch/s, Batch Loss=0.1461, Avg Loss=0.1312, Time Left=26.06 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 258/3393 [01:31<24:51,  2.10batch/s, Batch Loss=0.1131, Avg Loss=0.1311, Time Left=26.05 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 259/3393 [01:31<24:58,  2.09batch/s, Batch Loss=0.1131, Avg Loss=0.1311, Time Left=26.05 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 259/3393 [01:31<24:58,  2.09batch/s, Batch Loss=0.1650, Avg Loss=0.1313, Time Left=26.04 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 260/3393 [01:31<25:02,  2.09batch/s, Batch Loss=0.1650, Avg Loss=0.1313, Time Left=26.04 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 260/3393 [01:32<25:02,  2.09batch/s, Batch Loss=0.1633, Avg Loss=0.1315, Time Left=26.03 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 261/3393 [01:32<25:06,  2.08batch/s, Batch Loss=0.1633, Avg Loss=0.1315, Time Left=26.03 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 261/3393 [01:32<25:06,  2.08batch/s, Batch Loss=0.1452, Avg Loss=0.1316, Time Left=26.02 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:   8%| | 262/3393 [01:32<25:07,  2.08batch/s, Batch Loss=0.1452, Avg Loss=0.1316, Time Left=26.02 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 262/3393 [01:33<25:07,  2.08batch/s, Batch Loss=0.0247, Avg Loss=0.1310, Time Left=26.01 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 263/3393 [01:33<24:55,  2.09batch/s, Batch Loss=0.0247, Avg Loss=0.1310, Time Left=26.01 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 263/3393 [01:33<24:55,  2.09batch/s, Batch Loss=0.0643, Avg Loss=0.1307, Time Left=26.00 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 264/3393 [01:33<24:45,  2.11batch/s, Batch Loss=0.0643, Avg Loss=0.1307, Time Left=26.00 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 264/3393 [01:34<24:45,  2.11batch/s, Batch Loss=0.0796, Avg Loss=0.1304, Time Left=25.99 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 265/3393 [01:34<25:16,  2.06batch/s, Batch Loss=0.0796, Avg Loss=0.1304, Time Left=25.99 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 265/3393 [01:34<25:16,  2.06batch/s, Batch Loss=0.0550, Avg Loss=0.1300, Time Left=25.98 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 266/3393 [01:34<25:08,  2.07batch/s, Batch Loss=0.0550, Avg Loss=0.1300, Time Left=25.98 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 266/3393 [01:35<25:08,  2.07batch/s, Batch Loss=0.1066, Avg Loss=0.1299, Time Left=25.97 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 267/3393 [01:35<25:07,  2.07batch/s, Batch Loss=0.1066, Avg Loss=0.1299, Time Left=25.97 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 267/3393 [01:35<25:07,  2.07batch/s, Batch Loss=0.0638, Avg Loss=0.1295, Time Left=25.98 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 268/3393 [01:35<25:53,  2.01batch/s, Batch Loss=0.0638, Avg Loss=0.1295, Time Left=25.98 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 268/3393 [01:36<25:53,  2.01batch/s, Batch Loss=0.4575, Avg Loss=0.1312, Time Left=25.97 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 269/3393 [01:36<25:41,  2.03batch/s, Batch Loss=0.4575, Avg Loss=0.1312, Time Left=25.97 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 269/3393 [01:36<25:41,  2.03batch/s, Batch Loss=0.0191, Avg Loss=0.1306, Time Left=25.97 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 270/3393 [01:36<26:02,  2.00batch/s, Batch Loss=0.0191, Avg Loss=0.1306, Time Left=25.97 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 270/3393 [01:37<26:02,  2.00batch/s, Batch Loss=0.1783, Avg Loss=0.1309, Time Left=25.96 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 271/3393 [01:37<26:02,  2.00batch/s, Batch Loss=0.1783, Avg Loss=0.1309, Time Left=25.96 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 271/3393 [01:37<26:02,  2.00batch/s, Batch Loss=0.1980, Avg Loss=0.1312, Time Left=25.95 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 272/3393 [01:37<25:33,  2.03batch/s, Batch Loss=0.1980, Avg Loss=0.1312, Time Left=25.95 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 272/3393 [01:38<25:33,  2.03batch/s, Batch Loss=0.0746, Avg Loss=0.1309, Time Left=25.95 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 273/3393 [01:38<25:41,  2.02batch/s, Batch Loss=0.0746, Avg Loss=0.1309, Time Left=25.95 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 273/3393 [01:38<25:41,  2.02batch/s, Batch Loss=0.2341, Avg Loss=0.1314, Time Left=25.94 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 274/3393 [01:38<25:28,  2.04batch/s, Batch Loss=0.2341, Avg Loss=0.1314, Time Left=25.94 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 274/3393 [01:39<25:28,  2.04batch/s, Batch Loss=0.0708, Avg Loss=0.1311, Time Left=25.94 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 275/3393 [01:39<25:52,  2.01batch/s, Batch Loss=0.0708, Avg Loss=0.1311, Time Left=25.94 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 275/3393 [01:39<25:52,  2.01batch/s, Batch Loss=0.1157, Avg Loss=0.1311, Time Left=25.92 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 276/3393 [01:39<25:25,  2.04batch/s, Batch Loss=0.1157, Avg Loss=0.1311, Time Left=25.92 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 276/3393 [01:39<25:25,  2.04batch/s, Batch Loss=0.0394, Avg Loss=0.1306, Time Left=25.92 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 277/3393 [01:39<25:34,  2.03batch/s, Batch Loss=0.0394, Avg Loss=0.1306, Time Left=25.92 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 277/3393 [01:40<25:34,  2.03batch/s, Batch Loss=0.0235, Avg Loss=0.1301, Time Left=25.91 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 278/3393 [01:40<25:26,  2.04batch/s, Batch Loss=0.0235, Avg Loss=0.1301, Time Left=25.91 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 278/3393 [01:40<25:26,  2.04batch/s, Batch Loss=0.1322, Avg Loss=0.1301, Time Left=25.90 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 279/3393 [01:40<25:33,  2.03batch/s, Batch Loss=0.1322, Avg Loss=0.1301, Time Left=25.90 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 279/3393 [01:41<25:33,  2.03batch/s, Batch Loss=0.4086, Avg Loss=0.1315, Time Left=25.90 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 280/3393 [01:41<25:25,  2.04batch/s, Batch Loss=0.4086, Avg Loss=0.1315, Time Left=25.90 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 280/3393 [01:41<25:25,  2.04batch/s, Batch Loss=0.3025, Avg Loss=0.1323, Time Left=25.88 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 281/3393 [01:41<25:05,  2.07batch/s, Batch Loss=0.3025, Avg Loss=0.1323, Time Left=25.88 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 281/3393 [01:42<25:05,  2.07batch/s, Batch Loss=0.0760, Avg Loss=0.1320, Time Left=25.87 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 282/3393 [01:42<25:03,  2.07batch/s, Batch Loss=0.0760, Avg Loss=0.1320, Time Left=25.87 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 282/3393 [01:42<25:03,  2.07batch/s, Batch Loss=0.0734, Avg Loss=0.1317, Time Left=25.86 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 283/3393 [01:42<25:03,  2.07batch/s, Batch Loss=0.0734, Avg Loss=0.1317, Time Left=25.86 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 283/3393 [01:43<25:03,  2.07batch/s, Batch Loss=0.0343, Avg Loss=0.1313, Time Left=25.85 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 284/3393 [01:43<25:02,  2.07batch/s, Batch Loss=0.0343, Avg Loss=0.1313, Time Left=25.85 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 284/3393 [01:43<25:02,  2.07batch/s, Batch Loss=0.5415, Avg Loss=0.1332, Time Left=25.85 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 285/3393 [01:43<25:02,  2.07batch/s, Batch Loss=0.5415, Avg Loss=0.1332, Time Left=25.85 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 285/3393 [01:44<25:02,  2.07batch/s, Batch Loss=0.0733, Avg Loss=0.1329, Time Left=25.84 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 286/3393 [01:44<25:18,  2.05batch/s, Batch Loss=0.0733, Avg Loss=0.1329, Time Left=25.84 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 286/3393 [01:44<25:18,  2.05batch/s, Batch Loss=0.4394, Avg Loss=0.1343, Time Left=25.84 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 287/3393 [01:44<25:28,  2.03batch/s, Batch Loss=0.4394, Avg Loss=0.1343, Time Left=25.84 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 287/3393 [01:45<25:28,  2.03batch/s, Batch Loss=0.0720, Avg Loss=0.1340, Time Left=25.83 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 288/3393 [01:45<26:07,  1.98batch/s, Batch Loss=0.0720, Avg Loss=0.1340, Time Left=25.83 \u001b[A\n",
      "Epoch 2/3 - Training:   8%| | 288/3393 [01:45<26:07,  1.98batch/s, Batch Loss=0.0296, Avg Loss=0.1336, Time Left=25.83 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 289/3393 [01:45<25:46,  2.01batch/s, Batch Loss=0.0296, Avg Loss=0.1336, Time Left=25.83 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 289/3393 [01:46<25:46,  2.01batch/s, Batch Loss=0.0928, Avg Loss=0.1334, Time Left=25.83 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 290/3393 [01:46<26:16,  1.97batch/s, Batch Loss=0.0928, Avg Loss=0.1334, Time Left=25.83 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 290/3393 [01:46<26:16,  1.97batch/s, Batch Loss=0.4866, Avg Loss=0.1350, Time Left=25.82 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 291/3393 [01:46<25:27,  2.03batch/s, Batch Loss=0.4866, Avg Loss=0.1350, Time Left=25.82 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 291/3393 [01:47<25:27,  2.03batch/s, Batch Loss=0.0319, Avg Loss=0.1345, Time Left=25.81 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 292/3393 [01:47<25:17,  2.04batch/s, Batch Loss=0.0319, Avg Loss=0.1345, Time Left=25.81 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 292/3393 [01:47<25:17,  2.04batch/s, Batch Loss=0.0188, Avg Loss=0.1340, Time Left=25.80 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 293/3393 [01:47<25:25,  2.03batch/s, Batch Loss=0.0188, Avg Loss=0.1340, Time Left=25.80 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 293/3393 [01:48<25:25,  2.03batch/s, Batch Loss=0.0825, Avg Loss=0.1338, Time Left=25.80 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 294/3393 [01:48<25:32,  2.02batch/s, Batch Loss=0.0825, Avg Loss=0.1338, Time Left=25.80 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 294/3393 [01:48<25:32,  2.02batch/s, Batch Loss=0.0946, Avg Loss=0.1336, Time Left=25.79 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:   9%| | 295/3393 [01:48<25:36,  2.02batch/s, Batch Loss=0.0946, Avg Loss=0.1336, Time Left=25.79 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 295/3393 [01:49<25:36,  2.02batch/s, Batch Loss=0.1066, Avg Loss=0.1335, Time Left=25.78 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 296/3393 [01:49<25:25,  2.03batch/s, Batch Loss=0.1066, Avg Loss=0.1335, Time Left=25.78 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 296/3393 [01:49<25:25,  2.03batch/s, Batch Loss=0.0281, Avg Loss=0.1330, Time Left=25.77 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 297/3393 [01:49<25:17,  2.04batch/s, Batch Loss=0.0281, Avg Loss=0.1330, Time Left=25.77 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 297/3393 [01:50<25:17,  2.04batch/s, Batch Loss=0.0249, Avg Loss=0.1325, Time Left=25.77 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 298/3393 [01:50<25:24,  2.03batch/s, Batch Loss=0.0249, Avg Loss=0.1325, Time Left=25.77 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 298/3393 [01:50<25:24,  2.03batch/s, Batch Loss=0.0149, Avg Loss=0.1320, Time Left=25.75 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 299/3393 [01:50<25:17,  2.04batch/s, Batch Loss=0.0149, Avg Loss=0.1320, Time Left=25.75 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 299/3393 [01:51<25:17,  2.04batch/s, Batch Loss=0.3708, Avg Loss=0.1331, Time Left=25.75 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 300/3393 [01:51<25:30,  2.02batch/s, Batch Loss=0.3708, Avg Loss=0.1331, Time Left=25.75 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 300/3393 [01:51<25:30,  2.02batch/s, Batch Loss=0.3430, Avg Loss=0.1340, Time Left=25.74 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 301/3393 [01:51<25:26,  2.03batch/s, Batch Loss=0.3430, Avg Loss=0.1340, Time Left=25.74 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 301/3393 [01:52<25:26,  2.03batch/s, Batch Loss=0.1359, Avg Loss=0.1340, Time Left=25.74 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 302/3393 [01:52<25:25,  2.03batch/s, Batch Loss=0.1359, Avg Loss=0.1340, Time Left=25.74 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 302/3393 [01:52<25:25,  2.03batch/s, Batch Loss=0.4724, Avg Loss=0.1354, Time Left=25.73 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 303/3393 [01:52<24:57,  2.06batch/s, Batch Loss=0.4724, Avg Loss=0.1354, Time Left=25.73 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 303/3393 [01:53<24:57,  2.06batch/s, Batch Loss=0.6765, Avg Loss=0.1378, Time Left=25.71 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 304/3393 [01:53<24:45,  2.08batch/s, Batch Loss=0.6765, Avg Loss=0.1378, Time Left=25.71 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 304/3393 [01:53<24:45,  2.08batch/s, Batch Loss=0.0902, Avg Loss=0.1376, Time Left=25.71 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 305/3393 [01:53<25:10,  2.04batch/s, Batch Loss=0.0902, Avg Loss=0.1376, Time Left=25.71 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 305/3393 [01:54<25:10,  2.04batch/s, Batch Loss=0.1607, Avg Loss=0.1377, Time Left=25.70 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 306/3393 [01:54<25:13,  2.04batch/s, Batch Loss=0.1607, Avg Loss=0.1377, Time Left=25.70 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 306/3393 [01:54<25:13,  2.04batch/s, Batch Loss=0.0610, Avg Loss=0.1373, Time Left=25.69 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 307/3393 [01:54<25:00,  2.06batch/s, Batch Loss=0.0610, Avg Loss=0.1373, Time Left=25.69 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 307/3393 [01:55<25:00,  2.06batch/s, Batch Loss=0.1862, Avg Loss=0.1376, Time Left=25.69 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 308/3393 [01:55<25:11,  2.04batch/s, Batch Loss=0.1862, Avg Loss=0.1376, Time Left=25.69 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 308/3393 [01:55<25:11,  2.04batch/s, Batch Loss=0.1885, Avg Loss=0.1378, Time Left=25.67 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 309/3393 [01:55<25:06,  2.05batch/s, Batch Loss=0.1885, Avg Loss=0.1378, Time Left=25.67 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 309/3393 [01:56<25:06,  2.05batch/s, Batch Loss=0.2653, Avg Loss=0.1383, Time Left=25.67 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 310/3393 [01:56<25:01,  2.05batch/s, Batch Loss=0.2653, Avg Loss=0.1383, Time Left=25.67 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 310/3393 [01:56<25:01,  2.05batch/s, Batch Loss=0.1347, Avg Loss=0.1383, Time Left=25.66 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 311/3393 [01:56<24:59,  2.06batch/s, Batch Loss=0.1347, Avg Loss=0.1383, Time Left=25.66 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 311/3393 [01:57<24:59,  2.06batch/s, Batch Loss=0.1249, Avg Loss=0.1382, Time Left=25.65 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 312/3393 [01:57<24:40,  2.08batch/s, Batch Loss=0.1249, Avg Loss=0.1382, Time Left=25.65 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 312/3393 [01:57<24:40,  2.08batch/s, Batch Loss=0.0966, Avg Loss=0.1381, Time Left=25.64 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 313/3393 [01:57<24:41,  2.08batch/s, Batch Loss=0.0966, Avg Loss=0.1381, Time Left=25.64 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 313/3393 [01:58<24:41,  2.08batch/s, Batch Loss=0.1940, Avg Loss=0.1383, Time Left=25.63 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 314/3393 [01:58<24:31,  2.09batch/s, Batch Loss=0.1940, Avg Loss=0.1383, Time Left=25.63 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 314/3393 [01:58<24:31,  2.09batch/s, Batch Loss=0.1091, Avg Loss=0.1382, Time Left=25.62 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 315/3393 [01:58<24:48,  2.07batch/s, Batch Loss=0.1091, Avg Loss=0.1382, Time Left=25.62 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 315/3393 [01:59<24:48,  2.07batch/s, Batch Loss=0.2341, Avg Loss=0.1386, Time Left=25.62 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 316/3393 [01:59<25:17,  2.03batch/s, Batch Loss=0.2341, Avg Loss=0.1386, Time Left=25.62 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 316/3393 [01:59<25:17,  2.03batch/s, Batch Loss=0.0967, Avg Loss=0.1384, Time Left=25.61 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 317/3393 [01:59<25:10,  2.04batch/s, Batch Loss=0.0967, Avg Loss=0.1384, Time Left=25.61 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 317/3393 [02:00<25:10,  2.04batch/s, Batch Loss=0.0585, Avg Loss=0.1381, Time Left=25.60 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 318/3393 [02:00<25:17,  2.03batch/s, Batch Loss=0.0585, Avg Loss=0.1381, Time Left=25.60 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 318/3393 [02:00<25:17,  2.03batch/s, Batch Loss=0.1661, Avg Loss=0.1382, Time Left=25.59 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 319/3393 [02:00<25:09,  2.04batch/s, Batch Loss=0.1661, Avg Loss=0.1382, Time Left=25.59 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 319/3393 [02:01<25:09,  2.04batch/s, Batch Loss=0.2244, Avg Loss=0.1385, Time Left=25.58 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 320/3393 [02:01<24:46,  2.07batch/s, Batch Loss=0.2244, Avg Loss=0.1385, Time Left=25.58 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 320/3393 [02:01<24:46,  2.07batch/s, Batch Loss=0.2746, Avg Loss=0.1391, Time Left=25.58 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 321/3393 [02:01<25:13,  2.03batch/s, Batch Loss=0.2746, Avg Loss=0.1391, Time Left=25.58 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 321/3393 [02:02<25:13,  2.03batch/s, Batch Loss=0.2139, Avg Loss=0.1394, Time Left=25.57 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 322/3393 [02:02<25:07,  2.04batch/s, Batch Loss=0.2139, Avg Loss=0.1394, Time Left=25.57 \u001b[A\n",
      "Epoch 2/3 - Training:   9%| | 322/3393 [02:02<25:07,  2.04batch/s, Batch Loss=0.2718, Avg Loss=0.1399, Time Left=25.56 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 323/3393 [02:02<24:45,  2.07batch/s, Batch Loss=0.2718, Avg Loss=0.1399, Time Left=25.56 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 323/3393 [02:03<24:45,  2.07batch/s, Batch Loss=0.1709, Avg Loss=0.1400, Time Left=25.55 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 324/3393 [02:03<24:59,  2.05batch/s, Batch Loss=0.1709, Avg Loss=0.1400, Time Left=25.55 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 324/3393 [02:03<24:59,  2.05batch/s, Batch Loss=0.5677, Avg Loss=0.1417, Time Left=25.54 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 325/3393 [02:03<24:41,  2.07batch/s, Batch Loss=0.5677, Avg Loss=0.1417, Time Left=25.54 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 325/3393 [02:03<24:41,  2.07batch/s, Batch Loss=0.1235, Avg Loss=0.1416, Time Left=25.53 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 326/3393 [02:03<24:40,  2.07batch/s, Batch Loss=0.1235, Avg Loss=0.1416, Time Left=25.53 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 326/3393 [02:04<24:40,  2.07batch/s, Batch Loss=0.0282, Avg Loss=0.1412, Time Left=25.52 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 327/3393 [02:04<24:41,  2.07batch/s, Batch Loss=0.0282, Avg Loss=0.1412, Time Left=25.52 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 327/3393 [02:04<24:41,  2.07batch/s, Batch Loss=0.2157, Avg Loss=0.1415, Time Left=25.51 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  10%| | 328/3393 [02:04<24:34,  2.08batch/s, Batch Loss=0.2157, Avg Loss=0.1415, Time Left=25.51 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 328/3393 [02:05<24:34,  2.08batch/s, Batch Loss=0.1438, Avg Loss=0.1415, Time Left=25.50 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 329/3393 [02:05<24:28,  2.09batch/s, Batch Loss=0.1438, Avg Loss=0.1415, Time Left=25.50 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 329/3393 [02:05<24:28,  2.09batch/s, Batch Loss=0.1516, Avg Loss=0.1415, Time Left=25.49 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 330/3393 [02:05<24:35,  2.08batch/s, Batch Loss=0.1516, Avg Loss=0.1415, Time Left=25.49 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 330/3393 [02:06<24:35,  2.08batch/s, Batch Loss=0.0155, Avg Loss=0.1411, Time Left=25.48 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 331/3393 [02:06<24:16,  2.10batch/s, Batch Loss=0.0155, Avg Loss=0.1411, Time Left=25.48 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 331/3393 [02:06<24:16,  2.10batch/s, Batch Loss=0.0348, Avg Loss=0.1406, Time Left=25.47 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 332/3393 [02:06<24:23,  2.09batch/s, Batch Loss=0.0348, Avg Loss=0.1406, Time Left=25.47 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 332/3393 [02:07<24:23,  2.09batch/s, Batch Loss=0.0709, Avg Loss=0.1404, Time Left=25.46 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 333/3393 [02:07<24:36,  2.07batch/s, Batch Loss=0.0709, Avg Loss=0.1404, Time Left=25.46 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 333/3393 [02:07<24:36,  2.07batch/s, Batch Loss=0.0252, Avg Loss=0.1399, Time Left=25.45 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 334/3393 [02:07<24:15,  2.10batch/s, Batch Loss=0.0252, Avg Loss=0.1399, Time Left=25.45 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 334/3393 [02:08<24:15,  2.10batch/s, Batch Loss=0.0899, Avg Loss=0.1397, Time Left=25.44 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 335/3393 [02:08<24:05,  2.12batch/s, Batch Loss=0.0899, Avg Loss=0.1397, Time Left=25.44 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 335/3393 [02:08<24:05,  2.12batch/s, Batch Loss=0.0490, Avg Loss=0.1394, Time Left=25.43 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 336/3393 [02:08<24:14,  2.10batch/s, Batch Loss=0.0490, Avg Loss=0.1394, Time Left=25.43 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 336/3393 [02:09<24:14,  2.10batch/s, Batch Loss=0.1920, Avg Loss=0.1396, Time Left=25.42 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 337/3393 [02:09<24:07,  2.11batch/s, Batch Loss=0.1920, Avg Loss=0.1396, Time Left=25.42 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 337/3393 [02:09<24:07,  2.11batch/s, Batch Loss=0.3562, Avg Loss=0.1404, Time Left=25.41 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 338/3393 [02:09<24:15,  2.10batch/s, Batch Loss=0.3562, Avg Loss=0.1404, Time Left=25.41 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 338/3393 [02:10<24:15,  2.10batch/s, Batch Loss=0.1409, Avg Loss=0.1404, Time Left=25.40 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 339/3393 [02:10<24:35,  2.07batch/s, Batch Loss=0.1409, Avg Loss=0.1404, Time Left=25.40 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 339/3393 [02:10<24:35,  2.07batch/s, Batch Loss=0.0207, Avg Loss=0.1400, Time Left=25.39 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 340/3393 [02:10<24:07,  2.11batch/s, Batch Loss=0.0207, Avg Loss=0.1400, Time Left=25.39 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 340/3393 [02:11<24:07,  2.11batch/s, Batch Loss=0.0323, Avg Loss=0.1396, Time Left=25.38 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 341/3393 [02:11<24:14,  2.10batch/s, Batch Loss=0.0323, Avg Loss=0.1396, Time Left=25.38 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 341/3393 [02:11<24:14,  2.10batch/s, Batch Loss=0.0143, Avg Loss=0.1391, Time Left=25.37 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 342/3393 [02:11<24:49,  2.05batch/s, Batch Loss=0.0143, Avg Loss=0.1391, Time Left=25.37 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 342/3393 [02:12<24:49,  2.05batch/s, Batch Loss=0.1244, Avg Loss=0.1391, Time Left=25.37 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 343/3393 [02:12<24:47,  2.05batch/s, Batch Loss=0.1244, Avg Loss=0.1391, Time Left=25.37 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 343/3393 [02:12<24:47,  2.05batch/s, Batch Loss=0.0190, Avg Loss=0.1386, Time Left=25.36 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 344/3393 [02:12<24:56,  2.04batch/s, Batch Loss=0.0190, Avg Loss=0.1386, Time Left=25.36 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 344/3393 [02:13<24:56,  2.04batch/s, Batch Loss=0.0722, Avg Loss=0.1384, Time Left=25.35 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 345/3393 [02:13<24:51,  2.04batch/s, Batch Loss=0.0722, Avg Loss=0.1384, Time Left=25.35 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 345/3393 [02:13<24:51,  2.04batch/s, Batch Loss=0.0819, Avg Loss=0.1382, Time Left=25.34 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 346/3393 [02:13<24:50,  2.04batch/s, Batch Loss=0.0819, Avg Loss=0.1382, Time Left=25.34 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 346/3393 [02:14<24:50,  2.04batch/s, Batch Loss=0.1475, Avg Loss=0.1382, Time Left=25.33 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 347/3393 [02:14<24:40,  2.06batch/s, Batch Loss=0.1475, Avg Loss=0.1382, Time Left=25.33 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 347/3393 [02:14<24:40,  2.06batch/s, Batch Loss=0.3811, Avg Loss=0.1391, Time Left=25.32 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 348/3393 [02:14<24:08,  2.10batch/s, Batch Loss=0.3811, Avg Loss=0.1391, Time Left=25.32 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 348/3393 [02:14<24:08,  2.10batch/s, Batch Loss=0.0096, Avg Loss=0.1386, Time Left=25.31 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 349/3393 [02:14<24:13,  2.09batch/s, Batch Loss=0.0096, Avg Loss=0.1386, Time Left=25.31 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 349/3393 [02:15<24:13,  2.09batch/s, Batch Loss=0.0072, Avg Loss=0.1381, Time Left=25.30 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 350/3393 [02:15<24:18,  2.09batch/s, Batch Loss=0.0072, Avg Loss=0.1381, Time Left=25.30 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 350/3393 [02:15<24:18,  2.09batch/s, Batch Loss=0.3004, Avg Loss=0.1387, Time Left=25.29 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 351/3393 [02:15<24:27,  2.07batch/s, Batch Loss=0.3004, Avg Loss=0.1387, Time Left=25.29 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 351/3393 [02:16<24:27,  2.07batch/s, Batch Loss=0.2399, Avg Loss=0.1391, Time Left=25.29 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 352/3393 [02:16<25:20,  2.00batch/s, Batch Loss=0.2399, Avg Loss=0.1391, Time Left=25.29 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 352/3393 [02:16<25:20,  2.00batch/s, Batch Loss=0.0625, Avg Loss=0.1388, Time Left=25.28 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 353/3393 [02:16<25:07,  2.02batch/s, Batch Loss=0.0625, Avg Loss=0.1388, Time Left=25.28 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 353/3393 [02:17<25:07,  2.02batch/s, Batch Loss=0.0801, Avg Loss=0.1386, Time Left=25.28 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 354/3393 [02:17<24:55,  2.03batch/s, Batch Loss=0.0801, Avg Loss=0.1386, Time Left=25.28 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 354/3393 [02:17<24:55,  2.03batch/s, Batch Loss=0.0935, Avg Loss=0.1384, Time Left=25.27 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 355/3393 [02:17<24:35,  2.06batch/s, Batch Loss=0.0935, Avg Loss=0.1384, Time Left=25.27 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 355/3393 [02:18<24:35,  2.06batch/s, Batch Loss=0.0149, Avg Loss=0.1380, Time Left=25.25 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 356/3393 [02:18<24:16,  2.08batch/s, Batch Loss=0.0149, Avg Loss=0.1380, Time Left=25.25 \u001b[A\n",
      "Epoch 2/3 - Training:  10%| | 356/3393 [02:18<24:16,  2.08batch/s, Batch Loss=0.1471, Avg Loss=0.1380, Time Left=25.25 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 357/3393 [02:18<24:33,  2.06batch/s, Batch Loss=0.1471, Avg Loss=0.1380, Time Left=25.25 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 357/3393 [02:19<24:33,  2.06batch/s, Batch Loss=0.2013, Avg Loss=0.1383, Time Left=25.24 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 358/3393 [02:19<24:17,  2.08batch/s, Batch Loss=0.2013, Avg Loss=0.1383, Time Left=25.24 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 358/3393 [02:19<24:17,  2.08batch/s, Batch Loss=0.2699, Avg Loss=0.1387, Time Left=25.22 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 359/3393 [02:19<24:05,  2.10batch/s, Batch Loss=0.2699, Avg Loss=0.1387, Time Left=25.22 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 359/3393 [02:20<24:05,  2.10batch/s, Batch Loss=0.2519, Avg Loss=0.1391, Time Left=25.22 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 360/3393 [02:20<24:25,  2.07batch/s, Batch Loss=0.2519, Avg Loss=0.1391, Time Left=25.22 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 360/3393 [02:20<24:25,  2.07batch/s, Batch Loss=0.1491, Avg Loss=0.1391, Time Left=25.21 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  11%| | 361/3393 [02:20<24:27,  2.07batch/s, Batch Loss=0.1491, Avg Loss=0.1391, Time Left=25.21 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 361/3393 [02:21<24:27,  2.07batch/s, Batch Loss=0.0191, Avg Loss=0.1387, Time Left=25.20 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 362/3393 [02:21<24:26,  2.07batch/s, Batch Loss=0.0191, Avg Loss=0.1387, Time Left=25.20 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 362/3393 [02:21<24:26,  2.07batch/s, Batch Loss=0.0437, Avg Loss=0.1384, Time Left=25.20 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 363/3393 [02:21<24:53,  2.03batch/s, Batch Loss=0.0437, Avg Loss=0.1384, Time Left=25.20 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 363/3393 [02:22<24:53,  2.03batch/s, Batch Loss=0.0462, Avg Loss=0.1381, Time Left=25.19 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 364/3393 [02:22<24:45,  2.04batch/s, Batch Loss=0.0462, Avg Loss=0.1381, Time Left=25.19 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 364/3393 [02:22<24:45,  2.04batch/s, Batch Loss=0.2810, Avg Loss=0.1386, Time Left=25.18 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 365/3393 [02:22<24:32,  2.06batch/s, Batch Loss=0.2810, Avg Loss=0.1386, Time Left=25.18 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 365/3393 [02:23<24:32,  2.06batch/s, Batch Loss=0.1045, Avg Loss=0.1385, Time Left=25.17 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 366/3393 [02:23<24:51,  2.03batch/s, Batch Loss=0.1045, Avg Loss=0.1385, Time Left=25.17 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 366/3393 [02:23<24:51,  2.03batch/s, Batch Loss=0.0295, Avg Loss=0.1381, Time Left=25.17 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 367/3393 [02:23<24:44,  2.04batch/s, Batch Loss=0.0295, Avg Loss=0.1381, Time Left=25.17 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 367/3393 [02:24<24:44,  2.04batch/s, Batch Loss=0.1040, Avg Loss=0.1380, Time Left=25.16 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 368/3393 [02:24<24:50,  2.03batch/s, Batch Loss=0.1040, Avg Loss=0.1380, Time Left=25.16 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 368/3393 [02:24<24:50,  2.03batch/s, Batch Loss=0.0405, Avg Loss=0.1376, Time Left=25.15 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 369/3393 [02:24<25:08,  2.00batch/s, Batch Loss=0.0405, Avg Loss=0.1376, Time Left=25.15 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 369/3393 [02:25<25:08,  2.00batch/s, Batch Loss=0.0307, Avg Loss=0.1373, Time Left=25.15 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 370/3393 [02:25<25:12,  2.00batch/s, Batch Loss=0.0307, Avg Loss=0.1373, Time Left=25.15 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 370/3393 [02:25<25:12,  2.00batch/s, Batch Loss=0.0530, Avg Loss=0.1370, Time Left=25.14 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 371/3393 [02:25<24:43,  2.04batch/s, Batch Loss=0.0530, Avg Loss=0.1370, Time Left=25.14 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 371/3393 [02:26<24:43,  2.04batch/s, Batch Loss=0.1396, Avg Loss=0.1370, Time Left=25.13 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 372/3393 [02:26<24:37,  2.04batch/s, Batch Loss=0.1396, Avg Loss=0.1370, Time Left=25.13 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 372/3393 [02:26<24:37,  2.04batch/s, Batch Loss=0.1006, Avg Loss=0.1369, Time Left=25.13 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 373/3393 [02:26<24:59,  2.01batch/s, Batch Loss=0.1006, Avg Loss=0.1369, Time Left=25.13 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 373/3393 [02:27<24:59,  2.01batch/s, Batch Loss=0.1266, Avg Loss=0.1369, Time Left=25.12 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 374/3393 [02:27<25:03,  2.01batch/s, Batch Loss=0.1266, Avg Loss=0.1369, Time Left=25.12 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 374/3393 [02:27<25:03,  2.01batch/s, Batch Loss=0.2954, Avg Loss=0.1374, Time Left=25.12 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 375/3393 [02:27<25:18,  1.99batch/s, Batch Loss=0.2954, Avg Loss=0.1374, Time Left=25.12 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 375/3393 [02:28<25:18,  1.99batch/s, Batch Loss=0.0679, Avg Loss=0.1372, Time Left=25.11 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 376/3393 [02:28<25:02,  2.01batch/s, Batch Loss=0.0679, Avg Loss=0.1372, Time Left=25.11 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 376/3393 [02:28<25:02,  2.01batch/s, Batch Loss=0.0671, Avg Loss=0.1369, Time Left=25.11 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 377/3393 [02:28<25:18,  1.99batch/s, Batch Loss=0.0671, Avg Loss=0.1369, Time Left=25.11 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 377/3393 [02:29<25:18,  1.99batch/s, Batch Loss=0.1007, Avg Loss=0.1368, Time Left=25.10 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 378/3393 [02:29<25:16,  1.99batch/s, Batch Loss=0.1007, Avg Loss=0.1368, Time Left=25.10 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 378/3393 [02:29<25:16,  1.99batch/s, Batch Loss=0.1992, Avg Loss=0.1370, Time Left=25.09 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 379/3393 [02:29<24:29,  2.05batch/s, Batch Loss=0.1992, Avg Loss=0.1370, Time Left=25.09 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 379/3393 [02:30<24:29,  2.05batch/s, Batch Loss=0.0511, Avg Loss=0.1367, Time Left=25.08 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 380/3393 [02:30<24:38,  2.04batch/s, Batch Loss=0.0511, Avg Loss=0.1367, Time Left=25.08 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 380/3393 [02:30<24:38,  2.04batch/s, Batch Loss=0.3531, Avg Loss=0.1374, Time Left=25.07 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 381/3393 [02:30<24:47,  2.03batch/s, Batch Loss=0.3531, Avg Loss=0.1374, Time Left=25.07 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 381/3393 [02:31<24:47,  2.03batch/s, Batch Loss=0.0132, Avg Loss=0.1370, Time Left=25.07 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 382/3393 [02:31<25:05,  2.00batch/s, Batch Loss=0.0132, Avg Loss=0.1370, Time Left=25.07 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 382/3393 [02:31<25:05,  2.00batch/s, Batch Loss=0.0409, Avg Loss=0.1367, Time Left=25.06 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 383/3393 [02:31<24:52,  2.02batch/s, Batch Loss=0.0409, Avg Loss=0.1367, Time Left=25.06 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 383/3393 [02:32<24:52,  2.02batch/s, Batch Loss=0.1003, Avg Loss=0.1366, Time Left=25.05 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 384/3393 [02:32<24:40,  2.03batch/s, Batch Loss=0.1003, Avg Loss=0.1366, Time Left=25.05 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 384/3393 [02:32<24:40,  2.03batch/s, Batch Loss=0.1633, Avg Loss=0.1367, Time Left=25.04 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 385/3393 [02:32<24:34,  2.04batch/s, Batch Loss=0.1633, Avg Loss=0.1367, Time Left=25.04 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 385/3393 [02:33<24:34,  2.04batch/s, Batch Loss=0.0738, Avg Loss=0.1365, Time Left=25.03 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 386/3393 [02:33<24:14,  2.07batch/s, Batch Loss=0.0738, Avg Loss=0.1365, Time Left=25.03 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 386/3393 [02:33<24:14,  2.07batch/s, Batch Loss=0.0269, Avg Loss=0.1361, Time Left=25.02 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 387/3393 [02:33<24:10,  2.07batch/s, Batch Loss=0.0269, Avg Loss=0.1361, Time Left=25.02 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 387/3393 [02:34<24:10,  2.07batch/s, Batch Loss=0.1149, Avg Loss=0.1361, Time Left=25.02 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 388/3393 [02:34<24:13,  2.07batch/s, Batch Loss=0.1149, Avg Loss=0.1361, Time Left=25.02 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 388/3393 [02:34<24:13,  2.07batch/s, Batch Loss=0.1342, Avg Loss=0.1361, Time Left=25.00 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 389/3393 [02:34<23:57,  2.09batch/s, Batch Loss=0.1342, Avg Loss=0.1361, Time Left=25.00 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 389/3393 [02:35<23:57,  2.09batch/s, Batch Loss=0.0415, Avg Loss=0.1358, Time Left=25.00 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 390/3393 [02:35<24:44,  2.02batch/s, Batch Loss=0.0415, Avg Loss=0.1358, Time Left=25.00 \u001b[A\n",
      "Epoch 2/3 - Training:  11%| | 390/3393 [02:35<24:44,  2.02batch/s, Batch Loss=0.1066, Avg Loss=0.1357, Time Left=24.99 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 391/3393 [02:35<24:25,  2.05batch/s, Batch Loss=0.1066, Avg Loss=0.1357, Time Left=24.99 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 391/3393 [02:36<24:25,  2.05batch/s, Batch Loss=0.2635, Avg Loss=0.1361, Time Left=24.99 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 392/3393 [02:36<24:38,  2.03batch/s, Batch Loss=0.2635, Avg Loss=0.1361, Time Left=24.99 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 392/3393 [02:36<24:38,  2.03batch/s, Batch Loss=0.0468, Avg Loss=0.1358, Time Left=24.98 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 393/3393 [02:36<24:23,  2.05batch/s, Batch Loss=0.0468, Avg Loss=0.1358, Time Left=24.98 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 393/3393 [02:37<24:23,  2.05batch/s, Batch Loss=0.1064, Avg Loss=0.1357, Time Left=24.97 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  12%| | 394/3393 [02:37<24:18,  2.06batch/s, Batch Loss=0.1064, Avg Loss=0.1357, Time Left=24.97 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 394/3393 [02:37<24:18,  2.06batch/s, Batch Loss=0.1604, Avg Loss=0.1358, Time Left=24.96 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 395/3393 [02:37<24:57,  2.00batch/s, Batch Loss=0.1604, Avg Loss=0.1358, Time Left=24.96 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 395/3393 [02:38<24:57,  2.00batch/s, Batch Loss=0.0643, Avg Loss=0.1356, Time Left=24.96 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 396/3393 [02:38<24:44,  2.02batch/s, Batch Loss=0.0643, Avg Loss=0.1356, Time Left=24.96 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 396/3393 [02:38<24:44,  2.02batch/s, Batch Loss=0.0040, Avg Loss=0.1352, Time Left=24.95 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 397/3393 [02:38<25:02,  1.99batch/s, Batch Loss=0.0040, Avg Loss=0.1352, Time Left=24.95 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 397/3393 [02:39<25:02,  1.99batch/s, Batch Loss=0.0700, Avg Loss=0.1350, Time Left=24.95 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 398/3393 [02:39<25:05,  1.99batch/s, Batch Loss=0.0700, Avg Loss=0.1350, Time Left=24.95 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 398/3393 [02:39<25:05,  1.99batch/s, Batch Loss=0.0083, Avg Loss=0.1346, Time Left=24.94 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 399/3393 [02:39<24:44,  2.02batch/s, Batch Loss=0.0083, Avg Loss=0.1346, Time Left=24.94 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 399/3393 [02:40<24:44,  2.02batch/s, Batch Loss=0.0206, Avg Loss=0.1342, Time Left=24.93 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 400/3393 [02:40<24:34,  2.03batch/s, Batch Loss=0.0206, Avg Loss=0.1342, Time Left=24.93 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 400/3393 [02:40<24:34,  2.03batch/s, Batch Loss=0.0085, Avg Loss=0.1338, Time Left=24.92 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 401/3393 [02:40<24:24,  2.04batch/s, Batch Loss=0.0085, Avg Loss=0.1338, Time Left=24.92 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 401/3393 [02:41<24:24,  2.04batch/s, Batch Loss=0.0328, Avg Loss=0.1335, Time Left=24.92 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 402/3393 [02:41<24:46,  2.01batch/s, Batch Loss=0.0328, Avg Loss=0.1335, Time Left=24.92 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 402/3393 [02:41<24:46,  2.01batch/s, Batch Loss=0.0150, Avg Loss=0.1332, Time Left=24.91 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 403/3393 [02:41<25:05,  1.99batch/s, Batch Loss=0.0150, Avg Loss=0.1332, Time Left=24.91 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 403/3393 [02:42<25:05,  1.99batch/s, Batch Loss=0.1295, Avg Loss=0.1332, Time Left=24.92 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 404/3393 [02:42<25:43,  1.94batch/s, Batch Loss=0.1295, Avg Loss=0.1332, Time Left=24.92 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 404/3393 [02:42<25:43,  1.94batch/s, Batch Loss=0.2390, Avg Loss=0.1335, Time Left=24.91 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 405/3393 [02:42<25:31,  1.95batch/s, Batch Loss=0.2390, Avg Loss=0.1335, Time Left=24.91 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 405/3393 [02:43<25:31,  1.95batch/s, Batch Loss=0.4394, Avg Loss=0.1344, Time Left=24.91 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 406/3393 [02:43<25:39,  1.94batch/s, Batch Loss=0.4394, Avg Loss=0.1344, Time Left=24.91 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 406/3393 [02:43<25:39,  1.94batch/s, Batch Loss=0.0857, Avg Loss=0.1343, Time Left=24.90 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 407/3393 [02:43<25:07,  1.98batch/s, Batch Loss=0.0857, Avg Loss=0.1343, Time Left=24.90 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 407/3393 [02:44<25:07,  1.98batch/s, Batch Loss=0.0344, Avg Loss=0.1340, Time Left=24.90 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 408/3393 [02:44<25:47,  1.93batch/s, Batch Loss=0.0344, Avg Loss=0.1340, Time Left=24.90 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 408/3393 [02:44<25:47,  1.93batch/s, Batch Loss=0.0078, Avg Loss=0.1336, Time Left=24.89 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 409/3393 [02:44<25:17,  1.97batch/s, Batch Loss=0.0078, Avg Loss=0.1336, Time Left=24.89 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 409/3393 [02:45<25:17,  1.97batch/s, Batch Loss=0.0242, Avg Loss=0.1333, Time Left=24.89 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 410/3393 [02:45<25:23,  1.96batch/s, Batch Loss=0.0242, Avg Loss=0.1333, Time Left=24.89 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 410/3393 [02:45<25:23,  1.96batch/s, Batch Loss=0.0440, Avg Loss=0.1330, Time Left=24.88 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 411/3393 [02:45<25:03,  1.98batch/s, Batch Loss=0.0440, Avg Loss=0.1330, Time Left=24.88 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 411/3393 [02:46<25:03,  1.98batch/s, Batch Loss=0.0442, Avg Loss=0.1327, Time Left=24.88 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 412/3393 [02:46<25:22,  1.96batch/s, Batch Loss=0.0442, Avg Loss=0.1327, Time Left=24.88 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 412/3393 [02:46<25:22,  1.96batch/s, Batch Loss=0.0981, Avg Loss=0.1326, Time Left=24.87 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 413/3393 [02:46<24:59,  1.99batch/s, Batch Loss=0.0981, Avg Loss=0.1326, Time Left=24.87 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 413/3393 [02:47<24:59,  1.99batch/s, Batch Loss=0.0399, Avg Loss=0.1324, Time Left=24.86 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 414/3393 [02:47<25:10,  1.97batch/s, Batch Loss=0.0399, Avg Loss=0.1324, Time Left=24.86 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 414/3393 [02:47<25:10,  1.97batch/s, Batch Loss=0.0304, Avg Loss=0.1321, Time Left=24.85 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 415/3393 [02:47<24:51,  2.00batch/s, Batch Loss=0.0304, Avg Loss=0.1321, Time Left=24.85 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 415/3393 [02:48<24:51,  2.00batch/s, Batch Loss=0.0092, Avg Loss=0.1317, Time Left=24.85 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 416/3393 [02:48<24:36,  2.02batch/s, Batch Loss=0.0092, Avg Loss=0.1317, Time Left=24.85 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 416/3393 [02:48<24:36,  2.02batch/s, Batch Loss=0.3057, Avg Loss=0.1322, Time Left=24.83 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 417/3393 [02:48<24:11,  2.05batch/s, Batch Loss=0.3057, Avg Loss=0.1322, Time Left=24.83 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 417/3393 [02:49<24:11,  2.05batch/s, Batch Loss=0.0706, Avg Loss=0.1320, Time Left=24.82 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 418/3393 [02:49<24:05,  2.06batch/s, Batch Loss=0.0706, Avg Loss=0.1320, Time Left=24.82 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 418/3393 [02:49<24:05,  2.06batch/s, Batch Loss=0.2571, Avg Loss=0.1324, Time Left=24.83 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 419/3393 [02:49<24:58,  1.98batch/s, Batch Loss=0.2571, Avg Loss=0.1324, Time Left=24.83 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 419/3393 [02:50<24:58,  1.98batch/s, Batch Loss=0.0083, Avg Loss=0.1320, Time Left=24.82 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 420/3393 [02:50<24:56,  1.99batch/s, Batch Loss=0.0083, Avg Loss=0.1320, Time Left=24.82 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 420/3393 [02:50<24:56,  1.99batch/s, Batch Loss=0.1618, Avg Loss=0.1321, Time Left=24.81 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 421/3393 [02:50<25:07,  1.97batch/s, Batch Loss=0.1618, Avg Loss=0.1321, Time Left=24.81 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 421/3393 [02:51<25:07,  1.97batch/s, Batch Loss=0.0131, Avg Loss=0.1318, Time Left=24.81 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 422/3393 [02:51<24:58,  1.98batch/s, Batch Loss=0.0131, Avg Loss=0.1318, Time Left=24.81 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 422/3393 [02:51<24:58,  1.98batch/s, Batch Loss=0.0357, Avg Loss=0.1315, Time Left=24.80 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 423/3393 [02:51<24:57,  1.98batch/s, Batch Loss=0.0357, Avg Loss=0.1315, Time Left=24.80 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 423/3393 [02:52<24:57,  1.98batch/s, Batch Loss=0.0726, Avg Loss=0.1313, Time Left=24.79 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 424/3393 [02:52<24:26,  2.03batch/s, Batch Loss=0.0726, Avg Loss=0.1313, Time Left=24.79 \u001b[A\n",
      "Epoch 2/3 - Training:  12%| | 424/3393 [02:52<24:26,  2.03batch/s, Batch Loss=0.0369, Avg Loss=0.1311, Time Left=24.78 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 425/3393 [02:52<24:17,  2.04batch/s, Batch Loss=0.0369, Avg Loss=0.1311, Time Left=24.78 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 425/3393 [02:53<24:17,  2.04batch/s, Batch Loss=0.2764, Avg Loss=0.1315, Time Left=24.78 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 426/3393 [02:53<24:51,  1.99batch/s, Batch Loss=0.2764, Avg Loss=0.1315, Time Left=24.78 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 426/3393 [02:53<24:51,  1.99batch/s, Batch Loss=0.1573, Avg Loss=0.1316, Time Left=24.77 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  13%|▏| 427/3393 [02:53<24:21,  2.03batch/s, Batch Loss=0.1573, Avg Loss=0.1316, Time Left=24.77 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 427/3393 [02:54<24:21,  2.03batch/s, Batch Loss=0.1422, Avg Loss=0.1316, Time Left=24.76 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 428/3393 [02:54<24:28,  2.02batch/s, Batch Loss=0.1422, Avg Loss=0.1316, Time Left=24.76 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 428/3393 [02:54<24:28,  2.02batch/s, Batch Loss=0.2007, Avg Loss=0.1318, Time Left=24.75 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 429/3393 [02:54<24:10,  2.04batch/s, Batch Loss=0.2007, Avg Loss=0.1318, Time Left=24.75 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 429/3393 [02:55<24:10,  2.04batch/s, Batch Loss=0.1053, Avg Loss=0.1317, Time Left=24.74 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 430/3393 [02:55<24:11,  2.04batch/s, Batch Loss=0.1053, Avg Loss=0.1317, Time Left=24.74 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 430/3393 [02:55<24:11,  2.04batch/s, Batch Loss=0.1066, Avg Loss=0.1316, Time Left=24.74 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 431/3393 [02:55<24:33,  2.01batch/s, Batch Loss=0.1066, Avg Loss=0.1316, Time Left=24.74 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 431/3393 [02:56<24:33,  2.01batch/s, Batch Loss=0.0686, Avg Loss=0.1315, Time Left=24.73 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 432/3393 [02:56<24:35,  2.01batch/s, Batch Loss=0.0686, Avg Loss=0.1315, Time Left=24.73 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 432/3393 [02:56<24:35,  2.01batch/s, Batch Loss=0.0557, Avg Loss=0.1313, Time Left=24.73 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 433/3393 [02:56<24:36,  2.00batch/s, Batch Loss=0.0557, Avg Loss=0.1313, Time Left=24.73 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 433/3393 [02:57<24:36,  2.00batch/s, Batch Loss=0.0623, Avg Loss=0.1311, Time Left=24.72 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 434/3393 [02:57<24:24,  2.02batch/s, Batch Loss=0.0623, Avg Loss=0.1311, Time Left=24.72 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 434/3393 [02:57<24:24,  2.02batch/s, Batch Loss=0.0742, Avg Loss=0.1309, Time Left=24.71 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 435/3393 [02:57<24:13,  2.04batch/s, Batch Loss=0.0742, Avg Loss=0.1309, Time Left=24.71 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 435/3393 [02:58<24:13,  2.04batch/s, Batch Loss=0.1396, Avg Loss=0.1309, Time Left=24.70 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 436/3393 [02:58<24:06,  2.04batch/s, Batch Loss=0.1396, Avg Loss=0.1309, Time Left=24.70 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 436/3393 [02:58<24:06,  2.04batch/s, Batch Loss=0.0713, Avg Loss=0.1308, Time Left=24.69 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 437/3393 [02:58<24:01,  2.05batch/s, Batch Loss=0.0713, Avg Loss=0.1308, Time Left=24.69 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 437/3393 [02:59<24:01,  2.05batch/s, Batch Loss=0.0571, Avg Loss=0.1306, Time Left=24.69 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 438/3393 [02:59<24:39,  2.00batch/s, Batch Loss=0.0571, Avg Loss=0.1306, Time Left=24.69 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 438/3393 [02:59<24:39,  2.00batch/s, Batch Loss=0.0288, Avg Loss=0.1303, Time Left=24.68 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 439/3393 [02:59<24:26,  2.01batch/s, Batch Loss=0.0288, Avg Loss=0.1303, Time Left=24.68 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 439/3393 [03:00<24:26,  2.01batch/s, Batch Loss=0.0102, Avg Loss=0.1300, Time Left=24.67 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 440/3393 [03:00<24:00,  2.05batch/s, Batch Loss=0.0102, Avg Loss=0.1300, Time Left=24.67 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 440/3393 [03:00<24:00,  2.05batch/s, Batch Loss=0.0252, Avg Loss=0.1297, Time Left=24.66 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 441/3393 [03:00<23:55,  2.06batch/s, Batch Loss=0.0252, Avg Loss=0.1297, Time Left=24.66 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 441/3393 [03:01<23:55,  2.06batch/s, Batch Loss=0.3412, Avg Loss=0.1303, Time Left=24.65 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 442/3393 [03:01<23:53,  2.06batch/s, Batch Loss=0.3412, Avg Loss=0.1303, Time Left=24.65 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 442/3393 [03:01<23:53,  2.06batch/s, Batch Loss=0.3150, Avg Loss=0.1307, Time Left=24.65 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 443/3393 [03:01<24:18,  2.02batch/s, Batch Loss=0.3150, Avg Loss=0.1307, Time Left=24.65 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 443/3393 [03:02<24:18,  2.02batch/s, Batch Loss=0.0122, Avg Loss=0.1304, Time Left=24.64 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 444/3393 [03:02<24:24,  2.01batch/s, Batch Loss=0.0122, Avg Loss=0.1304, Time Left=24.64 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 444/3393 [03:02<24:24,  2.01batch/s, Batch Loss=0.1916, Avg Loss=0.1306, Time Left=24.63 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 445/3393 [03:02<24:00,  2.05batch/s, Batch Loss=0.1916, Avg Loss=0.1306, Time Left=24.63 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 445/3393 [03:03<24:00,  2.05batch/s, Batch Loss=0.1506, Avg Loss=0.1306, Time Left=24.62 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 446/3393 [03:03<24:07,  2.04batch/s, Batch Loss=0.1506, Avg Loss=0.1306, Time Left=24.62 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 446/3393 [03:03<24:07,  2.04batch/s, Batch Loss=0.2122, Avg Loss=0.1309, Time Left=24.61 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 447/3393 [03:03<24:00,  2.05batch/s, Batch Loss=0.2122, Avg Loss=0.1309, Time Left=24.61 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 447/3393 [03:04<24:00,  2.05batch/s, Batch Loss=0.0134, Avg Loss=0.1306, Time Left=24.61 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 448/3393 [03:04<24:23,  2.01batch/s, Batch Loss=0.0134, Avg Loss=0.1306, Time Left=24.61 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 448/3393 [03:04<24:23,  2.01batch/s, Batch Loss=0.1574, Avg Loss=0.1306, Time Left=24.60 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 449/3393 [03:04<24:13,  2.03batch/s, Batch Loss=0.1574, Avg Loss=0.1306, Time Left=24.60 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 449/3393 [03:04<24:13,  2.03batch/s, Batch Loss=0.0484, Avg Loss=0.1304, Time Left=24.59 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 450/3393 [03:04<24:05,  2.04batch/s, Batch Loss=0.0484, Avg Loss=0.1304, Time Left=24.59 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 450/3393 [03:05<24:05,  2.04batch/s, Batch Loss=0.2878, Avg Loss=0.1308, Time Left=24.58 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 451/3393 [03:05<23:57,  2.05batch/s, Batch Loss=0.2878, Avg Loss=0.1308, Time Left=24.58 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 451/3393 [03:05<23:57,  2.05batch/s, Batch Loss=0.0536, Avg Loss=0.1306, Time Left=24.57 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 452/3393 [03:05<23:24,  2.09batch/s, Batch Loss=0.0536, Avg Loss=0.1306, Time Left=24.57 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 452/3393 [03:06<23:24,  2.09batch/s, Batch Loss=0.2462, Avg Loss=0.1309, Time Left=24.56 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 453/3393 [03:06<23:42,  2.07batch/s, Batch Loss=0.2462, Avg Loss=0.1309, Time Left=24.56 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 453/3393 [03:06<23:42,  2.07batch/s, Batch Loss=0.1406, Avg Loss=0.1309, Time Left=24.55 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 454/3393 [03:06<23:42,  2.07batch/s, Batch Loss=0.1406, Avg Loss=0.1309, Time Left=24.55 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 454/3393 [03:07<23:42,  2.07batch/s, Batch Loss=0.0456, Avg Loss=0.1307, Time Left=24.54 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 455/3393 [03:07<23:54,  2.05batch/s, Batch Loss=0.0456, Avg Loss=0.1307, Time Left=24.54 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 455/3393 [03:07<23:54,  2.05batch/s, Batch Loss=0.0243, Avg Loss=0.1304, Time Left=24.54 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 456/3393 [03:07<24:17,  2.01batch/s, Batch Loss=0.0243, Avg Loss=0.1304, Time Left=24.54 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 456/3393 [03:08<24:17,  2.01batch/s, Batch Loss=0.1170, Avg Loss=0.1304, Time Left=24.53 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 457/3393 [03:08<24:08,  2.03batch/s, Batch Loss=0.1170, Avg Loss=0.1304, Time Left=24.53 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 457/3393 [03:08<24:08,  2.03batch/s, Batch Loss=0.0968, Avg Loss=0.1303, Time Left=24.53 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 458/3393 [03:08<24:27,  2.00batch/s, Batch Loss=0.0968, Avg Loss=0.1303, Time Left=24.53 \u001b[A\n",
      "Epoch 2/3 - Training:  13%|▏| 458/3393 [03:09<24:27,  2.00batch/s, Batch Loss=0.0113, Avg Loss=0.1300, Time Left=24.52 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 459/3393 [03:09<24:36,  1.99batch/s, Batch Loss=0.0113, Avg Loss=0.1300, Time Left=24.52 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 459/3393 [03:09<24:36,  1.99batch/s, Batch Loss=0.0281, Avg Loss=0.1298, Time Left=24.51 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  14%|▏| 460/3393 [03:09<24:12,  2.02batch/s, Batch Loss=0.0281, Avg Loss=0.1298, Time Left=24.51 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 460/3393 [03:10<24:12,  2.02batch/s, Batch Loss=0.0097, Avg Loss=0.1294, Time Left=24.50 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 461/3393 [03:10<24:04,  2.03batch/s, Batch Loss=0.0097, Avg Loss=0.1294, Time Left=24.50 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 461/3393 [03:10<24:04,  2.03batch/s, Batch Loss=0.0565, Avg Loss=0.1293, Time Left=24.49 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 462/3393 [03:10<23:38,  2.07batch/s, Batch Loss=0.0565, Avg Loss=0.1293, Time Left=24.49 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 462/3393 [03:11<23:38,  2.07batch/s, Batch Loss=0.0383, Avg Loss=0.1290, Time Left=24.49 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 463/3393 [03:11<23:50,  2.05batch/s, Batch Loss=0.0383, Avg Loss=0.1290, Time Left=24.49 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 463/3393 [03:11<23:50,  2.05batch/s, Batch Loss=0.0548, Avg Loss=0.1288, Time Left=24.48 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 464/3393 [03:11<23:48,  2.05batch/s, Batch Loss=0.0548, Avg Loss=0.1288, Time Left=24.48 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 464/3393 [03:12<23:48,  2.05batch/s, Batch Loss=0.0679, Avg Loss=0.1287, Time Left=24.47 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 465/3393 [03:12<23:29,  2.08batch/s, Batch Loss=0.0679, Avg Loss=0.1287, Time Left=24.47 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 465/3393 [03:12<23:29,  2.08batch/s, Batch Loss=0.1059, Avg Loss=0.1286, Time Left=24.46 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 466/3393 [03:12<24:11,  2.02batch/s, Batch Loss=0.1059, Avg Loss=0.1286, Time Left=24.46 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 466/3393 [03:13<24:11,  2.02batch/s, Batch Loss=0.0304, Avg Loss=0.1284, Time Left=24.45 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 467/3393 [03:13<23:48,  2.05batch/s, Batch Loss=0.0304, Avg Loss=0.1284, Time Left=24.45 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 467/3393 [03:13<23:48,  2.05batch/s, Batch Loss=0.2042, Avg Loss=0.1286, Time Left=24.44 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 468/3393 [03:13<23:29,  2.07batch/s, Batch Loss=0.2042, Avg Loss=0.1286, Time Left=24.44 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 468/3393 [03:14<23:29,  2.07batch/s, Batch Loss=0.1168, Avg Loss=0.1285, Time Left=24.44 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 469/3393 [03:14<23:59,  2.03batch/s, Batch Loss=0.1168, Avg Loss=0.1285, Time Left=24.44 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 469/3393 [03:14<23:59,  2.03batch/s, Batch Loss=0.0370, Avg Loss=0.1283, Time Left=24.43 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 470/3393 [03:14<23:51,  2.04batch/s, Batch Loss=0.0370, Avg Loss=0.1283, Time Left=24.43 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 470/3393 [03:15<23:51,  2.04batch/s, Batch Loss=0.0561, Avg Loss=0.1281, Time Left=24.42 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 471/3393 [03:15<24:00,  2.03batch/s, Batch Loss=0.0561, Avg Loss=0.1281, Time Left=24.42 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 471/3393 [03:15<24:00,  2.03batch/s, Batch Loss=0.0867, Avg Loss=0.1280, Time Left=24.41 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 472/3393 [03:15<23:51,  2.04batch/s, Batch Loss=0.0867, Avg Loss=0.1280, Time Left=24.41 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 472/3393 [03:16<23:51,  2.04batch/s, Batch Loss=0.0360, Avg Loss=0.1278, Time Left=24.41 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 473/3393 [03:16<24:01,  2.03batch/s, Batch Loss=0.0360, Avg Loss=0.1278, Time Left=24.41 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 473/3393 [03:16<24:01,  2.03batch/s, Batch Loss=0.0078, Avg Loss=0.1275, Time Left=24.40 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 474/3393 [03:16<23:56,  2.03batch/s, Batch Loss=0.0078, Avg Loss=0.1275, Time Left=24.40 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 474/3393 [03:17<23:56,  2.03batch/s, Batch Loss=0.1879, Avg Loss=0.1276, Time Left=24.39 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 475/3393 [03:17<23:44,  2.05batch/s, Batch Loss=0.1879, Avg Loss=0.1276, Time Left=24.39 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 475/3393 [03:17<23:44,  2.05batch/s, Batch Loss=0.0668, Avg Loss=0.1275, Time Left=24.38 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 476/3393 [03:17<23:52,  2.04batch/s, Batch Loss=0.0668, Avg Loss=0.1275, Time Left=24.38 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 476/3393 [03:18<23:52,  2.04batch/s, Batch Loss=0.0236, Avg Loss=0.1272, Time Left=24.37 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 477/3393 [03:18<23:33,  2.06batch/s, Batch Loss=0.0236, Avg Loss=0.1272, Time Left=24.37 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 477/3393 [03:18<23:33,  2.06batch/s, Batch Loss=0.2112, Avg Loss=0.1274, Time Left=24.36 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 478/3393 [03:18<23:45,  2.05batch/s, Batch Loss=0.2112, Avg Loss=0.1274, Time Left=24.36 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 478/3393 [03:19<23:45,  2.05batch/s, Batch Loss=0.0072, Avg Loss=0.1271, Time Left=24.35 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 479/3393 [03:19<23:41,  2.05batch/s, Batch Loss=0.0072, Avg Loss=0.1271, Time Left=24.35 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 479/3393 [03:19<23:41,  2.05batch/s, Batch Loss=0.0576, Avg Loss=0.1270, Time Left=24.35 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 480/3393 [03:19<23:36,  2.06batch/s, Batch Loss=0.0576, Avg Loss=0.1270, Time Left=24.35 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 480/3393 [03:20<23:36,  2.06batch/s, Batch Loss=0.1883, Avg Loss=0.1271, Time Left=24.34 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 481/3393 [03:20<23:47,  2.04batch/s, Batch Loss=0.1883, Avg Loss=0.1271, Time Left=24.34 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 481/3393 [03:20<23:47,  2.04batch/s, Batch Loss=0.1539, Avg Loss=0.1272, Time Left=24.33 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 482/3393 [03:20<23:27,  2.07batch/s, Batch Loss=0.1539, Avg Loss=0.1272, Time Left=24.33 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 482/3393 [03:21<23:27,  2.07batch/s, Batch Loss=0.0283, Avg Loss=0.1270, Time Left=24.32 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 483/3393 [03:21<23:40,  2.05batch/s, Batch Loss=0.0283, Avg Loss=0.1270, Time Left=24.32 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 483/3393 [03:21<23:40,  2.05batch/s, Batch Loss=0.0047, Avg Loss=0.1267, Time Left=24.31 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 484/3393 [03:21<23:51,  2.03batch/s, Batch Loss=0.0047, Avg Loss=0.1267, Time Left=24.31 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 484/3393 [03:22<23:51,  2.03batch/s, Batch Loss=0.2211, Avg Loss=0.1269, Time Left=24.30 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 485/3393 [03:22<23:29,  2.06batch/s, Batch Loss=0.2211, Avg Loss=0.1269, Time Left=24.30 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 485/3393 [03:22<23:29,  2.06batch/s, Batch Loss=0.0261, Avg Loss=0.1266, Time Left=24.30 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 486/3393 [03:22<23:28,  2.06batch/s, Batch Loss=0.0261, Avg Loss=0.1266, Time Left=24.30 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 486/3393 [03:23<23:28,  2.06batch/s, Batch Loss=0.0407, Avg Loss=0.1264, Time Left=24.29 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 487/3393 [03:23<23:54,  2.03batch/s, Batch Loss=0.0407, Avg Loss=0.1264, Time Left=24.29 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 487/3393 [03:23<23:54,  2.03batch/s, Batch Loss=0.1138, Avg Loss=0.1264, Time Left=24.28 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 488/3393 [03:23<23:46,  2.04batch/s, Batch Loss=0.1138, Avg Loss=0.1264, Time Left=24.28 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 488/3393 [03:24<23:46,  2.04batch/s, Batch Loss=0.2320, Avg Loss=0.1267, Time Left=24.28 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 489/3393 [03:24<24:06,  2.01batch/s, Batch Loss=0.2320, Avg Loss=0.1267, Time Left=24.28 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 489/3393 [03:24<24:06,  2.01batch/s, Batch Loss=0.0863, Avg Loss=0.1266, Time Left=24.27 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 490/3393 [03:24<23:55,  2.02batch/s, Batch Loss=0.0863, Avg Loss=0.1266, Time Left=24.27 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 490/3393 [03:25<23:55,  2.02batch/s, Batch Loss=0.1510, Avg Loss=0.1266, Time Left=24.26 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 491/3393 [03:25<24:28,  1.98batch/s, Batch Loss=0.1510, Avg Loss=0.1266, Time Left=24.26 \u001b[A\n",
      "Epoch 2/3 - Training:  14%|▏| 491/3393 [03:25<24:28,  1.98batch/s, Batch Loss=0.2644, Avg Loss=0.1269, Time Left=24.26 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 492/3393 [03:25<24:19,  1.99batch/s, Batch Loss=0.2644, Avg Loss=0.1269, Time Left=24.26 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 492/3393 [03:26<24:19,  1.99batch/s, Batch Loss=0.0169, Avg Loss=0.1267, Time Left=24.25 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  15%|▏| 493/3393 [03:26<24:06,  2.00batch/s, Batch Loss=0.0169, Avg Loss=0.1267, Time Left=24.25 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 493/3393 [03:26<24:06,  2.00batch/s, Batch Loss=0.0780, Avg Loss=0.1266, Time Left=24.24 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 494/3393 [03:26<23:39,  2.04batch/s, Batch Loss=0.0780, Avg Loss=0.1266, Time Left=24.24 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 494/3393 [03:27<23:39,  2.04batch/s, Batch Loss=0.0717, Avg Loss=0.1264, Time Left=24.23 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 495/3393 [03:27<23:33,  2.05batch/s, Batch Loss=0.0717, Avg Loss=0.1264, Time Left=24.23 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 495/3393 [03:27<23:33,  2.05batch/s, Batch Loss=0.0041, Avg Loss=0.1262, Time Left=24.22 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 496/3393 [03:27<23:55,  2.02batch/s, Batch Loss=0.0041, Avg Loss=0.1262, Time Left=24.22 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 496/3393 [03:28<23:55,  2.02batch/s, Batch Loss=0.1337, Avg Loss=0.1262, Time Left=24.22 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 497/3393 [03:28<23:59,  2.01batch/s, Batch Loss=0.1337, Avg Loss=0.1262, Time Left=24.22 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 497/3393 [03:28<23:59,  2.01batch/s, Batch Loss=0.1419, Avg Loss=0.1262, Time Left=24.21 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 498/3393 [03:28<23:35,  2.04batch/s, Batch Loss=0.1419, Avg Loss=0.1262, Time Left=24.21 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 498/3393 [03:29<23:35,  2.04batch/s, Batch Loss=0.0079, Avg Loss=0.1259, Time Left=24.20 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 499/3393 [03:29<23:30,  2.05batch/s, Batch Loss=0.0079, Avg Loss=0.1259, Time Left=24.20 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 499/3393 [03:29<23:30,  2.05batch/s, Batch Loss=0.8544, Avg Loss=0.1276, Time Left=24.19 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 500/3393 [03:29<23:40,  2.04batch/s, Batch Loss=0.8544, Avg Loss=0.1276, Time Left=24.19 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 500/3393 [03:30<23:40,  2.04batch/s, Batch Loss=0.0302, Avg Loss=0.1274, Time Left=24.19 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 501/3393 [03:30<24:00,  2.01batch/s, Batch Loss=0.0302, Avg Loss=0.1274, Time Left=24.19 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 501/3393 [03:30<24:00,  2.01batch/s, Batch Loss=0.0752, Avg Loss=0.1273, Time Left=24.17 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 502/3393 [03:30<23:33,  2.04batch/s, Batch Loss=0.0752, Avg Loss=0.1273, Time Left=24.17 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 502/3393 [03:30<23:33,  2.04batch/s, Batch Loss=0.1156, Avg Loss=0.1273, Time Left=24.16 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 503/3393 [03:30<23:29,  2.05batch/s, Batch Loss=0.1156, Avg Loss=0.1273, Time Left=24.16 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 503/3393 [03:31<23:29,  2.05batch/s, Batch Loss=0.1460, Avg Loss=0.1273, Time Left=24.16 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 504/3393 [03:31<23:26,  2.05batch/s, Batch Loss=0.1460, Avg Loss=0.1273, Time Left=24.16 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 504/3393 [03:31<23:26,  2.05batch/s, Batch Loss=0.0284, Avg Loss=0.1271, Time Left=24.15 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 505/3393 [03:31<23:09,  2.08batch/s, Batch Loss=0.0284, Avg Loss=0.1271, Time Left=24.15 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 505/3393 [03:32<23:09,  2.08batch/s, Batch Loss=0.0758, Avg Loss=0.1270, Time Left=24.14 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 506/3393 [03:32<23:11,  2.08batch/s, Batch Loss=0.0758, Avg Loss=0.1270, Time Left=24.14 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 506/3393 [03:32<23:11,  2.08batch/s, Batch Loss=0.0640, Avg Loss=0.1268, Time Left=24.13 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 507/3393 [03:32<23:12,  2.07batch/s, Batch Loss=0.0640, Avg Loss=0.1268, Time Left=24.13 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 507/3393 [03:33<23:12,  2.07batch/s, Batch Loss=0.1319, Avg Loss=0.1268, Time Left=24.12 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 508/3393 [03:33<23:12,  2.07batch/s, Batch Loss=0.1319, Avg Loss=0.1268, Time Left=24.12 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 508/3393 [03:33<23:12,  2.07batch/s, Batch Loss=0.2374, Avg Loss=0.1271, Time Left=24.12 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 509/3393 [03:33<23:39,  2.03batch/s, Batch Loss=0.2374, Avg Loss=0.1271, Time Left=24.12 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 509/3393 [03:34<23:39,  2.03batch/s, Batch Loss=0.0676, Avg Loss=0.1269, Time Left=24.11 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 510/3393 [03:34<23:47,  2.02batch/s, Batch Loss=0.0676, Avg Loss=0.1269, Time Left=24.11 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 510/3393 [03:34<23:47,  2.02batch/s, Batch Loss=0.1493, Avg Loss=0.1270, Time Left=24.10 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 511/3393 [03:34<23:37,  2.03batch/s, Batch Loss=0.1493, Avg Loss=0.1270, Time Left=24.10 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 511/3393 [03:35<23:37,  2.03batch/s, Batch Loss=0.2596, Avg Loss=0.1273, Time Left=24.09 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 512/3393 [03:35<23:31,  2.04batch/s, Batch Loss=0.2596, Avg Loss=0.1273, Time Left=24.09 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 512/3393 [03:35<23:31,  2.04batch/s, Batch Loss=0.0941, Avg Loss=0.1272, Time Left=24.08 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 513/3393 [03:35<23:11,  2.07batch/s, Batch Loss=0.0941, Avg Loss=0.1272, Time Left=24.08 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 513/3393 [03:36<23:11,  2.07batch/s, Batch Loss=0.0157, Avg Loss=0.1270, Time Left=24.08 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 514/3393 [03:36<23:50,  2.01batch/s, Batch Loss=0.0157, Avg Loss=0.1270, Time Left=24.08 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 514/3393 [03:36<23:50,  2.01batch/s, Batch Loss=0.3679, Avg Loss=0.1275, Time Left=24.06 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 515/3393 [03:36<23:13,  2.06batch/s, Batch Loss=0.3679, Avg Loss=0.1275, Time Left=24.06 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 515/3393 [03:37<23:13,  2.06batch/s, Batch Loss=0.1561, Avg Loss=0.1276, Time Left=24.06 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 516/3393 [03:37<23:26,  2.05batch/s, Batch Loss=0.1561, Avg Loss=0.1276, Time Left=24.06 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 516/3393 [03:37<23:26,  2.05batch/s, Batch Loss=0.0482, Avg Loss=0.1274, Time Left=24.05 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 517/3393 [03:37<23:47,  2.01batch/s, Batch Loss=0.0482, Avg Loss=0.1274, Time Left=24.05 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 517/3393 [03:38<23:47,  2.01batch/s, Batch Loss=0.0942, Avg Loss=0.1273, Time Left=24.04 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 518/3393 [03:38<23:37,  2.03batch/s, Batch Loss=0.0942, Avg Loss=0.1273, Time Left=24.04 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 518/3393 [03:38<23:37,  2.03batch/s, Batch Loss=0.2868, Avg Loss=0.1277, Time Left=24.04 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 519/3393 [03:38<23:57,  2.00batch/s, Batch Loss=0.2868, Avg Loss=0.1277, Time Left=24.04 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 519/3393 [03:39<23:57,  2.00batch/s, Batch Loss=0.1089, Avg Loss=0.1276, Time Left=24.03 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 520/3393 [03:39<23:44,  2.02batch/s, Batch Loss=0.1089, Avg Loss=0.1276, Time Left=24.03 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 520/3393 [03:39<23:44,  2.02batch/s, Batch Loss=0.0736, Avg Loss=0.1275, Time Left=24.03 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 521/3393 [03:39<24:14,  1.97batch/s, Batch Loss=0.0736, Avg Loss=0.1275, Time Left=24.03 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 521/3393 [03:40<24:14,  1.97batch/s, Batch Loss=0.0819, Avg Loss=0.1274, Time Left=24.02 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 522/3393 [03:40<23:55,  2.00batch/s, Batch Loss=0.0819, Avg Loss=0.1274, Time Left=24.02 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 522/3393 [03:40<23:55,  2.00batch/s, Batch Loss=0.1070, Avg Loss=0.1274, Time Left=24.01 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 523/3393 [03:40<23:55,  2.00batch/s, Batch Loss=0.1070, Avg Loss=0.1274, Time Left=24.01 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 523/3393 [03:41<23:55,  2.00batch/s, Batch Loss=0.1534, Avg Loss=0.1274, Time Left=24.00 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 524/3393 [03:41<23:41,  2.02batch/s, Batch Loss=0.1534, Avg Loss=0.1274, Time Left=24.00 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 524/3393 [03:41<23:41,  2.02batch/s, Batch Loss=0.0141, Avg Loss=0.1272, Time Left=23.99 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 525/3393 [03:41<23:30,  2.03batch/s, Batch Loss=0.0141, Avg Loss=0.1272, Time Left=23.99 \u001b[A\n",
      "Epoch 2/3 - Training:  15%|▏| 525/3393 [03:42<23:30,  2.03batch/s, Batch Loss=0.1898, Avg Loss=0.1273, Time Left=23.99 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  16%|▏| 526/3393 [03:42<24:03,  1.99batch/s, Batch Loss=0.1898, Avg Loss=0.1273, Time Left=23.99 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 526/3393 [03:42<24:03,  1.99batch/s, Batch Loss=0.0121, Avg Loss=0.1271, Time Left=23.98 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 527/3393 [03:42<23:48,  2.01batch/s, Batch Loss=0.0121, Avg Loss=0.1271, Time Left=23.98 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 527/3393 [03:43<23:48,  2.01batch/s, Batch Loss=0.0390, Avg Loss=0.1269, Time Left=23.98 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 528/3393 [03:43<24:01,  1.99batch/s, Batch Loss=0.0390, Avg Loss=0.1269, Time Left=23.98 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 528/3393 [03:43<24:01,  1.99batch/s, Batch Loss=0.0306, Avg Loss=0.1267, Time Left=23.97 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 529/3393 [03:43<23:33,  2.03batch/s, Batch Loss=0.0306, Avg Loss=0.1267, Time Left=23.97 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 529/3393 [03:44<23:33,  2.03batch/s, Batch Loss=0.0155, Avg Loss=0.1264, Time Left=23.96 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 530/3393 [03:44<23:24,  2.04batch/s, Batch Loss=0.0155, Avg Loss=0.1264, Time Left=23.96 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 530/3393 [03:44<23:24,  2.04batch/s, Batch Loss=0.2000, Avg Loss=0.1266, Time Left=23.95 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 531/3393 [03:44<23:17,  2.05batch/s, Batch Loss=0.2000, Avg Loss=0.1266, Time Left=23.95 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 531/3393 [03:45<23:17,  2.05batch/s, Batch Loss=0.0284, Avg Loss=0.1264, Time Left=23.94 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 532/3393 [03:45<23:26,  2.03batch/s, Batch Loss=0.0284, Avg Loss=0.1264, Time Left=23.94 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 532/3393 [03:45<23:26,  2.03batch/s, Batch Loss=0.1724, Avg Loss=0.1265, Time Left=23.93 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 533/3393 [03:45<23:45,  2.01batch/s, Batch Loss=0.1724, Avg Loss=0.1265, Time Left=23.93 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 533/3393 [03:46<23:45,  2.01batch/s, Batch Loss=0.1049, Avg Loss=0.1264, Time Left=23.93 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 534/3393 [03:46<23:48,  2.00batch/s, Batch Loss=0.1049, Avg Loss=0.1264, Time Left=23.93 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 534/3393 [03:46<23:48,  2.00batch/s, Batch Loss=0.0425, Avg Loss=0.1262, Time Left=23.92 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 535/3393 [03:46<24:01,  1.98batch/s, Batch Loss=0.0425, Avg Loss=0.1262, Time Left=23.92 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 535/3393 [03:47<24:01,  1.98batch/s, Batch Loss=0.1362, Avg Loss=0.1263, Time Left=23.91 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 536/3393 [03:47<23:59,  1.98batch/s, Batch Loss=0.1362, Avg Loss=0.1263, Time Left=23.91 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 536/3393 [03:47<23:59,  1.98batch/s, Batch Loss=0.0328, Avg Loss=0.1261, Time Left=23.91 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 537/3393 [03:47<24:39,  1.93batch/s, Batch Loss=0.0328, Avg Loss=0.1261, Time Left=23.91 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 537/3393 [03:48<24:39,  1.93batch/s, Batch Loss=0.0525, Avg Loss=0.1259, Time Left=23.90 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 538/3393 [03:48<24:08,  1.97batch/s, Batch Loss=0.0525, Avg Loss=0.1259, Time Left=23.90 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 538/3393 [03:48<24:08,  1.97batch/s, Batch Loss=0.0517, Avg Loss=0.1257, Time Left=23.90 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 539/3393 [03:48<24:02,  1.98batch/s, Batch Loss=0.0517, Avg Loss=0.1257, Time Left=23.90 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 539/3393 [03:49<24:02,  1.98batch/s, Batch Loss=0.0372, Avg Loss=0.1255, Time Left=23.89 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 540/3393 [03:49<23:30,  2.02batch/s, Batch Loss=0.0372, Avg Loss=0.1255, Time Left=23.89 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 540/3393 [03:49<23:30,  2.02batch/s, Batch Loss=0.6623, Avg Loss=0.1267, Time Left=23.88 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 541/3393 [03:49<23:33,  2.02batch/s, Batch Loss=0.6623, Avg Loss=0.1267, Time Left=23.88 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 541/3393 [03:50<23:33,  2.02batch/s, Batch Loss=0.2613, Avg Loss=0.1270, Time Left=23.87 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 542/3393 [03:50<23:49,  1.99batch/s, Batch Loss=0.2613, Avg Loss=0.1270, Time Left=23.87 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 542/3393 [03:50<23:49,  1.99batch/s, Batch Loss=0.0582, Avg Loss=0.1268, Time Left=23.87 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 543/3393 [03:50<23:36,  2.01batch/s, Batch Loss=0.0582, Avg Loss=0.1268, Time Left=23.87 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 543/3393 [03:51<23:36,  2.01batch/s, Batch Loss=0.0566, Avg Loss=0.1267, Time Left=23.86 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 544/3393 [03:51<23:29,  2.02batch/s, Batch Loss=0.0566, Avg Loss=0.1267, Time Left=23.86 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 544/3393 [03:51<23:29,  2.02batch/s, Batch Loss=0.0430, Avg Loss=0.1265, Time Left=23.85 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 545/3393 [03:51<23:14,  2.04batch/s, Batch Loss=0.0430, Avg Loss=0.1265, Time Left=23.85 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 545/3393 [03:52<23:14,  2.04batch/s, Batch Loss=0.0196, Avg Loss=0.1263, Time Left=23.85 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 546/3393 [03:52<24:08,  1.97batch/s, Batch Loss=0.0196, Avg Loss=0.1263, Time Left=23.85 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 546/3393 [03:52<24:08,  1.97batch/s, Batch Loss=0.1465, Avg Loss=0.1263, Time Left=23.84 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 547/3393 [03:52<23:58,  1.98batch/s, Batch Loss=0.1465, Avg Loss=0.1263, Time Left=23.84 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 547/3393 [03:53<23:58,  1.98batch/s, Batch Loss=0.1096, Avg Loss=0.1263, Time Left=23.83 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 548/3393 [03:53<23:25,  2.02batch/s, Batch Loss=0.1096, Avg Loss=0.1263, Time Left=23.83 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 548/3393 [03:53<23:25,  2.02batch/s, Batch Loss=0.2281, Avg Loss=0.1265, Time Left=23.82 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 549/3393 [03:53<23:16,  2.04batch/s, Batch Loss=0.2281, Avg Loss=0.1265, Time Left=23.82 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 549/3393 [03:54<23:16,  2.04batch/s, Batch Loss=0.2340, Avg Loss=0.1267, Time Left=23.81 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 550/3393 [03:54<23:28,  2.02batch/s, Batch Loss=0.2340, Avg Loss=0.1267, Time Left=23.81 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 550/3393 [03:54<23:28,  2.02batch/s, Batch Loss=0.0197, Avg Loss=0.1265, Time Left=23.81 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 551/3393 [03:54<23:40,  2.00batch/s, Batch Loss=0.0197, Avg Loss=0.1265, Time Left=23.81 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 551/3393 [03:55<23:40,  2.00batch/s, Batch Loss=0.0520, Avg Loss=0.1263, Time Left=23.80 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 552/3393 [03:55<23:40,  2.00batch/s, Batch Loss=0.0520, Avg Loss=0.1263, Time Left=23.80 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 552/3393 [03:55<23:40,  2.00batch/s, Batch Loss=0.4051, Avg Loss=0.1269, Time Left=23.79 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 553/3393 [03:55<23:53,  1.98batch/s, Batch Loss=0.4051, Avg Loss=0.1269, Time Left=23.79 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 553/3393 [03:56<23:53,  1.98batch/s, Batch Loss=0.0629, Avg Loss=0.1268, Time Left=23.79 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 554/3393 [03:56<23:51,  1.98batch/s, Batch Loss=0.0629, Avg Loss=0.1268, Time Left=23.79 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 554/3393 [03:56<23:51,  1.98batch/s, Batch Loss=0.2022, Avg Loss=0.1269, Time Left=23.78 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 555/3393 [03:56<23:59,  1.97batch/s, Batch Loss=0.2022, Avg Loss=0.1269, Time Left=23.78 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 555/3393 [03:57<23:59,  1.97batch/s, Batch Loss=0.2730, Avg Loss=0.1272, Time Left=23.77 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 556/3393 [03:57<23:54,  1.98batch/s, Batch Loss=0.2730, Avg Loss=0.1272, Time Left=23.77 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 556/3393 [03:57<23:54,  1.98batch/s, Batch Loss=0.0918, Avg Loss=0.1272, Time Left=23.76 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 557/3393 [03:57<23:22,  2.02batch/s, Batch Loss=0.0918, Avg Loss=0.1272, Time Left=23.76 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 557/3393 [03:58<23:22,  2.02batch/s, Batch Loss=0.1310, Avg Loss=0.1272, Time Left=23.76 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 558/3393 [03:58<23:26,  2.02batch/s, Batch Loss=0.1310, Avg Loss=0.1272, Time Left=23.76 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 558/3393 [03:58<23:26,  2.02batch/s, Batch Loss=0.0845, Avg Loss=0.1271, Time Left=23.75 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  16%|▏| 559/3393 [03:58<23:14,  2.03batch/s, Batch Loss=0.0845, Avg Loss=0.1271, Time Left=23.75 \u001b[A\n",
      "Epoch 2/3 - Training:  16%|▏| 559/3393 [03:59<23:14,  2.03batch/s, Batch Loss=0.0771, Avg Loss=0.1270, Time Left=23.74 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 560/3393 [03:59<23:33,  2.00batch/s, Batch Loss=0.0771, Avg Loss=0.1270, Time Left=23.74 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 560/3393 [03:59<23:33,  2.00batch/s, Batch Loss=0.0278, Avg Loss=0.1268, Time Left=23.73 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 561/3393 [03:59<23:08,  2.04batch/s, Batch Loss=0.0278, Avg Loss=0.1268, Time Left=23.73 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 561/3393 [04:00<23:08,  2.04batch/s, Batch Loss=0.0905, Avg Loss=0.1267, Time Left=23.72 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 562/3393 [04:00<23:02,  2.05batch/s, Batch Loss=0.0905, Avg Loss=0.1267, Time Left=23.72 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 562/3393 [04:00<23:02,  2.05batch/s, Batch Loss=0.1177, Avg Loss=0.1267, Time Left=23.71 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 563/3393 [04:00<23:11,  2.03batch/s, Batch Loss=0.1177, Avg Loss=0.1267, Time Left=23.71 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 563/3393 [04:01<23:11,  2.03batch/s, Batch Loss=0.0702, Avg Loss=0.1266, Time Left=23.70 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 564/3393 [04:01<22:37,  2.08batch/s, Batch Loss=0.0702, Avg Loss=0.1266, Time Left=23.70 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 564/3393 [04:01<22:37,  2.08batch/s, Batch Loss=0.1518, Avg Loss=0.1266, Time Left=23.69 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 565/3393 [04:01<22:39,  2.08batch/s, Batch Loss=0.1518, Avg Loss=0.1266, Time Left=23.69 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 565/3393 [04:02<22:39,  2.08batch/s, Batch Loss=0.3213, Avg Loss=0.1270, Time Left=23.69 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 566/3393 [04:02<23:07,  2.04batch/s, Batch Loss=0.3213, Avg Loss=0.1270, Time Left=23.69 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 566/3393 [04:02<23:07,  2.04batch/s, Batch Loss=0.1171, Avg Loss=0.1270, Time Left=23.68 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 567/3393 [04:02<22:49,  2.06batch/s, Batch Loss=0.1171, Avg Loss=0.1270, Time Left=23.68 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 567/3393 [04:03<22:49,  2.06batch/s, Batch Loss=0.0257, Avg Loss=0.1268, Time Left=23.67 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 568/3393 [04:03<23:14,  2.03batch/s, Batch Loss=0.0257, Avg Loss=0.1268, Time Left=23.67 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 568/3393 [04:03<23:14,  2.03batch/s, Batch Loss=0.2628, Avg Loss=0.1271, Time Left=23.67 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 569/3393 [04:03<23:20,  2.02batch/s, Batch Loss=0.2628, Avg Loss=0.1271, Time Left=23.67 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 569/3393 [04:04<23:20,  2.02batch/s, Batch Loss=0.0650, Avg Loss=0.1270, Time Left=23.66 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 570/3393 [04:04<22:56,  2.05batch/s, Batch Loss=0.0650, Avg Loss=0.1270, Time Left=23.66 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 570/3393 [04:04<22:56,  2.05batch/s, Batch Loss=0.1769, Avg Loss=0.1271, Time Left=23.65 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 571/3393 [04:04<22:52,  2.06batch/s, Batch Loss=0.1769, Avg Loss=0.1271, Time Left=23.65 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 571/3393 [04:05<22:52,  2.06batch/s, Batch Loss=0.0254, Avg Loss=0.1269, Time Left=23.64 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 572/3393 [04:05<22:49,  2.06batch/s, Batch Loss=0.0254, Avg Loss=0.1269, Time Left=23.64 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 572/3393 [04:05<22:49,  2.06batch/s, Batch Loss=0.1511, Avg Loss=0.1269, Time Left=23.63 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 573/3393 [04:05<23:27,  2.00batch/s, Batch Loss=0.1511, Avg Loss=0.1269, Time Left=23.63 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 573/3393 [04:06<23:27,  2.00batch/s, Batch Loss=0.1666, Avg Loss=0.1270, Time Left=23.63 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 574/3393 [04:06<23:15,  2.02batch/s, Batch Loss=0.1666, Avg Loss=0.1270, Time Left=23.63 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 574/3393 [04:06<23:15,  2.02batch/s, Batch Loss=0.1158, Avg Loss=0.1270, Time Left=23.62 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 575/3393 [04:06<22:54,  2.05batch/s, Batch Loss=0.1158, Avg Loss=0.1270, Time Left=23.62 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 575/3393 [04:07<22:54,  2.05batch/s, Batch Loss=0.1463, Avg Loss=0.1270, Time Left=23.61 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 576/3393 [04:07<23:02,  2.04batch/s, Batch Loss=0.1463, Avg Loss=0.1270, Time Left=23.61 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 576/3393 [04:07<23:02,  2.04batch/s, Batch Loss=0.1181, Avg Loss=0.1270, Time Left=23.60 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 577/3393 [04:07<22:56,  2.05batch/s, Batch Loss=0.1181, Avg Loss=0.1270, Time Left=23.60 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 577/3393 [04:08<22:56,  2.05batch/s, Batch Loss=0.1033, Avg Loss=0.1269, Time Left=23.59 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 578/3393 [04:08<23:18,  2.01batch/s, Batch Loss=0.1033, Avg Loss=0.1269, Time Left=23.59 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 578/3393 [04:08<23:18,  2.01batch/s, Batch Loss=0.0279, Avg Loss=0.1267, Time Left=23.59 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 579/3393 [04:08<23:07,  2.03batch/s, Batch Loss=0.0279, Avg Loss=0.1267, Time Left=23.59 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 579/3393 [04:09<23:07,  2.03batch/s, Batch Loss=0.0165, Avg Loss=0.1265, Time Left=23.58 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 580/3393 [04:09<22:59,  2.04batch/s, Batch Loss=0.0165, Avg Loss=0.1265, Time Left=23.58 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 580/3393 [04:09<22:59,  2.04batch/s, Batch Loss=0.0653, Avg Loss=0.1264, Time Left=23.57 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 581/3393 [04:09<22:39,  2.07batch/s, Batch Loss=0.0653, Avg Loss=0.1264, Time Left=23.57 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 581/3393 [04:10<22:39,  2.07batch/s, Batch Loss=0.0047, Avg Loss=0.1262, Time Left=23.56 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 582/3393 [04:10<22:25,  2.09batch/s, Batch Loss=0.0047, Avg Loss=0.1262, Time Left=23.56 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 582/3393 [04:10<22:25,  2.09batch/s, Batch Loss=0.2077, Avg Loss=0.1263, Time Left=23.55 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 583/3393 [04:10<22:41,  2.06batch/s, Batch Loss=0.2077, Avg Loss=0.1263, Time Left=23.55 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 583/3393 [04:10<22:41,  2.06batch/s, Batch Loss=0.0461, Avg Loss=0.1262, Time Left=23.54 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 584/3393 [04:10<22:30,  2.08batch/s, Batch Loss=0.0461, Avg Loss=0.1262, Time Left=23.54 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 584/3393 [04:11<22:30,  2.08batch/s, Batch Loss=0.1756, Avg Loss=0.1263, Time Left=23.53 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 585/3393 [04:11<22:17,  2.10batch/s, Batch Loss=0.1756, Avg Loss=0.1263, Time Left=23.53 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 585/3393 [04:11<22:17,  2.10batch/s, Batch Loss=0.4165, Avg Loss=0.1268, Time Left=23.52 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 586/3393 [04:11<22:21,  2.09batch/s, Batch Loss=0.4165, Avg Loss=0.1268, Time Left=23.52 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 586/3393 [04:12<22:21,  2.09batch/s, Batch Loss=0.5524, Avg Loss=0.1276, Time Left=23.51 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 587/3393 [04:12<22:25,  2.08batch/s, Batch Loss=0.5524, Avg Loss=0.1276, Time Left=23.51 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 587/3393 [04:12<22:25,  2.08batch/s, Batch Loss=0.0174, Avg Loss=0.1274, Time Left=23.50 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 588/3393 [04:12<22:15,  2.10batch/s, Batch Loss=0.0174, Avg Loss=0.1274, Time Left=23.50 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 588/3393 [04:13<22:15,  2.10batch/s, Batch Loss=0.2641, Avg Loss=0.1277, Time Left=23.49 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 589/3393 [04:13<22:33,  2.07batch/s, Batch Loss=0.2641, Avg Loss=0.1277, Time Left=23.49 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 589/3393 [04:13<22:33,  2.07batch/s, Batch Loss=0.2124, Avg Loss=0.1279, Time Left=23.48 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 590/3393 [04:13<22:20,  2.09batch/s, Batch Loss=0.2124, Avg Loss=0.1279, Time Left=23.48 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 590/3393 [04:14<22:20,  2.09batch/s, Batch Loss=0.1183, Avg Loss=0.1278, Time Left=23.47 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 591/3393 [04:14<22:25,  2.08batch/s, Batch Loss=0.1183, Avg Loss=0.1278, Time Left=23.47 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 591/3393 [04:14<22:25,  2.08batch/s, Batch Loss=0.2306, Avg Loss=0.1280, Time Left=23.46 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  17%|▏| 592/3393 [04:14<22:13,  2.10batch/s, Batch Loss=0.2306, Avg Loss=0.1280, Time Left=23.46 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 592/3393 [04:15<22:13,  2.10batch/s, Batch Loss=0.0237, Avg Loss=0.1278, Time Left=23.46 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 593/3393 [04:15<22:35,  2.07batch/s, Batch Loss=0.0237, Avg Loss=0.1278, Time Left=23.46 \u001b[A\n",
      "Epoch 2/3 - Training:  17%|▏| 593/3393 [04:15<22:35,  2.07batch/s, Batch Loss=0.0842, Avg Loss=0.1278, Time Left=23.45 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 594/3393 [04:15<22:31,  2.07batch/s, Batch Loss=0.0842, Avg Loss=0.1278, Time Left=23.45 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 594/3393 [04:16<22:31,  2.07batch/s, Batch Loss=0.1625, Avg Loss=0.1278, Time Left=23.44 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 595/3393 [04:16<22:18,  2.09batch/s, Batch Loss=0.1625, Avg Loss=0.1278, Time Left=23.44 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 595/3393 [04:16<22:18,  2.09batch/s, Batch Loss=0.0982, Avg Loss=0.1278, Time Left=23.43 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 596/3393 [04:16<22:17,  2.09batch/s, Batch Loss=0.0982, Avg Loss=0.1278, Time Left=23.43 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 596/3393 [04:17<22:17,  2.09batch/s, Batch Loss=0.1032, Avg Loss=0.1277, Time Left=23.42 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 597/3393 [04:17<22:19,  2.09batch/s, Batch Loss=0.1032, Avg Loss=0.1277, Time Left=23.42 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 597/3393 [04:17<22:19,  2.09batch/s, Batch Loss=0.0995, Avg Loss=0.1277, Time Left=23.41 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 598/3393 [04:17<23:09,  2.01batch/s, Batch Loss=0.0995, Avg Loss=0.1277, Time Left=23.41 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 598/3393 [04:18<23:09,  2.01batch/s, Batch Loss=0.6304, Avg Loss=0.1286, Time Left=23.41 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 599/3393 [04:18<22:46,  2.05batch/s, Batch Loss=0.6304, Avg Loss=0.1286, Time Left=23.41 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 599/3393 [04:18<22:46,  2.05batch/s, Batch Loss=0.0171, Avg Loss=0.1284, Time Left=23.40 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 600/3393 [04:18<22:41,  2.05batch/s, Batch Loss=0.0171, Avg Loss=0.1284, Time Left=23.40 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 600/3393 [04:19<22:41,  2.05batch/s, Batch Loss=0.0661, Avg Loss=0.1283, Time Left=23.39 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 601/3393 [04:19<22:25,  2.07batch/s, Batch Loss=0.0661, Avg Loss=0.1283, Time Left=23.39 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 601/3393 [04:19<22:25,  2.07batch/s, Batch Loss=0.0660, Avg Loss=0.1282, Time Left=23.38 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 602/3393 [04:19<22:11,  2.10batch/s, Batch Loss=0.0660, Avg Loss=0.1282, Time Left=23.38 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 602/3393 [04:20<22:11,  2.10batch/s, Batch Loss=0.1847, Avg Loss=0.1283, Time Left=23.37 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 603/3393 [04:20<22:15,  2.09batch/s, Batch Loss=0.1847, Avg Loss=0.1283, Time Left=23.37 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 603/3393 [04:20<22:15,  2.09batch/s, Batch Loss=0.0673, Avg Loss=0.1282, Time Left=23.36 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 604/3393 [04:20<22:45,  2.04batch/s, Batch Loss=0.0673, Avg Loss=0.1282, Time Left=23.36 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 604/3393 [04:21<22:45,  2.04batch/s, Batch Loss=0.0867, Avg Loss=0.1281, Time Left=23.35 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 605/3393 [04:21<22:41,  2.05batch/s, Batch Loss=0.0867, Avg Loss=0.1281, Time Left=23.35 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 605/3393 [04:21<22:41,  2.05batch/s, Batch Loss=0.0620, Avg Loss=0.1280, Time Left=23.35 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 606/3393 [04:21<22:50,  2.03batch/s, Batch Loss=0.0620, Avg Loss=0.1280, Time Left=23.35 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 606/3393 [04:22<22:50,  2.03batch/s, Batch Loss=0.0807, Avg Loss=0.1279, Time Left=23.34 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 607/3393 [04:22<23:10,  2.00batch/s, Batch Loss=0.0807, Avg Loss=0.1279, Time Left=23.34 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 607/3393 [04:22<23:10,  2.00batch/s, Batch Loss=0.0692, Avg Loss=0.1278, Time Left=23.33 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 608/3393 [04:22<23:23,  1.98batch/s, Batch Loss=0.0692, Avg Loss=0.1278, Time Left=23.33 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 608/3393 [04:23<23:23,  1.98batch/s, Batch Loss=0.1510, Avg Loss=0.1278, Time Left=23.33 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 609/3393 [04:23<23:21,  1.99batch/s, Batch Loss=0.1510, Avg Loss=0.1278, Time Left=23.33 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 609/3393 [04:23<23:21,  1.99batch/s, Batch Loss=0.0707, Avg Loss=0.1277, Time Left=23.32 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 610/3393 [04:23<22:52,  2.03batch/s, Batch Loss=0.0707, Avg Loss=0.1277, Time Left=23.32 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 610/3393 [04:24<22:52,  2.03batch/s, Batch Loss=0.0225, Avg Loss=0.1275, Time Left=23.31 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 611/3393 [04:24<22:31,  2.06batch/s, Batch Loss=0.0225, Avg Loss=0.1275, Time Left=23.31 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 611/3393 [04:24<22:31,  2.06batch/s, Batch Loss=0.4033, Avg Loss=0.1280, Time Left=23.30 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 612/3393 [04:24<22:40,  2.04batch/s, Batch Loss=0.4033, Avg Loss=0.1280, Time Left=23.30 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 612/3393 [04:25<22:40,  2.04batch/s, Batch Loss=0.1701, Avg Loss=0.1281, Time Left=23.29 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 613/3393 [04:25<23:01,  2.01batch/s, Batch Loss=0.1701, Avg Loss=0.1281, Time Left=23.29 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 613/3393 [04:25<23:01,  2.01batch/s, Batch Loss=0.0111, Avg Loss=0.1279, Time Left=23.28 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 614/3393 [04:25<22:38,  2.05batch/s, Batch Loss=0.0111, Avg Loss=0.1279, Time Left=23.28 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 614/3393 [04:26<22:38,  2.05batch/s, Batch Loss=0.0818, Avg Loss=0.1278, Time Left=23.27 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 615/3393 [04:26<22:34,  2.05batch/s, Batch Loss=0.0818, Avg Loss=0.1278, Time Left=23.27 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 615/3393 [04:26<22:34,  2.05batch/s, Batch Loss=0.0229, Avg Loss=0.1276, Time Left=23.27 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 616/3393 [04:26<22:56,  2.02batch/s, Batch Loss=0.0229, Avg Loss=0.1276, Time Left=23.27 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 616/3393 [04:27<22:56,  2.02batch/s, Batch Loss=0.2844, Avg Loss=0.1279, Time Left=23.26 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 617/3393 [04:27<22:46,  2.03batch/s, Batch Loss=0.2844, Avg Loss=0.1279, Time Left=23.26 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 617/3393 [04:27<22:46,  2.03batch/s, Batch Loss=0.0143, Avg Loss=0.1277, Time Left=23.25 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 618/3393 [04:27<22:52,  2.02batch/s, Batch Loss=0.0143, Avg Loss=0.1277, Time Left=23.25 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 618/3393 [04:28<22:52,  2.02batch/s, Batch Loss=0.1646, Avg Loss=0.1278, Time Left=23.24 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 619/3393 [04:28<22:57,  2.01batch/s, Batch Loss=0.1646, Avg Loss=0.1278, Time Left=23.24 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 619/3393 [04:28<22:57,  2.01batch/s, Batch Loss=0.0056, Avg Loss=0.1275, Time Left=23.23 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 620/3393 [04:28<22:33,  2.05batch/s, Batch Loss=0.0056, Avg Loss=0.1275, Time Left=23.23 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 620/3393 [04:29<22:33,  2.05batch/s, Batch Loss=0.0137, Avg Loss=0.1273, Time Left=23.23 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 621/3393 [04:29<22:41,  2.04batch/s, Batch Loss=0.0137, Avg Loss=0.1273, Time Left=23.23 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 621/3393 [04:29<22:41,  2.04batch/s, Batch Loss=0.3173, Avg Loss=0.1277, Time Left=23.22 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 622/3393 [04:29<22:23,  2.06batch/s, Batch Loss=0.3173, Avg Loss=0.1277, Time Left=23.22 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 622/3393 [04:30<22:23,  2.06batch/s, Batch Loss=0.0477, Avg Loss=0.1275, Time Left=23.21 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 623/3393 [04:30<22:47,  2.03batch/s, Batch Loss=0.0477, Avg Loss=0.1275, Time Left=23.21 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 623/3393 [04:30<22:47,  2.03batch/s, Batch Loss=0.2336, Avg Loss=0.1277, Time Left=23.20 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 624/3393 [04:30<22:50,  2.02batch/s, Batch Loss=0.2336, Avg Loss=0.1277, Time Left=23.20 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 624/3393 [04:30<22:50,  2.02batch/s, Batch Loss=0.2175, Avg Loss=0.1279, Time Left=23.19 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  18%|▏| 625/3393 [04:31<22:43,  2.03batch/s, Batch Loss=0.2175, Avg Loss=0.1279, Time Left=23.19 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 625/3393 [04:31<22:43,  2.03batch/s, Batch Loss=0.0762, Avg Loss=0.1278, Time Left=23.19 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 626/3393 [04:31<22:22,  2.06batch/s, Batch Loss=0.0762, Avg Loss=0.1278, Time Left=23.19 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 626/3393 [04:31<22:22,  2.06batch/s, Batch Loss=0.0428, Avg Loss=0.1276, Time Left=23.17 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 627/3393 [04:31<22:07,  2.08batch/s, Batch Loss=0.0428, Avg Loss=0.1276, Time Left=23.17 \u001b[A\n",
      "Epoch 2/3 - Training:  18%|▏| 627/3393 [04:32<22:07,  2.08batch/s, Batch Loss=0.1532, Avg Loss=0.1277, Time Left=23.17 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 628/3393 [04:32<21:55,  2.10batch/s, Batch Loss=0.1532, Avg Loss=0.1277, Time Left=23.17 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 628/3393 [04:32<21:55,  2.10batch/s, Batch Loss=0.0866, Avg Loss=0.1276, Time Left=23.16 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 629/3393 [04:32<22:27,  2.05batch/s, Batch Loss=0.0866, Avg Loss=0.1276, Time Left=23.16 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 629/3393 [04:33<22:27,  2.05batch/s, Batch Loss=0.1884, Avg Loss=0.1277, Time Left=23.15 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 630/3393 [04:33<22:24,  2.05batch/s, Batch Loss=0.1884, Avg Loss=0.1277, Time Left=23.15 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 630/3393 [04:33<22:24,  2.05batch/s, Batch Loss=0.2268, Avg Loss=0.1279, Time Left=23.15 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 631/3393 [04:33<22:59,  2.00batch/s, Batch Loss=0.2268, Avg Loss=0.1279, Time Left=23.15 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 631/3393 [04:34<22:59,  2.00batch/s, Batch Loss=0.0637, Avg Loss=0.1278, Time Left=23.14 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 632/3393 [04:34<22:36,  2.03batch/s, Batch Loss=0.0637, Avg Loss=0.1278, Time Left=23.14 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 632/3393 [04:34<22:36,  2.03batch/s, Batch Loss=0.1693, Avg Loss=0.1278, Time Left=23.13 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 633/3393 [04:34<22:43,  2.02batch/s, Batch Loss=0.1693, Avg Loss=0.1278, Time Left=23.13 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 633/3393 [04:35<22:43,  2.02batch/s, Batch Loss=0.0406, Avg Loss=0.1277, Time Left=23.12 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 634/3393 [04:35<22:35,  2.03batch/s, Batch Loss=0.0406, Avg Loss=0.1277, Time Left=23.12 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 634/3393 [04:35<22:35,  2.03batch/s, Batch Loss=0.1533, Avg Loss=0.1277, Time Left=23.11 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 635/3393 [04:35<22:28,  2.05batch/s, Batch Loss=0.1533, Avg Loss=0.1277, Time Left=23.11 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 635/3393 [04:36<22:28,  2.05batch/s, Batch Loss=0.0187, Avg Loss=0.1275, Time Left=23.11 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 636/3393 [04:36<23:03,  1.99batch/s, Batch Loss=0.0187, Avg Loss=0.1275, Time Left=23.11 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 636/3393 [04:36<23:03,  1.99batch/s, Batch Loss=0.0856, Avg Loss=0.1275, Time Left=23.10 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 637/3393 [04:36<22:48,  2.01batch/s, Batch Loss=0.0856, Avg Loss=0.1275, Time Left=23.10 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 637/3393 [04:37<22:48,  2.01batch/s, Batch Loss=0.0198, Avg Loss=0.1273, Time Left=23.09 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 638/3393 [04:37<23:03,  1.99batch/s, Batch Loss=0.0198, Avg Loss=0.1273, Time Left=23.09 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 638/3393 [04:37<23:03,  1.99batch/s, Batch Loss=0.0882, Avg Loss=0.1272, Time Left=23.08 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 639/3393 [04:37<22:49,  2.01batch/s, Batch Loss=0.0882, Avg Loss=0.1272, Time Left=23.08 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 639/3393 [04:38<22:49,  2.01batch/s, Batch Loss=0.1154, Avg Loss=0.1272, Time Left=23.08 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 640/3393 [04:38<22:37,  2.03batch/s, Batch Loss=0.1154, Avg Loss=0.1272, Time Left=23.08 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 640/3393 [04:38<22:37,  2.03batch/s, Batch Loss=0.1698, Avg Loss=0.1273, Time Left=23.07 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 641/3393 [04:38<22:31,  2.04batch/s, Batch Loss=0.1698, Avg Loss=0.1273, Time Left=23.07 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 641/3393 [04:39<22:31,  2.04batch/s, Batch Loss=0.0361, Avg Loss=0.1271, Time Left=23.06 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 642/3393 [04:39<22:10,  2.07batch/s, Batch Loss=0.0361, Avg Loss=0.1271, Time Left=23.06 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 642/3393 [04:39<22:10,  2.07batch/s, Batch Loss=0.1013, Avg Loss=0.1271, Time Left=23.05 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 643/3393 [04:39<22:49,  2.01batch/s, Batch Loss=0.1013, Avg Loss=0.1271, Time Left=23.05 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 643/3393 [04:40<22:49,  2.01batch/s, Batch Loss=0.0594, Avg Loss=0.1269, Time Left=23.04 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 644/3393 [04:40<22:38,  2.02batch/s, Batch Loss=0.0594, Avg Loss=0.1269, Time Left=23.04 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 644/3393 [04:40<22:38,  2.02batch/s, Batch Loss=0.2686, Avg Loss=0.1272, Time Left=23.03 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 645/3393 [04:40<22:29,  2.04batch/s, Batch Loss=0.2686, Avg Loss=0.1272, Time Left=23.03 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 645/3393 [04:41<22:29,  2.04batch/s, Batch Loss=0.1363, Avg Loss=0.1272, Time Left=23.03 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 646/3393 [04:41<22:23,  2.04batch/s, Batch Loss=0.1363, Avg Loss=0.1272, Time Left=23.03 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 646/3393 [04:41<22:23,  2.04batch/s, Batch Loss=0.1291, Avg Loss=0.1272, Time Left=23.02 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 647/3393 [04:41<22:18,  2.05batch/s, Batch Loss=0.1291, Avg Loss=0.1272, Time Left=23.02 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 647/3393 [04:42<22:18,  2.05batch/s, Batch Loss=0.1315, Avg Loss=0.1272, Time Left=23.01 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 648/3393 [04:42<22:52,  2.00batch/s, Batch Loss=0.1315, Avg Loss=0.1272, Time Left=23.01 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 648/3393 [04:42<22:52,  2.00batch/s, Batch Loss=0.0264, Avg Loss=0.1270, Time Left=23.00 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 649/3393 [04:42<22:40,  2.02batch/s, Batch Loss=0.0264, Avg Loss=0.1270, Time Left=23.00 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 649/3393 [04:43<22:40,  2.02batch/s, Batch Loss=0.0125, Avg Loss=0.1268, Time Left=23.00 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 650/3393 [04:43<23:09,  1.97batch/s, Batch Loss=0.0125, Avg Loss=0.1268, Time Left=23.00 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 650/3393 [04:43<23:09,  1.97batch/s, Batch Loss=0.0308, Avg Loss=0.1267, Time Left=22.99 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 651/3393 [04:43<22:53,  2.00batch/s, Batch Loss=0.0308, Avg Loss=0.1267, Time Left=22.99 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 651/3393 [04:44<22:53,  2.00batch/s, Batch Loss=0.1381, Avg Loss=0.1267, Time Left=22.98 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 652/3393 [04:44<22:37,  2.02batch/s, Batch Loss=0.1381, Avg Loss=0.1267, Time Left=22.98 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 652/3393 [04:44<22:37,  2.02batch/s, Batch Loss=0.0101, Avg Loss=0.1265, Time Left=22.97 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 653/3393 [04:44<22:28,  2.03batch/s, Batch Loss=0.0101, Avg Loss=0.1265, Time Left=22.97 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 653/3393 [04:45<22:28,  2.03batch/s, Batch Loss=0.0203, Avg Loss=0.1263, Time Left=22.96 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 654/3393 [04:45<22:20,  2.04batch/s, Batch Loss=0.0203, Avg Loss=0.1263, Time Left=22.96 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 654/3393 [04:45<22:20,  2.04batch/s, Batch Loss=0.1725, Avg Loss=0.1264, Time Left=22.96 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 655/3393 [04:45<22:42,  2.01batch/s, Batch Loss=0.1725, Avg Loss=0.1264, Time Left=22.96 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 655/3393 [04:46<22:42,  2.01batch/s, Batch Loss=0.0084, Avg Loss=0.1262, Time Left=22.95 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 656/3393 [04:46<22:44,  2.01batch/s, Batch Loss=0.0084, Avg Loss=0.1262, Time Left=22.95 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 656/3393 [04:46<22:44,  2.01batch/s, Batch Loss=0.0258, Avg Loss=0.1260, Time Left=22.95 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 657/3393 [04:46<23:11,  1.97batch/s, Batch Loss=0.0258, Avg Loss=0.1260, Time Left=22.95 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 657/3393 [04:47<23:11,  1.97batch/s, Batch Loss=0.1465, Avg Loss=0.1261, Time Left=22.94 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  19%|▏| 658/3393 [04:47<23:02,  1.98batch/s, Batch Loss=0.1465, Avg Loss=0.1261, Time Left=22.94 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 658/3393 [04:47<23:02,  1.98batch/s, Batch Loss=0.0446, Avg Loss=0.1259, Time Left=22.93 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 659/3393 [04:47<22:34,  2.02batch/s, Batch Loss=0.0446, Avg Loss=0.1259, Time Left=22.93 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 659/3393 [04:48<22:34,  2.02batch/s, Batch Loss=0.0663, Avg Loss=0.1258, Time Left=22.92 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 660/3393 [04:48<22:34,  2.02batch/s, Batch Loss=0.0663, Avg Loss=0.1258, Time Left=22.92 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 660/3393 [04:48<22:34,  2.02batch/s, Batch Loss=0.2727, Avg Loss=0.1261, Time Left=22.91 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 661/3393 [04:48<22:26,  2.03batch/s, Batch Loss=0.2727, Avg Loss=0.1261, Time Left=22.91 \u001b[A\n",
      "Epoch 2/3 - Training:  19%|▏| 661/3393 [04:49<22:26,  2.03batch/s, Batch Loss=0.3131, Avg Loss=0.1264, Time Left=22.91 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 662/3393 [04:49<22:31,  2.02batch/s, Batch Loss=0.3131, Avg Loss=0.1264, Time Left=22.91 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 662/3393 [04:49<22:31,  2.02batch/s, Batch Loss=0.1164, Avg Loss=0.1264, Time Left=22.90 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 663/3393 [04:49<22:37,  2.01batch/s, Batch Loss=0.1164, Avg Loss=0.1264, Time Left=22.90 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 663/3393 [04:50<22:37,  2.01batch/s, Batch Loss=0.0174, Avg Loss=0.1262, Time Left=22.89 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 664/3393 [04:50<22:12,  2.05batch/s, Batch Loss=0.0174, Avg Loss=0.1262, Time Left=22.89 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 664/3393 [04:50<22:12,  2.05batch/s, Batch Loss=0.0083, Avg Loss=0.1260, Time Left=22.88 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 665/3393 [04:50<22:07,  2.05batch/s, Batch Loss=0.0083, Avg Loss=0.1260, Time Left=22.88 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 665/3393 [04:51<22:07,  2.05batch/s, Batch Loss=0.0154, Avg Loss=0.1258, Time Left=22.87 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 666/3393 [04:51<21:59,  2.07batch/s, Batch Loss=0.0154, Avg Loss=0.1258, Time Left=22.87 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 666/3393 [04:51<21:59,  2.07batch/s, Batch Loss=0.0912, Avg Loss=0.1257, Time Left=22.86 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 667/3393 [04:51<22:16,  2.04batch/s, Batch Loss=0.0912, Avg Loss=0.1257, Time Left=22.86 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 667/3393 [04:52<22:16,  2.04batch/s, Batch Loss=0.1702, Avg Loss=0.1258, Time Left=22.85 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 668/3393 [04:52<22:08,  2.05batch/s, Batch Loss=0.1702, Avg Loss=0.1258, Time Left=22.85 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 668/3393 [04:52<22:08,  2.05batch/s, Batch Loss=0.0469, Avg Loss=0.1257, Time Left=22.84 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 669/3393 [04:52<21:54,  2.07batch/s, Batch Loss=0.0469, Avg Loss=0.1257, Time Left=22.84 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 669/3393 [04:53<21:54,  2.07batch/s, Batch Loss=0.0468, Avg Loss=0.1255, Time Left=22.84 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 670/3393 [04:53<22:34,  2.01batch/s, Batch Loss=0.0468, Avg Loss=0.1255, Time Left=22.84 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 670/3393 [04:53<22:34,  2.01batch/s, Batch Loss=0.2186, Avg Loss=0.1257, Time Left=22.83 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 671/3393 [04:53<22:37,  2.01batch/s, Batch Loss=0.2186, Avg Loss=0.1257, Time Left=22.83 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 671/3393 [04:54<22:37,  2.01batch/s, Batch Loss=0.0252, Avg Loss=0.1255, Time Left=22.82 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 672/3393 [04:54<22:37,  2.00batch/s, Batch Loss=0.0252, Avg Loss=0.1255, Time Left=22.82 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 672/3393 [04:54<22:37,  2.00batch/s, Batch Loss=0.0456, Avg Loss=0.1254, Time Left=22.82 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 673/3393 [04:54<22:25,  2.02batch/s, Batch Loss=0.0456, Avg Loss=0.1254, Time Left=22.82 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 673/3393 [04:55<22:25,  2.02batch/s, Batch Loss=0.3895, Avg Loss=0.1258, Time Left=22.81 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 674/3393 [04:55<22:29,  2.01batch/s, Batch Loss=0.3895, Avg Loss=0.1258, Time Left=22.81 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 674/3393 [04:55<22:29,  2.01batch/s, Batch Loss=0.0150, Avg Loss=0.1257, Time Left=22.80 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 675/3393 [04:55<22:21,  2.03batch/s, Batch Loss=0.0150, Avg Loss=0.1257, Time Left=22.80 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 675/3393 [04:56<22:21,  2.03batch/s, Batch Loss=0.3117, Avg Loss=0.1260, Time Left=22.79 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 676/3393 [04:56<22:22,  2.02batch/s, Batch Loss=0.3117, Avg Loss=0.1260, Time Left=22.79 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 676/3393 [04:56<22:22,  2.02batch/s, Batch Loss=0.1669, Avg Loss=0.1260, Time Left=22.79 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 677/3393 [04:56<22:40,  2.00batch/s, Batch Loss=0.1669, Avg Loss=0.1260, Time Left=22.79 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 677/3393 [04:57<22:40,  2.00batch/s, Batch Loss=0.0264, Avg Loss=0.1259, Time Left=22.78 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 678/3393 [04:57<22:29,  2.01batch/s, Batch Loss=0.0264, Avg Loss=0.1259, Time Left=22.78 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 678/3393 [04:57<22:29,  2.01batch/s, Batch Loss=0.2117, Avg Loss=0.1260, Time Left=22.77 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 679/3393 [04:57<22:42,  1.99batch/s, Batch Loss=0.2117, Avg Loss=0.1260, Time Left=22.77 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 679/3393 [04:58<22:42,  1.99batch/s, Batch Loss=0.0345, Avg Loss=0.1259, Time Left=22.76 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 680/3393 [04:58<22:15,  2.03batch/s, Batch Loss=0.0345, Avg Loss=0.1259, Time Left=22.76 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 680/3393 [04:58<22:15,  2.03batch/s, Batch Loss=0.0242, Avg Loss=0.1257, Time Left=22.75 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 681/3393 [04:58<22:23,  2.02batch/s, Batch Loss=0.0242, Avg Loss=0.1257, Time Left=22.75 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 681/3393 [04:59<22:23,  2.02batch/s, Batch Loss=0.0399, Avg Loss=0.1256, Time Left=22.74 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 682/3393 [04:59<21:57,  2.06batch/s, Batch Loss=0.0399, Avg Loss=0.1256, Time Left=22.74 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 682/3393 [04:59<21:57,  2.06batch/s, Batch Loss=0.1177, Avg Loss=0.1255, Time Left=22.74 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 683/3393 [04:59<21:55,  2.06batch/s, Batch Loss=0.1177, Avg Loss=0.1255, Time Left=22.74 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 683/3393 [05:00<21:55,  2.06batch/s, Batch Loss=0.0613, Avg Loss=0.1254, Time Left=22.73 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 684/3393 [05:00<22:30,  2.01batch/s, Batch Loss=0.0613, Avg Loss=0.1254, Time Left=22.73 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 684/3393 [05:00<22:30,  2.01batch/s, Batch Loss=0.0742, Avg Loss=0.1254, Time Left=22.72 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 685/3393 [05:00<22:33,  2.00batch/s, Batch Loss=0.0742, Avg Loss=0.1254, Time Left=22.72 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 685/3393 [05:01<22:33,  2.00batch/s, Batch Loss=0.0868, Avg Loss=0.1253, Time Left=22.72 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 686/3393 [05:01<22:46,  1.98batch/s, Batch Loss=0.0868, Avg Loss=0.1253, Time Left=22.72 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 686/3393 [05:01<22:46,  1.98batch/s, Batch Loss=0.0704, Avg Loss=0.1252, Time Left=22.71 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 687/3393 [05:01<22:18,  2.02batch/s, Batch Loss=0.0704, Avg Loss=0.1252, Time Left=22.71 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 687/3393 [05:02<22:18,  2.02batch/s, Batch Loss=0.3034, Avg Loss=0.1255, Time Left=22.70 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 688/3393 [05:02<22:09,  2.04batch/s, Batch Loss=0.3034, Avg Loss=0.1255, Time Left=22.70 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 688/3393 [05:02<22:09,  2.04batch/s, Batch Loss=0.0748, Avg Loss=0.1254, Time Left=22.69 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 689/3393 [05:02<22:27,  2.01batch/s, Batch Loss=0.0748, Avg Loss=0.1254, Time Left=22.69 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 689/3393 [05:03<22:27,  2.01batch/s, Batch Loss=0.0823, Avg Loss=0.1253, Time Left=22.68 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 690/3393 [05:03<22:17,  2.02batch/s, Batch Loss=0.0823, Avg Loss=0.1253, Time Left=22.68 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 690/3393 [05:03<22:17,  2.02batch/s, Batch Loss=0.6176, Avg Loss=0.1261, Time Left=22.68 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  20%|▏| 691/3393 [05:03<22:20,  2.02batch/s, Batch Loss=0.6176, Avg Loss=0.1261, Time Left=22.68 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 691/3393 [05:04<22:20,  2.02batch/s, Batch Loss=0.5837, Avg Loss=0.1269, Time Left=22.67 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 692/3393 [05:04<22:10,  2.03batch/s, Batch Loss=0.5837, Avg Loss=0.1269, Time Left=22.67 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 692/3393 [05:04<22:10,  2.03batch/s, Batch Loss=0.0693, Avg Loss=0.1268, Time Left=22.66 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 693/3393 [05:04<22:02,  2.04batch/s, Batch Loss=0.0693, Avg Loss=0.1268, Time Left=22.66 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 693/3393 [05:05<22:02,  2.04batch/s, Batch Loss=0.0489, Avg Loss=0.1267, Time Left=22.65 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 694/3393 [05:05<22:08,  2.03batch/s, Batch Loss=0.0489, Avg Loss=0.1267, Time Left=22.65 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 694/3393 [05:05<22:08,  2.03batch/s, Batch Loss=0.1008, Avg Loss=0.1266, Time Left=22.64 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 695/3393 [05:05<22:02,  2.04batch/s, Batch Loss=0.1008, Avg Loss=0.1266, Time Left=22.64 \u001b[A\n",
      "Epoch 2/3 - Training:  20%|▏| 695/3393 [05:06<22:02,  2.04batch/s, Batch Loss=0.0441, Avg Loss=0.1265, Time Left=22.64 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 696/3393 [05:06<22:23,  2.01batch/s, Batch Loss=0.0441, Avg Loss=0.1265, Time Left=22.64 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 696/3393 [05:06<22:23,  2.01batch/s, Batch Loss=0.0365, Avg Loss=0.1263, Time Left=22.63 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 697/3393 [05:06<21:45,  2.06batch/s, Batch Loss=0.0365, Avg Loss=0.1263, Time Left=22.63 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 697/3393 [05:07<21:45,  2.06batch/s, Batch Loss=0.0866, Avg Loss=0.1263, Time Left=22.62 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 698/3393 [05:07<21:48,  2.06batch/s, Batch Loss=0.0866, Avg Loss=0.1263, Time Left=22.62 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 698/3393 [05:07<21:48,  2.06batch/s, Batch Loss=0.1307, Avg Loss=0.1263, Time Left=22.61 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 699/3393 [05:07<22:21,  2.01batch/s, Batch Loss=0.1307, Avg Loss=0.1263, Time Left=22.61 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 699/3393 [05:08<22:21,  2.01batch/s, Batch Loss=0.1969, Avg Loss=0.1264, Time Left=22.60 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 700/3393 [05:08<21:56,  2.05batch/s, Batch Loss=0.1969, Avg Loss=0.1264, Time Left=22.60 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 700/3393 [05:08<21:56,  2.05batch/s, Batch Loss=0.4095, Avg Loss=0.1268, Time Left=22.60 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 701/3393 [05:08<22:17,  2.01batch/s, Batch Loss=0.4095, Avg Loss=0.1268, Time Left=22.60 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 701/3393 [05:09<22:17,  2.01batch/s, Batch Loss=0.0921, Avg Loss=0.1268, Time Left=22.59 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 702/3393 [05:09<22:07,  2.03batch/s, Batch Loss=0.0921, Avg Loss=0.1268, Time Left=22.59 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 702/3393 [05:09<22:07,  2.03batch/s, Batch Loss=0.1095, Avg Loss=0.1268, Time Left=22.58 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 703/3393 [05:09<22:12,  2.02batch/s, Batch Loss=0.1095, Avg Loss=0.1268, Time Left=22.58 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 703/3393 [05:09<22:12,  2.02batch/s, Batch Loss=0.0326, Avg Loss=0.1266, Time Left=22.57 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 704/3393 [05:09<21:50,  2.05batch/s, Batch Loss=0.0326, Avg Loss=0.1266, Time Left=22.57 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 704/3393 [05:10<21:50,  2.05batch/s, Batch Loss=0.1160, Avg Loss=0.1266, Time Left=22.56 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 705/3393 [05:10<21:47,  2.06batch/s, Batch Loss=0.1160, Avg Loss=0.1266, Time Left=22.56 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 705/3393 [05:10<21:47,  2.06batch/s, Batch Loss=0.1100, Avg Loss=0.1266, Time Left=22.56 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 706/3393 [05:10<22:22,  2.00batch/s, Batch Loss=0.1100, Avg Loss=0.1266, Time Left=22.56 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 706/3393 [05:11<22:22,  2.00batch/s, Batch Loss=0.0520, Avg Loss=0.1264, Time Left=22.55 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 707/3393 [05:11<22:22,  2.00batch/s, Batch Loss=0.0520, Avg Loss=0.1264, Time Left=22.55 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 707/3393 [05:12<22:22,  2.00batch/s, Batch Loss=0.5110, Avg Loss=0.1271, Time Left=22.54 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 708/3393 [05:12<22:42,  1.97batch/s, Batch Loss=0.5110, Avg Loss=0.1271, Time Left=22.54 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 708/3393 [05:12<22:42,  1.97batch/s, Batch Loss=0.2260, Avg Loss=0.1272, Time Left=22.53 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 709/3393 [05:12<22:28,  1.99batch/s, Batch Loss=0.2260, Avg Loss=0.1272, Time Left=22.53 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 709/3393 [05:13<22:28,  1.99batch/s, Batch Loss=0.1505, Avg Loss=0.1272, Time Left=22.53 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 710/3393 [05:13<22:27,  1.99batch/s, Batch Loss=0.1505, Avg Loss=0.1272, Time Left=22.53 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 710/3393 [05:13<22:27,  1.99batch/s, Batch Loss=0.1730, Avg Loss=0.1273, Time Left=22.52 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 711/3393 [05:13<22:13,  2.01batch/s, Batch Loss=0.1730, Avg Loss=0.1273, Time Left=22.52 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 711/3393 [05:13<22:13,  2.01batch/s, Batch Loss=0.1322, Avg Loss=0.1273, Time Left=22.51 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 712/3393 [05:13<22:01,  2.03batch/s, Batch Loss=0.1322, Avg Loss=0.1273, Time Left=22.51 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 712/3393 [05:14<22:01,  2.03batch/s, Batch Loss=0.0906, Avg Loss=0.1273, Time Left=22.50 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 713/3393 [05:14<22:07,  2.02batch/s, Batch Loss=0.0906, Avg Loss=0.1273, Time Left=22.50 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 713/3393 [05:14<22:07,  2.02batch/s, Batch Loss=0.0444, Avg Loss=0.1271, Time Left=22.50 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 714/3393 [05:14<22:10,  2.01batch/s, Batch Loss=0.0444, Avg Loss=0.1271, Time Left=22.50 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 714/3393 [05:15<22:10,  2.01batch/s, Batch Loss=0.0546, Avg Loss=0.1270, Time Left=22.49 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 715/3393 [05:15<22:13,  2.01batch/s, Batch Loss=0.0546, Avg Loss=0.1270, Time Left=22.49 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 715/3393 [05:15<22:13,  2.01batch/s, Batch Loss=0.0468, Avg Loss=0.1269, Time Left=22.48 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 716/3393 [05:15<21:50,  2.04batch/s, Batch Loss=0.0468, Avg Loss=0.1269, Time Left=22.48 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 716/3393 [05:16<21:50,  2.04batch/s, Batch Loss=0.0300, Avg Loss=0.1267, Time Left=22.47 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 717/3393 [05:16<21:56,  2.03batch/s, Batch Loss=0.0300, Avg Loss=0.1267, Time Left=22.47 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 717/3393 [05:16<21:56,  2.03batch/s, Batch Loss=0.1118, Avg Loss=0.1267, Time Left=22.46 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 718/3393 [05:16<22:20,  2.00batch/s, Batch Loss=0.1118, Avg Loss=0.1267, Time Left=22.46 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 718/3393 [05:17<22:20,  2.00batch/s, Batch Loss=0.0294, Avg Loss=0.1266, Time Left=22.46 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 719/3393 [05:17<22:03,  2.02batch/s, Batch Loss=0.0294, Avg Loss=0.1266, Time Left=22.46 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 719/3393 [05:17<22:03,  2.02batch/s, Batch Loss=0.4157, Avg Loss=0.1270, Time Left=22.45 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 720/3393 [05:17<22:30,  1.98batch/s, Batch Loss=0.4157, Avg Loss=0.1270, Time Left=22.45 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 720/3393 [05:18<22:30,  1.98batch/s, Batch Loss=0.4400, Avg Loss=0.1275, Time Left=22.44 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 721/3393 [05:18<22:14,  2.00batch/s, Batch Loss=0.4400, Avg Loss=0.1275, Time Left=22.44 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 721/3393 [05:18<22:14,  2.00batch/s, Batch Loss=0.0121, Avg Loss=0.1273, Time Left=22.43 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 722/3393 [05:18<22:14,  2.00batch/s, Batch Loss=0.0121, Avg Loss=0.1273, Time Left=22.43 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 722/3393 [05:19<22:14,  2.00batch/s, Batch Loss=0.0809, Avg Loss=0.1273, Time Left=22.43 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 723/3393 [05:19<22:15,  2.00batch/s, Batch Loss=0.0809, Avg Loss=0.1273, Time Left=22.43 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 723/3393 [05:19<22:15,  2.00batch/s, Batch Loss=0.0230, Avg Loss=0.1271, Time Left=22.42 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  21%|▏| 724/3393 [05:19<22:27,  1.98batch/s, Batch Loss=0.0230, Avg Loss=0.1271, Time Left=22.42 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 724/3393 [05:20<22:27,  1.98batch/s, Batch Loss=0.0387, Avg Loss=0.1270, Time Left=22.41 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 725/3393 [05:20<22:23,  1.99batch/s, Batch Loss=0.0387, Avg Loss=0.1270, Time Left=22.41 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 725/3393 [05:20<22:23,  1.99batch/s, Batch Loss=0.2680, Avg Loss=0.1272, Time Left=22.40 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 726/3393 [05:20<22:11,  2.00batch/s, Batch Loss=0.2680, Avg Loss=0.1272, Time Left=22.40 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 726/3393 [05:21<22:11,  2.00batch/s, Batch Loss=0.0100, Avg Loss=0.1270, Time Left=22.40 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 727/3393 [05:21<21:54,  2.03batch/s, Batch Loss=0.0100, Avg Loss=0.1270, Time Left=22.40 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 727/3393 [05:21<21:54,  2.03batch/s, Batch Loss=0.0194, Avg Loss=0.1268, Time Left=22.39 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 728/3393 [05:21<21:46,  2.04batch/s, Batch Loss=0.0194, Avg Loss=0.1268, Time Left=22.39 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 728/3393 [05:22<21:46,  2.04batch/s, Batch Loss=0.1123, Avg Loss=0.1268, Time Left=22.38 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 729/3393 [05:22<22:05,  2.01batch/s, Batch Loss=0.1123, Avg Loss=0.1268, Time Left=22.38 \u001b[A\n",
      "Epoch 2/3 - Training:  21%|▏| 729/3393 [05:22<22:05,  2.01batch/s, Batch Loss=0.0567, Avg Loss=0.1267, Time Left=22.37 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 730/3393 [05:22<22:09,  2.00batch/s, Batch Loss=0.0567, Avg Loss=0.1267, Time Left=22.37 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 730/3393 [05:23<22:09,  2.00batch/s, Batch Loss=0.2149, Avg Loss=0.1268, Time Left=22.37 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 731/3393 [05:23<22:21,  1.99batch/s, Batch Loss=0.2149, Avg Loss=0.1268, Time Left=22.37 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 731/3393 [05:23<22:21,  1.99batch/s, Batch Loss=0.0210, Avg Loss=0.1267, Time Left=22.36 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 732/3393 [05:23<22:06,  2.01batch/s, Batch Loss=0.0210, Avg Loss=0.1267, Time Left=22.36 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 732/3393 [05:24<22:06,  2.01batch/s, Batch Loss=0.0319, Avg Loss=0.1265, Time Left=22.35 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 733/3393 [05:24<21:54,  2.02batch/s, Batch Loss=0.0319, Avg Loss=0.1265, Time Left=22.35 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 733/3393 [05:24<21:54,  2.02batch/s, Batch Loss=0.0296, Avg Loss=0.1264, Time Left=22.34 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 734/3393 [05:24<21:45,  2.04batch/s, Batch Loss=0.0296, Avg Loss=0.1264, Time Left=22.34 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 734/3393 [05:25<21:45,  2.04batch/s, Batch Loss=0.0379, Avg Loss=0.1263, Time Left=22.33 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 735/3393 [05:25<21:38,  2.05batch/s, Batch Loss=0.0379, Avg Loss=0.1263, Time Left=22.33 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 735/3393 [05:25<21:38,  2.05batch/s, Batch Loss=0.0178, Avg Loss=0.1261, Time Left=22.32 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 736/3393 [05:25<21:34,  2.05batch/s, Batch Loss=0.0178, Avg Loss=0.1261, Time Left=22.32 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 736/3393 [05:26<21:34,  2.05batch/s, Batch Loss=0.0469, Avg Loss=0.1260, Time Left=22.31 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 737/3393 [05:26<21:32,  2.05batch/s, Batch Loss=0.0469, Avg Loss=0.1260, Time Left=22.31 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 737/3393 [05:26<21:32,  2.05batch/s, Batch Loss=0.3812, Avg Loss=0.1264, Time Left=22.31 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 738/3393 [05:26<21:29,  2.06batch/s, Batch Loss=0.3812, Avg Loss=0.1264, Time Left=22.31 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 738/3393 [05:27<21:29,  2.06batch/s, Batch Loss=0.1760, Avg Loss=0.1264, Time Left=22.30 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 739/3393 [05:27<22:04,  2.00batch/s, Batch Loss=0.1760, Avg Loss=0.1264, Time Left=22.30 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 739/3393 [05:27<22:04,  2.00batch/s, Batch Loss=0.1277, Avg Loss=0.1264, Time Left=22.29 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 740/3393 [05:27<21:55,  2.02batch/s, Batch Loss=0.1277, Avg Loss=0.1264, Time Left=22.29 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 740/3393 [05:28<21:55,  2.02batch/s, Batch Loss=0.0124, Avg Loss=0.1263, Time Left=22.29 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 741/3393 [05:28<22:21,  1.98batch/s, Batch Loss=0.0124, Avg Loss=0.1263, Time Left=22.29 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 741/3393 [05:28<22:21,  1.98batch/s, Batch Loss=0.0194, Avg Loss=0.1261, Time Left=22.28 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 742/3393 [05:28<22:05,  2.00batch/s, Batch Loss=0.0194, Avg Loss=0.1261, Time Left=22.28 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 742/3393 [05:29<22:05,  2.00batch/s, Batch Loss=0.0177, Avg Loss=0.1259, Time Left=22.27 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 743/3393 [05:29<22:31,  1.96batch/s, Batch Loss=0.0177, Avg Loss=0.1259, Time Left=22.27 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 743/3393 [05:29<22:31,  1.96batch/s, Batch Loss=0.1090, Avg Loss=0.1259, Time Left=22.26 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 744/3393 [05:29<22:09,  1.99batch/s, Batch Loss=0.1090, Avg Loss=0.1259, Time Left=22.26 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 744/3393 [05:30<22:09,  1.99batch/s, Batch Loss=0.1260, Avg Loss=0.1259, Time Left=22.26 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 745/3393 [05:30<22:20,  1.98batch/s, Batch Loss=0.1260, Avg Loss=0.1259, Time Left=22.26 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 745/3393 [05:30<22:20,  1.98batch/s, Batch Loss=0.3884, Avg Loss=0.1263, Time Left=22.25 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 746/3393 [05:30<21:38,  2.04batch/s, Batch Loss=0.3884, Avg Loss=0.1263, Time Left=22.25 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 746/3393 [05:31<21:38,  2.04batch/s, Batch Loss=0.2121, Avg Loss=0.1264, Time Left=22.24 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 747/3393 [05:31<21:43,  2.03batch/s, Batch Loss=0.2121, Avg Loss=0.1264, Time Left=22.24 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 747/3393 [05:31<21:43,  2.03batch/s, Batch Loss=0.1870, Avg Loss=0.1265, Time Left=22.23 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 748/3393 [05:31<21:48,  2.02batch/s, Batch Loss=0.1870, Avg Loss=0.1265, Time Left=22.23 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 748/3393 [05:32<21:48,  2.02batch/s, Batch Loss=0.1590, Avg Loss=0.1266, Time Left=22.22 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 749/3393 [05:32<21:45,  2.03batch/s, Batch Loss=0.1590, Avg Loss=0.1266, Time Left=22.22 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 749/3393 [05:32<21:45,  2.03batch/s, Batch Loss=0.3162, Avg Loss=0.1268, Time Left=22.21 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 750/3393 [05:32<21:33,  2.04batch/s, Batch Loss=0.3162, Avg Loss=0.1268, Time Left=22.21 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 750/3393 [05:33<21:33,  2.04batch/s, Batch Loss=0.1879, Avg Loss=0.1269, Time Left=22.21 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 751/3393 [05:33<21:28,  2.05batch/s, Batch Loss=0.1879, Avg Loss=0.1269, Time Left=22.21 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 751/3393 [05:33<21:28,  2.05batch/s, Batch Loss=0.0205, Avg Loss=0.1268, Time Left=22.20 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 752/3393 [05:33<21:11,  2.08batch/s, Batch Loss=0.0205, Avg Loss=0.1268, Time Left=22.20 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 752/3393 [05:34<21:11,  2.08batch/s, Batch Loss=0.0744, Avg Loss=0.1267, Time Left=22.19 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 753/3393 [05:34<21:24,  2.06batch/s, Batch Loss=0.0744, Avg Loss=0.1267, Time Left=22.19 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 753/3393 [05:34<21:24,  2.06batch/s, Batch Loss=0.3787, Avg Loss=0.1271, Time Left=22.18 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 754/3393 [05:34<21:11,  2.08batch/s, Batch Loss=0.3787, Avg Loss=0.1271, Time Left=22.18 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 754/3393 [05:35<21:11,  2.08batch/s, Batch Loss=0.3631, Avg Loss=0.1274, Time Left=22.17 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 755/3393 [05:35<21:22,  2.06batch/s, Batch Loss=0.3631, Avg Loss=0.1274, Time Left=22.17 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 755/3393 [05:35<21:22,  2.06batch/s, Batch Loss=0.0852, Avg Loss=0.1274, Time Left=22.16 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 756/3393 [05:35<21:22,  2.06batch/s, Batch Loss=0.0852, Avg Loss=0.1274, Time Left=22.16 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 756/3393 [05:36<21:22,  2.06batch/s, Batch Loss=0.0815, Avg Loss=0.1273, Time Left=22.15 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  22%|▏| 757/3393 [05:36<21:18,  2.06batch/s, Batch Loss=0.0815, Avg Loss=0.1273, Time Left=22.15 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 757/3393 [05:36<21:18,  2.06batch/s, Batch Loss=0.1896, Avg Loss=0.1274, Time Left=22.14 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 758/3393 [05:36<21:08,  2.08batch/s, Batch Loss=0.1896, Avg Loss=0.1274, Time Left=22.14 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 758/3393 [05:37<21:08,  2.08batch/s, Batch Loss=0.1648, Avg Loss=0.1274, Time Left=22.14 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 759/3393 [05:37<21:42,  2.02batch/s, Batch Loss=0.1648, Avg Loss=0.1274, Time Left=22.14 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 759/3393 [05:37<21:42,  2.02batch/s, Batch Loss=0.0753, Avg Loss=0.1274, Time Left=22.13 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 760/3393 [05:37<21:35,  2.03batch/s, Batch Loss=0.0753, Avg Loss=0.1274, Time Left=22.13 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 760/3393 [05:38<21:35,  2.03batch/s, Batch Loss=0.0863, Avg Loss=0.1273, Time Left=22.12 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 761/3393 [05:38<21:41,  2.02batch/s, Batch Loss=0.0863, Avg Loss=0.1273, Time Left=22.12 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 761/3393 [05:38<21:41,  2.02batch/s, Batch Loss=0.0433, Avg Loss=0.1272, Time Left=22.11 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 762/3393 [05:38<21:21,  2.05batch/s, Batch Loss=0.0433, Avg Loss=0.1272, Time Left=22.11 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 762/3393 [05:39<21:21,  2.05batch/s, Batch Loss=0.1080, Avg Loss=0.1272, Time Left=22.10 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 763/3393 [05:39<21:16,  2.06batch/s, Batch Loss=0.1080, Avg Loss=0.1272, Time Left=22.10 \u001b[A\n",
      "Epoch 2/3 - Training:  22%|▏| 763/3393 [05:39<21:16,  2.06batch/s, Batch Loss=0.0721, Avg Loss=0.1271, Time Left=22.10 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 764/3393 [05:39<21:51,  2.00batch/s, Batch Loss=0.0721, Avg Loss=0.1271, Time Left=22.10 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 764/3393 [05:40<21:51,  2.00batch/s, Batch Loss=0.0458, Avg Loss=0.1270, Time Left=22.09 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 765/3393 [05:40<21:40,  2.02batch/s, Batch Loss=0.0458, Avg Loss=0.1270, Time Left=22.09 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 765/3393 [05:40<21:40,  2.02batch/s, Batch Loss=0.0572, Avg Loss=0.1269, Time Left=22.08 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 766/3393 [05:40<21:56,  1.99batch/s, Batch Loss=0.0572, Avg Loss=0.1269, Time Left=22.08 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 766/3393 [05:41<21:56,  1.99batch/s, Batch Loss=0.1037, Avg Loss=0.1268, Time Left=22.08 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 767/3393 [05:41<21:56,  1.99batch/s, Batch Loss=0.1037, Avg Loss=0.1268, Time Left=22.08 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 767/3393 [05:41<21:56,  1.99batch/s, Batch Loss=0.0846, Avg Loss=0.1268, Time Left=22.07 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 768/3393 [05:41<22:06,  1.98batch/s, Batch Loss=0.0846, Avg Loss=0.1268, Time Left=22.07 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 768/3393 [05:42<22:06,  1.98batch/s, Batch Loss=0.1742, Avg Loss=0.1268, Time Left=22.06 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 769/3393 [05:42<21:38,  2.02batch/s, Batch Loss=0.1742, Avg Loss=0.1268, Time Left=22.06 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 769/3393 [05:42<21:38,  2.02batch/s, Batch Loss=0.1198, Avg Loss=0.1268, Time Left=22.05 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 770/3393 [05:42<21:16,  2.05batch/s, Batch Loss=0.1198, Avg Loss=0.1268, Time Left=22.05 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 770/3393 [05:43<21:16,  2.05batch/s, Batch Loss=0.0450, Avg Loss=0.1267, Time Left=22.04 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 771/3393 [05:43<21:37,  2.02batch/s, Batch Loss=0.0450, Avg Loss=0.1267, Time Left=22.04 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 771/3393 [05:43<21:37,  2.02batch/s, Batch Loss=0.1064, Avg Loss=0.1267, Time Left=22.03 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 772/3393 [05:43<21:17,  2.05batch/s, Batch Loss=0.1064, Avg Loss=0.1267, Time Left=22.03 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 772/3393 [05:44<21:17,  2.05batch/s, Batch Loss=0.0481, Avg Loss=0.1266, Time Left=22.03 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 773/3393 [05:44<21:50,  2.00batch/s, Batch Loss=0.0481, Avg Loss=0.1266, Time Left=22.03 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 773/3393 [05:44<21:50,  2.00batch/s, Batch Loss=0.2977, Avg Loss=0.1268, Time Left=22.02 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 774/3393 [05:44<21:38,  2.02batch/s, Batch Loss=0.2977, Avg Loss=0.1268, Time Left=22.02 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 774/3393 [05:45<21:38,  2.02batch/s, Batch Loss=0.0674, Avg Loss=0.1267, Time Left=22.01 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 775/3393 [05:45<21:16,  2.05batch/s, Batch Loss=0.0674, Avg Loss=0.1267, Time Left=22.01 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 775/3393 [05:45<21:16,  2.05batch/s, Batch Loss=0.0385, Avg Loss=0.1266, Time Left=22.00 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 776/3393 [05:45<21:00,  2.08batch/s, Batch Loss=0.0385, Avg Loss=0.1266, Time Left=22.00 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 776/3393 [05:46<21:00,  2.08batch/s, Batch Loss=0.1284, Avg Loss=0.1266, Time Left=21.99 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 777/3393 [05:46<20:48,  2.10batch/s, Batch Loss=0.1284, Avg Loss=0.1266, Time Left=21.99 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 777/3393 [05:46<20:48,  2.10batch/s, Batch Loss=0.0248, Avg Loss=0.1265, Time Left=21.98 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 778/3393 [05:46<20:52,  2.09batch/s, Batch Loss=0.0248, Avg Loss=0.1265, Time Left=21.98 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 778/3393 [05:47<20:52,  2.09batch/s, Batch Loss=0.0198, Avg Loss=0.1263, Time Left=21.98 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 779/3393 [05:47<21:19,  2.04batch/s, Batch Loss=0.0198, Avg Loss=0.1263, Time Left=21.98 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 779/3393 [05:47<21:19,  2.04batch/s, Batch Loss=0.0111, Avg Loss=0.1261, Time Left=21.97 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 780/3393 [05:47<21:17,  2.05batch/s, Batch Loss=0.0111, Avg Loss=0.1261, Time Left=21.97 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 780/3393 [05:48<21:17,  2.05batch/s, Batch Loss=0.0038, Avg Loss=0.1260, Time Left=21.96 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 781/3393 [05:48<21:25,  2.03batch/s, Batch Loss=0.0038, Avg Loss=0.1260, Time Left=21.96 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 781/3393 [05:48<21:25,  2.03batch/s, Batch Loss=0.2829, Avg Loss=0.1262, Time Left=21.95 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 782/3393 [05:48<20:54,  2.08batch/s, Batch Loss=0.2829, Avg Loss=0.1262, Time Left=21.95 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 782/3393 [05:48<20:54,  2.08batch/s, Batch Loss=0.2924, Avg Loss=0.1264, Time Left=21.94 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 783/3393 [05:48<20:54,  2.08batch/s, Batch Loss=0.2924, Avg Loss=0.1264, Time Left=21.94 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 783/3393 [05:49<20:54,  2.08batch/s, Batch Loss=0.1349, Avg Loss=0.1264, Time Left=21.93 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 784/3393 [05:49<21:34,  2.02batch/s, Batch Loss=0.1349, Avg Loss=0.1264, Time Left=21.93 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 784/3393 [05:49<21:34,  2.02batch/s, Batch Loss=0.1544, Avg Loss=0.1265, Time Left=21.92 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 785/3393 [05:49<21:11,  2.05batch/s, Batch Loss=0.1544, Avg Loss=0.1265, Time Left=21.92 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 785/3393 [05:50<21:11,  2.05batch/s, Batch Loss=0.3889, Avg Loss=0.1268, Time Left=21.92 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 786/3393 [05:50<21:44,  2.00batch/s, Batch Loss=0.3889, Avg Loss=0.1268, Time Left=21.92 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 786/3393 [05:50<21:44,  2.00batch/s, Batch Loss=0.0328, Avg Loss=0.1267, Time Left=21.91 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 787/3393 [05:50<21:28,  2.02batch/s, Batch Loss=0.0328, Avg Loss=0.1267, Time Left=21.91 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 787/3393 [05:51<21:28,  2.02batch/s, Batch Loss=0.3546, Avg Loss=0.1270, Time Left=21.90 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 788/3393 [05:51<21:25,  2.03batch/s, Batch Loss=0.3546, Avg Loss=0.1270, Time Left=21.90 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 788/3393 [05:51<21:25,  2.03batch/s, Batch Loss=0.5184, Avg Loss=0.1276, Time Left=21.90 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 789/3393 [05:51<21:36,  2.01batch/s, Batch Loss=0.5184, Avg Loss=0.1276, Time Left=21.90 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 789/3393 [05:52<21:36,  2.01batch/s, Batch Loss=0.0109, Avg Loss=0.1274, Time Left=21.89 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  23%|▏| 790/3393 [05:52<21:43,  2.00batch/s, Batch Loss=0.0109, Avg Loss=0.1274, Time Left=21.89 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 790/3393 [05:52<21:43,  2.00batch/s, Batch Loss=0.0589, Avg Loss=0.1273, Time Left=21.88 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 791/3393 [05:52<21:41,  2.00batch/s, Batch Loss=0.0589, Avg Loss=0.1273, Time Left=21.88 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 791/3393 [05:53<21:41,  2.00batch/s, Batch Loss=0.0904, Avg Loss=0.1273, Time Left=21.87 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 792/3393 [05:53<21:29,  2.02batch/s, Batch Loss=0.0904, Avg Loss=0.1273, Time Left=21.87 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 792/3393 [05:54<21:29,  2.02batch/s, Batch Loss=0.1199, Avg Loss=0.1273, Time Left=21.87 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 793/3393 [05:54<22:03,  1.96batch/s, Batch Loss=0.1199, Avg Loss=0.1273, Time Left=21.87 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 793/3393 [05:54<22:03,  1.96batch/s, Batch Loss=0.2663, Avg Loss=0.1274, Time Left=21.86 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 794/3393 [05:54<21:26,  2.02batch/s, Batch Loss=0.2663, Avg Loss=0.1274, Time Left=21.86 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 794/3393 [05:54<21:26,  2.02batch/s, Batch Loss=0.0334, Avg Loss=0.1273, Time Left=21.85 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 795/3393 [05:54<21:27,  2.02batch/s, Batch Loss=0.0334, Avg Loss=0.1273, Time Left=21.85 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 795/3393 [05:55<21:27,  2.02batch/s, Batch Loss=0.0682, Avg Loss=0.1272, Time Left=21.84 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 796/3393 [05:55<21:20,  2.03batch/s, Batch Loss=0.0682, Avg Loss=0.1272, Time Left=21.84 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 796/3393 [05:55<21:20,  2.03batch/s, Batch Loss=0.3636, Avg Loss=0.1276, Time Left=21.83 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 797/3393 [05:55<21:12,  2.04batch/s, Batch Loss=0.3636, Avg Loss=0.1276, Time Left=21.83 \u001b[A\n",
      "Epoch 2/3 - Training:  23%|▏| 797/3393 [05:56<21:12,  2.04batch/s, Batch Loss=0.1962, Avg Loss=0.1277, Time Left=21.83 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 798/3393 [05:56<21:43,  1.99batch/s, Batch Loss=0.1962, Avg Loss=0.1277, Time Left=21.83 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 798/3393 [05:56<21:43,  1.99batch/s, Batch Loss=0.3926, Avg Loss=0.1280, Time Left=21.82 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 799/3393 [05:56<21:29,  2.01batch/s, Batch Loss=0.3926, Avg Loss=0.1280, Time Left=21.82 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 799/3393 [05:57<21:29,  2.01batch/s, Batch Loss=0.0530, Avg Loss=0.1279, Time Left=21.81 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 800/3393 [05:57<21:42,  1.99batch/s, Batch Loss=0.0530, Avg Loss=0.1279, Time Left=21.81 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 800/3393 [05:57<21:42,  1.99batch/s, Batch Loss=0.1288, Avg Loss=0.1279, Time Left=21.80 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 801/3393 [05:57<21:31,  2.01batch/s, Batch Loss=0.1288, Avg Loss=0.1279, Time Left=21.80 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 801/3393 [05:58<21:31,  2.01batch/s, Batch Loss=0.0266, Avg Loss=0.1278, Time Left=21.79 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 802/3393 [05:58<21:30,  2.01batch/s, Batch Loss=0.0266, Avg Loss=0.1278, Time Left=21.79 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 802/3393 [05:58<21:30,  2.01batch/s, Batch Loss=0.1084, Avg Loss=0.1278, Time Left=21.79 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 803/3393 [05:58<21:20,  2.02batch/s, Batch Loss=0.1084, Avg Loss=0.1278, Time Left=21.79 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 803/3393 [05:59<21:20,  2.02batch/s, Batch Loss=0.1024, Avg Loss=0.1277, Time Left=21.78 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 804/3393 [05:59<21:11,  2.04batch/s, Batch Loss=0.1024, Avg Loss=0.1277, Time Left=21.78 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 804/3393 [05:59<21:11,  2.04batch/s, Batch Loss=0.1205, Avg Loss=0.1277, Time Left=21.77 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 805/3393 [05:59<21:29,  2.01batch/s, Batch Loss=0.1205, Avg Loss=0.1277, Time Left=21.77 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 805/3393 [06:00<21:29,  2.01batch/s, Batch Loss=0.0925, Avg Loss=0.1277, Time Left=21.76 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 806/3393 [06:00<21:19,  2.02batch/s, Batch Loss=0.0925, Avg Loss=0.1277, Time Left=21.76 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 806/3393 [06:00<21:19,  2.02batch/s, Batch Loss=0.1387, Avg Loss=0.1277, Time Left=21.76 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 807/3393 [06:00<21:34,  2.00batch/s, Batch Loss=0.1387, Avg Loss=0.1277, Time Left=21.76 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 807/3393 [06:01<21:34,  2.00batch/s, Batch Loss=0.0920, Avg Loss=0.1276, Time Left=21.75 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 808/3393 [06:01<21:35,  2.00batch/s, Batch Loss=0.0920, Avg Loss=0.1276, Time Left=21.75 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 808/3393 [06:01<21:35,  2.00batch/s, Batch Loss=0.2089, Avg Loss=0.1277, Time Left=21.74 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 809/3393 [06:01<21:20,  2.02batch/s, Batch Loss=0.2089, Avg Loss=0.1277, Time Left=21.74 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 809/3393 [06:02<21:20,  2.02batch/s, Batch Loss=0.0610, Avg Loss=0.1276, Time Left=21.73 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 810/3393 [06:02<21:11,  2.03batch/s, Batch Loss=0.0610, Avg Loss=0.1276, Time Left=21.73 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 810/3393 [06:02<21:11,  2.03batch/s, Batch Loss=0.0210, Avg Loss=0.1275, Time Left=21.72 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 811/3393 [06:02<20:52,  2.06batch/s, Batch Loss=0.0210, Avg Loss=0.1275, Time Left=21.72 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 811/3393 [06:03<20:52,  2.06batch/s, Batch Loss=0.4585, Avg Loss=0.1280, Time Left=21.71 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 812/3393 [06:03<21:15,  2.02batch/s, Batch Loss=0.4585, Avg Loss=0.1280, Time Left=21.71 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 812/3393 [06:03<21:15,  2.02batch/s, Batch Loss=0.0124, Avg Loss=0.1278, Time Left=21.70 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 813/3393 [06:03<20:54,  2.06batch/s, Batch Loss=0.0124, Avg Loss=0.1278, Time Left=21.70 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 813/3393 [06:04<20:54,  2.06batch/s, Batch Loss=0.0381, Avg Loss=0.1277, Time Left=21.70 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 814/3393 [06:04<21:04,  2.04batch/s, Batch Loss=0.0381, Avg Loss=0.1277, Time Left=21.70 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 814/3393 [06:04<21:04,  2.04batch/s, Batch Loss=0.0612, Avg Loss=0.1276, Time Left=21.69 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 815/3393 [06:04<20:46,  2.07batch/s, Batch Loss=0.0612, Avg Loss=0.1276, Time Left=21.69 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 815/3393 [06:05<20:46,  2.07batch/s, Batch Loss=0.1039, Avg Loss=0.1276, Time Left=21.68 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 816/3393 [06:05<20:46,  2.07batch/s, Batch Loss=0.1039, Avg Loss=0.1276, Time Left=21.68 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 816/3393 [06:05<20:46,  2.07batch/s, Batch Loss=0.1319, Avg Loss=0.1276, Time Left=21.67 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 817/3393 [06:05<20:45,  2.07batch/s, Batch Loss=0.1319, Avg Loss=0.1276, Time Left=21.67 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 817/3393 [06:06<20:45,  2.07batch/s, Batch Loss=0.0265, Avg Loss=0.1274, Time Left=21.66 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 818/3393 [06:06<20:56,  2.05batch/s, Batch Loss=0.0265, Avg Loss=0.1274, Time Left=21.66 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 818/3393 [06:06<20:56,  2.05batch/s, Batch Loss=0.0806, Avg Loss=0.1274, Time Left=21.65 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 819/3393 [06:06<20:53,  2.05batch/s, Batch Loss=0.0806, Avg Loss=0.1274, Time Left=21.65 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 819/3393 [06:07<20:53,  2.05batch/s, Batch Loss=0.0766, Avg Loss=0.1273, Time Left=21.65 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 820/3393 [06:07<21:15,  2.02batch/s, Batch Loss=0.0766, Avg Loss=0.1273, Time Left=21.65 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 820/3393 [06:07<21:15,  2.02batch/s, Batch Loss=0.2353, Avg Loss=0.1274, Time Left=21.64 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 821/3393 [06:07<21:07,  2.03batch/s, Batch Loss=0.2353, Avg Loss=0.1274, Time Left=21.64 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 821/3393 [06:08<21:07,  2.03batch/s, Batch Loss=0.0581, Avg Loss=0.1273, Time Left=21.63 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 822/3393 [06:08<21:11,  2.02batch/s, Batch Loss=0.0581, Avg Loss=0.1273, Time Left=21.63 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 822/3393 [06:08<21:11,  2.02batch/s, Batch Loss=0.0865, Avg Loss=0.1273, Time Left=21.62 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  24%|▏| 823/3393 [06:08<21:03,  2.03batch/s, Batch Loss=0.0865, Avg Loss=0.1273, Time Left=21.62 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 823/3393 [06:09<21:03,  2.03batch/s, Batch Loss=0.0630, Avg Loss=0.1272, Time Left=21.61 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 824/3393 [06:09<20:46,  2.06batch/s, Batch Loss=0.0630, Avg Loss=0.1272, Time Left=21.61 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 824/3393 [06:09<20:46,  2.06batch/s, Batch Loss=0.3429, Avg Loss=0.1275, Time Left=21.61 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 825/3393 [06:09<21:22,  2.00batch/s, Batch Loss=0.3429, Avg Loss=0.1275, Time Left=21.61 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 825/3393 [06:10<21:22,  2.00batch/s, Batch Loss=0.0277, Avg Loss=0.1274, Time Left=21.60 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 826/3393 [06:10<21:07,  2.02batch/s, Batch Loss=0.0277, Avg Loss=0.1274, Time Left=21.60 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 826/3393 [06:10<21:07,  2.02batch/s, Batch Loss=0.0854, Avg Loss=0.1273, Time Left=21.59 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 827/3393 [06:10<21:36,  1.98batch/s, Batch Loss=0.0854, Avg Loss=0.1273, Time Left=21.59 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 827/3393 [06:11<21:36,  1.98batch/s, Batch Loss=0.1349, Avg Loss=0.1273, Time Left=21.58 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 828/3393 [06:11<21:09,  2.02batch/s, Batch Loss=0.1349, Avg Loss=0.1273, Time Left=21.58 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 828/3393 [06:11<21:09,  2.02batch/s, Batch Loss=0.1842, Avg Loss=0.1274, Time Left=21.57 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 829/3393 [06:11<20:47,  2.06batch/s, Batch Loss=0.1842, Avg Loss=0.1274, Time Left=21.57 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 829/3393 [06:12<20:47,  2.06batch/s, Batch Loss=0.0063, Avg Loss=0.1272, Time Left=21.57 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 830/3393 [06:12<21:20,  2.00batch/s, Batch Loss=0.0063, Avg Loss=0.1272, Time Left=21.57 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 830/3393 [06:12<21:20,  2.00batch/s, Batch Loss=0.3637, Avg Loss=0.1275, Time Left=21.56 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 831/3393 [06:12<21:15,  2.01batch/s, Batch Loss=0.3637, Avg Loss=0.1275, Time Left=21.56 \u001b[A\n",
      "Epoch 2/3 - Training:  24%|▏| 831/3393 [06:13<21:15,  2.01batch/s, Batch Loss=0.0996, Avg Loss=0.1275, Time Left=21.55 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 832/3393 [06:13<21:35,  1.98batch/s, Batch Loss=0.0996, Avg Loss=0.1275, Time Left=21.55 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 832/3393 [06:13<21:35,  1.98batch/s, Batch Loss=0.1047, Avg Loss=0.1275, Time Left=21.55 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 833/3393 [06:13<21:20,  2.00batch/s, Batch Loss=0.1047, Avg Loss=0.1275, Time Left=21.55 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 833/3393 [06:14<21:20,  2.00batch/s, Batch Loss=0.1949, Avg Loss=0.1276, Time Left=21.54 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 834/3393 [06:14<21:18,  2.00batch/s, Batch Loss=0.1949, Avg Loss=0.1276, Time Left=21.54 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 834/3393 [06:14<21:18,  2.00batch/s, Batch Loss=0.1451, Avg Loss=0.1276, Time Left=21.53 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 835/3393 [06:14<20:42,  2.06batch/s, Batch Loss=0.1451, Avg Loss=0.1276, Time Left=21.53 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 835/3393 [06:15<20:42,  2.06batch/s, Batch Loss=0.0173, Avg Loss=0.1274, Time Left=21.52 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 836/3393 [06:15<20:38,  2.06batch/s, Batch Loss=0.0173, Avg Loss=0.1274, Time Left=21.52 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 836/3393 [06:15<20:38,  2.06batch/s, Batch Loss=0.0120, Avg Loss=0.1273, Time Left=21.51 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 837/3393 [06:15<20:37,  2.06batch/s, Batch Loss=0.0120, Avg Loss=0.1273, Time Left=21.51 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 837/3393 [06:16<20:37,  2.06batch/s, Batch Loss=0.1028, Avg Loss=0.1273, Time Left=21.50 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 838/3393 [06:16<20:25,  2.09batch/s, Batch Loss=0.1028, Avg Loss=0.1273, Time Left=21.50 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 838/3393 [06:16<20:25,  2.09batch/s, Batch Loss=0.0097, Avg Loss=0.1271, Time Left=21.49 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 839/3393 [06:16<20:26,  2.08batch/s, Batch Loss=0.0097, Avg Loss=0.1271, Time Left=21.49 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 839/3393 [06:17<20:26,  2.08batch/s, Batch Loss=0.1629, Avg Loss=0.1271, Time Left=21.48 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 840/3393 [06:17<20:41,  2.06batch/s, Batch Loss=0.1629, Avg Loss=0.1271, Time Left=21.48 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 840/3393 [06:17<20:41,  2.06batch/s, Batch Loss=0.0633, Avg Loss=0.1271, Time Left=21.47 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 841/3393 [06:17<20:14,  2.10batch/s, Batch Loss=0.0633, Avg Loss=0.1271, Time Left=21.47 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 841/3393 [06:18<20:14,  2.10batch/s, Batch Loss=0.4692, Avg Loss=0.1275, Time Left=21.46 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 842/3393 [06:18<20:19,  2.09batch/s, Batch Loss=0.4692, Avg Loss=0.1275, Time Left=21.46 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 842/3393 [06:18<20:19,  2.09batch/s, Batch Loss=0.0400, Avg Loss=0.1274, Time Left=21.46 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 843/3393 [06:18<20:59,  2.03batch/s, Batch Loss=0.0400, Avg Loss=0.1274, Time Left=21.46 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 843/3393 [06:19<20:59,  2.03batch/s, Batch Loss=0.0501, Avg Loss=0.1273, Time Left=21.45 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 844/3393 [06:19<20:52,  2.04batch/s, Batch Loss=0.0501, Avg Loss=0.1273, Time Left=21.45 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 844/3393 [06:19<20:52,  2.04batch/s, Batch Loss=0.0480, Avg Loss=0.1272, Time Left=21.44 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 845/3393 [06:19<20:58,  2.02batch/s, Batch Loss=0.0480, Avg Loss=0.1272, Time Left=21.44 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 845/3393 [06:20<20:58,  2.02batch/s, Batch Loss=0.0384, Avg Loss=0.1271, Time Left=21.43 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 846/3393 [06:20<20:26,  2.08batch/s, Batch Loss=0.0384, Avg Loss=0.1271, Time Left=21.43 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 846/3393 [06:20<20:26,  2.08batch/s, Batch Loss=0.2638, Avg Loss=0.1273, Time Left=21.42 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 847/3393 [06:20<20:26,  2.08batch/s, Batch Loss=0.2638, Avg Loss=0.1273, Time Left=21.42 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 847/3393 [06:21<20:26,  2.08batch/s, Batch Loss=0.0771, Avg Loss=0.1272, Time Left=21.42 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 848/3393 [06:21<20:52,  2.03batch/s, Batch Loss=0.0771, Avg Loss=0.1272, Time Left=21.42 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▏| 848/3393 [06:21<20:52,  2.03batch/s, Batch Loss=0.0405, Avg Loss=0.1271, Time Left=21.41 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 849/3393 [06:21<20:46,  2.04batch/s, Batch Loss=0.0405, Avg Loss=0.1271, Time Left=21.41 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 849/3393 [06:22<20:46,  2.04batch/s, Batch Loss=0.0421, Avg Loss=0.1270, Time Left=21.40 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 850/3393 [06:22<21:16,  1.99batch/s, Batch Loss=0.0421, Avg Loss=0.1270, Time Left=21.40 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 850/3393 [06:22<21:16,  1.99batch/s, Batch Loss=0.1029, Avg Loss=0.1269, Time Left=21.39 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 851/3393 [06:22<21:04,  2.01batch/s, Batch Loss=0.1029, Avg Loss=0.1269, Time Left=21.39 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 851/3393 [06:23<21:04,  2.01batch/s, Batch Loss=0.1219, Avg Loss=0.1269, Time Left=21.39 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 852/3393 [06:23<21:05,  2.01batch/s, Batch Loss=0.1219, Avg Loss=0.1269, Time Left=21.39 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 852/3393 [06:23<21:05,  2.01batch/s, Batch Loss=0.1338, Avg Loss=0.1269, Time Left=21.38 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 853/3393 [06:23<20:55,  2.02batch/s, Batch Loss=0.1338, Avg Loss=0.1269, Time Left=21.38 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 853/3393 [06:24<20:55,  2.02batch/s, Batch Loss=0.0755, Avg Loss=0.1269, Time Left=21.37 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 854/3393 [06:24<20:45,  2.04batch/s, Batch Loss=0.0755, Avg Loss=0.1269, Time Left=21.37 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 854/3393 [06:24<20:45,  2.04batch/s, Batch Loss=0.2996, Avg Loss=0.1271, Time Left=21.36 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 855/3393 [06:24<20:51,  2.03batch/s, Batch Loss=0.2996, Avg Loss=0.1271, Time Left=21.36 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 855/3393 [06:24<20:51,  2.03batch/s, Batch Loss=0.0982, Avg Loss=0.1271, Time Left=21.35 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  25%|▎| 856/3393 [06:25<20:45,  2.04batch/s, Batch Loss=0.0982, Avg Loss=0.1271, Time Left=21.35 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 856/3393 [06:25<20:45,  2.04batch/s, Batch Loss=0.1698, Avg Loss=0.1271, Time Left=21.35 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 857/3393 [06:25<20:51,  2.03batch/s, Batch Loss=0.1698, Avg Loss=0.1271, Time Left=21.35 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 857/3393 [06:25<20:51,  2.03batch/s, Batch Loss=0.3366, Avg Loss=0.1274, Time Left=21.34 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 858/3393 [06:25<20:33,  2.06batch/s, Batch Loss=0.3366, Avg Loss=0.1274, Time Left=21.34 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 858/3393 [06:26<20:33,  2.06batch/s, Batch Loss=0.0843, Avg Loss=0.1273, Time Left=21.33 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 859/3393 [06:26<20:40,  2.04batch/s, Batch Loss=0.0843, Avg Loss=0.1273, Time Left=21.33 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 859/3393 [06:26<20:40,  2.04batch/s, Batch Loss=0.0901, Avg Loss=0.1273, Time Left=21.32 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 860/3393 [06:26<20:37,  2.05batch/s, Batch Loss=0.0901, Avg Loss=0.1273, Time Left=21.32 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 860/3393 [06:27<20:37,  2.05batch/s, Batch Loss=0.0610, Avg Loss=0.1272, Time Left=21.31 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 861/3393 [06:27<20:20,  2.07batch/s, Batch Loss=0.0610, Avg Loss=0.1272, Time Left=21.31 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 861/3393 [06:27<20:20,  2.07batch/s, Batch Loss=0.0832, Avg Loss=0.1271, Time Left=21.30 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 862/3393 [06:27<20:08,  2.09batch/s, Batch Loss=0.0832, Avg Loss=0.1271, Time Left=21.30 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 862/3393 [06:28<20:08,  2.09batch/s, Batch Loss=0.0539, Avg Loss=0.1270, Time Left=21.29 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 863/3393 [06:28<20:35,  2.05batch/s, Batch Loss=0.0539, Avg Loss=0.1270, Time Left=21.29 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 863/3393 [06:28<20:35,  2.05batch/s, Batch Loss=0.1239, Avg Loss=0.1270, Time Left=21.29 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 864/3393 [06:28<20:45,  2.03batch/s, Batch Loss=0.1239, Avg Loss=0.1270, Time Left=21.29 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 864/3393 [06:29<20:45,  2.03batch/s, Batch Loss=0.0078, Avg Loss=0.1269, Time Left=21.28 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 865/3393 [06:29<20:26,  2.06batch/s, Batch Loss=0.0078, Avg Loss=0.1269, Time Left=21.28 \u001b[A\n",
      "Epoch 2/3 - Training:  25%|▎| 865/3393 [06:29<20:26,  2.06batch/s, Batch Loss=0.0808, Avg Loss=0.1268, Time Left=21.27 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 866/3393 [06:29<20:25,  2.06batch/s, Batch Loss=0.0808, Avg Loss=0.1268, Time Left=21.27 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 866/3393 [06:30<20:25,  2.06batch/s, Batch Loss=0.0712, Avg Loss=0.1268, Time Left=21.26 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 867/3393 [06:30<20:12,  2.08batch/s, Batch Loss=0.0712, Avg Loss=0.1268, Time Left=21.26 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 867/3393 [06:30<20:12,  2.08batch/s, Batch Loss=0.2769, Avg Loss=0.1270, Time Left=21.25 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 868/3393 [06:30<20:13,  2.08batch/s, Batch Loss=0.2769, Avg Loss=0.1270, Time Left=21.25 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 868/3393 [06:31<20:13,  2.08batch/s, Batch Loss=0.2240, Avg Loss=0.1271, Time Left=21.24 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 869/3393 [06:31<20:15,  2.08batch/s, Batch Loss=0.2240, Avg Loss=0.1271, Time Left=21.24 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 869/3393 [06:31<20:15,  2.08batch/s, Batch Loss=0.0572, Avg Loss=0.1270, Time Left=21.23 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 870/3393 [06:31<20:16,  2.07batch/s, Batch Loss=0.0572, Avg Loss=0.1270, Time Left=21.23 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 870/3393 [06:32<20:16,  2.07batch/s, Batch Loss=0.1200, Avg Loss=0.1270, Time Left=21.23 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 871/3393 [06:32<20:52,  2.01batch/s, Batch Loss=0.1200, Avg Loss=0.1270, Time Left=21.23 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 871/3393 [06:32<20:52,  2.01batch/s, Batch Loss=0.0620, Avg Loss=0.1269, Time Left=21.22 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 872/3393 [06:32<20:43,  2.03batch/s, Batch Loss=0.0620, Avg Loss=0.1269, Time Left=21.22 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 872/3393 [06:33<20:43,  2.03batch/s, Batch Loss=0.1149, Avg Loss=0.1269, Time Left=21.21 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 873/3393 [06:33<20:24,  2.06batch/s, Batch Loss=0.1149, Avg Loss=0.1269, Time Left=21.21 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 873/3393 [06:33<20:24,  2.06batch/s, Batch Loss=0.0876, Avg Loss=0.1268, Time Left=21.20 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 874/3393 [06:33<20:47,  2.02batch/s, Batch Loss=0.0876, Avg Loss=0.1268, Time Left=21.20 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 874/3393 [06:34<20:47,  2.02batch/s, Batch Loss=0.0618, Avg Loss=0.1268, Time Left=21.19 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 875/3393 [06:34<20:25,  2.05batch/s, Batch Loss=0.0618, Avg Loss=0.1268, Time Left=21.19 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 875/3393 [06:34<20:25,  2.05batch/s, Batch Loss=0.3659, Avg Loss=0.1271, Time Left=21.19 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 876/3393 [06:34<20:46,  2.02batch/s, Batch Loss=0.3659, Avg Loss=0.1271, Time Left=21.19 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 876/3393 [06:35<20:46,  2.02batch/s, Batch Loss=0.0395, Avg Loss=0.1269, Time Left=21.18 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 877/3393 [06:35<20:38,  2.03batch/s, Batch Loss=0.0395, Avg Loss=0.1269, Time Left=21.18 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 877/3393 [06:35<20:38,  2.03batch/s, Batch Loss=0.1370, Avg Loss=0.1270, Time Left=21.17 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 878/3393 [06:35<20:31,  2.04batch/s, Batch Loss=0.1370, Avg Loss=0.1270, Time Left=21.17 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 878/3393 [06:36<20:31,  2.04batch/s, Batch Loss=0.0463, Avg Loss=0.1269, Time Left=21.16 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 879/3393 [06:36<20:37,  2.03batch/s, Batch Loss=0.0463, Avg Loss=0.1269, Time Left=21.16 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 879/3393 [06:36<20:37,  2.03batch/s, Batch Loss=0.1109, Avg Loss=0.1268, Time Left=21.15 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 880/3393 [06:36<20:33,  2.04batch/s, Batch Loss=0.1109, Avg Loss=0.1268, Time Left=21.15 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 880/3393 [06:37<20:33,  2.04batch/s, Batch Loss=0.0194, Avg Loss=0.1267, Time Left=21.14 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 881/3393 [06:37<20:38,  2.03batch/s, Batch Loss=0.0194, Avg Loss=0.1267, Time Left=21.14 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 881/3393 [06:37<20:38,  2.03batch/s, Batch Loss=0.0146, Avg Loss=0.1266, Time Left=21.14 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 882/3393 [06:37<20:31,  2.04batch/s, Batch Loss=0.0146, Avg Loss=0.1266, Time Left=21.14 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 882/3393 [06:38<20:31,  2.04batch/s, Batch Loss=0.0063, Avg Loss=0.1264, Time Left=21.13 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 883/3393 [06:38<20:26,  2.05batch/s, Batch Loss=0.0063, Avg Loss=0.1264, Time Left=21.13 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 883/3393 [06:38<20:26,  2.05batch/s, Batch Loss=0.0426, Avg Loss=0.1263, Time Left=21.12 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 884/3393 [06:38<20:10,  2.07batch/s, Batch Loss=0.0426, Avg Loss=0.1263, Time Left=21.12 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 884/3393 [06:39<20:10,  2.07batch/s, Batch Loss=0.1886, Avg Loss=0.1264, Time Left=21.11 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 885/3393 [06:39<19:58,  2.09batch/s, Batch Loss=0.1886, Avg Loss=0.1264, Time Left=21.11 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 885/3393 [06:39<19:58,  2.09batch/s, Batch Loss=0.1319, Avg Loss=0.1264, Time Left=21.10 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 886/3393 [06:39<20:01,  2.09batch/s, Batch Loss=0.1319, Avg Loss=0.1264, Time Left=21.10 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 886/3393 [06:40<20:01,  2.09batch/s, Batch Loss=0.3300, Avg Loss=0.1266, Time Left=21.09 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 887/3393 [06:40<20:38,  2.02batch/s, Batch Loss=0.3300, Avg Loss=0.1266, Time Left=21.09 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 887/3393 [06:40<20:38,  2.02batch/s, Batch Loss=0.1067, Avg Loss=0.1266, Time Left=21.09 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 888/3393 [06:40<20:39,  2.02batch/s, Batch Loss=0.1067, Avg Loss=0.1266, Time Left=21.09 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 888/3393 [06:41<20:39,  2.02batch/s, Batch Loss=0.1376, Avg Loss=0.1266, Time Left=21.08 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  26%|▎| 889/3393 [06:41<21:00,  1.99batch/s, Batch Loss=0.1376, Avg Loss=0.1266, Time Left=21.08 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 889/3393 [06:41<21:00,  1.99batch/s, Batch Loss=0.2143, Avg Loss=0.1267, Time Left=21.07 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 890/3393 [06:41<20:46,  2.01batch/s, Batch Loss=0.2143, Avg Loss=0.1267, Time Left=21.07 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 890/3393 [06:42<20:46,  2.01batch/s, Batch Loss=0.0732, Avg Loss=0.1267, Time Left=21.06 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 891/3393 [06:42<20:52,  2.00batch/s, Batch Loss=0.0732, Avg Loss=0.1267, Time Left=21.06 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 891/3393 [06:42<20:52,  2.00batch/s, Batch Loss=0.1588, Avg Loss=0.1267, Time Left=21.05 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 892/3393 [06:42<20:11,  2.06batch/s, Batch Loss=0.1588, Avg Loss=0.1267, Time Left=21.05 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 892/3393 [06:43<20:11,  2.06batch/s, Batch Loss=0.3260, Avg Loss=0.1270, Time Left=21.04 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 893/3393 [06:43<20:09,  2.07batch/s, Batch Loss=0.3260, Avg Loss=0.1270, Time Left=21.04 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 893/3393 [06:43<20:09,  2.07batch/s, Batch Loss=0.0726, Avg Loss=0.1269, Time Left=21.04 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 894/3393 [06:43<20:43,  2.01batch/s, Batch Loss=0.0726, Avg Loss=0.1269, Time Left=21.04 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 894/3393 [06:44<20:43,  2.01batch/s, Batch Loss=0.0734, Avg Loss=0.1268, Time Left=21.03 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 895/3393 [06:44<20:35,  2.02batch/s, Batch Loss=0.0734, Avg Loss=0.1268, Time Left=21.03 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 895/3393 [06:44<20:35,  2.02batch/s, Batch Loss=0.2212, Avg Loss=0.1269, Time Left=21.02 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 896/3393 [06:44<20:50,  2.00batch/s, Batch Loss=0.2212, Avg Loss=0.1269, Time Left=21.02 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 896/3393 [06:45<20:50,  2.00batch/s, Batch Loss=0.0608, Avg Loss=0.1269, Time Left=21.01 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 897/3393 [06:45<20:37,  2.02batch/s, Batch Loss=0.0608, Avg Loss=0.1269, Time Left=21.01 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 897/3393 [06:45<20:37,  2.02batch/s, Batch Loss=0.2814, Avg Loss=0.1270, Time Left=21.01 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 898/3393 [06:45<20:16,  2.05batch/s, Batch Loss=0.2814, Avg Loss=0.1270, Time Left=21.01 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 898/3393 [06:46<20:16,  2.05batch/s, Batch Loss=0.3003, Avg Loss=0.1273, Time Left=21.00 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 899/3393 [06:46<20:14,  2.05batch/s, Batch Loss=0.3003, Avg Loss=0.1273, Time Left=21.00 \u001b[A\n",
      "Epoch 2/3 - Training:  26%|▎| 899/3393 [06:46<20:14,  2.05batch/s, Batch Loss=0.0088, Avg Loss=0.1271, Time Left=20.99 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 900/3393 [06:46<19:58,  2.08batch/s, Batch Loss=0.0088, Avg Loss=0.1271, Time Left=20.99 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 900/3393 [06:46<19:58,  2.08batch/s, Batch Loss=0.0461, Avg Loss=0.1270, Time Left=20.98 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 901/3393 [06:47<20:11,  2.06batch/s, Batch Loss=0.0461, Avg Loss=0.1270, Time Left=20.98 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 901/3393 [06:47<20:11,  2.06batch/s, Batch Loss=0.1213, Avg Loss=0.1270, Time Left=20.97 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 902/3393 [06:47<20:09,  2.06batch/s, Batch Loss=0.1213, Avg Loss=0.1270, Time Left=20.97 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 902/3393 [06:47<20:09,  2.06batch/s, Batch Loss=0.1632, Avg Loss=0.1271, Time Left=20.96 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 903/3393 [06:47<19:55,  2.08batch/s, Batch Loss=0.1632, Avg Loss=0.1271, Time Left=20.96 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 903/3393 [06:48<19:55,  2.08batch/s, Batch Loss=0.0384, Avg Loss=0.1269, Time Left=20.96 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 904/3393 [06:48<20:24,  2.03batch/s, Batch Loss=0.0384, Avg Loss=0.1269, Time Left=20.96 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 904/3393 [06:48<20:24,  2.03batch/s, Batch Loss=0.1995, Avg Loss=0.1270, Time Left=20.95 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 905/3393 [06:48<20:27,  2.03batch/s, Batch Loss=0.1995, Avg Loss=0.1270, Time Left=20.95 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 905/3393 [06:49<20:27,  2.03batch/s, Batch Loss=0.1808, Avg Loss=0.1271, Time Left=20.94 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 906/3393 [06:49<20:07,  2.06batch/s, Batch Loss=0.1808, Avg Loss=0.1271, Time Left=20.94 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 906/3393 [06:49<20:07,  2.06batch/s, Batch Loss=0.0443, Avg Loss=0.1270, Time Left=20.93 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 907/3393 [06:49<20:29,  2.02batch/s, Batch Loss=0.0443, Avg Loss=0.1270, Time Left=20.93 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 907/3393 [06:50<20:29,  2.02batch/s, Batch Loss=0.0084, Avg Loss=0.1269, Time Left=20.92 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 908/3393 [06:50<20:21,  2.03batch/s, Batch Loss=0.0084, Avg Loss=0.1269, Time Left=20.92 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 908/3393 [06:50<20:21,  2.03batch/s, Batch Loss=0.1525, Avg Loss=0.1269, Time Left=20.91 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 909/3393 [06:50<20:26,  2.02batch/s, Batch Loss=0.1525, Avg Loss=0.1269, Time Left=20.91 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 909/3393 [06:51<20:26,  2.02batch/s, Batch Loss=0.0373, Avg Loss=0.1268, Time Left=20.91 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 910/3393 [06:51<20:20,  2.03batch/s, Batch Loss=0.0373, Avg Loss=0.1268, Time Left=20.91 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 910/3393 [06:51<20:20,  2.03batch/s, Batch Loss=0.0582, Avg Loss=0.1267, Time Left=20.90 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 911/3393 [06:51<20:13,  2.04batch/s, Batch Loss=0.0582, Avg Loss=0.1267, Time Left=20.90 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 911/3393 [06:52<20:13,  2.04batch/s, Batch Loss=0.0918, Avg Loss=0.1267, Time Left=20.89 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 912/3393 [06:52<19:56,  2.07batch/s, Batch Loss=0.0918, Avg Loss=0.1267, Time Left=20.89 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 912/3393 [06:52<19:56,  2.07batch/s, Batch Loss=0.0866, Avg Loss=0.1266, Time Left=20.88 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 913/3393 [06:52<19:45,  2.09batch/s, Batch Loss=0.0866, Avg Loss=0.1266, Time Left=20.88 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 913/3393 [06:53<19:45,  2.09batch/s, Batch Loss=0.1503, Avg Loss=0.1266, Time Left=20.87 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 914/3393 [06:53<20:13,  2.04batch/s, Batch Loss=0.1503, Avg Loss=0.1266, Time Left=20.87 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 914/3393 [06:53<20:13,  2.04batch/s, Batch Loss=0.0650, Avg Loss=0.1266, Time Left=20.86 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 915/3393 [06:53<20:09,  2.05batch/s, Batch Loss=0.0650, Avg Loss=0.1266, Time Left=20.86 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 915/3393 [06:54<20:09,  2.05batch/s, Batch Loss=0.1427, Avg Loss=0.1266, Time Left=20.85 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 916/3393 [06:54<20:04,  2.06batch/s, Batch Loss=0.1427, Avg Loss=0.1266, Time Left=20.85 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 916/3393 [06:54<20:04,  2.06batch/s, Batch Loss=0.0921, Avg Loss=0.1265, Time Left=20.85 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 917/3393 [06:54<20:13,  2.04batch/s, Batch Loss=0.0921, Avg Loss=0.1265, Time Left=20.85 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 917/3393 [06:55<20:13,  2.04batch/s, Batch Loss=0.0451, Avg Loss=0.1264, Time Left=20.84 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 918/3393 [06:55<19:45,  2.09batch/s, Batch Loss=0.0451, Avg Loss=0.1264, Time Left=20.84 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 918/3393 [06:55<19:45,  2.09batch/s, Batch Loss=0.0726, Avg Loss=0.1264, Time Left=20.83 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 919/3393 [06:55<19:47,  2.08batch/s, Batch Loss=0.0726, Avg Loss=0.1264, Time Left=20.83 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 919/3393 [06:56<19:47,  2.08batch/s, Batch Loss=0.0086, Avg Loss=0.1262, Time Left=20.82 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 920/3393 [06:56<20:01,  2.06batch/s, Batch Loss=0.0086, Avg Loss=0.1262, Time Left=20.82 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 920/3393 [06:56<20:01,  2.06batch/s, Batch Loss=0.0980, Avg Loss=0.1262, Time Left=20.81 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 921/3393 [06:56<20:00,  2.06batch/s, Batch Loss=0.0980, Avg Loss=0.1262, Time Left=20.81 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 921/3393 [06:57<20:00,  2.06batch/s, Batch Loss=0.3592, Avg Loss=0.1265, Time Left=20.80 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  27%|▎| 922/3393 [06:57<19:58,  2.06batch/s, Batch Loss=0.3592, Avg Loss=0.1265, Time Left=20.80 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 922/3393 [06:57<19:58,  2.06batch/s, Batch Loss=0.2426, Avg Loss=0.1266, Time Left=20.79 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 923/3393 [06:57<19:58,  2.06batch/s, Batch Loss=0.2426, Avg Loss=0.1266, Time Left=20.79 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 923/3393 [06:58<19:58,  2.06batch/s, Batch Loss=0.0844, Avg Loss=0.1266, Time Left=20.79 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 924/3393 [06:58<19:43,  2.09batch/s, Batch Loss=0.0844, Avg Loss=0.1266, Time Left=20.79 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 924/3393 [06:58<19:43,  2.09batch/s, Batch Loss=0.0508, Avg Loss=0.1265, Time Left=20.78 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 925/3393 [06:58<19:46,  2.08batch/s, Batch Loss=0.0508, Avg Loss=0.1265, Time Left=20.78 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 925/3393 [06:59<19:46,  2.08batch/s, Batch Loss=0.0140, Avg Loss=0.1264, Time Left=20.77 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 926/3393 [06:59<19:47,  2.08batch/s, Batch Loss=0.0140, Avg Loss=0.1264, Time Left=20.77 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 926/3393 [06:59<19:47,  2.08batch/s, Batch Loss=0.0615, Avg Loss=0.1263, Time Left=20.76 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 927/3393 [06:59<19:49,  2.07batch/s, Batch Loss=0.0615, Avg Loss=0.1263, Time Left=20.76 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 927/3393 [07:00<19:49,  2.07batch/s, Batch Loss=0.0790, Avg Loss=0.1262, Time Left=20.75 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 928/3393 [07:00<20:24,  2.01batch/s, Batch Loss=0.0790, Avg Loss=0.1262, Time Left=20.75 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 928/3393 [07:00<20:24,  2.01batch/s, Batch Loss=0.1245, Avg Loss=0.1262, Time Left=20.74 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 929/3393 [07:00<19:52,  2.07batch/s, Batch Loss=0.1245, Avg Loss=0.1262, Time Left=20.74 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 929/3393 [07:01<19:52,  2.07batch/s, Batch Loss=0.0275, Avg Loss=0.1261, Time Left=20.74 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 930/3393 [07:01<20:02,  2.05batch/s, Batch Loss=0.0275, Avg Loss=0.1261, Time Left=20.74 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 930/3393 [07:01<20:02,  2.05batch/s, Batch Loss=0.0836, Avg Loss=0.1261, Time Left=20.73 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 931/3393 [07:01<20:01,  2.05batch/s, Batch Loss=0.0836, Avg Loss=0.1261, Time Left=20.73 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 931/3393 [07:02<20:01,  2.05batch/s, Batch Loss=0.0405, Avg Loss=0.1260, Time Left=20.72 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 932/3393 [07:02<20:07,  2.04batch/s, Batch Loss=0.0405, Avg Loss=0.1260, Time Left=20.72 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 932/3393 [07:02<20:07,  2.04batch/s, Batch Loss=0.1206, Avg Loss=0.1259, Time Left=20.71 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 933/3393 [07:02<20:13,  2.03batch/s, Batch Loss=0.1206, Avg Loss=0.1259, Time Left=20.71 \u001b[A\n",
      "Epoch 2/3 - Training:  27%|▎| 933/3393 [07:03<20:13,  2.03batch/s, Batch Loss=0.0973, Avg Loss=0.1259, Time Left=20.70 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 934/3393 [07:03<20:18,  2.02batch/s, Batch Loss=0.0973, Avg Loss=0.1259, Time Left=20.70 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 934/3393 [07:03<20:18,  2.02batch/s, Batch Loss=0.0656, Avg Loss=0.1258, Time Left=20.70 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 935/3393 [07:03<19:58,  2.05batch/s, Batch Loss=0.0656, Avg Loss=0.1258, Time Left=20.70 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 935/3393 [07:04<19:58,  2.05batch/s, Batch Loss=0.0227, Avg Loss=0.1257, Time Left=20.69 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 936/3393 [07:04<20:06,  2.04batch/s, Batch Loss=0.0227, Avg Loss=0.1257, Time Left=20.69 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 936/3393 [07:04<20:06,  2.04batch/s, Batch Loss=0.0276, Avg Loss=0.1256, Time Left=20.68 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 937/3393 [07:04<20:02,  2.04batch/s, Batch Loss=0.0276, Avg Loss=0.1256, Time Left=20.68 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 937/3393 [07:05<20:02,  2.04batch/s, Batch Loss=0.0902, Avg Loss=0.1256, Time Left=20.67 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 938/3393 [07:05<20:20,  2.01batch/s, Batch Loss=0.0902, Avg Loss=0.1256, Time Left=20.67 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 938/3393 [07:05<20:20,  2.01batch/s, Batch Loss=0.0035, Avg Loss=0.1254, Time Left=20.66 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 939/3393 [07:05<20:22,  2.01batch/s, Batch Loss=0.0035, Avg Loss=0.1254, Time Left=20.66 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 939/3393 [07:06<20:22,  2.01batch/s, Batch Loss=0.0595, Avg Loss=0.1254, Time Left=20.66 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 940/3393 [07:06<20:35,  1.99batch/s, Batch Loss=0.0595, Avg Loss=0.1254, Time Left=20.66 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 940/3393 [07:06<20:35,  1.99batch/s, Batch Loss=0.0064, Avg Loss=0.1252, Time Left=20.65 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 941/3393 [07:06<20:32,  1.99batch/s, Batch Loss=0.0064, Avg Loss=0.1252, Time Left=20.65 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 941/3393 [07:07<20:32,  1.99batch/s, Batch Loss=0.0804, Avg Loss=0.1252, Time Left=20.64 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 942/3393 [07:07<20:06,  2.03batch/s, Batch Loss=0.0804, Avg Loss=0.1252, Time Left=20.64 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 942/3393 [07:07<20:06,  2.03batch/s, Batch Loss=0.1040, Avg Loss=0.1251, Time Left=20.63 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 943/3393 [07:07<20:16,  2.01batch/s, Batch Loss=0.1040, Avg Loss=0.1251, Time Left=20.63 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 943/3393 [07:08<20:16,  2.01batch/s, Batch Loss=0.1656, Avg Loss=0.1252, Time Left=20.62 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 944/3393 [07:08<20:02,  2.04batch/s, Batch Loss=0.1656, Avg Loss=0.1252, Time Left=20.62 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 944/3393 [07:08<20:02,  2.04batch/s, Batch Loss=0.5485, Avg Loss=0.1257, Time Left=20.62 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 945/3393 [07:08<20:18,  2.01batch/s, Batch Loss=0.5485, Avg Loss=0.1257, Time Left=20.62 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 945/3393 [07:09<20:18,  2.01batch/s, Batch Loss=0.0388, Avg Loss=0.1256, Time Left=20.61 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 946/3393 [07:09<20:09,  2.02batch/s, Batch Loss=0.0388, Avg Loss=0.1256, Time Left=20.61 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 946/3393 [07:09<20:09,  2.02batch/s, Batch Loss=0.1573, Avg Loss=0.1256, Time Left=20.60 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 947/3393 [07:09<19:51,  2.05batch/s, Batch Loss=0.1573, Avg Loss=0.1256, Time Left=20.60 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 947/3393 [07:09<19:51,  2.05batch/s, Batch Loss=0.0634, Avg Loss=0.1255, Time Left=20.59 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 948/3393 [07:09<19:58,  2.04batch/s, Batch Loss=0.0634, Avg Loss=0.1255, Time Left=20.59 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 948/3393 [07:10<19:58,  2.04batch/s, Batch Loss=0.0045, Avg Loss=0.1254, Time Left=20.58 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 949/3393 [07:10<19:53,  2.05batch/s, Batch Loss=0.0045, Avg Loss=0.1254, Time Left=20.58 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 949/3393 [07:10<19:53,  2.05batch/s, Batch Loss=0.0451, Avg Loss=0.1253, Time Left=20.58 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 950/3393 [07:11<20:25,  1.99batch/s, Batch Loss=0.0451, Avg Loss=0.1253, Time Left=20.58 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 950/3393 [07:11<20:25,  1.99batch/s, Batch Loss=0.0496, Avg Loss=0.1252, Time Left=20.57 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 951/3393 [07:11<20:08,  2.02batch/s, Batch Loss=0.0496, Avg Loss=0.1252, Time Left=20.57 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 951/3393 [07:11<20:08,  2.02batch/s, Batch Loss=0.0588, Avg Loss=0.1251, Time Left=20.56 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 952/3393 [07:11<19:52,  2.05batch/s, Batch Loss=0.0588, Avg Loss=0.1251, Time Left=20.56 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 952/3393 [07:12<19:52,  2.05batch/s, Batch Loss=0.0322, Avg Loss=0.1250, Time Left=20.55 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 953/3393 [07:12<19:49,  2.05batch/s, Batch Loss=0.0322, Avg Loss=0.1250, Time Left=20.55 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 953/3393 [07:12<19:49,  2.05batch/s, Batch Loss=0.0234, Avg Loss=0.1249, Time Left=20.54 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 954/3393 [07:12<19:21,  2.10batch/s, Batch Loss=0.0234, Avg Loss=0.1249, Time Left=20.54 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 954/3393 [07:13<19:21,  2.10batch/s, Batch Loss=0.1272, Avg Loss=0.1249, Time Left=20.53 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  28%|▎| 955/3393 [07:13<19:25,  2.09batch/s, Batch Loss=0.1272, Avg Loss=0.1249, Time Left=20.53 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 955/3393 [07:13<19:25,  2.09batch/s, Batch Loss=0.0199, Avg Loss=0.1248, Time Left=20.52 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 956/3393 [07:13<19:40,  2.06batch/s, Batch Loss=0.0199, Avg Loss=0.1248, Time Left=20.52 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 956/3393 [07:14<19:40,  2.06batch/s, Batch Loss=0.5064, Avg Loss=0.1252, Time Left=20.51 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 957/3393 [07:14<19:16,  2.11batch/s, Batch Loss=0.5064, Avg Loss=0.1252, Time Left=20.51 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 957/3393 [07:14<19:16,  2.11batch/s, Batch Loss=0.0657, Avg Loss=0.1252, Time Left=20.51 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 958/3393 [07:14<19:21,  2.10batch/s, Batch Loss=0.0657, Avg Loss=0.1252, Time Left=20.51 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 958/3393 [07:15<19:21,  2.10batch/s, Batch Loss=0.0040, Avg Loss=0.1250, Time Left=20.50 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 959/3393 [07:15<20:01,  2.03batch/s, Batch Loss=0.0040, Avg Loss=0.1250, Time Left=20.50 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 959/3393 [07:15<20:01,  2.03batch/s, Batch Loss=0.0445, Avg Loss=0.1249, Time Left=20.49 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 960/3393 [07:15<19:42,  2.06batch/s, Batch Loss=0.0445, Avg Loss=0.1249, Time Left=20.49 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 960/3393 [07:16<19:42,  2.06batch/s, Batch Loss=0.0760, Avg Loss=0.1249, Time Left=20.48 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 961/3393 [07:16<20:04,  2.02batch/s, Batch Loss=0.0760, Avg Loss=0.1249, Time Left=20.48 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 961/3393 [07:16<20:04,  2.02batch/s, Batch Loss=0.1624, Avg Loss=0.1249, Time Left=20.47 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 962/3393 [07:16<19:56,  2.03batch/s, Batch Loss=0.1624, Avg Loss=0.1249, Time Left=20.47 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 962/3393 [07:17<19:56,  2.03batch/s, Batch Loss=0.1531, Avg Loss=0.1250, Time Left=20.47 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 963/3393 [07:17<20:24,  1.98batch/s, Batch Loss=0.1531, Avg Loss=0.1250, Time Left=20.47 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 963/3393 [07:17<20:24,  1.98batch/s, Batch Loss=0.1836, Avg Loss=0.1250, Time Left=20.46 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 964/3393 [07:17<20:09,  2.01batch/s, Batch Loss=0.1836, Avg Loss=0.1250, Time Left=20.46 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 964/3393 [07:18<20:09,  2.01batch/s, Batch Loss=0.0036, Avg Loss=0.1249, Time Left=20.45 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 965/3393 [07:18<20:34,  1.97batch/s, Batch Loss=0.0036, Avg Loss=0.1249, Time Left=20.45 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 965/3393 [07:18<20:34,  1.97batch/s, Batch Loss=0.0044, Avg Loss=0.1248, Time Left=20.44 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 966/3393 [07:18<20:16,  1.99batch/s, Batch Loss=0.0044, Avg Loss=0.1248, Time Left=20.44 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 966/3393 [07:19<20:16,  1.99batch/s, Batch Loss=0.0736, Avg Loss=0.1247, Time Left=20.44 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 967/3393 [07:19<19:52,  2.04batch/s, Batch Loss=0.0736, Avg Loss=0.1247, Time Left=20.44 \u001b[A\n",
      "Epoch 2/3 - Training:  28%|▎| 967/3393 [07:19<19:52,  2.04batch/s, Batch Loss=0.0386, Avg Loss=0.1246, Time Left=20.43 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 968/3393 [07:19<19:46,  2.04batch/s, Batch Loss=0.0386, Avg Loss=0.1246, Time Left=20.43 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 968/3393 [07:20<19:46,  2.04batch/s, Batch Loss=0.0543, Avg Loss=0.1245, Time Left=20.42 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 969/3393 [07:20<19:41,  2.05batch/s, Batch Loss=0.0543, Avg Loss=0.1245, Time Left=20.42 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 969/3393 [07:20<19:41,  2.05batch/s, Batch Loss=0.4163, Avg Loss=0.1249, Time Left=20.41 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 970/3393 [07:20<20:13,  2.00batch/s, Batch Loss=0.4163, Avg Loss=0.1249, Time Left=20.41 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 970/3393 [07:21<20:13,  2.00batch/s, Batch Loss=0.0343, Avg Loss=0.1248, Time Left=20.40 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 971/3393 [07:21<20:01,  2.02batch/s, Batch Loss=0.0343, Avg Loss=0.1248, Time Left=20.40 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 971/3393 [07:21<20:01,  2.02batch/s, Batch Loss=0.0094, Avg Loss=0.1246, Time Left=20.40 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 972/3393 [07:21<20:03,  2.01batch/s, Batch Loss=0.0094, Avg Loss=0.1246, Time Left=20.40 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 972/3393 [07:22<20:03,  2.01batch/s, Batch Loss=0.0203, Avg Loss=0.1245, Time Left=20.39 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 973/3393 [07:22<19:54,  2.03batch/s, Batch Loss=0.0203, Avg Loss=0.1245, Time Left=20.39 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 973/3393 [07:22<19:54,  2.03batch/s, Batch Loss=0.0102, Avg Loss=0.1244, Time Left=20.38 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 974/3393 [07:22<19:46,  2.04batch/s, Batch Loss=0.0102, Avg Loss=0.1244, Time Left=20.38 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 974/3393 [07:23<19:46,  2.04batch/s, Batch Loss=0.0465, Avg Loss=0.1243, Time Left=20.37 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 975/3393 [07:23<20:03,  2.01batch/s, Batch Loss=0.0465, Avg Loss=0.1243, Time Left=20.37 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 975/3393 [07:23<20:03,  2.01batch/s, Batch Loss=0.1119, Avg Loss=0.1243, Time Left=20.36 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 976/3393 [07:23<19:55,  2.02batch/s, Batch Loss=0.1119, Avg Loss=0.1243, Time Left=20.36 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 976/3393 [07:24<19:55,  2.02batch/s, Batch Loss=0.4725, Avg Loss=0.1247, Time Left=20.36 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 977/3393 [07:24<19:46,  2.04batch/s, Batch Loss=0.4725, Avg Loss=0.1247, Time Left=20.36 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 977/3393 [07:24<19:46,  2.04batch/s, Batch Loss=0.0135, Avg Loss=0.1245, Time Left=20.35 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 978/3393 [07:24<19:29,  2.06batch/s, Batch Loss=0.0135, Avg Loss=0.1245, Time Left=20.35 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 978/3393 [07:25<19:29,  2.06batch/s, Batch Loss=0.0090, Avg Loss=0.1244, Time Left=20.34 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 979/3393 [07:25<19:16,  2.09batch/s, Batch Loss=0.0090, Avg Loss=0.1244, Time Left=20.34 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 979/3393 [07:25<19:16,  2.09batch/s, Batch Loss=0.0933, Avg Loss=0.1244, Time Left=20.33 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 980/3393 [07:25<19:27,  2.07batch/s, Batch Loss=0.0933, Avg Loss=0.1244, Time Left=20.33 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 980/3393 [07:26<19:27,  2.07batch/s, Batch Loss=0.3113, Avg Loss=0.1246, Time Left=20.32 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 981/3393 [07:26<19:17,  2.08batch/s, Batch Loss=0.3113, Avg Loss=0.1246, Time Left=20.32 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 981/3393 [07:26<19:17,  2.08batch/s, Batch Loss=0.1753, Avg Loss=0.1246, Time Left=20.31 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 982/3393 [07:26<19:20,  2.08batch/s, Batch Loss=0.1753, Avg Loss=0.1246, Time Left=20.31 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 982/3393 [07:27<19:20,  2.08batch/s, Batch Loss=0.3170, Avg Loss=0.1249, Time Left=20.31 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 983/3393 [07:27<20:06,  2.00batch/s, Batch Loss=0.3170, Avg Loss=0.1249, Time Left=20.31 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 983/3393 [07:27<20:06,  2.00batch/s, Batch Loss=0.2260, Avg Loss=0.1250, Time Left=20.30 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 984/3393 [07:27<19:33,  2.05batch/s, Batch Loss=0.2260, Avg Loss=0.1250, Time Left=20.30 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 984/3393 [07:28<19:33,  2.05batch/s, Batch Loss=0.1272, Avg Loss=0.1250, Time Left=20.29 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 985/3393 [07:28<19:49,  2.02batch/s, Batch Loss=0.1272, Avg Loss=0.1250, Time Left=20.29 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 985/3393 [07:28<19:49,  2.02batch/s, Batch Loss=0.1376, Avg Loss=0.1250, Time Left=20.28 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 986/3393 [07:28<19:44,  2.03batch/s, Batch Loss=0.1376, Avg Loss=0.1250, Time Left=20.28 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 986/3393 [07:29<19:44,  2.03batch/s, Batch Loss=0.1216, Avg Loss=0.1250, Time Left=20.27 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 987/3393 [07:29<19:38,  2.04batch/s, Batch Loss=0.1216, Avg Loss=0.1250, Time Left=20.27 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 987/3393 [07:29<19:38,  2.04batch/s, Batch Loss=0.0673, Avg Loss=0.1249, Time Left=20.26 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  29%|▎| 988/3393 [07:29<19:56,  2.01batch/s, Batch Loss=0.0673, Avg Loss=0.1249, Time Left=20.26 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 988/3393 [07:30<19:56,  2.01batch/s, Batch Loss=0.0256, Avg Loss=0.1248, Time Left=20.26 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 989/3393 [07:30<19:47,  2.02batch/s, Batch Loss=0.0256, Avg Loss=0.1248, Time Left=20.26 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 989/3393 [07:30<19:47,  2.02batch/s, Batch Loss=0.0533, Avg Loss=0.1247, Time Left=20.25 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 990/3393 [07:30<20:02,  2.00batch/s, Batch Loss=0.0533, Avg Loss=0.1247, Time Left=20.25 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 990/3393 [07:31<20:02,  2.00batch/s, Batch Loss=0.1838, Avg Loss=0.1248, Time Left=20.24 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 991/3393 [07:31<19:41,  2.03batch/s, Batch Loss=0.1838, Avg Loss=0.1248, Time Left=20.24 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 991/3393 [07:31<19:41,  2.03batch/s, Batch Loss=0.0327, Avg Loss=0.1247, Time Left=20.23 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 992/3393 [07:31<19:32,  2.05batch/s, Batch Loss=0.0327, Avg Loss=0.1247, Time Left=20.23 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 992/3393 [07:32<19:32,  2.05batch/s, Batch Loss=0.1935, Avg Loss=0.1248, Time Left=20.22 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 993/3393 [07:32<19:28,  2.05batch/s, Batch Loss=0.1935, Avg Loss=0.1248, Time Left=20.22 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 993/3393 [07:32<19:28,  2.05batch/s, Batch Loss=0.1068, Avg Loss=0.1248, Time Left=20.21 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 994/3393 [07:32<19:15,  2.08batch/s, Batch Loss=0.1068, Avg Loss=0.1248, Time Left=20.21 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 994/3393 [07:33<19:15,  2.08batch/s, Batch Loss=0.1754, Avg Loss=0.1248, Time Left=20.21 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 995/3393 [07:33<19:15,  2.08batch/s, Batch Loss=0.1754, Avg Loss=0.1248, Time Left=20.21 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 995/3393 [07:33<19:15,  2.08batch/s, Batch Loss=0.0451, Avg Loss=0.1247, Time Left=20.20 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 996/3393 [07:33<19:50,  2.01batch/s, Batch Loss=0.0451, Avg Loss=0.1247, Time Left=20.20 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 996/3393 [07:34<19:50,  2.01batch/s, Batch Loss=0.0849, Avg Loss=0.1247, Time Left=20.19 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 997/3393 [07:34<19:48,  2.02batch/s, Batch Loss=0.0849, Avg Loss=0.1247, Time Left=20.19 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 997/3393 [07:34<19:48,  2.02batch/s, Batch Loss=0.0386, Avg Loss=0.1246, Time Left=20.18 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 998/3393 [07:34<19:44,  2.02batch/s, Batch Loss=0.0386, Avg Loss=0.1246, Time Left=20.18 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 998/3393 [07:35<19:44,  2.02batch/s, Batch Loss=0.1928, Avg Loss=0.1247, Time Left=20.17 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 999/3393 [07:35<19:25,  2.05batch/s, Batch Loss=0.1928, Avg Loss=0.1247, Time Left=20.17 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 999/3393 [07:35<19:25,  2.05batch/s, Batch Loss=0.0940, Avg Loss=0.1246, Time Left=20.17 \u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 1000/3393 [07:35<19:21,  2.06batch/s, Batch Loss=0.0940, Avg Loss=0.1246, Time Left=20.17\u001b[A\n",
      "Epoch 2/3 - Training:  29%|▎| 1000/3393 [07:36<19:21,  2.06batch/s, Batch Loss=0.0946, Avg Loss=0.1246, Time Left=20.16\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1001/3393 [07:36<19:42,  2.02batch/s, Batch Loss=0.0946, Avg Loss=0.1246, Time Left=20.16\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1001/3393 [07:36<19:42,  2.02batch/s, Batch Loss=0.1072, Avg Loss=0.1246, Time Left=20.15\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1002/3393 [07:36<19:23,  2.05batch/s, Batch Loss=0.1072, Avg Loss=0.1246, Time Left=20.15\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1002/3393 [07:37<19:23,  2.05batch/s, Batch Loss=0.2077, Avg Loss=0.1247, Time Left=20.14\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1003/3393 [07:37<19:54,  2.00batch/s, Batch Loss=0.2077, Avg Loss=0.1247, Time Left=20.14\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1003/3393 [07:37<19:54,  2.00batch/s, Batch Loss=0.0399, Avg Loss=0.1246, Time Left=20.13\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1004/3393 [07:37<19:50,  2.01batch/s, Batch Loss=0.0399, Avg Loss=0.1246, Time Left=20.13\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1004/3393 [07:38<19:50,  2.01batch/s, Batch Loss=0.0268, Avg Loss=0.1245, Time Left=20.13\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1005/3393 [07:38<19:57,  1.99batch/s, Batch Loss=0.0268, Avg Loss=0.1245, Time Left=20.13\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1005/3393 [07:38<19:57,  1.99batch/s, Batch Loss=0.2816, Avg Loss=0.1246, Time Left=20.12\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1006/3393 [07:38<19:22,  2.05batch/s, Batch Loss=0.2816, Avg Loss=0.1246, Time Left=20.12\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1006/3393 [07:38<19:22,  2.05batch/s, Batch Loss=0.0478, Avg Loss=0.1246, Time Left=20.11\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1007/3393 [07:38<19:18,  2.06batch/s, Batch Loss=0.0478, Avg Loss=0.1246, Time Left=20.11\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1007/3393 [07:39<19:18,  2.06batch/s, Batch Loss=0.0695, Avg Loss=0.1245, Time Left=20.10\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1008/3393 [07:39<19:38,  2.02batch/s, Batch Loss=0.0695, Avg Loss=0.1245, Time Left=20.10\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1008/3393 [07:39<19:38,  2.02batch/s, Batch Loss=0.1248, Avg Loss=0.1245, Time Left=20.09\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1009/3393 [07:39<19:32,  2.03batch/s, Batch Loss=0.1248, Avg Loss=0.1245, Time Left=20.09\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1009/3393 [07:40<19:32,  2.03batch/s, Batch Loss=0.1179, Avg Loss=0.1245, Time Left=20.08\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1010/3393 [07:40<19:28,  2.04batch/s, Batch Loss=0.1179, Avg Loss=0.1245, Time Left=20.08\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1010/3393 [07:40<19:28,  2.04batch/s, Batch Loss=0.0115, Avg Loss=0.1244, Time Left=20.08\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1011/3393 [07:40<19:08,  2.07batch/s, Batch Loss=0.0115, Avg Loss=0.1244, Time Left=20.08\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1011/3393 [07:41<19:08,  2.07batch/s, Batch Loss=0.4233, Avg Loss=0.1247, Time Left=20.07\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1012/3393 [07:41<19:09,  2.07batch/s, Batch Loss=0.4233, Avg Loss=0.1247, Time Left=20.07\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1012/3393 [07:41<19:09,  2.07batch/s, Batch Loss=0.1380, Avg Loss=0.1247, Time Left=20.06\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1013/3393 [07:41<19:08,  2.07batch/s, Batch Loss=0.1380, Avg Loss=0.1247, Time Left=20.06\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1013/3393 [07:42<19:08,  2.07batch/s, Batch Loss=0.0172, Avg Loss=0.1246, Time Left=20.05\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1014/3393 [07:42<19:10,  2.07batch/s, Batch Loss=0.0172, Avg Loss=0.1246, Time Left=20.05\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1014/3393 [07:42<19:10,  2.07batch/s, Batch Loss=0.0273, Avg Loss=0.1245, Time Left=20.04\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1015/3393 [07:42<19:09,  2.07batch/s, Batch Loss=0.0273, Avg Loss=0.1245, Time Left=20.04\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1015/3393 [07:43<19:09,  2.07batch/s, Batch Loss=0.1394, Avg Loss=0.1245, Time Left=20.03\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1016/3393 [07:43<19:31,  2.03batch/s, Batch Loss=0.1394, Avg Loss=0.1245, Time Left=20.03\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1016/3393 [07:43<19:31,  2.03batch/s, Batch Loss=0.0222, Avg Loss=0.1244, Time Left=20.03\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1017/3393 [07:43<19:25,  2.04batch/s, Batch Loss=0.0222, Avg Loss=0.1244, Time Left=20.03\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1017/3393 [07:44<19:25,  2.04batch/s, Batch Loss=0.3819, Avg Loss=0.1247, Time Left=20.02\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1018/3393 [07:44<19:20,  2.05batch/s, Batch Loss=0.3819, Avg Loss=0.1247, Time Left=20.02\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1018/3393 [07:44<19:20,  2.05batch/s, Batch Loss=0.0723, Avg Loss=0.1246, Time Left=20.01\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1019/3393 [07:44<19:05,  2.07batch/s, Batch Loss=0.0723, Avg Loss=0.1246, Time Left=20.01\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1019/3393 [07:45<19:05,  2.07batch/s, Batch Loss=0.1031, Avg Loss=0.1246, Time Left=20.00\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1020/3393 [07:45<18:53,  2.09batch/s, Batch Loss=0.1031, Avg Loss=0.1246, Time Left=20.00\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1020/3393 [07:45<18:53,  2.09batch/s, Batch Loss=0.0237, Avg Loss=0.1245, Time Left=19.99\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  30%|▎| 1021/3393 [07:45<18:57,  2.08batch/s, Batch Loss=0.0237, Avg Loss=0.1245, Time Left=19.99\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1021/3393 [07:46<18:57,  2.08batch/s, Batch Loss=0.1004, Avg Loss=0.1245, Time Left=19.98\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1022/3393 [07:46<19:33,  2.02batch/s, Batch Loss=0.1004, Avg Loss=0.1245, Time Left=19.98\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1022/3393 [07:46<19:33,  2.02batch/s, Batch Loss=0.0943, Avg Loss=0.1244, Time Left=19.98\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1023/3393 [07:46<19:14,  2.05batch/s, Batch Loss=0.0943, Avg Loss=0.1244, Time Left=19.98\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1023/3393 [07:47<19:14,  2.05batch/s, Batch Loss=0.1556, Avg Loss=0.1245, Time Left=19.97\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1024/3393 [07:47<19:22,  2.04batch/s, Batch Loss=0.1556, Avg Loss=0.1245, Time Left=19.97\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1024/3393 [07:47<19:22,  2.04batch/s, Batch Loss=0.0553, Avg Loss=0.1244, Time Left=19.96\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1025/3393 [07:47<18:54,  2.09batch/s, Batch Loss=0.0553, Avg Loss=0.1244, Time Left=19.96\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1025/3393 [07:48<18:54,  2.09batch/s, Batch Loss=0.0817, Avg Loss=0.1243, Time Left=19.95\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1026/3393 [07:48<18:56,  2.08batch/s, Batch Loss=0.0817, Avg Loss=0.1243, Time Left=19.95\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1026/3393 [07:48<18:56,  2.08batch/s, Batch Loss=0.2080, Avg Loss=0.1244, Time Left=19.94\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1027/3393 [07:48<19:09,  2.06batch/s, Batch Loss=0.2080, Avg Loss=0.1244, Time Left=19.94\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1027/3393 [07:49<19:09,  2.06batch/s, Batch Loss=0.0880, Avg Loss=0.1244, Time Left=19.93\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1028/3393 [07:49<19:19,  2.04batch/s, Batch Loss=0.0880, Avg Loss=0.1244, Time Left=19.93\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1028/3393 [07:49<19:19,  2.04batch/s, Batch Loss=0.2139, Avg Loss=0.1245, Time Left=19.92\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1029/3393 [07:49<19:14,  2.05batch/s, Batch Loss=0.2139, Avg Loss=0.1245, Time Left=19.92\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1029/3393 [07:50<19:14,  2.05batch/s, Batch Loss=0.1137, Avg Loss=0.1245, Time Left=19.92\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1030/3393 [07:50<19:08,  2.06batch/s, Batch Loss=0.1137, Avg Loss=0.1245, Time Left=19.92\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1030/3393 [07:50<19:08,  2.06batch/s, Batch Loss=0.0948, Avg Loss=0.1244, Time Left=19.91\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1031/3393 [07:50<18:57,  2.08batch/s, Batch Loss=0.0948, Avg Loss=0.1244, Time Left=19.91\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1031/3393 [07:51<18:57,  2.08batch/s, Batch Loss=0.2218, Avg Loss=0.1245, Time Left=19.90\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1032/3393 [07:51<19:04,  2.06batch/s, Batch Loss=0.2218, Avg Loss=0.1245, Time Left=19.90\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1032/3393 [07:51<19:04,  2.06batch/s, Batch Loss=0.0782, Avg Loss=0.1245, Time Left=19.89\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1033/3393 [07:51<18:52,  2.08batch/s, Batch Loss=0.0782, Avg Loss=0.1245, Time Left=19.89\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1033/3393 [07:52<18:52,  2.08batch/s, Batch Loss=0.1202, Avg Loss=0.1245, Time Left=19.88\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1034/3393 [07:52<19:01,  2.07batch/s, Batch Loss=0.1202, Avg Loss=0.1245, Time Left=19.88\u001b[A\n",
      "Epoch 2/3 - Training:  30%|▎| 1034/3393 [07:52<19:01,  2.07batch/s, Batch Loss=0.4720, Avg Loss=0.1248, Time Left=19.87\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1035/3393 [07:52<19:16,  2.04batch/s, Batch Loss=0.4720, Avg Loss=0.1248, Time Left=19.87\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1035/3393 [07:53<19:16,  2.04batch/s, Batch Loss=0.0425, Avg Loss=0.1248, Time Left=19.87\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1036/3393 [07:53<19:33,  2.01batch/s, Batch Loss=0.0425, Avg Loss=0.1248, Time Left=19.87\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1036/3393 [07:53<19:33,  2.01batch/s, Batch Loss=0.0975, Avg Loss=0.1247, Time Left=19.86\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1037/3393 [07:53<19:44,  1.99batch/s, Batch Loss=0.0975, Avg Loss=0.1247, Time Left=19.86\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1037/3393 [07:54<19:44,  1.99batch/s, Batch Loss=0.0601, Avg Loss=0.1247, Time Left=19.85\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1038/3393 [07:54<19:35,  2.00batch/s, Batch Loss=0.0601, Avg Loss=0.1247, Time Left=19.85\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1038/3393 [07:54<19:35,  2.00batch/s, Batch Loss=0.0748, Avg Loss=0.1246, Time Left=19.85\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1039/3393 [07:54<20:15,  1.94batch/s, Batch Loss=0.0748, Avg Loss=0.1246, Time Left=19.85\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1039/3393 [07:55<20:15,  1.94batch/s, Batch Loss=0.0200, Avg Loss=0.1245, Time Left=19.84\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1040/3393 [07:55<20:50,  1.88batch/s, Batch Loss=0.0200, Avg Loss=0.1245, Time Left=19.84\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1040/3393 [07:55<20:50,  1.88batch/s, Batch Loss=0.0286, Avg Loss=0.1244, Time Left=19.84\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1041/3393 [07:55<21:16,  1.84batch/s, Batch Loss=0.0286, Avg Loss=0.1244, Time Left=19.84\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1041/3393 [07:56<21:16,  1.84batch/s, Batch Loss=0.0476, Avg Loss=0.1243, Time Left=19.83\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1042/3393 [07:56<21:35,  1.81batch/s, Batch Loss=0.0476, Avg Loss=0.1243, Time Left=19.83\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1042/3393 [07:56<21:35,  1.81batch/s, Batch Loss=0.1530, Avg Loss=0.1244, Time Left=19.83\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1043/3393 [07:56<21:42,  1.80batch/s, Batch Loss=0.1530, Avg Loss=0.1244, Time Left=19.83\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1043/3393 [07:57<21:42,  1.80batch/s, Batch Loss=0.0028, Avg Loss=0.1242, Time Left=19.82\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1044/3393 [07:57<21:49,  1.79batch/s, Batch Loss=0.0028, Avg Loss=0.1242, Time Left=19.82\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1044/3393 [07:57<21:49,  1.79batch/s, Batch Loss=0.0866, Avg Loss=0.1242, Time Left=19.81\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1045/3393 [07:57<20:35,  1.90batch/s, Batch Loss=0.0866, Avg Loss=0.1242, Time Left=19.81\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1045/3393 [07:58<20:35,  1.90batch/s, Batch Loss=0.0155, Avg Loss=0.1241, Time Left=19.80\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1046/3393 [07:58<20:27,  1.91batch/s, Batch Loss=0.0155, Avg Loss=0.1241, Time Left=19.80\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1046/3393 [07:58<20:27,  1.91batch/s, Batch Loss=0.1996, Avg Loss=0.1242, Time Left=19.80\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1047/3393 [07:58<19:48,  1.97batch/s, Batch Loss=0.1996, Avg Loss=0.1242, Time Left=19.80\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1047/3393 [07:59<19:48,  1.97batch/s, Batch Loss=0.1950, Avg Loss=0.1242, Time Left=19.79\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1048/3393 [07:59<19:32,  2.00batch/s, Batch Loss=0.1950, Avg Loss=0.1242, Time Left=19.79\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1048/3393 [07:59<19:32,  2.00batch/s, Batch Loss=0.0331, Avg Loss=0.1241, Time Left=19.78\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1049/3393 [07:59<19:19,  2.02batch/s, Batch Loss=0.0331, Avg Loss=0.1241, Time Left=19.78\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1049/3393 [08:00<19:19,  2.02batch/s, Batch Loss=0.0225, Avg Loss=0.1240, Time Left=19.77\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1050/3393 [08:00<19:16,  2.03batch/s, Batch Loss=0.0225, Avg Loss=0.1240, Time Left=19.77\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1050/3393 [08:00<19:16,  2.03batch/s, Batch Loss=0.0833, Avg Loss=0.1240, Time Left=19.76\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1051/3393 [08:00<19:04,  2.05batch/s, Batch Loss=0.0833, Avg Loss=0.1240, Time Left=19.76\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1051/3393 [08:01<19:04,  2.05batch/s, Batch Loss=0.0501, Avg Loss=0.1239, Time Left=19.75\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1052/3393 [08:01<19:33,  2.00batch/s, Batch Loss=0.0501, Avg Loss=0.1239, Time Left=19.75\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1052/3393 [08:01<19:33,  2.00batch/s, Batch Loss=0.0151, Avg Loss=0.1238, Time Left=19.75\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1053/3393 [08:01<19:11,  2.03batch/s, Batch Loss=0.0151, Avg Loss=0.1238, Time Left=19.75\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1053/3393 [08:02<19:11,  2.03batch/s, Batch Loss=0.0143, Avg Loss=0.1237, Time Left=19.74\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  31%|▎| 1054/3393 [08:02<19:04,  2.04batch/s, Batch Loss=0.0143, Avg Loss=0.1237, Time Left=19.74\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1054/3393 [08:02<19:04,  2.04batch/s, Batch Loss=0.0672, Avg Loss=0.1236, Time Left=19.73\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1055/3393 [08:02<19:00,  2.05batch/s, Batch Loss=0.0672, Avg Loss=0.1236, Time Left=19.73\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1055/3393 [08:03<19:00,  2.05batch/s, Batch Loss=0.1480, Avg Loss=0.1237, Time Left=19.72\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1056/3393 [08:03<19:08,  2.04batch/s, Batch Loss=0.1480, Avg Loss=0.1237, Time Left=19.72\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1056/3393 [08:03<19:08,  2.04batch/s, Batch Loss=0.0520, Avg Loss=0.1236, Time Left=19.71\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1057/3393 [08:03<19:24,  2.01batch/s, Batch Loss=0.0520, Avg Loss=0.1236, Time Left=19.71\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1057/3393 [08:04<19:24,  2.01batch/s, Batch Loss=0.0743, Avg Loss=0.1235, Time Left=19.71\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1058/3393 [08:04<19:14,  2.02batch/s, Batch Loss=0.0743, Avg Loss=0.1235, Time Left=19.71\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1058/3393 [08:04<19:14,  2.02batch/s, Batch Loss=0.3044, Avg Loss=0.1237, Time Left=19.70\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1059/3393 [08:04<18:55,  2.06batch/s, Batch Loss=0.3044, Avg Loss=0.1237, Time Left=19.70\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1059/3393 [08:05<18:55,  2.06batch/s, Batch Loss=0.0327, Avg Loss=0.1236, Time Left=19.69\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1060/3393 [08:05<18:53,  2.06batch/s, Batch Loss=0.0327, Avg Loss=0.1236, Time Left=19.69\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1060/3393 [08:05<18:53,  2.06batch/s, Batch Loss=0.2016, Avg Loss=0.1237, Time Left=19.68\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1061/3393 [08:05<18:45,  2.07batch/s, Batch Loss=0.2016, Avg Loss=0.1237, Time Left=19.68\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1061/3393 [08:06<18:45,  2.07batch/s, Batch Loss=0.1663, Avg Loss=0.1238, Time Left=19.67\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1062/3393 [08:06<19:00,  2.04batch/s, Batch Loss=0.1663, Avg Loss=0.1238, Time Left=19.67\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1062/3393 [08:06<19:00,  2.04batch/s, Batch Loss=0.0920, Avg Loss=0.1237, Time Left=19.66\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1063/3393 [08:06<18:47,  2.07batch/s, Batch Loss=0.0920, Avg Loss=0.1237, Time Left=19.66\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1063/3393 [08:07<18:47,  2.07batch/s, Batch Loss=0.0495, Avg Loss=0.1236, Time Left=19.65\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1064/3393 [08:07<18:35,  2.09batch/s, Batch Loss=0.0495, Avg Loss=0.1236, Time Left=19.65\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1064/3393 [08:07<18:35,  2.09batch/s, Batch Loss=0.3221, Avg Loss=0.1238, Time Left=19.65\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1065/3393 [08:07<19:11,  2.02batch/s, Batch Loss=0.3221, Avg Loss=0.1238, Time Left=19.65\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1065/3393 [08:08<19:11,  2.02batch/s, Batch Loss=0.0956, Avg Loss=0.1238, Time Left=19.64\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1066/3393 [08:08<19:03,  2.03batch/s, Batch Loss=0.0956, Avg Loss=0.1238, Time Left=19.64\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1066/3393 [08:08<19:03,  2.03batch/s, Batch Loss=0.1045, Avg Loss=0.1238, Time Left=19.63\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1067/3393 [08:08<19:10,  2.02batch/s, Batch Loss=0.1045, Avg Loss=0.1238, Time Left=19.63\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1067/3393 [08:09<19:10,  2.02batch/s, Batch Loss=0.0713, Avg Loss=0.1237, Time Left=19.62\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1068/3393 [08:09<18:50,  2.06batch/s, Batch Loss=0.0713, Avg Loss=0.1237, Time Left=19.62\u001b[A\n",
      "Epoch 2/3 - Training:  31%|▎| 1068/3393 [08:09<18:50,  2.06batch/s, Batch Loss=0.4054, Avg Loss=0.1240, Time Left=19.61\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1069/3393 [08:09<18:48,  2.06batch/s, Batch Loss=0.4054, Avg Loss=0.1240, Time Left=19.61\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1069/3393 [08:10<18:48,  2.06batch/s, Batch Loss=0.0302, Avg Loss=0.1239, Time Left=19.61\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1070/3393 [08:10<19:19,  2.00batch/s, Batch Loss=0.0302, Avg Loss=0.1239, Time Left=19.61\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1070/3393 [08:10<19:19,  2.00batch/s, Batch Loss=0.2313, Avg Loss=0.1240, Time Left=19.60\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1071/3393 [08:10<19:10,  2.02batch/s, Batch Loss=0.2313, Avg Loss=0.1240, Time Left=19.60\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1071/3393 [08:11<19:10,  2.02batch/s, Batch Loss=0.0599, Avg Loss=0.1240, Time Left=19.59\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1072/3393 [08:11<19:34,  1.98batch/s, Batch Loss=0.0599, Avg Loss=0.1240, Time Left=19.59\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1072/3393 [08:11<19:34,  1.98batch/s, Batch Loss=0.0356, Avg Loss=0.1239, Time Left=19.58\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1073/3393 [08:11<19:19,  2.00batch/s, Batch Loss=0.0356, Avg Loss=0.1239, Time Left=19.58\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1073/3393 [08:12<19:19,  2.00batch/s, Batch Loss=0.1365, Avg Loss=0.1239, Time Left=19.57\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1074/3393 [08:12<19:08,  2.02batch/s, Batch Loss=0.1365, Avg Loss=0.1239, Time Left=19.57\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1074/3393 [08:12<19:08,  2.02batch/s, Batch Loss=0.0233, Avg Loss=0.1238, Time Left=19.57\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1075/3393 [08:12<19:01,  2.03batch/s, Batch Loss=0.0233, Avg Loss=0.1238, Time Left=19.57\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1075/3393 [08:13<19:01,  2.03batch/s, Batch Loss=0.0250, Avg Loss=0.1237, Time Left=19.56\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1076/3393 [08:13<18:43,  2.06batch/s, Batch Loss=0.0250, Avg Loss=0.1237, Time Left=19.56\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1076/3393 [08:13<18:43,  2.06batch/s, Batch Loss=0.0723, Avg Loss=0.1237, Time Left=19.55\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1077/3393 [08:13<18:41,  2.06batch/s, Batch Loss=0.0723, Avg Loss=0.1237, Time Left=19.55\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1077/3393 [08:14<18:41,  2.06batch/s, Batch Loss=0.3520, Avg Loss=0.1239, Time Left=19.54\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1078/3393 [08:14<18:29,  2.09batch/s, Batch Loss=0.3520, Avg Loss=0.1239, Time Left=19.54\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1078/3393 [08:14<18:29,  2.09batch/s, Batch Loss=0.1108, Avg Loss=0.1239, Time Left=19.53\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1079/3393 [08:14<18:35,  2.07batch/s, Batch Loss=0.1108, Avg Loss=0.1239, Time Left=19.53\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1079/3393 [08:15<18:35,  2.07batch/s, Batch Loss=0.4266, Avg Loss=0.1242, Time Left=19.52\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1080/3393 [08:15<18:32,  2.08batch/s, Batch Loss=0.4266, Avg Loss=0.1242, Time Left=19.52\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1080/3393 [08:15<18:32,  2.08batch/s, Batch Loss=0.0555, Avg Loss=0.1241, Time Left=19.51\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1081/3393 [08:15<18:33,  2.08batch/s, Batch Loss=0.0555, Avg Loss=0.1241, Time Left=19.51\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1081/3393 [08:15<18:33,  2.08batch/s, Batch Loss=0.0889, Avg Loss=0.1241, Time Left=19.50\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1082/3393 [08:15<18:23,  2.09batch/s, Batch Loss=0.0889, Avg Loss=0.1241, Time Left=19.50\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1082/3393 [08:16<18:23,  2.09batch/s, Batch Loss=0.1548, Avg Loss=0.1241, Time Left=19.50\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1083/3393 [08:16<18:27,  2.09batch/s, Batch Loss=0.1548, Avg Loss=0.1241, Time Left=19.50\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1083/3393 [08:16<18:27,  2.09batch/s, Batch Loss=0.0238, Avg Loss=0.1240, Time Left=19.49\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1084/3393 [08:16<18:18,  2.10batch/s, Batch Loss=0.0238, Avg Loss=0.1240, Time Left=19.49\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1084/3393 [08:17<18:18,  2.10batch/s, Batch Loss=0.0906, Avg Loss=0.1240, Time Left=19.48\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1085/3393 [08:17<18:11,  2.11batch/s, Batch Loss=0.0906, Avg Loss=0.1240, Time Left=19.48\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1085/3393 [08:17<18:11,  2.11batch/s, Batch Loss=0.1016, Avg Loss=0.1239, Time Left=19.47\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1086/3393 [08:17<18:07,  2.12batch/s, Batch Loss=0.1016, Avg Loss=0.1239, Time Left=19.47\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1086/3393 [08:18<18:07,  2.12batch/s, Batch Loss=0.0952, Avg Loss=0.1239, Time Left=19.46\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  32%|▎| 1087/3393 [08:18<18:36,  2.07batch/s, Batch Loss=0.0952, Avg Loss=0.1239, Time Left=19.46\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1087/3393 [08:18<18:36,  2.07batch/s, Batch Loss=0.0236, Avg Loss=0.1238, Time Left=19.45\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1088/3393 [08:18<18:36,  2.06batch/s, Batch Loss=0.0236, Avg Loss=0.1238, Time Left=19.45\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1088/3393 [08:19<18:36,  2.06batch/s, Batch Loss=0.1232, Avg Loss=0.1238, Time Left=19.45\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1089/3393 [08:19<18:56,  2.03batch/s, Batch Loss=0.1232, Avg Loss=0.1238, Time Left=19.45\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1089/3393 [08:19<18:56,  2.03batch/s, Batch Loss=0.2079, Avg Loss=0.1239, Time Left=19.44\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1090/3393 [08:19<18:57,  2.02batch/s, Batch Loss=0.2079, Avg Loss=0.1239, Time Left=19.44\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1090/3393 [08:20<18:57,  2.02batch/s, Batch Loss=0.2213, Avg Loss=0.1240, Time Left=19.43\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1091/3393 [08:20<18:44,  2.05batch/s, Batch Loss=0.2213, Avg Loss=0.1240, Time Left=19.43\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1091/3393 [08:20<18:44,  2.05batch/s, Batch Loss=0.0626, Avg Loss=0.1239, Time Left=19.42\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1092/3393 [08:20<18:30,  2.07batch/s, Batch Loss=0.0626, Avg Loss=0.1239, Time Left=19.42\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1092/3393 [08:21<18:30,  2.07batch/s, Batch Loss=0.0539, Avg Loss=0.1239, Time Left=19.41\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1093/3393 [08:21<18:29,  2.07batch/s, Batch Loss=0.0539, Avg Loss=0.1239, Time Left=19.41\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1093/3393 [08:21<18:29,  2.07batch/s, Batch Loss=0.0145, Avg Loss=0.1238, Time Left=19.40\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1094/3393 [08:21<18:29,  2.07batch/s, Batch Loss=0.0145, Avg Loss=0.1238, Time Left=19.40\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1094/3393 [08:22<18:29,  2.07batch/s, Batch Loss=0.0709, Avg Loss=0.1237, Time Left=19.39\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1095/3393 [08:22<18:29,  2.07batch/s, Batch Loss=0.0709, Avg Loss=0.1237, Time Left=19.39\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1095/3393 [08:22<18:29,  2.07batch/s, Batch Loss=0.2475, Avg Loss=0.1238, Time Left=19.39\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1096/3393 [08:22<18:18,  2.09batch/s, Batch Loss=0.2475, Avg Loss=0.1238, Time Left=19.39\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1096/3393 [08:23<18:18,  2.09batch/s, Batch Loss=0.0813, Avg Loss=0.1238, Time Left=19.38\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1097/3393 [08:23<18:43,  2.04batch/s, Batch Loss=0.0813, Avg Loss=0.1238, Time Left=19.38\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1097/3393 [08:23<18:43,  2.04batch/s, Batch Loss=0.2951, Avg Loss=0.1239, Time Left=19.37\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1098/3393 [08:23<18:28,  2.07batch/s, Batch Loss=0.2951, Avg Loss=0.1239, Time Left=19.37\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1098/3393 [08:24<18:28,  2.07batch/s, Batch Loss=0.0558, Avg Loss=0.1239, Time Left=19.36\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1099/3393 [08:24<18:28,  2.07batch/s, Batch Loss=0.0558, Avg Loss=0.1239, Time Left=19.36\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1099/3393 [08:24<18:28,  2.07batch/s, Batch Loss=0.0583, Avg Loss=0.1238, Time Left=19.35\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1100/3393 [08:24<19:00,  2.01batch/s, Batch Loss=0.0583, Avg Loss=0.1238, Time Left=19.35\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1100/3393 [08:25<19:00,  2.01batch/s, Batch Loss=0.2081, Avg Loss=0.1239, Time Left=19.35\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1101/3393 [08:25<18:52,  2.02batch/s, Batch Loss=0.2081, Avg Loss=0.1239, Time Left=19.35\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1101/3393 [08:25<18:52,  2.02batch/s, Batch Loss=0.2524, Avg Loss=0.1240, Time Left=19.34\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1102/3393 [08:25<19:10,  1.99batch/s, Batch Loss=0.2524, Avg Loss=0.1240, Time Left=19.34\u001b[A\n",
      "Epoch 2/3 - Training:  32%|▎| 1102/3393 [08:26<19:10,  1.99batch/s, Batch Loss=0.0229, Avg Loss=0.1239, Time Left=19.33\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1103/3393 [08:26<18:43,  2.04batch/s, Batch Loss=0.0229, Avg Loss=0.1239, Time Left=19.33\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1103/3393 [08:26<18:43,  2.04batch/s, Batch Loss=0.0332, Avg Loss=0.1238, Time Left=19.32\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1104/3393 [08:26<18:37,  2.05batch/s, Batch Loss=0.0332, Avg Loss=0.1238, Time Left=19.32\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1104/3393 [08:27<18:37,  2.05batch/s, Batch Loss=0.0962, Avg Loss=0.1238, Time Left=19.31\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1105/3393 [08:27<18:44,  2.03batch/s, Batch Loss=0.0962, Avg Loss=0.1238, Time Left=19.31\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1105/3393 [08:27<18:44,  2.03batch/s, Batch Loss=0.0413, Avg Loss=0.1237, Time Left=19.30\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1106/3393 [08:27<18:39,  2.04batch/s, Batch Loss=0.0413, Avg Loss=0.1237, Time Left=19.30\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1106/3393 [08:28<18:39,  2.04batch/s, Batch Loss=0.1233, Avg Loss=0.1237, Time Left=19.30\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1107/3393 [08:28<19:07,  1.99batch/s, Batch Loss=0.1233, Avg Loss=0.1237, Time Left=19.30\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1107/3393 [08:28<19:07,  1.99batch/s, Batch Loss=0.2106, Avg Loss=0.1238, Time Left=19.29\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1108/3393 [08:28<18:56,  2.01batch/s, Batch Loss=0.2106, Avg Loss=0.1238, Time Left=19.29\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1108/3393 [08:29<18:56,  2.01batch/s, Batch Loss=0.0809, Avg Loss=0.1238, Time Left=19.28\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1109/3393 [08:29<19:19,  1.97batch/s, Batch Loss=0.0809, Avg Loss=0.1238, Time Left=19.28\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1109/3393 [08:29<19:19,  1.97batch/s, Batch Loss=0.0738, Avg Loss=0.1237, Time Left=19.27\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1110/3393 [08:29<19:15,  1.98batch/s, Batch Loss=0.0738, Avg Loss=0.1237, Time Left=19.27\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1110/3393 [08:30<19:15,  1.98batch/s, Batch Loss=0.0739, Avg Loss=0.1237, Time Left=19.27\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1111/3393 [08:30<19:20,  1.97batch/s, Batch Loss=0.0739, Avg Loss=0.1237, Time Left=19.27\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1111/3393 [08:30<19:20,  1.97batch/s, Batch Loss=0.2127, Avg Loss=0.1238, Time Left=19.26\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1112/3393 [08:30<19:16,  1.97batch/s, Batch Loss=0.2127, Avg Loss=0.1238, Time Left=19.26\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1112/3393 [08:31<19:16,  1.97batch/s, Batch Loss=0.1234, Avg Loss=0.1238, Time Left=19.25\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1113/3393 [08:31<19:21,  1.96batch/s, Batch Loss=0.1234, Avg Loss=0.1238, Time Left=19.25\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1113/3393 [08:31<19:21,  1.96batch/s, Batch Loss=0.1423, Avg Loss=0.1238, Time Left=19.24\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1114/3393 [08:31<18:54,  2.01batch/s, Batch Loss=0.1423, Avg Loss=0.1238, Time Left=19.24\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1114/3393 [08:32<18:54,  2.01batch/s, Batch Loss=0.1176, Avg Loss=0.1238, Time Left=19.24\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1115/3393 [08:32<18:43,  2.03batch/s, Batch Loss=0.1176, Avg Loss=0.1238, Time Left=19.24\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1115/3393 [08:32<18:43,  2.03batch/s, Batch Loss=0.0074, Avg Loss=0.1237, Time Left=19.23\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1116/3393 [08:32<18:57,  2.00batch/s, Batch Loss=0.0074, Avg Loss=0.1237, Time Left=19.23\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1116/3393 [08:33<18:57,  2.00batch/s, Batch Loss=0.0270, Avg Loss=0.1236, Time Left=19.22\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1117/3393 [08:33<18:47,  2.02batch/s, Batch Loss=0.0270, Avg Loss=0.1236, Time Left=19.22\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1117/3393 [08:33<18:47,  2.02batch/s, Batch Loss=0.1051, Avg Loss=0.1236, Time Left=19.21\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1118/3393 [08:33<18:51,  2.01batch/s, Batch Loss=0.1051, Avg Loss=0.1236, Time Left=19.21\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1118/3393 [08:34<18:51,  2.01batch/s, Batch Loss=0.0218, Avg Loss=0.1235, Time Left=19.20\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1119/3393 [08:34<18:52,  2.01batch/s, Batch Loss=0.0218, Avg Loss=0.1235, Time Left=19.20\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1119/3393 [08:34<18:52,  2.01batch/s, Batch Loss=0.0581, Avg Loss=0.1234, Time Left=19.20\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  33%|▎| 1120/3393 [08:34<19:18,  1.96batch/s, Batch Loss=0.0581, Avg Loss=0.1234, Time Left=19.20\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1120/3393 [08:35<19:18,  1.96batch/s, Batch Loss=0.1422, Avg Loss=0.1234, Time Left=19.19\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1121/3393 [08:35<19:10,  1.97batch/s, Batch Loss=0.1422, Avg Loss=0.1234, Time Left=19.19\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1121/3393 [08:35<19:10,  1.97batch/s, Batch Loss=0.0727, Avg Loss=0.1234, Time Left=19.18\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1122/3393 [08:35<19:03,  1.99batch/s, Batch Loss=0.0727, Avg Loss=0.1234, Time Left=19.18\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1122/3393 [08:36<19:03,  1.99batch/s, Batch Loss=0.0934, Avg Loss=0.1233, Time Left=19.17\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1123/3393 [08:36<18:51,  2.01batch/s, Batch Loss=0.0934, Avg Loss=0.1233, Time Left=19.17\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1123/3393 [08:36<18:51,  2.01batch/s, Batch Loss=0.0283, Avg Loss=0.1232, Time Left=19.16\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1124/3393 [08:36<18:29,  2.05batch/s, Batch Loss=0.0283, Avg Loss=0.1232, Time Left=19.16\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1124/3393 [08:37<18:29,  2.05batch/s, Batch Loss=0.0174, Avg Loss=0.1231, Time Left=19.16\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1125/3393 [08:37<18:36,  2.03batch/s, Batch Loss=0.0174, Avg Loss=0.1231, Time Left=19.16\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1125/3393 [08:37<18:36,  2.03batch/s, Batch Loss=0.1170, Avg Loss=0.1231, Time Left=19.15\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1126/3393 [08:37<18:20,  2.06batch/s, Batch Loss=0.1170, Avg Loss=0.1231, Time Left=19.15\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1126/3393 [08:38<18:20,  2.06batch/s, Batch Loss=0.2183, Avg Loss=0.1232, Time Left=19.14\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1127/3393 [08:38<18:28,  2.04batch/s, Batch Loss=0.2183, Avg Loss=0.1232, Time Left=19.14\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1127/3393 [08:38<18:28,  2.04batch/s, Batch Loss=0.3319, Avg Loss=0.1234, Time Left=19.13\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1128/3393 [08:38<18:34,  2.03batch/s, Batch Loss=0.3319, Avg Loss=0.1234, Time Left=19.13\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1128/3393 [08:39<18:34,  2.03batch/s, Batch Loss=0.3187, Avg Loss=0.1236, Time Left=19.12\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1129/3393 [08:39<18:29,  2.04batch/s, Batch Loss=0.3187, Avg Loss=0.1236, Time Left=19.12\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1129/3393 [08:39<18:29,  2.04batch/s, Batch Loss=0.0791, Avg Loss=0.1236, Time Left=19.12\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1130/3393 [08:39<18:45,  2.01batch/s, Batch Loss=0.0791, Avg Loss=0.1236, Time Left=19.12\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1130/3393 [08:40<18:45,  2.01batch/s, Batch Loss=0.2838, Avg Loss=0.1237, Time Left=19.11\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1131/3393 [08:40<18:16,  2.06batch/s, Batch Loss=0.2838, Avg Loss=0.1237, Time Left=19.11\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1131/3393 [08:40<18:16,  2.06batch/s, Batch Loss=0.0183, Avg Loss=0.1236, Time Left=19.10\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1132/3393 [08:40<18:25,  2.04batch/s, Batch Loss=0.0183, Avg Loss=0.1236, Time Left=19.10\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1132/3393 [08:41<18:25,  2.04batch/s, Batch Loss=0.0471, Avg Loss=0.1235, Time Left=19.09\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1133/3393 [08:41<18:29,  2.04batch/s, Batch Loss=0.0471, Avg Loss=0.1235, Time Left=19.09\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1133/3393 [08:41<18:29,  2.04batch/s, Batch Loss=0.0639, Avg Loss=0.1235, Time Left=19.08\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1134/3393 [08:41<18:05,  2.08batch/s, Batch Loss=0.0639, Avg Loss=0.1235, Time Left=19.08\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1134/3393 [08:42<18:05,  2.08batch/s, Batch Loss=0.0208, Avg Loss=0.1234, Time Left=19.07\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1135/3393 [08:42<17:55,  2.10batch/s, Batch Loss=0.0208, Avg Loss=0.1234, Time Left=19.07\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1135/3393 [08:42<17:55,  2.10batch/s, Batch Loss=0.1848, Avg Loss=0.1235, Time Left=19.06\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1136/3393 [08:42<18:30,  2.03batch/s, Batch Loss=0.1848, Avg Loss=0.1235, Time Left=19.06\u001b[A\n",
      "Epoch 2/3 - Training:  33%|▎| 1136/3393 [08:43<18:30,  2.03batch/s, Batch Loss=0.0052, Avg Loss=0.1233, Time Left=19.06\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1137/3393 [08:43<18:16,  2.06batch/s, Batch Loss=0.0052, Avg Loss=0.1233, Time Left=19.06\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1137/3393 [08:43<18:16,  2.06batch/s, Batch Loss=0.0741, Avg Loss=0.1233, Time Left=19.05\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1138/3393 [08:43<18:35,  2.02batch/s, Batch Loss=0.0741, Avg Loss=0.1233, Time Left=19.05\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1138/3393 [08:44<18:35,  2.02batch/s, Batch Loss=0.0225, Avg Loss=0.1232, Time Left=19.04\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1139/3393 [08:44<18:28,  2.03batch/s, Batch Loss=0.0225, Avg Loss=0.1232, Time Left=19.04\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1139/3393 [08:44<18:28,  2.03batch/s, Batch Loss=0.1035, Avg Loss=0.1232, Time Left=19.03\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1140/3393 [08:44<18:12,  2.06batch/s, Batch Loss=0.1035, Avg Loss=0.1232, Time Left=19.03\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1140/3393 [08:45<18:12,  2.06batch/s, Batch Loss=0.0395, Avg Loss=0.1231, Time Left=19.02\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1141/3393 [08:45<18:31,  2.03batch/s, Batch Loss=0.0395, Avg Loss=0.1231, Time Left=19.02\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1141/3393 [08:45<18:31,  2.03batch/s, Batch Loss=0.1468, Avg Loss=0.1231, Time Left=19.01\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1142/3393 [08:45<18:14,  2.06batch/s, Batch Loss=0.1468, Avg Loss=0.1231, Time Left=19.01\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1142/3393 [08:46<18:14,  2.06batch/s, Batch Loss=0.2565, Avg Loss=0.1233, Time Left=19.01\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1143/3393 [08:46<18:38,  2.01batch/s, Batch Loss=0.2565, Avg Loss=0.1233, Time Left=19.01\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1143/3393 [08:46<18:38,  2.01batch/s, Batch Loss=0.0588, Avg Loss=0.1232, Time Left=19.00\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1144/3393 [08:46<18:14,  2.05batch/s, Batch Loss=0.0588, Avg Loss=0.1232, Time Left=19.00\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1144/3393 [08:46<18:14,  2.05batch/s, Batch Loss=0.1541, Avg Loss=0.1232, Time Left=18.99\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1145/3393 [08:46<18:12,  2.06batch/s, Batch Loss=0.1541, Avg Loss=0.1232, Time Left=18.99\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1145/3393 [08:47<18:12,  2.06batch/s, Batch Loss=0.1013, Avg Loss=0.1232, Time Left=18.98\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1146/3393 [08:47<18:09,  2.06batch/s, Batch Loss=0.1013, Avg Loss=0.1232, Time Left=18.98\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1146/3393 [08:47<18:09,  2.06batch/s, Batch Loss=0.1813, Avg Loss=0.1233, Time Left=18.97\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1147/3393 [08:47<18:31,  2.02batch/s, Batch Loss=0.1813, Avg Loss=0.1233, Time Left=18.97\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1147/3393 [08:48<18:31,  2.02batch/s, Batch Loss=0.0420, Avg Loss=0.1232, Time Left=18.97\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1148/3393 [08:48<18:34,  2.01batch/s, Batch Loss=0.0420, Avg Loss=0.1232, Time Left=18.97\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1148/3393 [08:48<18:34,  2.01batch/s, Batch Loss=0.1230, Avg Loss=0.1232, Time Left=18.96\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1149/3393 [08:48<18:13,  2.05batch/s, Batch Loss=0.1230, Avg Loss=0.1232, Time Left=18.96\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1149/3393 [08:49<18:13,  2.05batch/s, Batch Loss=0.0768, Avg Loss=0.1231, Time Left=18.95\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1150/3393 [08:49<18:09,  2.06batch/s, Batch Loss=0.0768, Avg Loss=0.1231, Time Left=18.95\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1150/3393 [08:49<18:09,  2.06batch/s, Batch Loss=0.2073, Avg Loss=0.1232, Time Left=18.94\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1151/3393 [08:49<17:57,  2.08batch/s, Batch Loss=0.2073, Avg Loss=0.1232, Time Left=18.94\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1151/3393 [08:50<17:57,  2.08batch/s, Batch Loss=0.2908, Avg Loss=0.1234, Time Left=18.93\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1152/3393 [08:50<17:48,  2.10batch/s, Batch Loss=0.2908, Avg Loss=0.1234, Time Left=18.93\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1152/3393 [08:50<17:48,  2.10batch/s, Batch Loss=0.4464, Avg Loss=0.1237, Time Left=18.92\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  34%|▎| 1153/3393 [08:50<17:52,  2.09batch/s, Batch Loss=0.4464, Avg Loss=0.1237, Time Left=18.92\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1153/3393 [08:51<17:52,  2.09batch/s, Batch Loss=0.0728, Avg Loss=0.1236, Time Left=18.91\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1154/3393 [08:51<18:10,  2.05batch/s, Batch Loss=0.0728, Avg Loss=0.1236, Time Left=18.91\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1154/3393 [08:51<18:10,  2.05batch/s, Batch Loss=0.1259, Avg Loss=0.1236, Time Left=18.91\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1155/3393 [08:51<18:03,  2.07batch/s, Batch Loss=0.1259, Avg Loss=0.1236, Time Left=18.91\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1155/3393 [08:52<18:03,  2.07batch/s, Batch Loss=0.3751, Avg Loss=0.1239, Time Left=18.90\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1156/3393 [08:52<18:23,  2.03batch/s, Batch Loss=0.3751, Avg Loss=0.1239, Time Left=18.90\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1156/3393 [08:52<18:23,  2.03batch/s, Batch Loss=0.0288, Avg Loss=0.1238, Time Left=18.89\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1157/3393 [08:52<18:18,  2.04batch/s, Batch Loss=0.0288, Avg Loss=0.1238, Time Left=18.89\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1157/3393 [08:53<18:18,  2.04batch/s, Batch Loss=0.1883, Avg Loss=0.1238, Time Left=18.88\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1158/3393 [08:53<18:01,  2.07batch/s, Batch Loss=0.1883, Avg Loss=0.1238, Time Left=18.88\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1158/3393 [08:53<18:01,  2.07batch/s, Batch Loss=0.0614, Avg Loss=0.1238, Time Left=18.87\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1159/3393 [08:53<18:21,  2.03batch/s, Batch Loss=0.0614, Avg Loss=0.1238, Time Left=18.87\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1159/3393 [08:54<18:21,  2.03batch/s, Batch Loss=0.0913, Avg Loss=0.1237, Time Left=18.87\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1160/3393 [08:54<18:16,  2.04batch/s, Batch Loss=0.0913, Avg Loss=0.1237, Time Left=18.87\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1160/3393 [08:54<18:16,  2.04batch/s, Batch Loss=0.2349, Avg Loss=0.1238, Time Left=18.86\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1161/3393 [08:54<18:42,  1.99batch/s, Batch Loss=0.2349, Avg Loss=0.1238, Time Left=18.86\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1161/3393 [08:55<18:42,  1.99batch/s, Batch Loss=0.1241, Avg Loss=0.1238, Time Left=18.85\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1162/3393 [08:55<18:30,  2.01batch/s, Batch Loss=0.1241, Avg Loss=0.1238, Time Left=18.85\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1162/3393 [08:55<18:30,  2.01batch/s, Batch Loss=0.0774, Avg Loss=0.1238, Time Left=18.84\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1163/3393 [08:55<18:10,  2.04batch/s, Batch Loss=0.0774, Avg Loss=0.1238, Time Left=18.84\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1163/3393 [08:56<18:10,  2.04batch/s, Batch Loss=0.2747, Avg Loss=0.1239, Time Left=18.83\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1164/3393 [08:56<18:05,  2.05batch/s, Batch Loss=0.2747, Avg Loss=0.1239, Time Left=18.83\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1164/3393 [08:56<18:05,  2.05batch/s, Batch Loss=0.1369, Avg Loss=0.1239, Time Left=18.82\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1165/3393 [08:56<18:13,  2.04batch/s, Batch Loss=0.1369, Avg Loss=0.1239, Time Left=18.82\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1165/3393 [08:57<18:13,  2.04batch/s, Batch Loss=0.1588, Avg Loss=0.1240, Time Left=18.82\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1166/3393 [08:57<18:08,  2.05batch/s, Batch Loss=0.1588, Avg Loss=0.1240, Time Left=18.82\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1166/3393 [08:57<18:08,  2.05batch/s, Batch Loss=0.1988, Avg Loss=0.1240, Time Left=18.81\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1167/3393 [08:57<18:05,  2.05batch/s, Batch Loss=0.1988, Avg Loss=0.1240, Time Left=18.81\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1167/3393 [08:58<18:05,  2.05batch/s, Batch Loss=0.0929, Avg Loss=0.1240, Time Left=18.80\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1168/3393 [08:58<18:01,  2.06batch/s, Batch Loss=0.0929, Avg Loss=0.1240, Time Left=18.80\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1168/3393 [08:58<18:01,  2.06batch/s, Batch Loss=0.0731, Avg Loss=0.1240, Time Left=18.79\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1169/3393 [08:58<18:09,  2.04batch/s, Batch Loss=0.0731, Avg Loss=0.1240, Time Left=18.79\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1169/3393 [08:59<18:09,  2.04batch/s, Batch Loss=0.1505, Avg Loss=0.1240, Time Left=18.78\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1170/3393 [08:59<18:06,  2.05batch/s, Batch Loss=0.1505, Avg Loss=0.1240, Time Left=18.78\u001b[A\n",
      "Epoch 2/3 - Training:  34%|▎| 1170/3393 [08:59<18:06,  2.05batch/s, Batch Loss=0.2238, Avg Loss=0.1241, Time Left=18.78\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1171/3393 [08:59<18:02,  2.05batch/s, Batch Loss=0.2238, Avg Loss=0.1241, Time Left=18.78\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1171/3393 [09:00<18:02,  2.05batch/s, Batch Loss=0.1349, Avg Loss=0.1241, Time Left=18.77\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1172/3393 [09:00<17:59,  2.06batch/s, Batch Loss=0.1349, Avg Loss=0.1241, Time Left=18.77\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1172/3393 [09:00<17:59,  2.06batch/s, Batch Loss=0.0260, Avg Loss=0.1240, Time Left=18.76\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1173/3393 [09:00<17:57,  2.06batch/s, Batch Loss=0.0260, Avg Loss=0.1240, Time Left=18.76\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1173/3393 [09:01<17:57,  2.06batch/s, Batch Loss=0.1267, Avg Loss=0.1240, Time Left=18.75\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1174/3393 [09:01<18:27,  2.00batch/s, Batch Loss=0.1267, Avg Loss=0.1240, Time Left=18.75\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1174/3393 [09:01<18:27,  2.00batch/s, Batch Loss=0.0461, Avg Loss=0.1239, Time Left=18.74\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1175/3393 [09:01<18:07,  2.04batch/s, Batch Loss=0.0461, Avg Loss=0.1239, Time Left=18.74\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1175/3393 [09:02<18:07,  2.04batch/s, Batch Loss=0.2952, Avg Loss=0.1241, Time Left=18.73\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1176/3393 [09:02<18:13,  2.03batch/s, Batch Loss=0.2952, Avg Loss=0.1241, Time Left=18.73\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1176/3393 [09:02<18:13,  2.03batch/s, Batch Loss=0.0537, Avg Loss=0.1240, Time Left=18.73\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1177/3393 [09:02<17:56,  2.06batch/s, Batch Loss=0.0537, Avg Loss=0.1240, Time Left=18.73\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1177/3393 [09:03<17:56,  2.06batch/s, Batch Loss=0.1684, Avg Loss=0.1241, Time Left=18.72\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1178/3393 [09:03<17:54,  2.06batch/s, Batch Loss=0.1684, Avg Loss=0.1241, Time Left=18.72\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1178/3393 [09:03<17:54,  2.06batch/s, Batch Loss=0.0357, Avg Loss=0.1240, Time Left=18.71\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1179/3393 [09:03<18:24,  2.00batch/s, Batch Loss=0.0357, Avg Loss=0.1240, Time Left=18.71\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1179/3393 [09:04<18:24,  2.00batch/s, Batch Loss=0.0081, Avg Loss=0.1239, Time Left=18.70\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1180/3393 [09:04<18:26,  2.00batch/s, Batch Loss=0.0081, Avg Loss=0.1239, Time Left=18.70\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1180/3393 [09:04<18:26,  2.00batch/s, Batch Loss=0.2601, Avg Loss=0.1240, Time Left=18.69\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1181/3393 [09:04<18:04,  2.04batch/s, Batch Loss=0.2601, Avg Loss=0.1240, Time Left=18.69\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1181/3393 [09:05<18:04,  2.04batch/s, Batch Loss=0.0058, Avg Loss=0.1239, Time Left=18.68\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1182/3393 [09:05<18:00,  2.05batch/s, Batch Loss=0.0058, Avg Loss=0.1239, Time Left=18.68\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1182/3393 [09:05<18:00,  2.05batch/s, Batch Loss=0.0780, Avg Loss=0.1239, Time Left=18.68\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1183/3393 [09:05<17:34,  2.10batch/s, Batch Loss=0.0780, Avg Loss=0.1239, Time Left=18.68\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1183/3393 [09:06<17:34,  2.10batch/s, Batch Loss=0.0472, Avg Loss=0.1238, Time Left=18.67\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1184/3393 [09:06<17:39,  2.09batch/s, Batch Loss=0.0472, Avg Loss=0.1238, Time Left=18.67\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1184/3393 [09:06<17:39,  2.09batch/s, Batch Loss=0.2779, Avg Loss=0.1239, Time Left=18.66\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1185/3393 [09:06<17:59,  2.04batch/s, Batch Loss=0.2779, Avg Loss=0.1239, Time Left=18.66\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1185/3393 [09:07<17:59,  2.04batch/s, Batch Loss=0.2096, Avg Loss=0.1240, Time Left=18.65\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  35%|▎| 1186/3393 [09:07<17:46,  2.07batch/s, Batch Loss=0.2096, Avg Loss=0.1240, Time Left=18.65\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1186/3393 [09:07<17:46,  2.07batch/s, Batch Loss=0.1216, Avg Loss=0.1240, Time Left=18.64\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1187/3393 [09:07<18:17,  2.01batch/s, Batch Loss=0.1216, Avg Loss=0.1240, Time Left=18.64\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1187/3393 [09:07<18:17,  2.01batch/s, Batch Loss=0.0387, Avg Loss=0.1239, Time Left=18.63\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1188/3393 [09:08<17:49,  2.06batch/s, Batch Loss=0.0387, Avg Loss=0.1239, Time Left=18.63\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1188/3393 [09:08<17:49,  2.06batch/s, Batch Loss=0.1250, Avg Loss=0.1239, Time Left=18.63\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1189/3393 [09:08<17:57,  2.05batch/s, Batch Loss=0.1250, Avg Loss=0.1239, Time Left=18.63\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1189/3393 [09:08<17:57,  2.05batch/s, Batch Loss=0.0219, Avg Loss=0.1238, Time Left=18.62\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1190/3393 [09:08<18:03,  2.03batch/s, Batch Loss=0.0219, Avg Loss=0.1238, Time Left=18.62\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1190/3393 [09:09<18:03,  2.03batch/s, Batch Loss=0.0278, Avg Loss=0.1238, Time Left=18.61\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1191/3393 [09:09<17:58,  2.04batch/s, Batch Loss=0.0278, Avg Loss=0.1238, Time Left=18.61\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1191/3393 [09:09<17:58,  2.04batch/s, Batch Loss=0.1252, Avg Loss=0.1238, Time Left=18.60\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1192/3393 [09:09<18:04,  2.03batch/s, Batch Loss=0.1252, Avg Loss=0.1238, Time Left=18.60\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1192/3393 [09:10<18:04,  2.03batch/s, Batch Loss=0.0082, Avg Loss=0.1237, Time Left=18.59\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1193/3393 [09:10<18:10,  2.02batch/s, Batch Loss=0.0082, Avg Loss=0.1237, Time Left=18.59\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1193/3393 [09:10<18:10,  2.02batch/s, Batch Loss=0.2778, Avg Loss=0.1238, Time Left=18.59\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1194/3393 [09:10<17:50,  2.05batch/s, Batch Loss=0.2778, Avg Loss=0.1238, Time Left=18.59\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1194/3393 [09:11<17:50,  2.05batch/s, Batch Loss=0.0431, Avg Loss=0.1237, Time Left=18.58\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1195/3393 [09:11<17:50,  2.05batch/s, Batch Loss=0.0431, Avg Loss=0.1237, Time Left=18.58\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1195/3393 [09:11<17:50,  2.05batch/s, Batch Loss=0.1850, Avg Loss=0.1238, Time Left=18.57\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1196/3393 [09:11<17:34,  2.08batch/s, Batch Loss=0.1850, Avg Loss=0.1238, Time Left=18.57\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1196/3393 [09:12<17:34,  2.08batch/s, Batch Loss=0.0327, Avg Loss=0.1237, Time Left=18.56\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1197/3393 [09:12<17:36,  2.08batch/s, Batch Loss=0.0327, Avg Loss=0.1237, Time Left=18.56\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1197/3393 [09:12<17:36,  2.08batch/s, Batch Loss=0.1402, Avg Loss=0.1237, Time Left=18.55\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1198/3393 [09:12<17:44,  2.06batch/s, Batch Loss=0.1402, Avg Loss=0.1237, Time Left=18.55\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1198/3393 [09:13<17:44,  2.06batch/s, Batch Loss=0.0058, Avg Loss=0.1236, Time Left=18.54\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1199/3393 [09:13<17:36,  2.08batch/s, Batch Loss=0.0058, Avg Loss=0.1236, Time Left=18.54\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1199/3393 [09:13<17:36,  2.08batch/s, Batch Loss=0.1249, Avg Loss=0.1236, Time Left=18.54\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1200/3393 [09:13<17:46,  2.06batch/s, Batch Loss=0.1249, Avg Loss=0.1236, Time Left=18.54\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1200/3393 [09:14<17:46,  2.06batch/s, Batch Loss=0.0771, Avg Loss=0.1236, Time Left=18.53\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1201/3393 [09:14<17:35,  2.08batch/s, Batch Loss=0.0771, Avg Loss=0.1236, Time Left=18.53\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1201/3393 [09:14<17:35,  2.08batch/s, Batch Loss=0.0971, Avg Loss=0.1235, Time Left=18.52\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1202/3393 [09:14<17:36,  2.07batch/s, Batch Loss=0.0971, Avg Loss=0.1235, Time Left=18.52\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1202/3393 [09:15<17:36,  2.07batch/s, Batch Loss=0.0539, Avg Loss=0.1235, Time Left=18.51\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1203/3393 [09:15<17:57,  2.03batch/s, Batch Loss=0.0539, Avg Loss=0.1235, Time Left=18.51\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1203/3393 [09:15<17:57,  2.03batch/s, Batch Loss=0.0919, Avg Loss=0.1234, Time Left=18.50\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1204/3393 [09:15<17:54,  2.04batch/s, Batch Loss=0.0919, Avg Loss=0.1234, Time Left=18.50\u001b[A\n",
      "Epoch 2/3 - Training:  35%|▎| 1204/3393 [09:16<17:54,  2.04batch/s, Batch Loss=0.1244, Avg Loss=0.1235, Time Left=18.49\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1205/3393 [09:16<17:47,  2.05batch/s, Batch Loss=0.1244, Avg Loss=0.1235, Time Left=18.49\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1205/3393 [09:16<17:47,  2.05batch/s, Batch Loss=0.0370, Avg Loss=0.1234, Time Left=18.49\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1206/3393 [09:16<17:44,  2.05batch/s, Batch Loss=0.0370, Avg Loss=0.1234, Time Left=18.49\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1206/3393 [09:17<17:44,  2.05batch/s, Batch Loss=0.0994, Avg Loss=0.1234, Time Left=18.48\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1207/3393 [09:17<17:42,  2.06batch/s, Batch Loss=0.0994, Avg Loss=0.1234, Time Left=18.48\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1207/3393 [09:17<17:42,  2.06batch/s, Batch Loss=0.0602, Avg Loss=0.1233, Time Left=18.47\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1208/3393 [09:17<18:00,  2.02batch/s, Batch Loss=0.0602, Avg Loss=0.1233, Time Left=18.47\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1208/3393 [09:18<18:00,  2.02batch/s, Batch Loss=0.2145, Avg Loss=0.1234, Time Left=18.46\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1209/3393 [09:18<18:04,  2.01batch/s, Batch Loss=0.2145, Avg Loss=0.1234, Time Left=18.46\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1209/3393 [09:18<18:04,  2.01batch/s, Batch Loss=0.0644, Avg Loss=0.1233, Time Left=18.45\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1210/3393 [09:18<18:26,  1.97batch/s, Batch Loss=0.0644, Avg Loss=0.1233, Time Left=18.45\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1210/3393 [09:19<18:26,  1.97batch/s, Batch Loss=0.1090, Avg Loss=0.1233, Time Left=18.45\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1211/3393 [09:19<18:27,  1.97batch/s, Batch Loss=0.1090, Avg Loss=0.1233, Time Left=18.45\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1211/3393 [09:19<18:27,  1.97batch/s, Batch Loss=0.0401, Avg Loss=0.1232, Time Left=18.44\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1212/3393 [09:19<18:28,  1.97batch/s, Batch Loss=0.0401, Avg Loss=0.1232, Time Left=18.44\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1212/3393 [09:20<18:28,  1.97batch/s, Batch Loss=0.0361, Avg Loss=0.1232, Time Left=18.43\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1213/3393 [09:20<18:24,  1.97batch/s, Batch Loss=0.0361, Avg Loss=0.1232, Time Left=18.43\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1213/3393 [09:20<18:24,  1.97batch/s, Batch Loss=0.4899, Avg Loss=0.1235, Time Left=18.42\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1214/3393 [09:20<18:29,  1.96batch/s, Batch Loss=0.4899, Avg Loss=0.1235, Time Left=18.42\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1214/3393 [09:21<18:29,  1.96batch/s, Batch Loss=0.0300, Avg Loss=0.1234, Time Left=18.42\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1215/3393 [09:21<18:24,  1.97batch/s, Batch Loss=0.0300, Avg Loss=0.1234, Time Left=18.42\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1215/3393 [09:21<18:24,  1.97batch/s, Batch Loss=0.0096, Avg Loss=0.1233, Time Left=18.41\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1216/3393 [09:21<18:08,  2.00batch/s, Batch Loss=0.0096, Avg Loss=0.1233, Time Left=18.41\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1216/3393 [09:22<18:08,  2.00batch/s, Batch Loss=0.0218, Avg Loss=0.1232, Time Left=18.40\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1217/3393 [09:22<17:57,  2.02batch/s, Batch Loss=0.0218, Avg Loss=0.1232, Time Left=18.40\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1217/3393 [09:22<17:57,  2.02batch/s, Batch Loss=0.0422, Avg Loss=0.1231, Time Left=18.39\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1218/3393 [09:22<17:50,  2.03batch/s, Batch Loss=0.0422, Avg Loss=0.1231, Time Left=18.39\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1218/3393 [09:23<17:50,  2.03batch/s, Batch Loss=0.0256, Avg Loss=0.1231, Time Left=18.38\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  36%|▎| 1219/3393 [09:23<18:03,  2.01batch/s, Batch Loss=0.0256, Avg Loss=0.1231, Time Left=18.38\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1219/3393 [09:23<18:03,  2.01batch/s, Batch Loss=0.0155, Avg Loss=0.1230, Time Left=18.38\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1220/3393 [09:23<17:55,  2.02batch/s, Batch Loss=0.0155, Avg Loss=0.1230, Time Left=18.38\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1220/3393 [09:24<17:55,  2.02batch/s, Batch Loss=0.0170, Avg Loss=0.1229, Time Left=18.37\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1221/3393 [09:24<17:36,  2.06batch/s, Batch Loss=0.0170, Avg Loss=0.1229, Time Left=18.37\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1221/3393 [09:24<17:36,  2.06batch/s, Batch Loss=0.2838, Avg Loss=0.1230, Time Left=18.36\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1222/3393 [09:24<17:35,  2.06batch/s, Batch Loss=0.2838, Avg Loss=0.1230, Time Left=18.36\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1222/3393 [09:25<17:35,  2.06batch/s, Batch Loss=0.0363, Avg Loss=0.1229, Time Left=18.35\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1223/3393 [09:25<17:23,  2.08batch/s, Batch Loss=0.0363, Avg Loss=0.1229, Time Left=18.35\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1223/3393 [09:25<17:23,  2.08batch/s, Batch Loss=0.0503, Avg Loss=0.1229, Time Left=18.34\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1224/3393 [09:25<17:22,  2.08batch/s, Batch Loss=0.0503, Avg Loss=0.1229, Time Left=18.34\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1224/3393 [09:26<17:22,  2.08batch/s, Batch Loss=0.0209, Avg Loss=0.1228, Time Left=18.33\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1225/3393 [09:26<17:44,  2.04batch/s, Batch Loss=0.0209, Avg Loss=0.1228, Time Left=18.33\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1225/3393 [09:26<17:44,  2.04batch/s, Batch Loss=0.0105, Avg Loss=0.1227, Time Left=18.32\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1226/3393 [09:26<17:30,  2.06batch/s, Batch Loss=0.0105, Avg Loss=0.1227, Time Left=18.32\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1226/3393 [09:27<17:30,  2.06batch/s, Batch Loss=0.1050, Avg Loss=0.1227, Time Left=18.32\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1227/3393 [09:27<17:49,  2.03batch/s, Batch Loss=0.1050, Avg Loss=0.1227, Time Left=18.32\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1227/3393 [09:27<17:49,  2.03batch/s, Batch Loss=0.0706, Avg Loss=0.1226, Time Left=18.31\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1228/3393 [09:27<17:43,  2.04batch/s, Batch Loss=0.0706, Avg Loss=0.1226, Time Left=18.31\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1228/3393 [09:28<17:43,  2.04batch/s, Batch Loss=0.0801, Avg Loss=0.1226, Time Left=18.30\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1229/3393 [09:28<17:39,  2.04batch/s, Batch Loss=0.0801, Avg Loss=0.1226, Time Left=18.30\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1229/3393 [09:28<17:39,  2.04batch/s, Batch Loss=0.0454, Avg Loss=0.1225, Time Left=18.29\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1230/3393 [09:28<17:24,  2.07batch/s, Batch Loss=0.0454, Avg Loss=0.1225, Time Left=18.29\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1230/3393 [09:29<17:24,  2.07batch/s, Batch Loss=0.0062, Avg Loss=0.1224, Time Left=18.28\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1231/3393 [09:29<17:33,  2.05batch/s, Batch Loss=0.0062, Avg Loss=0.1224, Time Left=18.28\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1231/3393 [09:29<17:33,  2.05batch/s, Batch Loss=0.0210, Avg Loss=0.1223, Time Left=18.28\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1232/3393 [09:29<17:51,  2.02batch/s, Batch Loss=0.0210, Avg Loss=0.1223, Time Left=18.28\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1232/3393 [09:30<17:51,  2.02batch/s, Batch Loss=0.4565, Avg Loss=0.1226, Time Left=18.27\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1233/3393 [09:30<17:34,  2.05batch/s, Batch Loss=0.4565, Avg Loss=0.1226, Time Left=18.27\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1233/3393 [09:30<17:34,  2.05batch/s, Batch Loss=0.0138, Avg Loss=0.1225, Time Left=18.26\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1234/3393 [09:30<17:40,  2.03batch/s, Batch Loss=0.0138, Avg Loss=0.1225, Time Left=18.26\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1234/3393 [09:31<17:40,  2.03batch/s, Batch Loss=0.0656, Avg Loss=0.1225, Time Left=18.25\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1235/3393 [09:31<17:45,  2.03batch/s, Batch Loss=0.0656, Avg Loss=0.1225, Time Left=18.25\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1235/3393 [09:31<17:45,  2.03batch/s, Batch Loss=0.0286, Avg Loss=0.1224, Time Left=18.24\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1236/3393 [09:31<17:29,  2.06batch/s, Batch Loss=0.0286, Avg Loss=0.1224, Time Left=18.24\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1236/3393 [09:32<17:29,  2.06batch/s, Batch Loss=0.0077, Avg Loss=0.1223, Time Left=18.23\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1237/3393 [09:32<17:17,  2.08batch/s, Batch Loss=0.0077, Avg Loss=0.1223, Time Left=18.23\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1237/3393 [09:32<17:17,  2.08batch/s, Batch Loss=0.1924, Avg Loss=0.1224, Time Left=18.22\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1238/3393 [09:32<17:26,  2.06batch/s, Batch Loss=0.1924, Avg Loss=0.1224, Time Left=18.22\u001b[A\n",
      "Epoch 2/3 - Training:  36%|▎| 1238/3393 [09:33<17:26,  2.06batch/s, Batch Loss=0.3713, Avg Loss=0.1226, Time Left=18.22\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1239/3393 [09:33<17:25,  2.06batch/s, Batch Loss=0.3713, Avg Loss=0.1226, Time Left=18.22\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1239/3393 [09:33<17:25,  2.06batch/s, Batch Loss=0.1677, Avg Loss=0.1226, Time Left=18.21\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1240/3393 [09:33<17:44,  2.02batch/s, Batch Loss=0.1677, Avg Loss=0.1226, Time Left=18.21\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1240/3393 [09:34<17:44,  2.02batch/s, Batch Loss=0.2222, Avg Loss=0.1227, Time Left=18.20\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1241/3393 [09:34<17:38,  2.03batch/s, Batch Loss=0.2222, Avg Loss=0.1227, Time Left=18.20\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1241/3393 [09:34<17:38,  2.03batch/s, Batch Loss=0.0958, Avg Loss=0.1227, Time Left=18.19\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1242/3393 [09:34<17:22,  2.06batch/s, Batch Loss=0.0958, Avg Loss=0.1227, Time Left=18.19\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1242/3393 [09:34<17:22,  2.06batch/s, Batch Loss=0.0098, Avg Loss=0.1226, Time Left=18.18\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1243/3393 [09:34<17:21,  2.06batch/s, Batch Loss=0.0098, Avg Loss=0.1226, Time Left=18.18\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1243/3393 [09:35<17:21,  2.06batch/s, Batch Loss=0.2545, Avg Loss=0.1227, Time Left=18.17\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1244/3393 [09:35<17:10,  2.09batch/s, Batch Loss=0.2545, Avg Loss=0.1227, Time Left=18.17\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1244/3393 [09:35<17:10,  2.09batch/s, Batch Loss=0.1844, Avg Loss=0.1227, Time Left=18.17\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1245/3393 [09:35<17:22,  2.06batch/s, Batch Loss=0.1844, Avg Loss=0.1227, Time Left=18.17\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1245/3393 [09:36<17:22,  2.06batch/s, Batch Loss=0.1268, Avg Loss=0.1228, Time Left=18.16\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1246/3393 [09:36<17:10,  2.08batch/s, Batch Loss=0.1268, Avg Loss=0.1228, Time Left=18.16\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1246/3393 [09:36<17:10,  2.08batch/s, Batch Loss=0.0634, Avg Loss=0.1227, Time Left=18.15\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1247/3393 [09:36<17:02,  2.10batch/s, Batch Loss=0.0634, Avg Loss=0.1227, Time Left=18.15\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1247/3393 [09:37<17:02,  2.10batch/s, Batch Loss=0.0040, Avg Loss=0.1226, Time Left=18.14\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1248/3393 [09:37<17:15,  2.07batch/s, Batch Loss=0.0040, Avg Loss=0.1226, Time Left=18.14\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1248/3393 [09:37<17:15,  2.07batch/s, Batch Loss=0.0813, Avg Loss=0.1226, Time Left=18.13\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1249/3393 [09:37<17:05,  2.09batch/s, Batch Loss=0.0813, Avg Loss=0.1226, Time Left=18.13\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1249/3393 [09:38<17:05,  2.09batch/s, Batch Loss=0.1022, Avg Loss=0.1225, Time Left=18.12\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1250/3393 [09:38<17:08,  2.08batch/s, Batch Loss=0.1022, Avg Loss=0.1225, Time Left=18.12\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1250/3393 [09:38<17:08,  2.08batch/s, Batch Loss=0.3838, Avg Loss=0.1228, Time Left=18.12\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1251/3393 [09:38<17:41,  2.02batch/s, Batch Loss=0.3838, Avg Loss=0.1228, Time Left=18.12\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1251/3393 [09:39<17:41,  2.02batch/s, Batch Loss=0.2664, Avg Loss=0.1229, Time Left=18.11\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  37%|▎| 1252/3393 [09:39<17:34,  2.03batch/s, Batch Loss=0.2664, Avg Loss=0.1229, Time Left=18.11\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1252/3393 [09:39<17:34,  2.03batch/s, Batch Loss=0.0438, Avg Loss=0.1228, Time Left=18.10\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1253/3393 [09:39<17:38,  2.02batch/s, Batch Loss=0.0438, Avg Loss=0.1228, Time Left=18.10\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1253/3393 [09:40<17:38,  2.02batch/s, Batch Loss=0.3341, Avg Loss=0.1230, Time Left=18.09\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1254/3393 [09:40<17:41,  2.01batch/s, Batch Loss=0.3341, Avg Loss=0.1230, Time Left=18.09\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1254/3393 [09:40<17:41,  2.01batch/s, Batch Loss=0.0595, Avg Loss=0.1229, Time Left=18.08\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1255/3393 [09:40<17:33,  2.03batch/s, Batch Loss=0.0595, Avg Loss=0.1229, Time Left=18.08\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1255/3393 [09:41<17:33,  2.03batch/s, Batch Loss=0.2956, Avg Loss=0.1231, Time Left=18.08\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1256/3393 [09:41<17:47,  2.00batch/s, Batch Loss=0.2956, Avg Loss=0.1231, Time Left=18.08\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1256/3393 [09:41<17:47,  2.00batch/s, Batch Loss=0.0029, Avg Loss=0.1230, Time Left=18.07\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1257/3393 [09:41<17:38,  2.02batch/s, Batch Loss=0.0029, Avg Loss=0.1230, Time Left=18.07\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1257/3393 [09:42<17:38,  2.02batch/s, Batch Loss=0.0232, Avg Loss=0.1229, Time Left=18.06\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1258/3393 [09:42<17:51,  1.99batch/s, Batch Loss=0.0232, Avg Loss=0.1229, Time Left=18.06\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1258/3393 [09:42<17:51,  1.99batch/s, Batch Loss=0.0800, Avg Loss=0.1229, Time Left=18.05\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1259/3393 [09:42<17:19,  2.05batch/s, Batch Loss=0.0800, Avg Loss=0.1229, Time Left=18.05\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1259/3393 [09:43<17:19,  2.05batch/s, Batch Loss=0.0133, Avg Loss=0.1228, Time Left=18.04\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1260/3393 [09:43<17:16,  2.06batch/s, Batch Loss=0.0133, Avg Loss=0.1228, Time Left=18.04\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1260/3393 [09:43<17:16,  2.06batch/s, Batch Loss=0.0365, Avg Loss=0.1227, Time Left=18.04\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1261/3393 [09:43<17:33,  2.02batch/s, Batch Loss=0.0365, Avg Loss=0.1227, Time Left=18.04\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1261/3393 [09:44<17:33,  2.02batch/s, Batch Loss=0.0995, Avg Loss=0.1227, Time Left=18.03\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1262/3393 [09:44<17:37,  2.01batch/s, Batch Loss=0.0995, Avg Loss=0.1227, Time Left=18.03\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1262/3393 [09:44<17:37,  2.01batch/s, Batch Loss=0.4730, Avg Loss=0.1230, Time Left=18.02\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1263/3393 [09:44<17:32,  2.02batch/s, Batch Loss=0.4730, Avg Loss=0.1230, Time Left=18.02\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1263/3393 [09:45<17:32,  2.02batch/s, Batch Loss=0.0122, Avg Loss=0.1229, Time Left=18.01\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1264/3393 [09:45<17:12,  2.06batch/s, Batch Loss=0.0122, Avg Loss=0.1229, Time Left=18.01\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1264/3393 [09:45<17:12,  2.06batch/s, Batch Loss=0.1009, Avg Loss=0.1229, Time Left=18.00\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1265/3393 [09:45<17:11,  2.06batch/s, Batch Loss=0.1009, Avg Loss=0.1229, Time Left=18.00\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1265/3393 [09:46<17:11,  2.06batch/s, Batch Loss=0.0398, Avg Loss=0.1228, Time Left=18.00\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1266/3393 [09:46<17:29,  2.03batch/s, Batch Loss=0.0398, Avg Loss=0.1228, Time Left=18.00\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1266/3393 [09:46<17:29,  2.03batch/s, Batch Loss=0.0488, Avg Loss=0.1227, Time Left=17.99\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1267/3393 [09:46<17:25,  2.03batch/s, Batch Loss=0.0488, Avg Loss=0.1227, Time Left=17.99\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1267/3393 [09:47<17:25,  2.03batch/s, Batch Loss=0.1318, Avg Loss=0.1227, Time Left=17.98\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1268/3393 [09:47<17:49,  1.99batch/s, Batch Loss=0.1318, Avg Loss=0.1227, Time Left=17.98\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1268/3393 [09:47<17:49,  1.99batch/s, Batch Loss=0.1512, Avg Loss=0.1228, Time Left=17.97\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1269/3393 [09:47<17:36,  2.01batch/s, Batch Loss=0.1512, Avg Loss=0.1228, Time Left=17.97\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1269/3393 [09:48<17:36,  2.01batch/s, Batch Loss=0.0213, Avg Loss=0.1227, Time Left=17.96\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1270/3393 [09:48<17:48,  1.99batch/s, Batch Loss=0.0213, Avg Loss=0.1227, Time Left=17.96\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1270/3393 [09:48<17:48,  1.99batch/s, Batch Loss=0.3796, Avg Loss=0.1229, Time Left=17.96\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1271/3393 [09:48<17:37,  2.01batch/s, Batch Loss=0.3796, Avg Loss=0.1229, Time Left=17.96\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1271/3393 [09:49<17:37,  2.01batch/s, Batch Loss=0.0842, Avg Loss=0.1229, Time Left=17.95\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1272/3393 [09:49<17:36,  2.01batch/s, Batch Loss=0.0842, Avg Loss=0.1229, Time Left=17.95\u001b[A\n",
      "Epoch 2/3 - Training:  37%|▎| 1272/3393 [09:49<17:36,  2.01batch/s, Batch Loss=0.1914, Avg Loss=0.1229, Time Left=17.94\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1273/3393 [09:49<17:28,  2.02batch/s, Batch Loss=0.1914, Avg Loss=0.1229, Time Left=17.94\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1273/3393 [09:50<17:28,  2.02batch/s, Batch Loss=0.0826, Avg Loss=0.1229, Time Left=17.93\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1274/3393 [09:50<17:20,  2.04batch/s, Batch Loss=0.0826, Avg Loss=0.1229, Time Left=17.93\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1274/3393 [09:50<17:20,  2.04batch/s, Batch Loss=0.0907, Avg Loss=0.1229, Time Left=17.92\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1275/3393 [09:50<17:34,  2.01batch/s, Batch Loss=0.0907, Avg Loss=0.1229, Time Left=17.92\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1275/3393 [09:51<17:34,  2.01batch/s, Batch Loss=0.0727, Avg Loss=0.1228, Time Left=17.92\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1276/3393 [09:51<17:34,  2.01batch/s, Batch Loss=0.0727, Avg Loss=0.1228, Time Left=17.92\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1276/3393 [09:51<17:34,  2.01batch/s, Batch Loss=0.0253, Avg Loss=0.1227, Time Left=17.91\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1277/3393 [09:51<17:47,  1.98batch/s, Batch Loss=0.0253, Avg Loss=0.1227, Time Left=17.91\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1277/3393 [09:52<17:47,  1.98batch/s, Batch Loss=0.1593, Avg Loss=0.1228, Time Left=17.90\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1278/3393 [09:52<18:14,  1.93batch/s, Batch Loss=0.1593, Avg Loss=0.1228, Time Left=17.90\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1278/3393 [09:52<18:14,  1.93batch/s, Batch Loss=0.0472, Avg Loss=0.1227, Time Left=17.89\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1279/3393 [09:52<18:16,  1.93batch/s, Batch Loss=0.0472, Avg Loss=0.1227, Time Left=17.89\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1279/3393 [09:53<18:16,  1.93batch/s, Batch Loss=0.0264, Avg Loss=0.1226, Time Left=17.89\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1280/3393 [09:53<17:53,  1.97batch/s, Batch Loss=0.0264, Avg Loss=0.1226, Time Left=17.89\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1280/3393 [09:53<17:53,  1.97batch/s, Batch Loss=0.1685, Avg Loss=0.1227, Time Left=17.88\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1281/3393 [09:53<17:37,  2.00batch/s, Batch Loss=0.1685, Avg Loss=0.1227, Time Left=17.88\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1281/3393 [09:54<17:37,  2.00batch/s, Batch Loss=0.0083, Avg Loss=0.1226, Time Left=17.87\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1282/3393 [09:54<17:41,  1.99batch/s, Batch Loss=0.0083, Avg Loss=0.1226, Time Left=17.87\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1282/3393 [09:54<17:41,  1.99batch/s, Batch Loss=0.3324, Avg Loss=0.1227, Time Left=17.86\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1283/3393 [09:54<17:35,  2.00batch/s, Batch Loss=0.3324, Avg Loss=0.1227, Time Left=17.86\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1283/3393 [09:55<17:35,  2.00batch/s, Batch Loss=0.1401, Avg Loss=0.1228, Time Left=17.85\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1284/3393 [09:55<17:24,  2.02batch/s, Batch Loss=0.1401, Avg Loss=0.1228, Time Left=17.85\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1284/3393 [09:55<17:24,  2.02batch/s, Batch Loss=0.3502, Avg Loss=0.1229, Time Left=17.84\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  38%|▍| 1285/3393 [09:55<17:17,  2.03batch/s, Batch Loss=0.3502, Avg Loss=0.1229, Time Left=17.84\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1285/3393 [09:56<17:17,  2.03batch/s, Batch Loss=0.2719, Avg Loss=0.1231, Time Left=17.84\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1286/3393 [09:56<17:10,  2.04batch/s, Batch Loss=0.2719, Avg Loss=0.1231, Time Left=17.84\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1286/3393 [09:56<17:10,  2.04batch/s, Batch Loss=0.2258, Avg Loss=0.1232, Time Left=17.83\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1287/3393 [09:56<17:07,  2.05batch/s, Batch Loss=0.2258, Avg Loss=0.1232, Time Left=17.83\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1287/3393 [09:57<17:07,  2.05batch/s, Batch Loss=0.0541, Avg Loss=0.1231, Time Left=17.82\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1288/3393 [09:57<16:53,  2.08batch/s, Batch Loss=0.0541, Avg Loss=0.1231, Time Left=17.82\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1288/3393 [09:57<16:53,  2.08batch/s, Batch Loss=0.0854, Avg Loss=0.1231, Time Left=17.81\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1289/3393 [09:57<17:04,  2.05batch/s, Batch Loss=0.0854, Avg Loss=0.1231, Time Left=17.81\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1289/3393 [09:58<17:04,  2.05batch/s, Batch Loss=0.1301, Avg Loss=0.1231, Time Left=17.80\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1290/3393 [09:58<17:12,  2.04batch/s, Batch Loss=0.1301, Avg Loss=0.1231, Time Left=17.80\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1290/3393 [09:58<17:12,  2.04batch/s, Batch Loss=0.0252, Avg Loss=0.1230, Time Left=17.80\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1291/3393 [09:58<17:18,  2.02batch/s, Batch Loss=0.0252, Avg Loss=0.1230, Time Left=17.80\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1291/3393 [09:59<17:18,  2.02batch/s, Batch Loss=0.0382, Avg Loss=0.1229, Time Left=17.79\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1292/3393 [09:59<17:11,  2.04batch/s, Batch Loss=0.0382, Avg Loss=0.1229, Time Left=17.79\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1292/3393 [09:59<17:11,  2.04batch/s, Batch Loss=0.0520, Avg Loss=0.1229, Time Left=17.78\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1293/3393 [09:59<16:57,  2.06batch/s, Batch Loss=0.0520, Avg Loss=0.1229, Time Left=17.78\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1293/3393 [10:00<16:57,  2.06batch/s, Batch Loss=0.0461, Avg Loss=0.1228, Time Left=17.77\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1294/3393 [10:00<17:15,  2.03batch/s, Batch Loss=0.0461, Avg Loss=0.1228, Time Left=17.77\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1294/3393 [10:00<17:15,  2.03batch/s, Batch Loss=0.0746, Avg Loss=0.1228, Time Left=17.76\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1295/3393 [10:00<17:00,  2.06batch/s, Batch Loss=0.0746, Avg Loss=0.1228, Time Left=17.76\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1295/3393 [10:01<17:00,  2.06batch/s, Batch Loss=0.0649, Avg Loss=0.1227, Time Left=17.75\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1296/3393 [10:01<17:06,  2.04batch/s, Batch Loss=0.0649, Avg Loss=0.1227, Time Left=17.75\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1296/3393 [10:01<17:06,  2.04batch/s, Batch Loss=0.2678, Avg Loss=0.1228, Time Left=17.75\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1297/3393 [10:01<16:52,  2.07batch/s, Batch Loss=0.2678, Avg Loss=0.1228, Time Left=17.75\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1297/3393 [10:02<16:52,  2.07batch/s, Batch Loss=0.0386, Avg Loss=0.1228, Time Left=17.74\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1298/3393 [10:02<16:43,  2.09batch/s, Batch Loss=0.0386, Avg Loss=0.1228, Time Left=17.74\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1298/3393 [10:02<16:43,  2.09batch/s, Batch Loss=0.0551, Avg Loss=0.1227, Time Left=17.73\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1299/3393 [10:02<17:05,  2.04batch/s, Batch Loss=0.0551, Avg Loss=0.1227, Time Left=17.73\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1299/3393 [10:03<17:05,  2.04batch/s, Batch Loss=0.0106, Avg Loss=0.1226, Time Left=17.72\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1300/3393 [10:03<17:02,  2.05batch/s, Batch Loss=0.0106, Avg Loss=0.1226, Time Left=17.72\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1300/3393 [10:03<17:02,  2.05batch/s, Batch Loss=0.1349, Avg Loss=0.1226, Time Left=17.71\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1301/3393 [10:03<16:49,  2.07batch/s, Batch Loss=0.1349, Avg Loss=0.1226, Time Left=17.71\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1301/3393 [10:04<16:49,  2.07batch/s, Batch Loss=0.1851, Avg Loss=0.1227, Time Left=17.70\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1302/3393 [10:04<17:18,  2.01batch/s, Batch Loss=0.1851, Avg Loss=0.1227, Time Left=17.70\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1302/3393 [10:04<17:18,  2.01batch/s, Batch Loss=0.0486, Avg Loss=0.1226, Time Left=17.70\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1303/3393 [10:04<17:11,  2.03batch/s, Batch Loss=0.0486, Avg Loss=0.1226, Time Left=17.70\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1303/3393 [10:05<17:11,  2.03batch/s, Batch Loss=0.1511, Avg Loss=0.1226, Time Left=17.69\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1304/3393 [10:05<17:14,  2.02batch/s, Batch Loss=0.1511, Avg Loss=0.1226, Time Left=17.69\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1304/3393 [10:05<17:14,  2.02batch/s, Batch Loss=0.2165, Avg Loss=0.1227, Time Left=17.68\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1305/3393 [10:05<17:17,  2.01batch/s, Batch Loss=0.2165, Avg Loss=0.1227, Time Left=17.68\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1305/3393 [10:06<17:17,  2.01batch/s, Batch Loss=0.0045, Avg Loss=0.1226, Time Left=17.67\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1306/3393 [10:06<16:58,  2.05batch/s, Batch Loss=0.0045, Avg Loss=0.1226, Time Left=17.67\u001b[A\n",
      "Epoch 2/3 - Training:  38%|▍| 1306/3393 [10:06<16:58,  2.05batch/s, Batch Loss=0.4201, Avg Loss=0.1229, Time Left=17.66\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1307/3393 [10:06<16:56,  2.05batch/s, Batch Loss=0.4201, Avg Loss=0.1229, Time Left=17.66\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1307/3393 [10:06<16:56,  2.05batch/s, Batch Loss=0.0918, Avg Loss=0.1228, Time Left=17.65\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1308/3393 [10:06<16:32,  2.10batch/s, Batch Loss=0.0918, Avg Loss=0.1228, Time Left=17.65\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1308/3393 [10:07<16:32,  2.10batch/s, Batch Loss=0.0460, Avg Loss=0.1228, Time Left=17.65\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1309/3393 [10:07<16:44,  2.07batch/s, Batch Loss=0.0460, Avg Loss=0.1228, Time Left=17.65\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1309/3393 [10:07<16:44,  2.07batch/s, Batch Loss=0.1365, Avg Loss=0.1228, Time Left=17.64\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1310/3393 [10:07<16:46,  2.07batch/s, Batch Loss=0.1365, Avg Loss=0.1228, Time Left=17.64\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1310/3393 [10:08<16:46,  2.07batch/s, Batch Loss=0.2310, Avg Loss=0.1229, Time Left=17.63\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1311/3393 [10:08<16:36,  2.09batch/s, Batch Loss=0.2310, Avg Loss=0.1229, Time Left=17.63\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1311/3393 [10:08<16:36,  2.09batch/s, Batch Loss=0.0583, Avg Loss=0.1228, Time Left=17.62\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1312/3393 [10:08<16:45,  2.07batch/s, Batch Loss=0.0583, Avg Loss=0.1228, Time Left=17.62\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1312/3393 [10:09<16:45,  2.07batch/s, Batch Loss=0.1949, Avg Loss=0.1229, Time Left=17.61\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1313/3393 [10:09<16:39,  2.08batch/s, Batch Loss=0.1949, Avg Loss=0.1229, Time Left=17.61\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1313/3393 [10:09<16:39,  2.08batch/s, Batch Loss=0.0762, Avg Loss=0.1228, Time Left=17.60\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1314/3393 [10:09<16:31,  2.10batch/s, Batch Loss=0.0762, Avg Loss=0.1228, Time Left=17.60\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1314/3393 [10:10<16:31,  2.10batch/s, Batch Loss=0.1330, Avg Loss=0.1229, Time Left=17.60\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1315/3393 [10:10<17:03,  2.03batch/s, Batch Loss=0.1330, Avg Loss=0.1229, Time Left=17.60\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1315/3393 [10:10<17:03,  2.03batch/s, Batch Loss=0.0887, Avg Loss=0.1228, Time Left=17.59\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1316/3393 [10:10<16:58,  2.04batch/s, Batch Loss=0.0887, Avg Loss=0.1228, Time Left=17.59\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1316/3393 [10:11<16:58,  2.04batch/s, Batch Loss=0.0120, Avg Loss=0.1227, Time Left=17.58\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1317/3393 [10:11<16:53,  2.05batch/s, Batch Loss=0.0120, Avg Loss=0.1227, Time Left=17.58\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1317/3393 [10:11<16:53,  2.05batch/s, Batch Loss=0.1224, Avg Loss=0.1227, Time Left=17.57\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  39%|▍| 1318/3393 [10:11<16:50,  2.05batch/s, Batch Loss=0.1224, Avg Loss=0.1227, Time Left=17.57\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1318/3393 [10:12<16:50,  2.05batch/s, Batch Loss=0.1860, Avg Loss=0.1228, Time Left=17.56\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1319/3393 [10:12<16:39,  2.07batch/s, Batch Loss=0.1860, Avg Loss=0.1228, Time Left=17.56\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1319/3393 [10:12<16:39,  2.07batch/s, Batch Loss=0.0100, Avg Loss=0.1227, Time Left=17.55\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1320/3393 [10:12<16:39,  2.07batch/s, Batch Loss=0.0100, Avg Loss=0.1227, Time Left=17.55\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1320/3393 [10:13<16:39,  2.07batch/s, Batch Loss=0.0975, Avg Loss=0.1227, Time Left=17.55\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1321/3393 [10:13<16:49,  2.05batch/s, Batch Loss=0.0975, Avg Loss=0.1227, Time Left=17.55\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1321/3393 [10:13<16:49,  2.05batch/s, Batch Loss=0.0163, Avg Loss=0.1226, Time Left=17.54\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1322/3393 [10:13<16:36,  2.08batch/s, Batch Loss=0.0163, Avg Loss=0.1226, Time Left=17.54\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1322/3393 [10:14<16:36,  2.08batch/s, Batch Loss=0.0497, Avg Loss=0.1225, Time Left=17.53\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1323/3393 [10:14<16:37,  2.08batch/s, Batch Loss=0.0497, Avg Loss=0.1225, Time Left=17.53\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1323/3393 [10:14<16:37,  2.08batch/s, Batch Loss=0.0836, Avg Loss=0.1225, Time Left=17.52\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1324/3393 [10:14<16:32,  2.08batch/s, Batch Loss=0.0836, Avg Loss=0.1225, Time Left=17.52\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1324/3393 [10:15<16:32,  2.08batch/s, Batch Loss=0.1777, Avg Loss=0.1225, Time Left=17.51\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1325/3393 [10:15<16:21,  2.11batch/s, Batch Loss=0.1777, Avg Loss=0.1225, Time Left=17.51\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1325/3393 [10:15<16:21,  2.11batch/s, Batch Loss=0.0250, Avg Loss=0.1225, Time Left=17.50\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1326/3393 [10:15<16:34,  2.08batch/s, Batch Loss=0.0250, Avg Loss=0.1225, Time Left=17.50\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1326/3393 [10:16<16:34,  2.08batch/s, Batch Loss=0.0826, Avg Loss=0.1224, Time Left=17.49\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1327/3393 [10:16<16:26,  2.09batch/s, Batch Loss=0.0826, Avg Loss=0.1224, Time Left=17.49\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1327/3393 [10:16<16:26,  2.09batch/s, Batch Loss=0.1578, Avg Loss=0.1225, Time Left=17.49\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1328/3393 [10:16<16:19,  2.11batch/s, Batch Loss=0.1578, Avg Loss=0.1225, Time Left=17.49\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1328/3393 [10:17<16:19,  2.11batch/s, Batch Loss=0.1899, Avg Loss=0.1225, Time Left=17.48\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1329/3393 [10:17<16:24,  2.10batch/s, Batch Loss=0.1899, Avg Loss=0.1225, Time Left=17.48\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1329/3393 [10:17<16:24,  2.10batch/s, Batch Loss=0.2399, Avg Loss=0.1226, Time Left=17.47\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1330/3393 [10:17<16:28,  2.09batch/s, Batch Loss=0.2399, Avg Loss=0.1226, Time Left=17.47\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1330/3393 [10:18<16:28,  2.09batch/s, Batch Loss=0.0058, Avg Loss=0.1225, Time Left=17.46\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1331/3393 [10:18<16:40,  2.06batch/s, Batch Loss=0.0058, Avg Loss=0.1225, Time Left=17.46\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1331/3393 [10:18<16:40,  2.06batch/s, Batch Loss=0.2375, Avg Loss=0.1226, Time Left=17.45\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1332/3393 [10:18<17:00,  2.02batch/s, Batch Loss=0.2375, Avg Loss=0.1226, Time Left=17.45\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1332/3393 [10:19<17:00,  2.02batch/s, Batch Loss=0.0931, Avg Loss=0.1226, Time Left=17.44\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1333/3393 [10:19<16:52,  2.04batch/s, Batch Loss=0.0931, Avg Loss=0.1226, Time Left=17.44\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1333/3393 [10:19<16:52,  2.04batch/s, Batch Loss=0.1997, Avg Loss=0.1226, Time Left=17.44\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1334/3393 [10:19<16:37,  2.06batch/s, Batch Loss=0.1997, Avg Loss=0.1226, Time Left=17.44\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1334/3393 [10:20<16:37,  2.06batch/s, Batch Loss=0.1756, Avg Loss=0.1227, Time Left=17.43\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1335/3393 [10:20<16:36,  2.07batch/s, Batch Loss=0.1756, Avg Loss=0.1227, Time Left=17.43\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1335/3393 [10:20<16:36,  2.07batch/s, Batch Loss=0.0301, Avg Loss=0.1226, Time Left=17.42\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1336/3393 [10:20<16:34,  2.07batch/s, Batch Loss=0.0301, Avg Loss=0.1226, Time Left=17.42\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1336/3393 [10:20<16:34,  2.07batch/s, Batch Loss=0.0222, Avg Loss=0.1225, Time Left=17.41\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1337/3393 [10:20<16:35,  2.06batch/s, Batch Loss=0.0222, Avg Loss=0.1225, Time Left=17.41\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1337/3393 [10:21<16:35,  2.06batch/s, Batch Loss=0.1222, Avg Loss=0.1225, Time Left=17.40\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1338/3393 [10:21<16:34,  2.07batch/s, Batch Loss=0.1222, Avg Loss=0.1225, Time Left=17.40\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1338/3393 [10:21<16:34,  2.07batch/s, Batch Loss=0.2312, Avg Loss=0.1226, Time Left=17.39\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1339/3393 [10:21<16:34,  2.07batch/s, Batch Loss=0.2312, Avg Loss=0.1226, Time Left=17.39\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1339/3393 [10:22<16:34,  2.07batch/s, Batch Loss=0.0136, Avg Loss=0.1225, Time Left=17.39\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1340/3393 [10:22<17:01,  2.01batch/s, Batch Loss=0.0136, Avg Loss=0.1225, Time Left=17.39\u001b[A\n",
      "Epoch 2/3 - Training:  39%|▍| 1340/3393 [10:22<17:01,  2.01batch/s, Batch Loss=0.0303, Avg Loss=0.1225, Time Left=17.38\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1341/3393 [10:23<17:03,  2.00batch/s, Batch Loss=0.0303, Avg Loss=0.1225, Time Left=17.38\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1341/3393 [10:23<17:03,  2.00batch/s, Batch Loss=0.2447, Avg Loss=0.1226, Time Left=17.37\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1342/3393 [10:23<17:05,  2.00batch/s, Batch Loss=0.2447, Avg Loss=0.1226, Time Left=17.37\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1342/3393 [10:23<17:05,  2.00batch/s, Batch Loss=0.0878, Avg Loss=0.1225, Time Left=17.36\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1343/3393 [10:23<16:54,  2.02batch/s, Batch Loss=0.0878, Avg Loss=0.1225, Time Left=17.36\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1343/3393 [10:24<16:54,  2.02batch/s, Batch Loss=0.2551, Avg Loss=0.1226, Time Left=17.36\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1344/3393 [10:24<17:06,  2.00batch/s, Batch Loss=0.2551, Avg Loss=0.1226, Time Left=17.36\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1344/3393 [10:24<17:06,  2.00batch/s, Batch Loss=0.4680, Avg Loss=0.1229, Time Left=17.35\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1345/3393 [10:24<16:57,  2.01batch/s, Batch Loss=0.4680, Avg Loss=0.1229, Time Left=17.35\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1345/3393 [10:25<16:57,  2.01batch/s, Batch Loss=0.0253, Avg Loss=0.1228, Time Left=17.34\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1346/3393 [10:25<16:59,  2.01batch/s, Batch Loss=0.0253, Avg Loss=0.1228, Time Left=17.34\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1346/3393 [10:25<16:59,  2.01batch/s, Batch Loss=0.0146, Avg Loss=0.1227, Time Left=17.33\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1347/3393 [10:25<16:57,  2.01batch/s, Batch Loss=0.0146, Avg Loss=0.1227, Time Left=17.33\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1347/3393 [10:26<16:57,  2.01batch/s, Batch Loss=0.0278, Avg Loss=0.1227, Time Left=17.32\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1348/3393 [10:26<16:50,  2.02batch/s, Batch Loss=0.0278, Avg Loss=0.1227, Time Left=17.32\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1348/3393 [10:26<16:50,  2.02batch/s, Batch Loss=0.1245, Avg Loss=0.1227, Time Left=17.31\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1349/3393 [10:26<16:51,  2.02batch/s, Batch Loss=0.1245, Avg Loss=0.1227, Time Left=17.31\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1349/3393 [10:27<16:51,  2.02batch/s, Batch Loss=0.1085, Avg Loss=0.1227, Time Left=17.31\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1350/3393 [10:27<16:46,  2.03batch/s, Batch Loss=0.1085, Avg Loss=0.1227, Time Left=17.31\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1350/3393 [10:27<16:46,  2.03batch/s, Batch Loss=0.2610, Avg Loss=0.1228, Time Left=17.30\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  40%|▍| 1351/3393 [10:27<16:49,  2.02batch/s, Batch Loss=0.2610, Avg Loss=0.1228, Time Left=17.30\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1351/3393 [10:28<16:49,  2.02batch/s, Batch Loss=0.0591, Avg Loss=0.1227, Time Left=17.29\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1352/3393 [10:28<16:43,  2.03batch/s, Batch Loss=0.0591, Avg Loss=0.1227, Time Left=17.29\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1352/3393 [10:28<16:43,  2.03batch/s, Batch Loss=0.0522, Avg Loss=0.1227, Time Left=17.28\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1353/3393 [10:28<16:18,  2.08batch/s, Batch Loss=0.0522, Avg Loss=0.1227, Time Left=17.28\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1353/3393 [10:29<16:18,  2.08batch/s, Batch Loss=0.4082, Avg Loss=0.1229, Time Left=17.27\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1354/3393 [10:29<16:28,  2.06batch/s, Batch Loss=0.4082, Avg Loss=0.1229, Time Left=17.27\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1354/3393 [10:29<16:28,  2.06batch/s, Batch Loss=0.2269, Avg Loss=0.1230, Time Left=17.26\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1355/3393 [10:29<16:18,  2.08batch/s, Batch Loss=0.2269, Avg Loss=0.1230, Time Left=17.26\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1355/3393 [10:30<16:18,  2.08batch/s, Batch Loss=0.2675, Avg Loss=0.1231, Time Left=17.26\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1356/3393 [10:30<16:29,  2.06batch/s, Batch Loss=0.2675, Avg Loss=0.1231, Time Left=17.26\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1356/3393 [10:30<16:29,  2.06batch/s, Batch Loss=0.2409, Avg Loss=0.1232, Time Left=17.25\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1357/3393 [10:30<16:56,  2.00batch/s, Batch Loss=0.2409, Avg Loss=0.1232, Time Left=17.25\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1357/3393 [10:31<16:56,  2.00batch/s, Batch Loss=0.1759, Avg Loss=0.1232, Time Left=17.24\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1358/3393 [10:31<16:48,  2.02batch/s, Batch Loss=0.1759, Avg Loss=0.1232, Time Left=17.24\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1358/3393 [10:31<16:48,  2.02batch/s, Batch Loss=0.0832, Avg Loss=0.1232, Time Left=17.23\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1359/3393 [10:31<16:31,  2.05batch/s, Batch Loss=0.0832, Avg Loss=0.1232, Time Left=17.23\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1359/3393 [10:32<16:31,  2.05batch/s, Batch Loss=0.1450, Avg Loss=0.1232, Time Left=17.22\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1360/3393 [10:32<16:28,  2.06batch/s, Batch Loss=0.1450, Avg Loss=0.1232, Time Left=17.22\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1360/3393 [10:32<16:28,  2.06batch/s, Batch Loss=0.1121, Avg Loss=0.1232, Time Left=17.21\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1361/3393 [10:32<16:17,  2.08batch/s, Batch Loss=0.1121, Avg Loss=0.1232, Time Left=17.21\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1361/3393 [10:33<16:17,  2.08batch/s, Batch Loss=0.1238, Avg Loss=0.1232, Time Left=17.21\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1362/3393 [10:33<16:26,  2.06batch/s, Batch Loss=0.1238, Avg Loss=0.1232, Time Left=17.21\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1362/3393 [10:33<16:26,  2.06batch/s, Batch Loss=0.0796, Avg Loss=0.1232, Time Left=17.20\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1363/3393 [10:33<16:26,  2.06batch/s, Batch Loss=0.0796, Avg Loss=0.1232, Time Left=17.20\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1363/3393 [10:34<16:26,  2.06batch/s, Batch Loss=0.0552, Avg Loss=0.1231, Time Left=17.19\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1364/3393 [10:34<16:04,  2.10batch/s, Batch Loss=0.0552, Avg Loss=0.1231, Time Left=17.19\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1364/3393 [10:34<16:04,  2.10batch/s, Batch Loss=0.1632, Avg Loss=0.1231, Time Left=17.18\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1365/3393 [10:34<15:58,  2.12batch/s, Batch Loss=0.1632, Avg Loss=0.1231, Time Left=17.18\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1365/3393 [10:35<15:58,  2.12batch/s, Batch Loss=0.1781, Avg Loss=0.1232, Time Left=17.17\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1366/3393 [10:35<16:13,  2.08batch/s, Batch Loss=0.1781, Avg Loss=0.1232, Time Left=17.17\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1366/3393 [10:35<16:13,  2.08batch/s, Batch Loss=0.0566, Avg Loss=0.1231, Time Left=17.16\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1367/3393 [10:35<16:16,  2.07batch/s, Batch Loss=0.0566, Avg Loss=0.1231, Time Left=17.16\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1367/3393 [10:36<16:16,  2.07batch/s, Batch Loss=0.1360, Avg Loss=0.1231, Time Left=17.16\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1368/3393 [10:36<16:26,  2.05batch/s, Batch Loss=0.1360, Avg Loss=0.1231, Time Left=17.16\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1368/3393 [10:36<16:26,  2.05batch/s, Batch Loss=0.0670, Avg Loss=0.1231, Time Left=17.15\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1369/3393 [10:36<16:24,  2.06batch/s, Batch Loss=0.0670, Avg Loss=0.1231, Time Left=17.15\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1369/3393 [10:37<16:24,  2.06batch/s, Batch Loss=0.1712, Avg Loss=0.1231, Time Left=17.14\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1370/3393 [10:37<16:11,  2.08batch/s, Batch Loss=0.1712, Avg Loss=0.1231, Time Left=17.14\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1370/3393 [10:37<16:11,  2.08batch/s, Batch Loss=0.0166, Avg Loss=0.1231, Time Left=17.13\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1371/3393 [10:37<16:33,  2.04batch/s, Batch Loss=0.0166, Avg Loss=0.1231, Time Left=17.13\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1371/3393 [10:38<16:33,  2.04batch/s, Batch Loss=0.3122, Avg Loss=0.1232, Time Left=17.12\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1372/3393 [10:38<16:18,  2.07batch/s, Batch Loss=0.3122, Avg Loss=0.1232, Time Left=17.12\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1372/3393 [10:38<16:18,  2.07batch/s, Batch Loss=0.0539, Avg Loss=0.1231, Time Left=17.11\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1373/3393 [10:38<16:07,  2.09batch/s, Batch Loss=0.0539, Avg Loss=0.1231, Time Left=17.11\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1373/3393 [10:39<16:07,  2.09batch/s, Batch Loss=0.0036, Avg Loss=0.1231, Time Left=17.11\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1374/3393 [10:39<16:19,  2.06batch/s, Batch Loss=0.0036, Avg Loss=0.1231, Time Left=17.11\u001b[A\n",
      "Epoch 2/3 - Training:  40%|▍| 1374/3393 [10:39<16:19,  2.06batch/s, Batch Loss=0.2022, Avg Loss=0.1231, Time Left=17.10\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1375/3393 [10:39<16:18,  2.06batch/s, Batch Loss=0.2022, Avg Loss=0.1231, Time Left=17.10\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1375/3393 [10:40<16:18,  2.06batch/s, Batch Loss=0.0089, Avg Loss=0.1230, Time Left=17.09\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1376/3393 [10:40<16:45,  2.01batch/s, Batch Loss=0.0089, Avg Loss=0.1230, Time Left=17.09\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1376/3393 [10:40<16:45,  2.01batch/s, Batch Loss=0.0148, Avg Loss=0.1229, Time Left=17.08\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1377/3393 [10:40<16:37,  2.02batch/s, Batch Loss=0.0148, Avg Loss=0.1229, Time Left=17.08\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1377/3393 [10:41<16:37,  2.02batch/s, Batch Loss=0.1347, Avg Loss=0.1229, Time Left=17.07\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1378/3393 [10:41<16:31,  2.03batch/s, Batch Loss=0.1347, Avg Loss=0.1229, Time Left=17.07\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1378/3393 [10:41<16:31,  2.03batch/s, Batch Loss=0.0766, Avg Loss=0.1229, Time Left=17.06\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1379/3393 [10:41<16:16,  2.06batch/s, Batch Loss=0.0766, Avg Loss=0.1229, Time Left=17.06\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1379/3393 [10:41<16:16,  2.06batch/s, Batch Loss=0.2822, Avg Loss=0.1230, Time Left=17.06\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1380/3393 [10:41<16:04,  2.09batch/s, Batch Loss=0.2822, Avg Loss=0.1230, Time Left=17.06\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1380/3393 [10:42<16:04,  2.09batch/s, Batch Loss=0.1582, Avg Loss=0.1231, Time Left=17.05\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1381/3393 [10:42<16:06,  2.08batch/s, Batch Loss=0.1582, Avg Loss=0.1231, Time Left=17.05\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1381/3393 [10:42<16:06,  2.08batch/s, Batch Loss=0.0417, Avg Loss=0.1230, Time Left=17.04\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1382/3393 [10:42<16:07,  2.08batch/s, Batch Loss=0.0417, Avg Loss=0.1230, Time Left=17.04\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1382/3393 [10:43<16:07,  2.08batch/s, Batch Loss=0.0262, Avg Loss=0.1229, Time Left=17.03\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1383/3393 [10:43<16:09,  2.07batch/s, Batch Loss=0.0262, Avg Loss=0.1229, Time Left=17.03\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1383/3393 [10:43<16:09,  2.07batch/s, Batch Loss=0.0132, Avg Loss=0.1228, Time Left=17.02\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  41%|▍| 1384/3393 [10:43<16:19,  2.05batch/s, Batch Loss=0.0132, Avg Loss=0.1228, Time Left=17.02\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1384/3393 [10:44<16:19,  2.05batch/s, Batch Loss=0.0030, Avg Loss=0.1228, Time Left=17.01\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1385/3393 [10:44<16:16,  2.06batch/s, Batch Loss=0.0030, Avg Loss=0.1228, Time Left=17.01\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1385/3393 [10:44<16:16,  2.06batch/s, Batch Loss=0.1309, Avg Loss=0.1228, Time Left=17.01\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1386/3393 [10:44<16:05,  2.08batch/s, Batch Loss=0.1309, Avg Loss=0.1228, Time Left=17.01\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1386/3393 [10:45<16:05,  2.08batch/s, Batch Loss=0.0559, Avg Loss=0.1227, Time Left=17.00\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1387/3393 [10:45<16:15,  2.06batch/s, Batch Loss=0.0559, Avg Loss=0.1227, Time Left=17.00\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1387/3393 [10:45<16:15,  2.06batch/s, Batch Loss=0.0436, Avg Loss=0.1226, Time Left=16.99\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1388/3393 [10:45<16:13,  2.06batch/s, Batch Loss=0.0436, Avg Loss=0.1226, Time Left=16.99\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1388/3393 [10:46<16:13,  2.06batch/s, Batch Loss=0.0628, Avg Loss=0.1226, Time Left=16.98\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1389/3393 [10:46<16:01,  2.08batch/s, Batch Loss=0.0628, Avg Loss=0.1226, Time Left=16.98\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1389/3393 [10:46<16:01,  2.08batch/s, Batch Loss=0.0046, Avg Loss=0.1225, Time Left=16.97\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1390/3393 [10:46<16:32,  2.02batch/s, Batch Loss=0.0046, Avg Loss=0.1225, Time Left=16.97\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1390/3393 [10:47<16:32,  2.02batch/s, Batch Loss=0.0735, Avg Loss=0.1225, Time Left=16.97\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1391/3393 [10:47<16:26,  2.03batch/s, Batch Loss=0.0735, Avg Loss=0.1225, Time Left=16.97\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1391/3393 [10:47<16:26,  2.03batch/s, Batch Loss=0.0096, Avg Loss=0.1224, Time Left=16.96\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1392/3393 [10:47<16:29,  2.02batch/s, Batch Loss=0.0096, Avg Loss=0.1224, Time Left=16.96\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1392/3393 [10:48<16:29,  2.02batch/s, Batch Loss=0.0090, Avg Loss=0.1223, Time Left=16.95\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1393/3393 [10:48<16:13,  2.05batch/s, Batch Loss=0.0090, Avg Loss=0.1223, Time Left=16.95\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1393/3393 [10:48<16:13,  2.05batch/s, Batch Loss=0.0411, Avg Loss=0.1222, Time Left=16.94\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1394/3393 [10:48<16:01,  2.08batch/s, Batch Loss=0.0411, Avg Loss=0.1222, Time Left=16.94\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1394/3393 [10:49<16:01,  2.08batch/s, Batch Loss=0.0505, Avg Loss=0.1222, Time Left=16.93\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1395/3393 [10:49<16:30,  2.02batch/s, Batch Loss=0.0505, Avg Loss=0.1222, Time Left=16.93\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1395/3393 [10:49<16:30,  2.02batch/s, Batch Loss=0.0045, Avg Loss=0.1221, Time Left=16.92\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1396/3393 [10:49<16:23,  2.03batch/s, Batch Loss=0.0045, Avg Loss=0.1221, Time Left=16.92\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1396/3393 [10:50<16:23,  2.03batch/s, Batch Loss=0.0700, Avg Loss=0.1221, Time Left=16.92\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1397/3393 [10:50<16:36,  2.00batch/s, Batch Loss=0.0700, Avg Loss=0.1221, Time Left=16.92\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1397/3393 [10:50<16:36,  2.00batch/s, Batch Loss=0.1254, Avg Loss=0.1221, Time Left=16.91\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1398/3393 [10:50<16:28,  2.02batch/s, Batch Loss=0.1254, Avg Loss=0.1221, Time Left=16.91\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1398/3393 [10:51<16:28,  2.02batch/s, Batch Loss=0.0051, Avg Loss=0.1220, Time Left=16.90\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1399/3393 [10:51<16:35,  2.00batch/s, Batch Loss=0.0051, Avg Loss=0.1220, Time Left=16.90\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1399/3393 [10:51<16:35,  2.00batch/s, Batch Loss=0.0754, Avg Loss=0.1219, Time Left=16.89\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1400/3393 [10:51<16:23,  2.03batch/s, Batch Loss=0.0754, Avg Loss=0.1219, Time Left=16.89\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1400/3393 [10:52<16:23,  2.03batch/s, Batch Loss=0.1875, Avg Loss=0.1220, Time Left=16.88\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1401/3393 [10:52<16:14,  2.04batch/s, Batch Loss=0.1875, Avg Loss=0.1220, Time Left=16.88\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1401/3393 [10:52<16:14,  2.04batch/s, Batch Loss=0.0569, Avg Loss=0.1219, Time Left=16.88\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1402/3393 [10:52<16:20,  2.03batch/s, Batch Loss=0.0569, Avg Loss=0.1219, Time Left=16.88\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1402/3393 [10:53<16:20,  2.03batch/s, Batch Loss=0.0441, Avg Loss=0.1219, Time Left=16.87\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1403/3393 [10:53<16:03,  2.07batch/s, Batch Loss=0.0441, Avg Loss=0.1219, Time Left=16.87\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1403/3393 [10:53<16:03,  2.07batch/s, Batch Loss=0.1080, Avg Loss=0.1219, Time Left=16.86\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1404/3393 [10:53<16:05,  2.06batch/s, Batch Loss=0.1080, Avg Loss=0.1219, Time Left=16.86\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1404/3393 [10:54<16:05,  2.06batch/s, Batch Loss=0.0064, Avg Loss=0.1218, Time Left=16.85\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1405/3393 [10:54<16:04,  2.06batch/s, Batch Loss=0.0064, Avg Loss=0.1218, Time Left=16.85\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1405/3393 [10:54<16:04,  2.06batch/s, Batch Loss=0.0488, Avg Loss=0.1217, Time Left=16.84\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1406/3393 [10:54<15:53,  2.08batch/s, Batch Loss=0.0488, Avg Loss=0.1217, Time Left=16.84\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1406/3393 [10:55<15:53,  2.08batch/s, Batch Loss=0.0113, Avg Loss=0.1216, Time Left=16.83\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1407/3393 [10:55<15:55,  2.08batch/s, Batch Loss=0.0113, Avg Loss=0.1216, Time Left=16.83\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1407/3393 [10:55<15:55,  2.08batch/s, Batch Loss=0.0586, Avg Loss=0.1216, Time Left=16.83\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1408/3393 [10:55<16:23,  2.02batch/s, Batch Loss=0.0586, Avg Loss=0.1216, Time Left=16.83\u001b[A\n",
      "Epoch 2/3 - Training:  41%|▍| 1408/3393 [10:56<16:23,  2.02batch/s, Batch Loss=0.0753, Avg Loss=0.1216, Time Left=16.82\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1409/3393 [10:56<16:17,  2.03batch/s, Batch Loss=0.0753, Avg Loss=0.1216, Time Left=16.82\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1409/3393 [10:56<16:17,  2.03batch/s, Batch Loss=0.0937, Avg Loss=0.1215, Time Left=16.81\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1410/3393 [10:56<16:11,  2.04batch/s, Batch Loss=0.0937, Avg Loss=0.1215, Time Left=16.81\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1410/3393 [10:57<16:11,  2.04batch/s, Batch Loss=0.0245, Avg Loss=0.1215, Time Left=16.80\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1411/3393 [10:57<15:58,  2.07batch/s, Batch Loss=0.0245, Avg Loss=0.1215, Time Left=16.80\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1411/3393 [10:57<15:58,  2.07batch/s, Batch Loss=0.0219, Avg Loss=0.1214, Time Left=16.79\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1412/3393 [10:57<15:48,  2.09batch/s, Batch Loss=0.0219, Avg Loss=0.1214, Time Left=16.79\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1412/3393 [10:58<15:48,  2.09batch/s, Batch Loss=0.0082, Avg Loss=0.1213, Time Left=16.78\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1413/3393 [10:58<16:00,  2.06batch/s, Batch Loss=0.0082, Avg Loss=0.1213, Time Left=16.78\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1413/3393 [10:58<16:00,  2.06batch/s, Batch Loss=0.0041, Avg Loss=0.1212, Time Left=16.77\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1414/3393 [10:58<15:39,  2.11batch/s, Batch Loss=0.0041, Avg Loss=0.1212, Time Left=16.77\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1414/3393 [10:59<15:39,  2.11batch/s, Batch Loss=0.2163, Avg Loss=0.1213, Time Left=16.77\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1415/3393 [10:59<15:43,  2.10batch/s, Batch Loss=0.2163, Avg Loss=0.1213, Time Left=16.77\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1415/3393 [10:59<15:43,  2.10batch/s, Batch Loss=0.0072, Avg Loss=0.1212, Time Left=16.76\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1416/3393 [10:59<15:47,  2.09batch/s, Batch Loss=0.0072, Avg Loss=0.1212, Time Left=16.76\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1416/3393 [10:59<15:47,  2.09batch/s, Batch Loss=0.0107, Avg Loss=0.1211, Time Left=16.75\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  42%|▍| 1417/3393 [10:59<15:40,  2.10batch/s, Batch Loss=0.0107, Avg Loss=0.1211, Time Left=16.75\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1417/3393 [11:00<15:40,  2.10batch/s, Batch Loss=0.0873, Avg Loss=0.1211, Time Left=16.74\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1418/3393 [11:00<15:34,  2.11batch/s, Batch Loss=0.0873, Avg Loss=0.1211, Time Left=16.74\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1418/3393 [11:00<15:34,  2.11batch/s, Batch Loss=0.0384, Avg Loss=0.1210, Time Left=16.73\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1419/3393 [11:00<15:48,  2.08batch/s, Batch Loss=0.0384, Avg Loss=0.1210, Time Left=16.73\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1419/3393 [11:01<15:48,  2.08batch/s, Batch Loss=0.1343, Avg Loss=0.1211, Time Left=16.72\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1420/3393 [11:01<15:51,  2.07batch/s, Batch Loss=0.1343, Avg Loss=0.1211, Time Left=16.72\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1420/3393 [11:01<15:51,  2.07batch/s, Batch Loss=0.2449, Avg Loss=0.1211, Time Left=16.72\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1421/3393 [11:01<15:50,  2.07batch/s, Batch Loss=0.2449, Avg Loss=0.1211, Time Left=16.72\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1421/3393 [11:02<15:50,  2.07batch/s, Batch Loss=0.0053, Avg Loss=0.1211, Time Left=16.71\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1422/3393 [11:02<16:09,  2.03batch/s, Batch Loss=0.0053, Avg Loss=0.1211, Time Left=16.71\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1422/3393 [11:02<16:09,  2.03batch/s, Batch Loss=0.3850, Avg Loss=0.1213, Time Left=16.70\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1423/3393 [11:02<16:05,  2.04batch/s, Batch Loss=0.3850, Avg Loss=0.1213, Time Left=16.70\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1423/3393 [11:03<16:05,  2.04batch/s, Batch Loss=0.0785, Avg Loss=0.1212, Time Left=16.69\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1424/3393 [11:03<16:19,  2.01batch/s, Batch Loss=0.0785, Avg Loss=0.1212, Time Left=16.69\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1424/3393 [11:03<16:19,  2.01batch/s, Batch Loss=0.3406, Avg Loss=0.1214, Time Left=16.68\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1425/3393 [11:03<15:53,  2.06batch/s, Batch Loss=0.3406, Avg Loss=0.1214, Time Left=16.68\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1425/3393 [11:04<15:53,  2.06batch/s, Batch Loss=0.1186, Avg Loss=0.1214, Time Left=16.67\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1426/3393 [11:04<15:52,  2.07batch/s, Batch Loss=0.1186, Avg Loss=0.1214, Time Left=16.67\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1426/3393 [11:04<15:52,  2.07batch/s, Batch Loss=0.0640, Avg Loss=0.1213, Time Left=16.67\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1427/3393 [11:04<16:09,  2.03batch/s, Batch Loss=0.0640, Avg Loss=0.1213, Time Left=16.67\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1427/3393 [11:05<16:09,  2.03batch/s, Batch Loss=0.0590, Avg Loss=0.1213, Time Left=16.66\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1428/3393 [11:05<16:05,  2.04batch/s, Batch Loss=0.0590, Avg Loss=0.1213, Time Left=16.66\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1428/3393 [11:05<16:05,  2.04batch/s, Batch Loss=0.0166, Avg Loss=0.1212, Time Left=16.65\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1429/3393 [11:05<16:09,  2.03batch/s, Batch Loss=0.0166, Avg Loss=0.1212, Time Left=16.65\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1429/3393 [11:06<16:09,  2.03batch/s, Batch Loss=0.0222, Avg Loss=0.1211, Time Left=16.64\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1430/3393 [11:06<16:03,  2.04batch/s, Batch Loss=0.0222, Avg Loss=0.1211, Time Left=16.64\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1430/3393 [11:06<16:03,  2.04batch/s, Batch Loss=0.1667, Avg Loss=0.1212, Time Left=16.63\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1431/3393 [11:06<15:58,  2.05batch/s, Batch Loss=0.1667, Avg Loss=0.1212, Time Left=16.63\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1431/3393 [11:07<15:58,  2.05batch/s, Batch Loss=0.0201, Avg Loss=0.1211, Time Left=16.63\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1432/3393 [11:07<16:07,  2.03batch/s, Batch Loss=0.0201, Avg Loss=0.1211, Time Left=16.63\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1432/3393 [11:07<16:07,  2.03batch/s, Batch Loss=0.0042, Avg Loss=0.1210, Time Left=16.62\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1433/3393 [11:07<15:58,  2.04batch/s, Batch Loss=0.0042, Avg Loss=0.1210, Time Left=16.62\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1433/3393 [11:08<15:58,  2.04batch/s, Batch Loss=0.0207, Avg Loss=0.1209, Time Left=16.61\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1434/3393 [11:08<16:14,  2.01batch/s, Batch Loss=0.0207, Avg Loss=0.1209, Time Left=16.61\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1434/3393 [11:08<16:14,  2.01batch/s, Batch Loss=0.0640, Avg Loss=0.1209, Time Left=16.60\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1435/3393 [11:08<16:06,  2.03batch/s, Batch Loss=0.0640, Avg Loss=0.1209, Time Left=16.60\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1435/3393 [11:09<16:06,  2.03batch/s, Batch Loss=0.0629, Avg Loss=0.1209, Time Left=16.59\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1436/3393 [11:09<16:08,  2.02batch/s, Batch Loss=0.0629, Avg Loss=0.1209, Time Left=16.59\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1436/3393 [11:09<16:08,  2.02batch/s, Batch Loss=0.0802, Avg Loss=0.1208, Time Left=16.59\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1437/3393 [11:09<15:53,  2.05batch/s, Batch Loss=0.0802, Avg Loss=0.1208, Time Left=16.59\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1437/3393 [11:10<15:53,  2.05batch/s, Batch Loss=0.1091, Avg Loss=0.1208, Time Left=16.58\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1438/3393 [11:10<15:50,  2.06batch/s, Batch Loss=0.1091, Avg Loss=0.1208, Time Left=16.58\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1438/3393 [11:10<15:50,  2.06batch/s, Batch Loss=0.0653, Avg Loss=0.1208, Time Left=16.57\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1439/3393 [11:10<16:07,  2.02batch/s, Batch Loss=0.0653, Avg Loss=0.1208, Time Left=16.57\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1439/3393 [11:11<16:07,  2.02batch/s, Batch Loss=0.0080, Avg Loss=0.1207, Time Left=16.56\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1440/3393 [11:11<15:52,  2.05batch/s, Batch Loss=0.0080, Avg Loss=0.1207, Time Left=16.56\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1440/3393 [11:11<15:52,  2.05batch/s, Batch Loss=0.0243, Avg Loss=0.1206, Time Left=16.55\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1441/3393 [11:11<15:49,  2.06batch/s, Batch Loss=0.0243, Avg Loss=0.1206, Time Left=16.55\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1441/3393 [11:12<15:49,  2.06batch/s, Batch Loss=0.0107, Avg Loss=0.1205, Time Left=16.55\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1442/3393 [11:12<16:14,  2.00batch/s, Batch Loss=0.0107, Avg Loss=0.1205, Time Left=16.55\u001b[A\n",
      "Epoch 2/3 - Training:  42%|▍| 1442/3393 [11:12<16:14,  2.00batch/s, Batch Loss=0.0173, Avg Loss=0.1205, Time Left=16.54\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1443/3393 [11:12<15:56,  2.04batch/s, Batch Loss=0.0173, Avg Loss=0.1205, Time Left=16.54\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1443/3393 [11:13<15:56,  2.04batch/s, Batch Loss=0.1049, Avg Loss=0.1205, Time Left=16.53\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1444/3393 [11:13<16:17,  1.99batch/s, Batch Loss=0.1049, Avg Loss=0.1205, Time Left=16.53\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1444/3393 [11:13<16:17,  1.99batch/s, Batch Loss=0.0669, Avg Loss=0.1204, Time Left=16.52\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1445/3393 [11:13<16:00,  2.03batch/s, Batch Loss=0.0669, Avg Loss=0.1204, Time Left=16.52\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1445/3393 [11:14<16:00,  2.03batch/s, Batch Loss=0.1609, Avg Loss=0.1205, Time Left=16.51\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1446/3393 [11:14<15:55,  2.04batch/s, Batch Loss=0.1609, Avg Loss=0.1205, Time Left=16.51\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1446/3393 [11:14<15:55,  2.04batch/s, Batch Loss=0.0609, Avg Loss=0.1204, Time Left=16.50\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1447/3393 [11:14<15:41,  2.07batch/s, Batch Loss=0.0609, Avg Loss=0.1204, Time Left=16.50\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1447/3393 [11:15<15:41,  2.07batch/s, Batch Loss=0.2598, Avg Loss=0.1205, Time Left=16.49\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1448/3393 [11:15<15:20,  2.11batch/s, Batch Loss=0.2598, Avg Loss=0.1205, Time Left=16.49\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1448/3393 [11:15<15:20,  2.11batch/s, Batch Loss=0.0180, Avg Loss=0.1204, Time Left=16.49\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1449/3393 [11:15<15:25,  2.10batch/s, Batch Loss=0.0180, Avg Loss=0.1204, Time Left=16.49\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1449/3393 [11:16<15:25,  2.10batch/s, Batch Loss=0.0347, Avg Loss=0.1204, Time Left=16.48\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  43%|▍| 1450/3393 [11:16<15:57,  2.03batch/s, Batch Loss=0.0347, Avg Loss=0.1204, Time Left=16.48\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1450/3393 [11:16<15:57,  2.03batch/s, Batch Loss=0.3048, Avg Loss=0.1205, Time Left=16.47\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1451/3393 [11:16<15:43,  2.06batch/s, Batch Loss=0.3048, Avg Loss=0.1205, Time Left=16.47\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1451/3393 [11:17<15:43,  2.06batch/s, Batch Loss=0.1342, Avg Loss=0.1205, Time Left=16.46\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1452/3393 [11:17<16:08,  2.00batch/s, Batch Loss=0.1342, Avg Loss=0.1205, Time Left=16.46\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1452/3393 [11:17<16:08,  2.00batch/s, Batch Loss=0.0374, Avg Loss=0.1205, Time Left=16.45\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1453/3393 [11:17<15:51,  2.04batch/s, Batch Loss=0.0374, Avg Loss=0.1205, Time Left=16.45\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1453/3393 [11:18<15:51,  2.04batch/s, Batch Loss=0.0876, Avg Loss=0.1204, Time Left=16.45\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1454/3393 [11:18<16:07,  2.01batch/s, Batch Loss=0.0876, Avg Loss=0.1204, Time Left=16.45\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1454/3393 [11:18<16:07,  2.01batch/s, Batch Loss=0.1540, Avg Loss=0.1205, Time Left=16.44\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1455/3393 [11:18<15:57,  2.02batch/s, Batch Loss=0.1540, Avg Loss=0.1205, Time Left=16.44\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1455/3393 [11:19<15:57,  2.02batch/s, Batch Loss=0.0175, Avg Loss=0.1204, Time Left=16.43\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1456/3393 [11:19<15:40,  2.06batch/s, Batch Loss=0.0175, Avg Loss=0.1204, Time Left=16.43\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1456/3393 [11:19<15:40,  2.06batch/s, Batch Loss=0.1945, Avg Loss=0.1204, Time Left=16.42\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1457/3393 [11:19<16:01,  2.01batch/s, Batch Loss=0.1945, Avg Loss=0.1204, Time Left=16.42\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1457/3393 [11:20<16:01,  2.01batch/s, Batch Loss=0.0576, Avg Loss=0.1204, Time Left=16.41\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1458/3393 [11:20<15:32,  2.08batch/s, Batch Loss=0.0576, Avg Loss=0.1204, Time Left=16.41\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1458/3393 [11:20<15:32,  2.08batch/s, Batch Loss=0.0146, Avg Loss=0.1203, Time Left=16.40\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1459/3393 [11:20<15:32,  2.07batch/s, Batch Loss=0.0146, Avg Loss=0.1203, Time Left=16.40\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1459/3393 [11:21<15:32,  2.07batch/s, Batch Loss=0.0094, Avg Loss=0.1202, Time Left=16.40\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1460/3393 [11:21<15:50,  2.03batch/s, Batch Loss=0.0094, Avg Loss=0.1202, Time Left=16.40\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1460/3393 [11:21<15:50,  2.03batch/s, Batch Loss=0.0579, Avg Loss=0.1202, Time Left=16.39\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1461/3393 [11:21<15:44,  2.04batch/s, Batch Loss=0.0579, Avg Loss=0.1202, Time Left=16.39\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1461/3393 [11:22<15:44,  2.04batch/s, Batch Loss=0.0375, Avg Loss=0.1201, Time Left=16.38\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1462/3393 [11:22<15:51,  2.03batch/s, Batch Loss=0.0375, Avg Loss=0.1201, Time Left=16.38\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1462/3393 [11:22<15:51,  2.03batch/s, Batch Loss=0.0404, Avg Loss=0.1201, Time Left=16.37\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1463/3393 [11:22<15:28,  2.08batch/s, Batch Loss=0.0404, Avg Loss=0.1201, Time Left=16.37\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1463/3393 [11:22<15:28,  2.08batch/s, Batch Loss=0.1436, Avg Loss=0.1201, Time Left=16.36\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1464/3393 [11:22<15:28,  2.08batch/s, Batch Loss=0.1436, Avg Loss=0.1201, Time Left=16.36\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1464/3393 [11:23<15:28,  2.08batch/s, Batch Loss=0.1947, Avg Loss=0.1201, Time Left=16.36\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1465/3393 [11:23<15:42,  2.04batch/s, Batch Loss=0.1947, Avg Loss=0.1201, Time Left=16.36\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1465/3393 [11:23<15:42,  2.04batch/s, Batch Loss=0.0681, Avg Loss=0.1201, Time Left=16.35\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1466/3393 [11:23<15:27,  2.08batch/s, Batch Loss=0.0681, Avg Loss=0.1201, Time Left=16.35\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1466/3393 [11:24<15:27,  2.08batch/s, Batch Loss=0.0573, Avg Loss=0.1201, Time Left=16.34\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1467/3393 [11:24<15:36,  2.06batch/s, Batch Loss=0.0573, Avg Loss=0.1201, Time Left=16.34\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1467/3393 [11:24<15:36,  2.06batch/s, Batch Loss=0.0611, Avg Loss=0.1200, Time Left=16.33\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1468/3393 [11:24<15:35,  2.06batch/s, Batch Loss=0.0611, Avg Loss=0.1200, Time Left=16.33\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1468/3393 [11:25<15:35,  2.06batch/s, Batch Loss=0.0547, Avg Loss=0.1200, Time Left=16.32\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1469/3393 [11:25<15:42,  2.04batch/s, Batch Loss=0.0547, Avg Loss=0.1200, Time Left=16.32\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1469/3393 [11:25<15:42,  2.04batch/s, Batch Loss=0.0585, Avg Loss=0.1199, Time Left=16.31\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1470/3393 [11:25<15:56,  2.01batch/s, Batch Loss=0.0585, Avg Loss=0.1199, Time Left=16.31\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1470/3393 [11:26<15:56,  2.01batch/s, Batch Loss=0.0699, Avg Loss=0.1199, Time Left=16.31\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1471/3393 [11:26<15:58,  2.01batch/s, Batch Loss=0.0699, Avg Loss=0.1199, Time Left=16.31\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1471/3393 [11:26<15:58,  2.01batch/s, Batch Loss=0.0937, Avg Loss=0.1199, Time Left=16.30\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1472/3393 [11:26<16:03,  1.99batch/s, Batch Loss=0.0937, Avg Loss=0.1199, Time Left=16.30\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1472/3393 [11:27<16:03,  1.99batch/s, Batch Loss=0.2125, Avg Loss=0.1199, Time Left=16.29\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1473/3393 [11:27<15:49,  2.02batch/s, Batch Loss=0.2125, Avg Loss=0.1199, Time Left=16.29\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1473/3393 [11:27<15:49,  2.02batch/s, Batch Loss=0.3155, Avg Loss=0.1201, Time Left=16.28\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1474/3393 [11:27<15:33,  2.06batch/s, Batch Loss=0.3155, Avg Loss=0.1201, Time Left=16.28\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1474/3393 [11:28<15:33,  2.06batch/s, Batch Loss=0.0802, Avg Loss=0.1200, Time Left=16.27\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1475/3393 [11:28<15:58,  2.00batch/s, Batch Loss=0.0802, Avg Loss=0.1200, Time Left=16.27\u001b[A\n",
      "Epoch 2/3 - Training:  43%|▍| 1475/3393 [11:28<15:58,  2.00batch/s, Batch Loss=0.1294, Avg Loss=0.1201, Time Left=16.27\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1476/3393 [11:28<15:50,  2.02batch/s, Batch Loss=0.1294, Avg Loss=0.1201, Time Left=16.27\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1476/3393 [11:29<15:50,  2.02batch/s, Batch Loss=0.1354, Avg Loss=0.1201, Time Left=16.26\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1477/3393 [11:29<15:42,  2.03batch/s, Batch Loss=0.1354, Avg Loss=0.1201, Time Left=16.26\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1477/3393 [11:29<15:42,  2.03batch/s, Batch Loss=0.1355, Avg Loss=0.1201, Time Left=16.25\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1478/3393 [11:29<15:37,  2.04batch/s, Batch Loss=0.1355, Avg Loss=0.1201, Time Left=16.25\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1478/3393 [11:30<15:37,  2.04batch/s, Batch Loss=0.2785, Avg Loss=0.1202, Time Left=16.24\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1479/3393 [11:30<15:23,  2.07batch/s, Batch Loss=0.2785, Avg Loss=0.1202, Time Left=16.24\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1479/3393 [11:30<15:23,  2.07batch/s, Batch Loss=0.2464, Avg Loss=0.1203, Time Left=16.23\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1480/3393 [11:30<15:23,  2.07batch/s, Batch Loss=0.2464, Avg Loss=0.1203, Time Left=16.23\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1480/3393 [11:31<15:23,  2.07batch/s, Batch Loss=0.2383, Avg Loss=0.1204, Time Left=16.22\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1481/3393 [11:31<15:23,  2.07batch/s, Batch Loss=0.2383, Avg Loss=0.1204, Time Left=16.22\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1481/3393 [11:31<15:23,  2.07batch/s, Batch Loss=0.0116, Avg Loss=0.1203, Time Left=16.22\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1482/3393 [11:31<15:14,  2.09batch/s, Batch Loss=0.0116, Avg Loss=0.1203, Time Left=16.22\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1482/3393 [11:32<15:14,  2.09batch/s, Batch Loss=0.1047, Avg Loss=0.1203, Time Left=16.21\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  44%|▍| 1483/3393 [11:32<15:25,  2.06batch/s, Batch Loss=0.1047, Avg Loss=0.1203, Time Left=16.21\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1483/3393 [11:32<15:25,  2.06batch/s, Batch Loss=0.1258, Avg Loss=0.1203, Time Left=16.20\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1484/3393 [11:32<15:23,  2.07batch/s, Batch Loss=0.1258, Avg Loss=0.1203, Time Left=16.20\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1484/3393 [11:33<15:23,  2.07batch/s, Batch Loss=0.0790, Avg Loss=0.1203, Time Left=16.19\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1485/3393 [11:33<15:14,  2.09batch/s, Batch Loss=0.0790, Avg Loss=0.1203, Time Left=16.19\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1485/3393 [11:33<15:14,  2.09batch/s, Batch Loss=0.1139, Avg Loss=0.1202, Time Left=16.18\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1486/3393 [11:33<15:41,  2.03batch/s, Batch Loss=0.1139, Avg Loss=0.1202, Time Left=16.18\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1486/3393 [11:34<15:41,  2.03batch/s, Batch Loss=0.0635, Avg Loss=0.1202, Time Left=16.18\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1487/3393 [11:34<15:39,  2.03batch/s, Batch Loss=0.0635, Avg Loss=0.1202, Time Left=16.18\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1487/3393 [11:34<15:39,  2.03batch/s, Batch Loss=0.1100, Avg Loss=0.1202, Time Left=16.17\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1488/3393 [11:34<15:41,  2.02batch/s, Batch Loss=0.1100, Avg Loss=0.1202, Time Left=16.17\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1488/3393 [11:35<15:41,  2.02batch/s, Batch Loss=0.1778, Avg Loss=0.1202, Time Left=16.16\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1489/3393 [11:35<15:45,  2.01batch/s, Batch Loss=0.1778, Avg Loss=0.1202, Time Left=16.16\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1489/3393 [11:35<15:45,  2.01batch/s, Batch Loss=0.0163, Avg Loss=0.1202, Time Left=16.15\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1490/3393 [11:35<16:04,  1.97batch/s, Batch Loss=0.0163, Avg Loss=0.1202, Time Left=16.15\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1490/3393 [11:36<16:04,  1.97batch/s, Batch Loss=0.0216, Avg Loss=0.1201, Time Left=16.14\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1491/3393 [11:36<15:43,  2.02batch/s, Batch Loss=0.0216, Avg Loss=0.1201, Time Left=16.14\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1491/3393 [11:36<15:43,  2.02batch/s, Batch Loss=0.0375, Avg Loss=0.1200, Time Left=16.13\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1492/3393 [11:36<15:36,  2.03batch/s, Batch Loss=0.0375, Avg Loss=0.1200, Time Left=16.13\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1492/3393 [11:37<15:36,  2.03batch/s, Batch Loss=0.0577, Avg Loss=0.1200, Time Left=16.13\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1493/3393 [11:37<15:31,  2.04batch/s, Batch Loss=0.0577, Avg Loss=0.1200, Time Left=16.13\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1493/3393 [11:37<15:31,  2.04batch/s, Batch Loss=0.0176, Avg Loss=0.1199, Time Left=16.12\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1494/3393 [11:37<15:26,  2.05batch/s, Batch Loss=0.0176, Avg Loss=0.1199, Time Left=16.12\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1494/3393 [11:38<15:26,  2.05batch/s, Batch Loss=0.3115, Avg Loss=0.1201, Time Left=16.11\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1495/3393 [11:38<15:24,  2.05batch/s, Batch Loss=0.3115, Avg Loss=0.1201, Time Left=16.11\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1495/3393 [11:38<15:24,  2.05batch/s, Batch Loss=0.1677, Avg Loss=0.1201, Time Left=16.10\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1496/3393 [11:38<15:13,  2.08batch/s, Batch Loss=0.1677, Avg Loss=0.1201, Time Left=16.10\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1496/3393 [11:39<15:13,  2.08batch/s, Batch Loss=0.0462, Avg Loss=0.1200, Time Left=16.09\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1497/3393 [11:39<15:12,  2.08batch/s, Batch Loss=0.0462, Avg Loss=0.1200, Time Left=16.09\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1497/3393 [11:39<15:12,  2.08batch/s, Batch Loss=0.0849, Avg Loss=0.1200, Time Left=16.08\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1498/3393 [11:39<15:04,  2.10batch/s, Batch Loss=0.0849, Avg Loss=0.1200, Time Left=16.08\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1498/3393 [11:40<15:04,  2.10batch/s, Batch Loss=0.0565, Avg Loss=0.1200, Time Left=16.08\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1499/3393 [11:40<15:07,  2.09batch/s, Batch Loss=0.0565, Avg Loss=0.1200, Time Left=16.08\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1499/3393 [11:40<15:07,  2.09batch/s, Batch Loss=0.1348, Avg Loss=0.1200, Time Left=16.07\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1500/3393 [11:40<14:51,  2.12batch/s, Batch Loss=0.1348, Avg Loss=0.1200, Time Left=16.07\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1500/3393 [11:41<14:51,  2.12batch/s, Batch Loss=0.1204, Avg Loss=0.1200, Time Left=16.06\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1501/3393 [11:41<15:06,  2.09batch/s, Batch Loss=0.1204, Avg Loss=0.1200, Time Left=16.06\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1501/3393 [11:41<15:06,  2.09batch/s, Batch Loss=0.2734, Avg Loss=0.1201, Time Left=16.05\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1502/3393 [11:41<15:08,  2.08batch/s, Batch Loss=0.2734, Avg Loss=0.1201, Time Left=16.05\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1502/3393 [11:42<15:08,  2.08batch/s, Batch Loss=0.1148, Avg Loss=0.1201, Time Left=16.04\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1503/3393 [11:42<15:19,  2.06batch/s, Batch Loss=0.1148, Avg Loss=0.1201, Time Left=16.04\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1503/3393 [11:42<15:19,  2.06batch/s, Batch Loss=0.0870, Avg Loss=0.1201, Time Left=16.03\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1504/3393 [11:42<15:25,  2.04batch/s, Batch Loss=0.0870, Avg Loss=0.1201, Time Left=16.03\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1504/3393 [11:43<15:25,  2.04batch/s, Batch Loss=0.1204, Avg Loss=0.1201, Time Left=16.03\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1505/3393 [11:43<15:32,  2.03batch/s, Batch Loss=0.1204, Avg Loss=0.1201, Time Left=16.03\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1505/3393 [11:43<15:32,  2.03batch/s, Batch Loss=0.0945, Avg Loss=0.1200, Time Left=16.02\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1506/3393 [11:43<15:16,  2.06batch/s, Batch Loss=0.0945, Avg Loss=0.1200, Time Left=16.02\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1506/3393 [11:43<15:16,  2.06batch/s, Batch Loss=0.0584, Avg Loss=0.1200, Time Left=16.01\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1507/3393 [11:43<15:16,  2.06batch/s, Batch Loss=0.0584, Avg Loss=0.1200, Time Left=16.01\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1507/3393 [11:44<15:16,  2.06batch/s, Batch Loss=0.3426, Avg Loss=0.1202, Time Left=16.00\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1508/3393 [11:44<15:13,  2.06batch/s, Batch Loss=0.3426, Avg Loss=0.1202, Time Left=16.00\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1508/3393 [11:44<15:13,  2.06batch/s, Batch Loss=0.0310, Avg Loss=0.1201, Time Left=15.99\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1509/3393 [11:44<15:02,  2.09batch/s, Batch Loss=0.0310, Avg Loss=0.1201, Time Left=15.99\u001b[A\n",
      "Epoch 2/3 - Training:  44%|▍| 1509/3393 [11:45<15:02,  2.09batch/s, Batch Loss=0.3796, Avg Loss=0.1203, Time Left=15.98\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1510/3393 [11:45<15:05,  2.08batch/s, Batch Loss=0.3796, Avg Loss=0.1203, Time Left=15.98\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1510/3393 [11:45<15:05,  2.08batch/s, Batch Loss=0.3436, Avg Loss=0.1204, Time Left=15.98\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1511/3393 [11:45<14:57,  2.10batch/s, Batch Loss=0.3436, Avg Loss=0.1204, Time Left=15.98\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1511/3393 [11:46<14:57,  2.10batch/s, Batch Loss=0.0696, Avg Loss=0.1204, Time Left=15.97\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1512/3393 [11:46<15:00,  2.09batch/s, Batch Loss=0.0696, Avg Loss=0.1204, Time Left=15.97\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1512/3393 [11:46<15:00,  2.09batch/s, Batch Loss=0.2128, Avg Loss=0.1205, Time Left=15.96\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1513/3393 [11:46<15:03,  2.08batch/s, Batch Loss=0.2128, Avg Loss=0.1205, Time Left=15.96\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1513/3393 [11:47<15:03,  2.08batch/s, Batch Loss=0.0522, Avg Loss=0.1204, Time Left=15.95\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1514/3393 [11:47<15:12,  2.06batch/s, Batch Loss=0.0522, Avg Loss=0.1204, Time Left=15.95\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1514/3393 [11:47<15:12,  2.06batch/s, Batch Loss=0.0741, Avg Loss=0.1204, Time Left=15.94\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1515/3393 [11:47<15:28,  2.02batch/s, Batch Loss=0.0741, Avg Loss=0.1204, Time Left=15.94\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1515/3393 [11:48<15:28,  2.02batch/s, Batch Loss=0.2837, Avg Loss=0.1205, Time Left=15.93\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  45%|▍| 1516/3393 [11:48<15:24,  2.03batch/s, Batch Loss=0.2837, Avg Loss=0.1205, Time Left=15.93\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1516/3393 [11:48<15:24,  2.03batch/s, Batch Loss=0.0233, Avg Loss=0.1204, Time Left=15.93\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1517/3393 [11:48<15:27,  2.02batch/s, Batch Loss=0.0233, Avg Loss=0.1204, Time Left=15.93\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1517/3393 [11:49<15:27,  2.02batch/s, Batch Loss=0.1954, Avg Loss=0.1205, Time Left=15.92\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1518/3393 [11:49<15:12,  2.06batch/s, Batch Loss=0.1954, Avg Loss=0.1205, Time Left=15.92\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1518/3393 [11:49<15:12,  2.06batch/s, Batch Loss=0.3110, Avg Loss=0.1206, Time Left=15.91\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1519/3393 [11:49<15:10,  2.06batch/s, Batch Loss=0.3110, Avg Loss=0.1206, Time Left=15.91\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1519/3393 [11:50<15:10,  2.06batch/s, Batch Loss=0.0758, Avg Loss=0.1206, Time Left=15.90\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1520/3393 [11:50<15:34,  2.00batch/s, Batch Loss=0.0758, Avg Loss=0.1206, Time Left=15.90\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1520/3393 [11:50<15:34,  2.00batch/s, Batch Loss=0.0373, Avg Loss=0.1205, Time Left=15.89\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1521/3393 [11:50<15:09,  2.06batch/s, Batch Loss=0.0373, Avg Loss=0.1205, Time Left=15.89\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1521/3393 [11:51<15:09,  2.06batch/s, Batch Loss=0.1725, Avg Loss=0.1206, Time Left=15.89\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1522/3393 [11:51<15:22,  2.03batch/s, Batch Loss=0.1725, Avg Loss=0.1206, Time Left=15.89\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1522/3393 [11:51<15:22,  2.03batch/s, Batch Loss=0.1431, Avg Loss=0.1206, Time Left=15.88\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1523/3393 [11:51<15:10,  2.05batch/s, Batch Loss=0.1431, Avg Loss=0.1206, Time Left=15.88\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1523/3393 [11:52<15:10,  2.05batch/s, Batch Loss=0.0699, Avg Loss=0.1205, Time Left=15.87\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1524/3393 [11:52<15:38,  1.99batch/s, Batch Loss=0.0699, Avg Loss=0.1205, Time Left=15.87\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1524/3393 [11:52<15:38,  1.99batch/s, Batch Loss=0.0609, Avg Loss=0.1205, Time Left=15.86\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1525/3393 [11:52<15:25,  2.02batch/s, Batch Loss=0.0609, Avg Loss=0.1205, Time Left=15.86\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1525/3393 [11:53<15:25,  2.02batch/s, Batch Loss=0.0253, Avg Loss=0.1204, Time Left=15.85\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1526/3393 [11:53<15:20,  2.03batch/s, Batch Loss=0.0253, Avg Loss=0.1204, Time Left=15.85\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1526/3393 [11:53<15:20,  2.03batch/s, Batch Loss=0.2093, Avg Loss=0.1205, Time Left=15.85\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1527/3393 [11:53<15:39,  1.99batch/s, Batch Loss=0.2093, Avg Loss=0.1205, Time Left=15.85\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1527/3393 [11:54<15:39,  1.99batch/s, Batch Loss=0.1332, Avg Loss=0.1205, Time Left=15.84\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1528/3393 [11:54<15:43,  1.98batch/s, Batch Loss=0.1332, Avg Loss=0.1205, Time Left=15.84\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1528/3393 [11:54<15:43,  1.98batch/s, Batch Loss=0.0887, Avg Loss=0.1205, Time Left=15.83\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1529/3393 [11:54<15:43,  1.98batch/s, Batch Loss=0.0887, Avg Loss=0.1205, Time Left=15.83\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1529/3393 [11:55<15:43,  1.98batch/s, Batch Loss=0.0318, Avg Loss=0.1204, Time Left=15.82\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1530/3393 [11:55<15:39,  1.98batch/s, Batch Loss=0.0318, Avg Loss=0.1204, Time Left=15.82\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1530/3393 [11:55<15:39,  1.98batch/s, Batch Loss=0.0142, Avg Loss=0.1203, Time Left=15.81\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1531/3393 [11:55<15:28,  2.01batch/s, Batch Loss=0.0142, Avg Loss=0.1203, Time Left=15.81\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1531/3393 [11:56<15:28,  2.01batch/s, Batch Loss=0.2537, Avg Loss=0.1204, Time Left=15.81\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1532/3393 [11:56<15:10,  2.04batch/s, Batch Loss=0.2537, Avg Loss=0.1204, Time Left=15.81\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1532/3393 [11:56<15:10,  2.04batch/s, Batch Loss=0.0197, Avg Loss=0.1204, Time Left=15.80\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1533/3393 [11:56<15:06,  2.05batch/s, Batch Loss=0.0197, Avg Loss=0.1204, Time Left=15.80\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1533/3393 [11:57<15:06,  2.05batch/s, Batch Loss=0.1180, Avg Loss=0.1204, Time Left=15.79\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1534/3393 [11:57<15:12,  2.04batch/s, Batch Loss=0.1180, Avg Loss=0.1204, Time Left=15.79\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1534/3393 [11:57<15:12,  2.04batch/s, Batch Loss=0.0450, Avg Loss=0.1203, Time Left=15.78\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1535/3393 [11:57<15:09,  2.04batch/s, Batch Loss=0.0450, Avg Loss=0.1203, Time Left=15.78\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1535/3393 [11:58<15:09,  2.04batch/s, Batch Loss=0.0157, Avg Loss=0.1202, Time Left=15.77\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1536/3393 [11:58<14:47,  2.09batch/s, Batch Loss=0.0157, Avg Loss=0.1202, Time Left=15.77\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1536/3393 [11:58<14:47,  2.09batch/s, Batch Loss=0.0750, Avg Loss=0.1202, Time Left=15.76\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1537/3393 [11:58<15:15,  2.03batch/s, Batch Loss=0.0750, Avg Loss=0.1202, Time Left=15.76\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1537/3393 [11:59<15:15,  2.03batch/s, Batch Loss=0.0182, Avg Loss=0.1201, Time Left=15.76\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1538/3393 [11:59<15:02,  2.06batch/s, Batch Loss=0.0182, Avg Loss=0.1201, Time Left=15.76\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1538/3393 [11:59<15:02,  2.06batch/s, Batch Loss=0.4167, Avg Loss=0.1203, Time Left=15.75\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1539/3393 [11:59<15:00,  2.06batch/s, Batch Loss=0.4167, Avg Loss=0.1203, Time Left=15.75\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1539/3393 [12:00<15:00,  2.06batch/s, Batch Loss=0.1864, Avg Loss=0.1204, Time Left=15.74\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1540/3393 [12:00<15:15,  2.02batch/s, Batch Loss=0.1864, Avg Loss=0.1204, Time Left=15.74\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1540/3393 [12:00<15:15,  2.02batch/s, Batch Loss=0.0642, Avg Loss=0.1204, Time Left=15.73\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1541/3393 [12:00<15:11,  2.03batch/s, Batch Loss=0.0642, Avg Loss=0.1204, Time Left=15.73\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1541/3393 [12:01<15:11,  2.03batch/s, Batch Loss=0.1050, Avg Loss=0.1203, Time Left=15.72\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1542/3393 [12:01<15:14,  2.03batch/s, Batch Loss=0.1050, Avg Loss=0.1203, Time Left=15.72\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1542/3393 [12:01<15:14,  2.03batch/s, Batch Loss=0.2024, Avg Loss=0.1204, Time Left=15.72\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1543/3393 [12:01<15:08,  2.04batch/s, Batch Loss=0.2024, Avg Loss=0.1204, Time Left=15.72\u001b[A\n",
      "Epoch 2/3 - Training:  45%|▍| 1543/3393 [12:02<15:08,  2.04batch/s, Batch Loss=0.1421, Avg Loss=0.1204, Time Left=15.71\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1544/3393 [12:02<14:56,  2.06batch/s, Batch Loss=0.1421, Avg Loss=0.1204, Time Left=15.71\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1544/3393 [12:02<14:56,  2.06batch/s, Batch Loss=0.1346, Avg Loss=0.1204, Time Left=15.70\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1545/3393 [12:02<15:02,  2.05batch/s, Batch Loss=0.1346, Avg Loss=0.1204, Time Left=15.70\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1545/3393 [12:03<15:02,  2.05batch/s, Batch Loss=0.1510, Avg Loss=0.1204, Time Left=15.69\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1546/3393 [12:03<14:59,  2.05batch/s, Batch Loss=0.1510, Avg Loss=0.1204, Time Left=15.69\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1546/3393 [12:03<14:59,  2.05batch/s, Batch Loss=0.0290, Avg Loss=0.1204, Time Left=15.68\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1547/3393 [12:03<15:15,  2.02batch/s, Batch Loss=0.0290, Avg Loss=0.1204, Time Left=15.68\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1547/3393 [12:04<15:15,  2.02batch/s, Batch Loss=0.0473, Avg Loss=0.1203, Time Left=15.67\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1548/3393 [12:04<15:18,  2.01batch/s, Batch Loss=0.0473, Avg Loss=0.1203, Time Left=15.67\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1548/3393 [12:04<15:18,  2.01batch/s, Batch Loss=0.1144, Avg Loss=0.1203, Time Left=15.67\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  46%|▍| 1549/3393 [12:04<15:00,  2.05batch/s, Batch Loss=0.1144, Avg Loss=0.1203, Time Left=15.67\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1549/3393 [12:05<15:00,  2.05batch/s, Batch Loss=0.2049, Avg Loss=0.1204, Time Left=15.66\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1550/3393 [12:05<14:49,  2.07batch/s, Batch Loss=0.2049, Avg Loss=0.1204, Time Left=15.66\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1550/3393 [12:05<14:49,  2.07batch/s, Batch Loss=0.1406, Avg Loss=0.1204, Time Left=15.65\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1551/3393 [12:05<14:48,  2.07batch/s, Batch Loss=0.1406, Avg Loss=0.1204, Time Left=15.65\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1551/3393 [12:06<14:48,  2.07batch/s, Batch Loss=0.3121, Avg Loss=0.1205, Time Left=15.64\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1552/3393 [12:06<14:39,  2.09batch/s, Batch Loss=0.3121, Avg Loss=0.1205, Time Left=15.64\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1552/3393 [12:06<14:39,  2.09batch/s, Batch Loss=0.1237, Avg Loss=0.1205, Time Left=15.63\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1553/3393 [12:06<14:51,  2.06batch/s, Batch Loss=0.1237, Avg Loss=0.1205, Time Left=15.63\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1553/3393 [12:07<14:51,  2.06batch/s, Batch Loss=0.0195, Avg Loss=0.1205, Time Left=15.62\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1554/3393 [12:07<14:58,  2.05batch/s, Batch Loss=0.0195, Avg Loss=0.1205, Time Left=15.62\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1554/3393 [12:07<14:58,  2.05batch/s, Batch Loss=0.1639, Avg Loss=0.1205, Time Left=15.62\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1555/3393 [12:07<15:13,  2.01batch/s, Batch Loss=0.1639, Avg Loss=0.1205, Time Left=15.62\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1555/3393 [12:08<15:13,  2.01batch/s, Batch Loss=0.4938, Avg Loss=0.1207, Time Left=15.61\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1556/3393 [12:08<15:06,  2.03batch/s, Batch Loss=0.4938, Avg Loss=0.1207, Time Left=15.61\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1556/3393 [12:08<15:06,  2.03batch/s, Batch Loss=0.0440, Avg Loss=0.1207, Time Left=15.60\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1557/3393 [12:08<14:52,  2.06batch/s, Batch Loss=0.0440, Avg Loss=0.1207, Time Left=15.60\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1557/3393 [12:08<14:52,  2.06batch/s, Batch Loss=0.1320, Avg Loss=0.1207, Time Left=15.59\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1558/3393 [12:08<14:50,  2.06batch/s, Batch Loss=0.1320, Avg Loss=0.1207, Time Left=15.59\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1558/3393 [12:09<14:50,  2.06batch/s, Batch Loss=0.0638, Avg Loss=0.1207, Time Left=15.58\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1559/3393 [12:09<14:58,  2.04batch/s, Batch Loss=0.0638, Avg Loss=0.1207, Time Left=15.58\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1559/3393 [12:09<14:58,  2.04batch/s, Batch Loss=0.2165, Avg Loss=0.1207, Time Left=15.58\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1560/3393 [12:09<15:11,  2.01batch/s, Batch Loss=0.2165, Avg Loss=0.1207, Time Left=15.58\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1560/3393 [12:10<15:11,  2.01batch/s, Batch Loss=0.1064, Avg Loss=0.1207, Time Left=15.57\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1561/3393 [12:10<15:05,  2.02batch/s, Batch Loss=0.1064, Avg Loss=0.1207, Time Left=15.57\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1561/3393 [12:10<15:05,  2.02batch/s, Batch Loss=0.1162, Avg Loss=0.1207, Time Left=15.56\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1562/3393 [12:11<15:16,  2.00batch/s, Batch Loss=0.1162, Avg Loss=0.1207, Time Left=15.56\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1562/3393 [12:11<15:16,  2.00batch/s, Batch Loss=0.0200, Avg Loss=0.1206, Time Left=15.55\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1563/3393 [12:11<15:04,  2.02batch/s, Batch Loss=0.0200, Avg Loss=0.1206, Time Left=15.55\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1563/3393 [12:11<15:04,  2.02batch/s, Batch Loss=0.1035, Avg Loss=0.1206, Time Left=15.54\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1564/3393 [12:11<14:43,  2.07batch/s, Batch Loss=0.1035, Avg Loss=0.1206, Time Left=15.54\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1564/3393 [12:12<14:43,  2.07batch/s, Batch Loss=0.2481, Avg Loss=0.1207, Time Left=15.53\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1565/3393 [12:12<14:51,  2.05batch/s, Batch Loss=0.2481, Avg Loss=0.1207, Time Left=15.53\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1565/3393 [12:12<14:51,  2.05batch/s, Batch Loss=0.0201, Avg Loss=0.1206, Time Left=15.53\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1566/3393 [12:12<14:40,  2.07batch/s, Batch Loss=0.0201, Avg Loss=0.1206, Time Left=15.53\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1566/3393 [12:13<14:40,  2.07batch/s, Batch Loss=0.0324, Avg Loss=0.1206, Time Left=15.52\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1567/3393 [12:13<14:40,  2.07batch/s, Batch Loss=0.0324, Avg Loss=0.1206, Time Left=15.52\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1567/3393 [12:13<14:40,  2.07batch/s, Batch Loss=0.1233, Avg Loss=0.1206, Time Left=15.51\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1568/3393 [12:13<14:40,  2.07batch/s, Batch Loss=0.1233, Avg Loss=0.1206, Time Left=15.51\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1568/3393 [12:14<14:40,  2.07batch/s, Batch Loss=0.0587, Avg Loss=0.1206, Time Left=15.50\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1569/3393 [12:14<14:33,  2.09batch/s, Batch Loss=0.0587, Avg Loss=0.1206, Time Left=15.50\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1569/3393 [12:14<14:33,  2.09batch/s, Batch Loss=0.0519, Avg Loss=0.1205, Time Left=15.49\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1570/3393 [12:14<14:40,  2.07batch/s, Batch Loss=0.0519, Avg Loss=0.1205, Time Left=15.49\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1570/3393 [12:15<14:40,  2.07batch/s, Batch Loss=0.1333, Avg Loss=0.1205, Time Left=15.48\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1571/3393 [12:15<14:42,  2.06batch/s, Batch Loss=0.1333, Avg Loss=0.1205, Time Left=15.48\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1571/3393 [12:15<14:42,  2.06batch/s, Batch Loss=0.0235, Avg Loss=0.1204, Time Left=15.48\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1572/3393 [12:15<14:24,  2.11batch/s, Batch Loss=0.0235, Avg Loss=0.1204, Time Left=15.48\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1572/3393 [12:16<14:24,  2.11batch/s, Batch Loss=0.0727, Avg Loss=0.1204, Time Left=15.47\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1573/3393 [12:16<14:29,  2.09batch/s, Batch Loss=0.0727, Avg Loss=0.1204, Time Left=15.47\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1573/3393 [12:16<14:29,  2.09batch/s, Batch Loss=0.1528, Avg Loss=0.1204, Time Left=15.46\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1574/3393 [12:16<14:31,  2.09batch/s, Batch Loss=0.1528, Avg Loss=0.1204, Time Left=15.46\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1574/3393 [12:17<14:31,  2.09batch/s, Batch Loss=0.1602, Avg Loss=0.1205, Time Left=15.45\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1575/3393 [12:17<14:28,  2.09batch/s, Batch Loss=0.1602, Avg Loss=0.1205, Time Left=15.45\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1575/3393 [12:17<14:28,  2.09batch/s, Batch Loss=0.2061, Avg Loss=0.1205, Time Left=15.44\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1576/3393 [12:17<14:18,  2.12batch/s, Batch Loss=0.2061, Avg Loss=0.1205, Time Left=15.44\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1576/3393 [12:18<14:18,  2.12batch/s, Batch Loss=0.2776, Avg Loss=0.1206, Time Left=15.43\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1577/3393 [12:18<14:32,  2.08batch/s, Batch Loss=0.2776, Avg Loss=0.1206, Time Left=15.43\u001b[A\n",
      "Epoch 2/3 - Training:  46%|▍| 1577/3393 [12:18<14:32,  2.08batch/s, Batch Loss=0.0268, Avg Loss=0.1206, Time Left=15.43\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1578/3393 [12:18<14:26,  2.10batch/s, Batch Loss=0.0268, Avg Loss=0.1206, Time Left=15.43\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1578/3393 [12:19<14:26,  2.10batch/s, Batch Loss=0.0794, Avg Loss=0.1205, Time Left=15.42\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1579/3393 [12:19<14:28,  2.09batch/s, Batch Loss=0.0794, Avg Loss=0.1205, Time Left=15.42\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1579/3393 [12:19<14:28,  2.09batch/s, Batch Loss=0.2105, Avg Loss=0.1206, Time Left=15.41\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1580/3393 [12:19<14:47,  2.04batch/s, Batch Loss=0.2105, Avg Loss=0.1206, Time Left=15.41\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1580/3393 [12:20<14:47,  2.04batch/s, Batch Loss=0.0603, Avg Loss=0.1206, Time Left=15.40\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1581/3393 [12:20<14:53,  2.03batch/s, Batch Loss=0.0603, Avg Loss=0.1206, Time Left=15.40\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1581/3393 [12:20<14:53,  2.03batch/s, Batch Loss=0.0516, Avg Loss=0.1205, Time Left=15.39\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  47%|▍| 1582/3393 [12:20<14:48,  2.04batch/s, Batch Loss=0.0516, Avg Loss=0.1205, Time Left=15.39\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1582/3393 [12:21<14:48,  2.04batch/s, Batch Loss=0.1452, Avg Loss=0.1205, Time Left=15.38\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1583/3393 [12:21<14:36,  2.07batch/s, Batch Loss=0.1452, Avg Loss=0.1205, Time Left=15.38\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1583/3393 [12:21<14:36,  2.07batch/s, Batch Loss=0.0085, Avg Loss=0.1205, Time Left=15.38\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1584/3393 [12:21<14:34,  2.07batch/s, Batch Loss=0.0085, Avg Loss=0.1205, Time Left=15.38\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1584/3393 [12:22<14:34,  2.07batch/s, Batch Loss=0.0499, Avg Loss=0.1204, Time Left=15.37\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1585/3393 [12:22<14:42,  2.05batch/s, Batch Loss=0.0499, Avg Loss=0.1204, Time Left=15.37\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1585/3393 [12:22<14:42,  2.05batch/s, Batch Loss=0.1084, Avg Loss=0.1204, Time Left=15.36\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1586/3393 [12:22<14:40,  2.05batch/s, Batch Loss=0.1084, Avg Loss=0.1204, Time Left=15.36\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1586/3393 [12:23<14:40,  2.05batch/s, Batch Loss=0.1123, Avg Loss=0.1204, Time Left=15.35\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1587/3393 [12:23<14:39,  2.05batch/s, Batch Loss=0.1123, Avg Loss=0.1204, Time Left=15.35\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1587/3393 [12:23<14:39,  2.05batch/s, Batch Loss=0.2275, Avg Loss=0.1205, Time Left=15.34\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1588/3393 [12:23<14:34,  2.06batch/s, Batch Loss=0.2275, Avg Loss=0.1205, Time Left=15.34\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1588/3393 [12:24<14:34,  2.06batch/s, Batch Loss=0.0270, Avg Loss=0.1204, Time Left=15.33\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1589/3393 [12:24<14:35,  2.06batch/s, Batch Loss=0.0270, Avg Loss=0.1204, Time Left=15.33\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1589/3393 [12:24<14:35,  2.06batch/s, Batch Loss=0.0198, Avg Loss=0.1203, Time Left=15.33\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1590/3393 [12:24<14:50,  2.03batch/s, Batch Loss=0.0198, Avg Loss=0.1203, Time Left=15.33\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1590/3393 [12:24<14:50,  2.03batch/s, Batch Loss=0.1695, Avg Loss=0.1204, Time Left=15.32\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1591/3393 [12:24<14:27,  2.08batch/s, Batch Loss=0.1695, Avg Loss=0.1204, Time Left=15.32\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1591/3393 [12:25<14:27,  2.08batch/s, Batch Loss=0.1364, Avg Loss=0.1204, Time Left=15.31\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1592/3393 [12:25<14:36,  2.05batch/s, Batch Loss=0.1364, Avg Loss=0.1204, Time Left=15.31\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1592/3393 [12:25<14:36,  2.05batch/s, Batch Loss=0.0206, Avg Loss=0.1203, Time Left=15.30\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1593/3393 [12:25<14:42,  2.04batch/s, Batch Loss=0.0206, Avg Loss=0.1203, Time Left=15.30\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1593/3393 [12:26<14:42,  2.04batch/s, Batch Loss=0.1241, Avg Loss=0.1203, Time Left=15.29\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1594/3393 [12:26<14:39,  2.04batch/s, Batch Loss=0.1241, Avg Loss=0.1203, Time Left=15.29\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1594/3393 [12:26<14:39,  2.04batch/s, Batch Loss=0.0116, Avg Loss=0.1202, Time Left=15.29\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1595/3393 [12:26<15:01,  1.99batch/s, Batch Loss=0.0116, Avg Loss=0.1202, Time Left=15.29\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1595/3393 [12:27<15:01,  1.99batch/s, Batch Loss=0.0075, Avg Loss=0.1202, Time Left=15.28\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1596/3393 [12:27<15:01,  1.99batch/s, Batch Loss=0.0075, Avg Loss=0.1202, Time Left=15.28\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1596/3393 [12:27<15:01,  1.99batch/s, Batch Loss=0.0076, Avg Loss=0.1201, Time Left=15.27\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1597/3393 [12:27<14:43,  2.03batch/s, Batch Loss=0.0076, Avg Loss=0.1201, Time Left=15.27\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1597/3393 [12:28<14:43,  2.03batch/s, Batch Loss=0.0180, Avg Loss=0.1200, Time Left=15.26\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1598/3393 [12:28<14:38,  2.04batch/s, Batch Loss=0.0180, Avg Loss=0.1200, Time Left=15.26\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1598/3393 [12:28<14:38,  2.04batch/s, Batch Loss=0.0167, Avg Loss=0.1200, Time Left=15.25\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1599/3393 [12:28<14:25,  2.07batch/s, Batch Loss=0.0167, Avg Loss=0.1200, Time Left=15.25\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1599/3393 [12:29<14:25,  2.07batch/s, Batch Loss=0.0092, Avg Loss=0.1199, Time Left=15.24\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1600/3393 [12:29<14:25,  2.07batch/s, Batch Loss=0.0092, Avg Loss=0.1199, Time Left=15.24\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1600/3393 [12:29<14:25,  2.07batch/s, Batch Loss=0.0201, Avg Loss=0.1198, Time Left=15.24\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1601/3393 [12:29<14:17,  2.09batch/s, Batch Loss=0.0201, Avg Loss=0.1198, Time Left=15.24\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1601/3393 [12:30<14:17,  2.09batch/s, Batch Loss=0.0095, Avg Loss=0.1198, Time Left=15.23\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1602/3393 [12:30<14:11,  2.10batch/s, Batch Loss=0.0095, Avg Loss=0.1198, Time Left=15.23\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1602/3393 [12:30<14:11,  2.10batch/s, Batch Loss=0.0930, Avg Loss=0.1197, Time Left=15.22\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1603/3393 [12:30<14:23,  2.07batch/s, Batch Loss=0.0930, Avg Loss=0.1197, Time Left=15.22\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1603/3393 [12:31<14:23,  2.07batch/s, Batch Loss=0.0771, Avg Loss=0.1197, Time Left=15.21\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1604/3393 [12:31<14:32,  2.05batch/s, Batch Loss=0.0771, Avg Loss=0.1197, Time Left=15.21\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1604/3393 [12:31<14:32,  2.05batch/s, Batch Loss=0.1064, Avg Loss=0.1197, Time Left=15.20\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1605/3393 [12:31<14:29,  2.06batch/s, Batch Loss=0.1064, Avg Loss=0.1197, Time Left=15.20\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1605/3393 [12:32<14:29,  2.06batch/s, Batch Loss=0.0899, Avg Loss=0.1197, Time Left=15.20\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1606/3393 [12:32<14:37,  2.04batch/s, Batch Loss=0.0899, Avg Loss=0.1197, Time Left=15.20\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1606/3393 [12:32<14:37,  2.04batch/s, Batch Loss=0.3245, Avg Loss=0.1198, Time Left=15.19\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1607/3393 [12:32<14:25,  2.06batch/s, Batch Loss=0.3245, Avg Loss=0.1198, Time Left=15.19\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1607/3393 [12:33<14:25,  2.06batch/s, Batch Loss=0.0253, Avg Loss=0.1198, Time Left=15.18\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1608/3393 [12:33<14:23,  2.07batch/s, Batch Loss=0.0253, Avg Loss=0.1198, Time Left=15.18\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1608/3393 [12:33<14:23,  2.07batch/s, Batch Loss=0.1647, Avg Loss=0.1198, Time Left=15.17\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1609/3393 [12:33<14:30,  2.05batch/s, Batch Loss=0.1647, Avg Loss=0.1198, Time Left=15.17\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1609/3393 [12:34<14:30,  2.05batch/s, Batch Loss=0.0548, Avg Loss=0.1197, Time Left=15.16\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1610/3393 [12:34<14:28,  2.05batch/s, Batch Loss=0.0548, Avg Loss=0.1197, Time Left=15.16\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1610/3393 [12:34<14:28,  2.05batch/s, Batch Loss=0.0115, Avg Loss=0.1197, Time Left=15.15\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1611/3393 [12:34<14:42,  2.02batch/s, Batch Loss=0.0115, Avg Loss=0.1197, Time Left=15.15\u001b[A\n",
      "Epoch 2/3 - Training:  47%|▍| 1611/3393 [12:35<14:42,  2.02batch/s, Batch Loss=0.0953, Avg Loss=0.1197, Time Left=15.15\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1612/3393 [12:35<14:45,  2.01batch/s, Batch Loss=0.0953, Avg Loss=0.1197, Time Left=15.15\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1612/3393 [12:35<14:45,  2.01batch/s, Batch Loss=0.0208, Avg Loss=0.1196, Time Left=15.14\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1613/3393 [12:35<15:03,  1.97batch/s, Batch Loss=0.0208, Avg Loss=0.1196, Time Left=15.14\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1613/3393 [12:36<15:03,  1.97batch/s, Batch Loss=0.2040, Avg Loss=0.1196, Time Left=15.13\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1614/3393 [12:36<14:47,  2.00batch/s, Batch Loss=0.2040, Avg Loss=0.1196, Time Left=15.13\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1614/3393 [12:36<14:47,  2.00batch/s, Batch Loss=0.1945, Avg Loss=0.1197, Time Left=15.12\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  48%|▍| 1615/3393 [12:36<14:35,  2.03batch/s, Batch Loss=0.1945, Avg Loss=0.1197, Time Left=15.12\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1615/3393 [12:37<14:35,  2.03batch/s, Batch Loss=0.0258, Avg Loss=0.1196, Time Left=15.11\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1616/3393 [12:37<14:29,  2.04batch/s, Batch Loss=0.0258, Avg Loss=0.1196, Time Left=15.11\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1616/3393 [12:37<14:29,  2.04batch/s, Batch Loss=0.0427, Avg Loss=0.1196, Time Left=15.11\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1617/3393 [12:37<14:17,  2.07batch/s, Batch Loss=0.0427, Avg Loss=0.1196, Time Left=15.11\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1617/3393 [12:38<14:17,  2.07batch/s, Batch Loss=0.4492, Avg Loss=0.1198, Time Left=15.10\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1618/3393 [12:38<14:11,  2.08batch/s, Batch Loss=0.4492, Avg Loss=0.1198, Time Left=15.10\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1618/3393 [12:38<14:11,  2.08batch/s, Batch Loss=0.1809, Avg Loss=0.1198, Time Left=15.09\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1619/3393 [12:38<14:18,  2.07batch/s, Batch Loss=0.1809, Avg Loss=0.1198, Time Left=15.09\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1619/3393 [12:39<14:18,  2.07batch/s, Batch Loss=0.0111, Avg Loss=0.1198, Time Left=15.08\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1620/3393 [12:39<14:26,  2.05batch/s, Batch Loss=0.0111, Avg Loss=0.1198, Time Left=15.08\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1620/3393 [12:39<14:26,  2.05batch/s, Batch Loss=0.0052, Avg Loss=0.1197, Time Left=15.07\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1621/3393 [12:39<14:41,  2.01batch/s, Batch Loss=0.0052, Avg Loss=0.1197, Time Left=15.07\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1621/3393 [12:40<14:41,  2.01batch/s, Batch Loss=0.0353, Avg Loss=0.1196, Time Left=15.06\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1622/3393 [12:40<14:42,  2.01batch/s, Batch Loss=0.0353, Avg Loss=0.1196, Time Left=15.06\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1622/3393 [12:40<14:42,  2.01batch/s, Batch Loss=0.0493, Avg Loss=0.1196, Time Left=15.06\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1623/3393 [12:40<14:34,  2.03batch/s, Batch Loss=0.0493, Avg Loss=0.1196, Time Left=15.06\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1623/3393 [12:41<14:34,  2.03batch/s, Batch Loss=0.2199, Avg Loss=0.1197, Time Left=15.05\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1624/3393 [12:41<14:28,  2.04batch/s, Batch Loss=0.2199, Avg Loss=0.1197, Time Left=15.05\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1624/3393 [12:41<14:28,  2.04batch/s, Batch Loss=0.4485, Avg Loss=0.1199, Time Left=15.04\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1625/3393 [12:41<14:20,  2.06batch/s, Batch Loss=0.4485, Avg Loss=0.1199, Time Left=15.04\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1625/3393 [12:42<14:20,  2.06batch/s, Batch Loss=0.0323, Avg Loss=0.1198, Time Left=15.03\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1626/3393 [12:42<14:38,  2.01batch/s, Batch Loss=0.0323, Avg Loss=0.1198, Time Left=15.03\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1626/3393 [12:42<14:38,  2.01batch/s, Batch Loss=0.0189, Avg Loss=0.1197, Time Left=15.02\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1627/3393 [12:42<14:23,  2.04batch/s, Batch Loss=0.0189, Avg Loss=0.1197, Time Left=15.02\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1627/3393 [12:43<14:23,  2.04batch/s, Batch Loss=0.0865, Avg Loss=0.1197, Time Left=15.02\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1628/3393 [12:43<14:19,  2.05batch/s, Batch Loss=0.0865, Avg Loss=0.1197, Time Left=15.02\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1628/3393 [12:43<14:19,  2.05batch/s, Batch Loss=0.0131, Avg Loss=0.1197, Time Left=15.01\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1629/3393 [12:43<14:26,  2.04batch/s, Batch Loss=0.0131, Avg Loss=0.1197, Time Left=15.01\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1629/3393 [12:44<14:26,  2.04batch/s, Batch Loss=0.0487, Avg Loss=0.1196, Time Left=15.00\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1630/3393 [12:44<14:21,  2.05batch/s, Batch Loss=0.0487, Avg Loss=0.1196, Time Left=15.00\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1630/3393 [12:44<14:21,  2.05batch/s, Batch Loss=0.0242, Avg Loss=0.1195, Time Left=14.99\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1631/3393 [12:44<14:35,  2.01batch/s, Batch Loss=0.0242, Avg Loss=0.1195, Time Left=14.99\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1631/3393 [12:45<14:35,  2.01batch/s, Batch Loss=0.2080, Avg Loss=0.1196, Time Left=14.98\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1632/3393 [12:45<14:21,  2.04batch/s, Batch Loss=0.2080, Avg Loss=0.1196, Time Left=14.98\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1632/3393 [12:45<14:21,  2.04batch/s, Batch Loss=0.1827, Avg Loss=0.1196, Time Left=14.97\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1633/3393 [12:45<14:18,  2.05batch/s, Batch Loss=0.1827, Avg Loss=0.1196, Time Left=14.97\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1633/3393 [12:46<14:18,  2.05batch/s, Batch Loss=0.0375, Avg Loss=0.1196, Time Left=14.97\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1634/3393 [12:46<14:21,  2.04batch/s, Batch Loss=0.0375, Avg Loss=0.1196, Time Left=14.97\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1634/3393 [12:46<14:21,  2.04batch/s, Batch Loss=0.1105, Avg Loss=0.1196, Time Left=14.96\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1635/3393 [12:46<14:11,  2.06batch/s, Batch Loss=0.1105, Avg Loss=0.1196, Time Left=14.96\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1635/3393 [12:47<14:11,  2.06batch/s, Batch Loss=0.0393, Avg Loss=0.1195, Time Left=14.95\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1636/3393 [12:47<14:35,  2.01batch/s, Batch Loss=0.0393, Avg Loss=0.1195, Time Left=14.95\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1636/3393 [12:47<14:35,  2.01batch/s, Batch Loss=0.0353, Avg Loss=0.1195, Time Left=14.94\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1637/3393 [12:47<14:36,  2.00batch/s, Batch Loss=0.0353, Avg Loss=0.1195, Time Left=14.94\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1637/3393 [12:48<14:36,  2.00batch/s, Batch Loss=0.1850, Avg Loss=0.1195, Time Left=14.93\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1638/3393 [12:48<14:45,  1.98batch/s, Batch Loss=0.1850, Avg Loss=0.1195, Time Left=14.93\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1638/3393 [12:48<14:45,  1.98batch/s, Batch Loss=0.1602, Avg Loss=0.1195, Time Left=14.93\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1639/3393 [12:48<14:42,  1.99batch/s, Batch Loss=0.1602, Avg Loss=0.1195, Time Left=14.93\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1639/3393 [12:49<14:42,  1.99batch/s, Batch Loss=0.1102, Avg Loss=0.1195, Time Left=14.92\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1640/3393 [12:49<14:23,  2.03batch/s, Batch Loss=0.1102, Avg Loss=0.1195, Time Left=14.92\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1640/3393 [12:49<14:23,  2.03batch/s, Batch Loss=0.1028, Avg Loss=0.1195, Time Left=14.91\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1641/3393 [12:49<14:19,  2.04batch/s, Batch Loss=0.1028, Avg Loss=0.1195, Time Left=14.91\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1641/3393 [12:49<14:19,  2.04batch/s, Batch Loss=0.0471, Avg Loss=0.1195, Time Left=14.90\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1642/3393 [12:49<13:58,  2.09batch/s, Batch Loss=0.0471, Avg Loss=0.1195, Time Left=14.90\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1642/3393 [12:50<13:58,  2.09batch/s, Batch Loss=0.0258, Avg Loss=0.1194, Time Left=14.89\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1643/3393 [12:50<13:59,  2.08batch/s, Batch Loss=0.0258, Avg Loss=0.1194, Time Left=14.89\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1643/3393 [12:50<13:59,  2.08batch/s, Batch Loss=0.1845, Avg Loss=0.1195, Time Left=14.89\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1644/3393 [12:50<14:25,  2.02batch/s, Batch Loss=0.1845, Avg Loss=0.1195, Time Left=14.89\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1644/3393 [12:51<14:25,  2.02batch/s, Batch Loss=0.0450, Avg Loss=0.1194, Time Left=14.88\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1645/3393 [12:51<14:11,  2.05batch/s, Batch Loss=0.0450, Avg Loss=0.1194, Time Left=14.88\u001b[A\n",
      "Epoch 2/3 - Training:  48%|▍| 1645/3393 [12:51<14:11,  2.05batch/s, Batch Loss=0.2872, Avg Loss=0.1195, Time Left=14.87\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1646/3393 [12:51<14:25,  2.02batch/s, Batch Loss=0.2872, Avg Loss=0.1195, Time Left=14.87\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1646/3393 [12:52<14:25,  2.02batch/s, Batch Loss=0.2510, Avg Loss=0.1196, Time Left=14.86\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1647/3393 [12:52<14:19,  2.03batch/s, Batch Loss=0.2510, Avg Loss=0.1196, Time Left=14.86\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1647/3393 [12:52<14:19,  2.03batch/s, Batch Loss=0.2597, Avg Loss=0.1197, Time Left=14.85\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  49%|▍| 1648/3393 [12:52<14:06,  2.06batch/s, Batch Loss=0.2597, Avg Loss=0.1197, Time Left=14.85\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1648/3393 [12:53<14:06,  2.06batch/s, Batch Loss=0.0595, Avg Loss=0.1197, Time Left=14.84\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1649/3393 [12:53<14:06,  2.06batch/s, Batch Loss=0.0595, Avg Loss=0.1197, Time Left=14.84\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1649/3393 [12:53<14:06,  2.06batch/s, Batch Loss=0.0840, Avg Loss=0.1196, Time Left=14.84\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1650/3393 [12:53<13:55,  2.09batch/s, Batch Loss=0.0840, Avg Loss=0.1196, Time Left=14.84\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1650/3393 [12:54<13:55,  2.09batch/s, Batch Loss=0.0111, Avg Loss=0.1196, Time Left=14.83\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1651/3393 [12:54<13:57,  2.08batch/s, Batch Loss=0.0111, Avg Loss=0.1196, Time Left=14.83\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1651/3393 [12:54<13:57,  2.08batch/s, Batch Loss=0.0100, Avg Loss=0.1195, Time Left=14.82\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1652/3393 [12:54<14:06,  2.06batch/s, Batch Loss=0.0100, Avg Loss=0.1195, Time Left=14.82\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1652/3393 [12:55<14:06,  2.06batch/s, Batch Loss=0.0760, Avg Loss=0.1195, Time Left=14.81\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1653/3393 [12:55<14:04,  2.06batch/s, Batch Loss=0.0760, Avg Loss=0.1195, Time Left=14.81\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1653/3393 [12:55<14:04,  2.06batch/s, Batch Loss=0.0760, Avg Loss=0.1194, Time Left=14.80\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1654/3393 [12:55<14:19,  2.02batch/s, Batch Loss=0.0760, Avg Loss=0.1194, Time Left=14.80\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1654/3393 [12:56<14:19,  2.02batch/s, Batch Loss=0.0230, Avg Loss=0.1194, Time Left=14.79\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1655/3393 [12:56<13:58,  2.07batch/s, Batch Loss=0.0230, Avg Loss=0.1194, Time Left=14.79\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1655/3393 [12:56<13:58,  2.07batch/s, Batch Loss=0.0163, Avg Loss=0.1193, Time Left=14.79\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1656/3393 [12:56<13:57,  2.07batch/s, Batch Loss=0.0163, Avg Loss=0.1193, Time Left=14.79\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1656/3393 [12:57<13:57,  2.07batch/s, Batch Loss=0.0444, Avg Loss=0.1193, Time Left=14.78\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1657/3393 [12:57<13:58,  2.07batch/s, Batch Loss=0.0444, Avg Loss=0.1193, Time Left=14.78\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1657/3393 [12:57<13:58,  2.07batch/s, Batch Loss=0.0336, Avg Loss=0.1192, Time Left=14.77\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1658/3393 [12:57<13:57,  2.07batch/s, Batch Loss=0.0336, Avg Loss=0.1192, Time Left=14.77\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1658/3393 [12:58<13:57,  2.07batch/s, Batch Loss=0.0483, Avg Loss=0.1192, Time Left=14.76\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1659/3393 [12:58<13:57,  2.07batch/s, Batch Loss=0.0483, Avg Loss=0.1192, Time Left=14.76\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1659/3393 [12:58<13:57,  2.07batch/s, Batch Loss=0.1073, Avg Loss=0.1192, Time Left=14.75\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1660/3393 [12:58<14:13,  2.03batch/s, Batch Loss=0.1073, Avg Loss=0.1192, Time Left=14.75\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1660/3393 [12:59<14:13,  2.03batch/s, Batch Loss=0.0637, Avg Loss=0.1191, Time Left=14.74\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1661/3393 [12:59<14:10,  2.04batch/s, Batch Loss=0.0637, Avg Loss=0.1191, Time Left=14.74\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1661/3393 [12:59<14:10,  2.04batch/s, Batch Loss=0.0355, Avg Loss=0.1191, Time Left=14.74\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1662/3393 [12:59<14:13,  2.03batch/s, Batch Loss=0.0355, Avg Loss=0.1191, Time Left=14.74\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1662/3393 [13:00<14:13,  2.03batch/s, Batch Loss=0.0722, Avg Loss=0.1190, Time Left=14.73\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1663/3393 [13:00<14:17,  2.02batch/s, Batch Loss=0.0722, Avg Loss=0.1190, Time Left=14.73\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1663/3393 [13:00<14:17,  2.02batch/s, Batch Loss=0.0808, Avg Loss=0.1190, Time Left=14.72\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1664/3393 [13:00<14:10,  2.03batch/s, Batch Loss=0.0808, Avg Loss=0.1190, Time Left=14.72\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1664/3393 [13:01<14:10,  2.03batch/s, Batch Loss=0.0060, Avg Loss=0.1190, Time Left=14.71\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1665/3393 [13:01<13:57,  2.06batch/s, Batch Loss=0.0060, Avg Loss=0.1190, Time Left=14.71\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1665/3393 [13:01<13:57,  2.06batch/s, Batch Loss=0.1334, Avg Loss=0.1190, Time Left=14.70\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1666/3393 [13:01<13:57,  2.06batch/s, Batch Loss=0.1334, Avg Loss=0.1190, Time Left=14.70\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1666/3393 [13:02<13:57,  2.06batch/s, Batch Loss=0.3841, Avg Loss=0.1191, Time Left=14.70\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1667/3393 [13:02<14:11,  2.03batch/s, Batch Loss=0.3841, Avg Loss=0.1191, Time Left=14.70\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1667/3393 [13:02<14:11,  2.03batch/s, Batch Loss=0.0107, Avg Loss=0.1191, Time Left=14.69\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1668/3393 [13:02<13:59,  2.05batch/s, Batch Loss=0.0107, Avg Loss=0.1191, Time Left=14.69\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1668/3393 [13:03<13:59,  2.05batch/s, Batch Loss=0.0032, Avg Loss=0.1190, Time Left=14.68\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1669/3393 [13:03<13:57,  2.06batch/s, Batch Loss=0.0032, Avg Loss=0.1190, Time Left=14.68\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1669/3393 [13:03<13:57,  2.06batch/s, Batch Loss=0.0664, Avg Loss=0.1190, Time Left=14.67\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1670/3393 [13:03<14:18,  2.01batch/s, Batch Loss=0.0664, Avg Loss=0.1190, Time Left=14.67\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1670/3393 [13:04<14:18,  2.01batch/s, Batch Loss=0.3731, Avg Loss=0.1191, Time Left=14.66\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1671/3393 [13:04<14:12,  2.02batch/s, Batch Loss=0.3731, Avg Loss=0.1191, Time Left=14.66\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1671/3393 [13:04<14:12,  2.02batch/s, Batch Loss=0.0042, Avg Loss=0.1190, Time Left=14.66\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1672/3393 [13:04<14:07,  2.03batch/s, Batch Loss=0.0042, Avg Loss=0.1190, Time Left=14.66\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1672/3393 [13:05<14:07,  2.03batch/s, Batch Loss=0.0153, Avg Loss=0.1190, Time Left=14.65\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1673/3393 [13:05<14:09,  2.02batch/s, Batch Loss=0.0153, Avg Loss=0.1190, Time Left=14.65\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1673/3393 [13:05<14:09,  2.02batch/s, Batch Loss=0.0306, Avg Loss=0.1189, Time Left=14.64\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1674/3393 [13:05<14:04,  2.03batch/s, Batch Loss=0.0306, Avg Loss=0.1189, Time Left=14.64\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1674/3393 [13:06<14:04,  2.03batch/s, Batch Loss=0.1025, Avg Loss=0.1189, Time Left=14.63\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1675/3393 [13:06<13:59,  2.05batch/s, Batch Loss=0.1025, Avg Loss=0.1189, Time Left=14.63\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1675/3393 [13:06<13:59,  2.05batch/s, Batch Loss=0.1421, Avg Loss=0.1189, Time Left=14.62\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1676/3393 [13:06<13:57,  2.05batch/s, Batch Loss=0.1421, Avg Loss=0.1189, Time Left=14.62\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1676/3393 [13:07<13:57,  2.05batch/s, Batch Loss=0.1297, Avg Loss=0.1189, Time Left=14.61\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1677/3393 [13:07<14:17,  2.00batch/s, Batch Loss=0.1297, Avg Loss=0.1189, Time Left=14.61\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1677/3393 [13:07<14:17,  2.00batch/s, Batch Loss=0.0254, Avg Loss=0.1189, Time Left=14.61\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1678/3393 [13:07<14:02,  2.04batch/s, Batch Loss=0.0254, Avg Loss=0.1189, Time Left=14.61\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1678/3393 [13:08<14:02,  2.04batch/s, Batch Loss=0.0523, Avg Loss=0.1188, Time Left=14.60\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1679/3393 [13:08<13:58,  2.04batch/s, Batch Loss=0.0523, Avg Loss=0.1188, Time Left=14.60\u001b[A\n",
      "Epoch 2/3 - Training:  49%|▍| 1679/3393 [13:08<13:58,  2.04batch/s, Batch Loss=0.2082, Avg Loss=0.1189, Time Left=14.59\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1680/3393 [13:08<13:55,  2.05batch/s, Batch Loss=0.2082, Avg Loss=0.1189, Time Left=14.59\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1680/3393 [13:09<13:55,  2.05batch/s, Batch Loss=0.2452, Avg Loss=0.1190, Time Left=14.58\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  50%|▍| 1681/3393 [13:09<14:01,  2.04batch/s, Batch Loss=0.2452, Avg Loss=0.1190, Time Left=14.58\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1681/3393 [13:09<14:01,  2.04batch/s, Batch Loss=0.3821, Avg Loss=0.1191, Time Left=14.57\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1682/3393 [13:09<14:04,  2.03batch/s, Batch Loss=0.3821, Avg Loss=0.1191, Time Left=14.57\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1682/3393 [13:10<14:04,  2.03batch/s, Batch Loss=0.0261, Avg Loss=0.1191, Time Left=14.57\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1683/3393 [13:10<13:52,  2.05batch/s, Batch Loss=0.0261, Avg Loss=0.1191, Time Left=14.57\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1683/3393 [13:10<13:52,  2.05batch/s, Batch Loss=0.0268, Avg Loss=0.1190, Time Left=14.56\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1684/3393 [13:10<13:41,  2.08batch/s, Batch Loss=0.0268, Avg Loss=0.1190, Time Left=14.56\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1684/3393 [13:10<13:41,  2.08batch/s, Batch Loss=0.0102, Avg Loss=0.1189, Time Left=14.55\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1685/3393 [13:10<13:42,  2.08batch/s, Batch Loss=0.0102, Avg Loss=0.1189, Time Left=14.55\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1685/3393 [13:11<13:42,  2.08batch/s, Batch Loss=0.0553, Avg Loss=0.1189, Time Left=14.54\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1686/3393 [13:11<13:42,  2.07batch/s, Batch Loss=0.0553, Avg Loss=0.1189, Time Left=14.54\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1686/3393 [13:11<13:42,  2.07batch/s, Batch Loss=0.1252, Avg Loss=0.1189, Time Left=14.53\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1687/3393 [13:11<13:43,  2.07batch/s, Batch Loss=0.1252, Avg Loss=0.1189, Time Left=14.53\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1687/3393 [13:12<13:43,  2.07batch/s, Batch Loss=0.0536, Avg Loss=0.1189, Time Left=14.52\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1688/3393 [13:12<13:58,  2.03batch/s, Batch Loss=0.0536, Avg Loss=0.1189, Time Left=14.52\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1688/3393 [13:12<13:58,  2.03batch/s, Batch Loss=0.0610, Avg Loss=0.1188, Time Left=14.52\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1689/3393 [13:12<14:02,  2.02batch/s, Batch Loss=0.0610, Avg Loss=0.1188, Time Left=14.52\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1689/3393 [13:13<14:02,  2.02batch/s, Batch Loss=0.0346, Avg Loss=0.1188, Time Left=14.51\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1690/3393 [13:13<13:49,  2.05batch/s, Batch Loss=0.0346, Avg Loss=0.1188, Time Left=14.51\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1690/3393 [13:13<13:49,  2.05batch/s, Batch Loss=0.0333, Avg Loss=0.1187, Time Left=14.50\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1691/3393 [13:13<13:47,  2.06batch/s, Batch Loss=0.0333, Avg Loss=0.1187, Time Left=14.50\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1691/3393 [13:14<13:47,  2.06batch/s, Batch Loss=0.0232, Avg Loss=0.1187, Time Left=14.49\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1692/3393 [13:14<13:37,  2.08batch/s, Batch Loss=0.0232, Avg Loss=0.1187, Time Left=14.49\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1692/3393 [13:14<13:37,  2.08batch/s, Batch Loss=0.1406, Avg Loss=0.1187, Time Left=14.48\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1693/3393 [13:14<14:10,  2.00batch/s, Batch Loss=0.1406, Avg Loss=0.1187, Time Left=14.48\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1693/3393 [13:15<14:10,  2.00batch/s, Batch Loss=0.1113, Avg Loss=0.1187, Time Left=14.48\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1694/3393 [13:15<14:03,  2.01batch/s, Batch Loss=0.1113, Avg Loss=0.1187, Time Left=14.48\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1694/3393 [13:15<14:03,  2.01batch/s, Batch Loss=0.0325, Avg Loss=0.1186, Time Left=14.47\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1695/3393 [13:15<14:04,  2.01batch/s, Batch Loss=0.0325, Avg Loss=0.1186, Time Left=14.47\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1695/3393 [13:16<14:04,  2.01batch/s, Batch Loss=0.0110, Avg Loss=0.1186, Time Left=14.46\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1696/3393 [13:16<13:49,  2.05batch/s, Batch Loss=0.0110, Avg Loss=0.1186, Time Left=14.46\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▍| 1696/3393 [13:16<13:49,  2.05batch/s, Batch Loss=0.1338, Avg Loss=0.1186, Time Left=14.45\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1697/3393 [13:16<13:38,  2.07batch/s, Batch Loss=0.1338, Avg Loss=0.1186, Time Left=14.45\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1697/3393 [13:17<13:38,  2.07batch/s, Batch Loss=0.0814, Avg Loss=0.1185, Time Left=14.44\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1698/3393 [13:17<14:01,  2.01batch/s, Batch Loss=0.0814, Avg Loss=0.1185, Time Left=14.44\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1698/3393 [13:17<14:01,  2.01batch/s, Batch Loss=0.0450, Avg Loss=0.1185, Time Left=14.43\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1699/3393 [13:17<13:55,  2.03batch/s, Batch Loss=0.0450, Avg Loss=0.1185, Time Left=14.43\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1699/3393 [13:18<13:55,  2.03batch/s, Batch Loss=0.1251, Avg Loss=0.1185, Time Left=14.43\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1700/3393 [13:18<13:58,  2.02batch/s, Batch Loss=0.1251, Avg Loss=0.1185, Time Left=14.43\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1700/3393 [13:18<13:58,  2.02batch/s, Batch Loss=0.0283, Avg Loss=0.1185, Time Left=14.42\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1701/3393 [13:18<13:44,  2.05batch/s, Batch Loss=0.0283, Avg Loss=0.1185, Time Left=14.42\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1701/3393 [13:19<13:44,  2.05batch/s, Batch Loss=0.1604, Avg Loss=0.1185, Time Left=14.41\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1702/3393 [13:19<13:44,  2.05batch/s, Batch Loss=0.1604, Avg Loss=0.1185, Time Left=14.41\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1702/3393 [13:19<13:44,  2.05batch/s, Batch Loss=0.3619, Avg Loss=0.1186, Time Left=14.40\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1703/3393 [13:19<13:55,  2.02batch/s, Batch Loss=0.3619, Avg Loss=0.1186, Time Left=14.40\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1703/3393 [13:20<13:55,  2.02batch/s, Batch Loss=0.2673, Avg Loss=0.1187, Time Left=14.39\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1704/3393 [13:20<13:50,  2.03batch/s, Batch Loss=0.2673, Avg Loss=0.1187, Time Left=14.39\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1704/3393 [13:20<13:50,  2.03batch/s, Batch Loss=0.1037, Avg Loss=0.1187, Time Left=14.39\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1705/3393 [13:20<14:01,  2.01batch/s, Batch Loss=0.1037, Avg Loss=0.1187, Time Left=14.39\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1705/3393 [13:21<14:01,  2.01batch/s, Batch Loss=0.0669, Avg Loss=0.1187, Time Left=14.38\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1706/3393 [13:21<13:46,  2.04batch/s, Batch Loss=0.0669, Avg Loss=0.1187, Time Left=14.38\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1706/3393 [13:21<13:46,  2.04batch/s, Batch Loss=0.0920, Avg Loss=0.1187, Time Left=14.37\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1707/3393 [13:21<13:42,  2.05batch/s, Batch Loss=0.0920, Avg Loss=0.1187, Time Left=14.37\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1707/3393 [13:22<13:42,  2.05batch/s, Batch Loss=0.0224, Avg Loss=0.1186, Time Left=14.36\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1708/3393 [13:22<13:55,  2.02batch/s, Batch Loss=0.0224, Avg Loss=0.1186, Time Left=14.36\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1708/3393 [13:22<13:55,  2.02batch/s, Batch Loss=0.0928, Avg Loss=0.1186, Time Left=14.35\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1709/3393 [13:22<13:50,  2.03batch/s, Batch Loss=0.0928, Avg Loss=0.1186, Time Left=14.35\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1709/3393 [13:23<13:50,  2.03batch/s, Batch Loss=0.2755, Avg Loss=0.1187, Time Left=14.34\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1710/3393 [13:23<13:52,  2.02batch/s, Batch Loss=0.2755, Avg Loss=0.1187, Time Left=14.34\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1710/3393 [13:23<13:52,  2.02batch/s, Batch Loss=0.4802, Avg Loss=0.1189, Time Left=14.34\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1711/3393 [13:23<13:47,  2.03batch/s, Batch Loss=0.4802, Avg Loss=0.1189, Time Left=14.34\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1711/3393 [13:24<13:47,  2.03batch/s, Batch Loss=0.0744, Avg Loss=0.1189, Time Left=14.33\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1712/3393 [13:24<13:43,  2.04batch/s, Batch Loss=0.0744, Avg Loss=0.1189, Time Left=14.33\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1712/3393 [13:24<13:43,  2.04batch/s, Batch Loss=0.0715, Avg Loss=0.1188, Time Left=14.32\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1713/3393 [13:24<13:47,  2.03batch/s, Batch Loss=0.0715, Avg Loss=0.1188, Time Left=14.32\u001b[A\n",
      "Epoch 2/3 - Training:  50%|▌| 1713/3393 [13:25<13:47,  2.03batch/s, Batch Loss=0.2209, Avg Loss=0.1189, Time Left=14.31\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  51%|▌| 1714/3393 [13:25<13:42,  2.04batch/s, Batch Loss=0.2209, Avg Loss=0.1189, Time Left=14.31\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1714/3393 [13:25<13:42,  2.04batch/s, Batch Loss=0.0984, Avg Loss=0.1189, Time Left=14.30\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1715/3393 [13:25<13:39,  2.05batch/s, Batch Loss=0.0984, Avg Loss=0.1189, Time Left=14.30\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1715/3393 [13:26<13:39,  2.05batch/s, Batch Loss=0.0339, Avg Loss=0.1188, Time Left=14.30\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1716/3393 [13:26<13:36,  2.05batch/s, Batch Loss=0.0339, Avg Loss=0.1188, Time Left=14.30\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1716/3393 [13:26<13:36,  2.05batch/s, Batch Loss=0.1133, Avg Loss=0.1188, Time Left=14.29\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1717/3393 [13:26<13:33,  2.06batch/s, Batch Loss=0.1133, Avg Loss=0.1188, Time Left=14.29\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1717/3393 [13:27<13:33,  2.06batch/s, Batch Loss=0.3047, Avg Loss=0.1190, Time Left=14.28\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1718/3393 [13:27<13:40,  2.04batch/s, Batch Loss=0.3047, Avg Loss=0.1190, Time Left=14.28\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1718/3393 [13:27<13:40,  2.04batch/s, Batch Loss=0.0965, Avg Loss=0.1189, Time Left=14.27\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1719/3393 [13:27<13:37,  2.05batch/s, Batch Loss=0.0965, Avg Loss=0.1189, Time Left=14.27\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1719/3393 [13:28<13:37,  2.05batch/s, Batch Loss=0.0504, Avg Loss=0.1189, Time Left=14.26\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1720/3393 [13:28<13:35,  2.05batch/s, Batch Loss=0.0504, Avg Loss=0.1189, Time Left=14.26\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1720/3393 [13:28<13:35,  2.05batch/s, Batch Loss=0.0135, Avg Loss=0.1188, Time Left=14.25\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1721/3393 [13:28<13:32,  2.06batch/s, Batch Loss=0.0135, Avg Loss=0.1188, Time Left=14.25\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1721/3393 [13:29<13:32,  2.06batch/s, Batch Loss=0.0639, Avg Loss=0.1188, Time Left=14.25\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1722/3393 [13:29<13:30,  2.06batch/s, Batch Loss=0.0639, Avg Loss=0.1188, Time Left=14.25\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1722/3393 [13:29<13:30,  2.06batch/s, Batch Loss=0.1391, Avg Loss=0.1188, Time Left=14.24\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1723/3393 [13:29<13:45,  2.02batch/s, Batch Loss=0.1391, Avg Loss=0.1188, Time Left=14.24\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1723/3393 [13:30<13:45,  2.02batch/s, Batch Loss=0.4124, Avg Loss=0.1190, Time Left=14.23\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1724/3393 [13:30<13:40,  2.03batch/s, Batch Loss=0.4124, Avg Loss=0.1190, Time Left=14.23\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1724/3393 [13:30<13:40,  2.03batch/s, Batch Loss=0.0297, Avg Loss=0.1189, Time Left=14.22\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1725/3393 [13:30<13:35,  2.05batch/s, Batch Loss=0.0297, Avg Loss=0.1189, Time Left=14.22\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1725/3393 [13:31<13:35,  2.05batch/s, Batch Loss=0.4697, Avg Loss=0.1192, Time Left=14.21\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1726/3393 [13:31<13:40,  2.03batch/s, Batch Loss=0.4697, Avg Loss=0.1192, Time Left=14.21\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1726/3393 [13:31<13:40,  2.03batch/s, Batch Loss=0.1109, Avg Loss=0.1191, Time Left=14.20\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1727/3393 [13:31<13:36,  2.04batch/s, Batch Loss=0.1109, Avg Loss=0.1191, Time Left=14.20\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1727/3393 [13:32<13:36,  2.04batch/s, Batch Loss=0.0987, Avg Loss=0.1191, Time Left=14.20\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1728/3393 [13:32<13:32,  2.05batch/s, Batch Loss=0.0987, Avg Loss=0.1191, Time Left=14.20\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1728/3393 [13:32<13:32,  2.05batch/s, Batch Loss=0.0120, Avg Loss=0.1191, Time Left=14.19\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1729/3393 [13:32<13:23,  2.07batch/s, Batch Loss=0.0120, Avg Loss=0.1191, Time Left=14.19\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1729/3393 [13:33<13:23,  2.07batch/s, Batch Loss=0.0342, Avg Loss=0.1190, Time Left=14.18\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1730/3393 [13:33<13:24,  2.07batch/s, Batch Loss=0.0342, Avg Loss=0.1190, Time Left=14.18\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1730/3393 [13:33<13:24,  2.07batch/s, Batch Loss=0.0489, Avg Loss=0.1190, Time Left=14.17\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1731/3393 [13:33<13:21,  2.07batch/s, Batch Loss=0.0489, Avg Loss=0.1190, Time Left=14.17\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1731/3393 [13:34<13:21,  2.07batch/s, Batch Loss=0.0028, Avg Loss=0.1189, Time Left=14.16\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1732/3393 [13:34<13:14,  2.09batch/s, Batch Loss=0.0028, Avg Loss=0.1189, Time Left=14.16\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1732/3393 [13:34<13:14,  2.09batch/s, Batch Loss=0.0069, Avg Loss=0.1188, Time Left=14.15\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1733/3393 [13:34<13:15,  2.09batch/s, Batch Loss=0.0069, Avg Loss=0.1188, Time Left=14.15\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1733/3393 [13:34<13:15,  2.09batch/s, Batch Loss=0.1697, Avg Loss=0.1189, Time Left=14.15\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1734/3393 [13:34<13:25,  2.06batch/s, Batch Loss=0.1697, Avg Loss=0.1189, Time Left=14.15\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1734/3393 [13:35<13:25,  2.06batch/s, Batch Loss=0.7355, Avg Loss=0.1192, Time Left=14.14\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1735/3393 [13:35<13:24,  2.06batch/s, Batch Loss=0.7355, Avg Loss=0.1192, Time Left=14.14\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1735/3393 [13:35<13:24,  2.06batch/s, Batch Loss=0.0853, Avg Loss=0.1192, Time Left=14.13\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1736/3393 [13:35<13:23,  2.06batch/s, Batch Loss=0.0853, Avg Loss=0.1192, Time Left=14.13\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1736/3393 [13:36<13:23,  2.06batch/s, Batch Loss=0.1729, Avg Loss=0.1193, Time Left=14.12\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1737/3393 [13:36<13:14,  2.08batch/s, Batch Loss=0.1729, Avg Loss=0.1193, Time Left=14.12\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1737/3393 [13:36<13:14,  2.08batch/s, Batch Loss=0.0261, Avg Loss=0.1192, Time Left=14.11\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1738/3393 [13:36<13:07,  2.10batch/s, Batch Loss=0.0261, Avg Loss=0.1192, Time Left=14.11\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1738/3393 [13:37<13:07,  2.10batch/s, Batch Loss=0.1241, Avg Loss=0.1192, Time Left=14.11\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1739/3393 [13:37<13:18,  2.07batch/s, Batch Loss=0.1241, Avg Loss=0.1192, Time Left=14.11\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1739/3393 [13:37<13:18,  2.07batch/s, Batch Loss=0.2229, Avg Loss=0.1193, Time Left=14.10\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1740/3393 [13:37<13:10,  2.09batch/s, Batch Loss=0.2229, Avg Loss=0.1193, Time Left=14.10\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1740/3393 [13:38<13:10,  2.09batch/s, Batch Loss=0.5123, Avg Loss=0.1195, Time Left=14.09\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1741/3393 [13:38<13:12,  2.08batch/s, Batch Loss=0.5123, Avg Loss=0.1195, Time Left=14.09\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1741/3393 [13:38<13:12,  2.08batch/s, Batch Loss=0.0486, Avg Loss=0.1195, Time Left=14.08\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1742/3393 [13:38<13:45,  2.00batch/s, Batch Loss=0.0486, Avg Loss=0.1195, Time Left=14.08\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1742/3393 [13:39<13:45,  2.00batch/s, Batch Loss=0.1217, Avg Loss=0.1195, Time Left=14.07\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1743/3393 [13:39<13:31,  2.03batch/s, Batch Loss=0.1217, Avg Loss=0.1195, Time Left=14.07\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1743/3393 [13:39<13:31,  2.03batch/s, Batch Loss=0.2236, Avg Loss=0.1195, Time Left=14.06\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1744/3393 [13:39<13:33,  2.03batch/s, Batch Loss=0.2236, Avg Loss=0.1195, Time Left=14.06\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1744/3393 [13:40<13:33,  2.03batch/s, Batch Loss=0.0335, Avg Loss=0.1195, Time Left=14.06\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1745/3393 [13:40<13:20,  2.06batch/s, Batch Loss=0.0335, Avg Loss=0.1195, Time Left=14.06\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1745/3393 [13:40<13:20,  2.06batch/s, Batch Loss=0.0699, Avg Loss=0.1194, Time Left=14.05\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1746/3393 [13:40<13:12,  2.08batch/s, Batch Loss=0.0699, Avg Loss=0.1194, Time Left=14.05\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1746/3393 [13:41<13:12,  2.08batch/s, Batch Loss=0.3752, Avg Loss=0.1196, Time Left=14.04\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  51%|▌| 1747/3393 [13:41<13:19,  2.06batch/s, Batch Loss=0.3752, Avg Loss=0.1196, Time Left=14.04\u001b[A\n",
      "Epoch 2/3 - Training:  51%|▌| 1747/3393 [13:41<13:19,  2.06batch/s, Batch Loss=0.0462, Avg Loss=0.1195, Time Left=14.03\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1748/3393 [13:41<13:19,  2.06batch/s, Batch Loss=0.0462, Avg Loss=0.1195, Time Left=14.03\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1748/3393 [13:42<13:19,  2.06batch/s, Batch Loss=0.0748, Avg Loss=0.1195, Time Left=14.02\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1749/3393 [13:42<13:08,  2.08batch/s, Batch Loss=0.0748, Avg Loss=0.1195, Time Left=14.02\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1749/3393 [13:42<13:08,  2.08batch/s, Batch Loss=0.0392, Avg Loss=0.1195, Time Left=14.02\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1750/3393 [13:42<13:33,  2.02batch/s, Batch Loss=0.0392, Avg Loss=0.1195, Time Left=14.02\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1750/3393 [13:43<13:33,  2.02batch/s, Batch Loss=0.0924, Avg Loss=0.1195, Time Left=14.01\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1751/3393 [13:43<13:28,  2.03batch/s, Batch Loss=0.0924, Avg Loss=0.1195, Time Left=14.01\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1751/3393 [13:43<13:28,  2.03batch/s, Batch Loss=0.0746, Avg Loss=0.1194, Time Left=14.00\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1752/3393 [13:43<13:47,  1.98batch/s, Batch Loss=0.0746, Avg Loss=0.1194, Time Left=14.00\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1752/3393 [13:44<13:47,  1.98batch/s, Batch Loss=0.1712, Avg Loss=0.1195, Time Left=13.99\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1753/3393 [13:44<13:45,  1.99batch/s, Batch Loss=0.1712, Avg Loss=0.1195, Time Left=13.99\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1753/3393 [13:44<13:45,  1.99batch/s, Batch Loss=0.1136, Avg Loss=0.1195, Time Left=13.98\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1754/3393 [13:44<13:58,  1.96batch/s, Batch Loss=0.1136, Avg Loss=0.1195, Time Left=13.98\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1754/3393 [13:45<13:58,  1.96batch/s, Batch Loss=0.1281, Avg Loss=0.1195, Time Left=13.98\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1755/3393 [13:45<13:46,  1.98batch/s, Batch Loss=0.1281, Avg Loss=0.1195, Time Left=13.98\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1755/3393 [13:45<13:46,  1.98batch/s, Batch Loss=0.1117, Avg Loss=0.1195, Time Left=13.97\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1756/3393 [13:45<13:27,  2.03batch/s, Batch Loss=0.1117, Avg Loss=0.1195, Time Left=13.97\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1756/3393 [13:46<13:27,  2.03batch/s, Batch Loss=0.0407, Avg Loss=0.1194, Time Left=13.96\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1757/3393 [13:46<13:22,  2.04batch/s, Batch Loss=0.0407, Avg Loss=0.1194, Time Left=13.96\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1757/3393 [13:46<13:22,  2.04batch/s, Batch Loss=0.0563, Avg Loss=0.1194, Time Left=13.95\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1758/3393 [13:46<13:18,  2.05batch/s, Batch Loss=0.0563, Avg Loss=0.1194, Time Left=13.95\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1758/3393 [13:47<13:18,  2.05batch/s, Batch Loss=0.0498, Avg Loss=0.1193, Time Left=13.94\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1759/3393 [13:47<13:30,  2.02batch/s, Batch Loss=0.0498, Avg Loss=0.1193, Time Left=13.94\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1759/3393 [13:47<13:30,  2.02batch/s, Batch Loss=0.2258, Avg Loss=0.1194, Time Left=13.93\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1760/3393 [13:47<13:30,  2.02batch/s, Batch Loss=0.2258, Avg Loss=0.1194, Time Left=13.93\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1760/3393 [13:48<13:30,  2.02batch/s, Batch Loss=0.1677, Avg Loss=0.1194, Time Left=13.93\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1761/3393 [13:48<13:26,  2.02batch/s, Batch Loss=0.1677, Avg Loss=0.1194, Time Left=13.93\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1761/3393 [13:48<13:26,  2.02batch/s, Batch Loss=0.0429, Avg Loss=0.1194, Time Left=13.92\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1762/3393 [13:48<13:21,  2.04batch/s, Batch Loss=0.0429, Avg Loss=0.1194, Time Left=13.92\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1762/3393 [13:49<13:21,  2.04batch/s, Batch Loss=0.0787, Avg Loss=0.1194, Time Left=13.91\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1763/3393 [13:49<13:16,  2.05batch/s, Batch Loss=0.0787, Avg Loss=0.1194, Time Left=13.91\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1763/3393 [13:49<13:16,  2.05batch/s, Batch Loss=0.0168, Avg Loss=0.1193, Time Left=13.90\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1764/3393 [13:49<13:29,  2.01batch/s, Batch Loss=0.0168, Avg Loss=0.1193, Time Left=13.90\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1764/3393 [13:50<13:29,  2.01batch/s, Batch Loss=0.0206, Avg Loss=0.1192, Time Left=13.89\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1765/3393 [13:50<13:31,  2.01batch/s, Batch Loss=0.0206, Avg Loss=0.1192, Time Left=13.89\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1765/3393 [13:50<13:31,  2.01batch/s, Batch Loss=0.0153, Avg Loss=0.1192, Time Left=13.89\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1766/3393 [13:50<13:22,  2.03batch/s, Batch Loss=0.0153, Avg Loss=0.1192, Time Left=13.89\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1766/3393 [13:51<13:22,  2.03batch/s, Batch Loss=0.0110, Avg Loss=0.1191, Time Left=13.88\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1767/3393 [13:51<13:11,  2.05batch/s, Batch Loss=0.0110, Avg Loss=0.1191, Time Left=13.88\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1767/3393 [13:51<13:11,  2.05batch/s, Batch Loss=0.4075, Avg Loss=0.1193, Time Left=13.87\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1768/3393 [13:51<13:01,  2.08batch/s, Batch Loss=0.4075, Avg Loss=0.1193, Time Left=13.87\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1768/3393 [13:52<13:01,  2.08batch/s, Batch Loss=0.0130, Avg Loss=0.1192, Time Left=13.86\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1769/3393 [13:52<13:40,  1.98batch/s, Batch Loss=0.0130, Avg Loss=0.1192, Time Left=13.86\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1769/3393 [13:52<13:40,  1.98batch/s, Batch Loss=0.0386, Avg Loss=0.1192, Time Left=13.85\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1770/3393 [13:52<13:37,  1.99batch/s, Batch Loss=0.0386, Avg Loss=0.1192, Time Left=13.85\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1770/3393 [13:53<13:37,  1.99batch/s, Batch Loss=0.0366, Avg Loss=0.1191, Time Left=13.85\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1771/3393 [13:53<13:36,  1.99batch/s, Batch Loss=0.0366, Avg Loss=0.1191, Time Left=13.85\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1771/3393 [13:53<13:36,  1.99batch/s, Batch Loss=0.0115, Avg Loss=0.1191, Time Left=13.84\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1772/3393 [13:53<13:34,  1.99batch/s, Batch Loss=0.0115, Avg Loss=0.1191, Time Left=13.84\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1772/3393 [13:54<13:34,  1.99batch/s, Batch Loss=0.4771, Avg Loss=0.1193, Time Left=13.83\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1773/3393 [13:54<14:01,  1.93batch/s, Batch Loss=0.4771, Avg Loss=0.1193, Time Left=13.83\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1773/3393 [13:54<14:01,  1.93batch/s, Batch Loss=0.0171, Avg Loss=0.1192, Time Left=13.82\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1774/3393 [13:54<13:47,  1.96batch/s, Batch Loss=0.0171, Avg Loss=0.1192, Time Left=13.82\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1774/3393 [13:55<13:47,  1.96batch/s, Batch Loss=0.1653, Avg Loss=0.1192, Time Left=13.81\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1775/3393 [13:55<13:41,  1.97batch/s, Batch Loss=0.1653, Avg Loss=0.1192, Time Left=13.81\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1775/3393 [13:55<13:41,  1.97batch/s, Batch Loss=0.0259, Avg Loss=0.1192, Time Left=13.81\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1776/3393 [13:55<13:22,  2.01batch/s, Batch Loss=0.0259, Avg Loss=0.1192, Time Left=13.81\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1776/3393 [13:56<13:22,  2.01batch/s, Batch Loss=0.2358, Avg Loss=0.1192, Time Left=13.80\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1777/3393 [13:56<13:15,  2.03batch/s, Batch Loss=0.2358, Avg Loss=0.1192, Time Left=13.80\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1777/3393 [13:56<13:15,  2.03batch/s, Batch Loss=0.4042, Avg Loss=0.1194, Time Left=13.79\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1778/3393 [13:56<13:10,  2.04batch/s, Batch Loss=0.4042, Avg Loss=0.1194, Time Left=13.79\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1778/3393 [13:57<13:10,  2.04batch/s, Batch Loss=0.0874, Avg Loss=0.1194, Time Left=13.78\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1779/3393 [13:57<12:51,  2.09batch/s, Batch Loss=0.0874, Avg Loss=0.1194, Time Left=13.78\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1779/3393 [13:57<12:51,  2.09batch/s, Batch Loss=0.0402, Avg Loss=0.1193, Time Left=13.77\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  52%|▌| 1780/3393 [13:57<12:53,  2.09batch/s, Batch Loss=0.0402, Avg Loss=0.1193, Time Left=13.77\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1780/3393 [13:58<12:53,  2.09batch/s, Batch Loss=0.0452, Avg Loss=0.1193, Time Left=13.76\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1781/3393 [13:58<13:18,  2.02batch/s, Batch Loss=0.0452, Avg Loss=0.1193, Time Left=13.76\u001b[A\n",
      "Epoch 2/3 - Training:  52%|▌| 1781/3393 [13:58<13:18,  2.02batch/s, Batch Loss=0.0305, Avg Loss=0.1193, Time Left=13.76\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1782/3393 [13:58<13:12,  2.03batch/s, Batch Loss=0.0305, Avg Loss=0.1193, Time Left=13.76\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1782/3393 [13:59<13:12,  2.03batch/s, Batch Loss=0.0514, Avg Loss=0.1192, Time Left=13.75\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1783/3393 [13:59<13:15,  2.02batch/s, Batch Loss=0.0514, Avg Loss=0.1192, Time Left=13.75\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1783/3393 [13:59<13:15,  2.02batch/s, Batch Loss=0.1042, Avg Loss=0.1192, Time Left=13.74\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1784/3393 [13:59<12:55,  2.07batch/s, Batch Loss=0.1042, Avg Loss=0.1192, Time Left=13.74\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1784/3393 [14:00<12:55,  2.07batch/s, Batch Loss=0.0794, Avg Loss=0.1192, Time Left=13.73\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1785/3393 [14:00<13:02,  2.05batch/s, Batch Loss=0.0794, Avg Loss=0.1192, Time Left=13.73\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1785/3393 [14:00<13:02,  2.05batch/s, Batch Loss=0.0800, Avg Loss=0.1192, Time Left=13.72\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1786/3393 [14:00<13:02,  2.05batch/s, Batch Loss=0.0800, Avg Loss=0.1192, Time Left=13.72\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1786/3393 [14:01<13:02,  2.05batch/s, Batch Loss=0.1561, Avg Loss=0.1192, Time Left=13.71\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1787/3393 [14:01<12:43,  2.10batch/s, Batch Loss=0.1561, Avg Loss=0.1192, Time Left=13.71\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1787/3393 [14:01<12:43,  2.10batch/s, Batch Loss=0.0219, Avg Loss=0.1191, Time Left=13.71\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1788/3393 [14:01<12:54,  2.07batch/s, Batch Loss=0.0219, Avg Loss=0.1191, Time Left=13.71\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1788/3393 [14:02<12:54,  2.07batch/s, Batch Loss=0.0452, Avg Loss=0.1191, Time Left=13.70\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1789/3393 [14:02<13:01,  2.05batch/s, Batch Loss=0.0452, Avg Loss=0.1191, Time Left=13.70\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1789/3393 [14:02<13:01,  2.05batch/s, Batch Loss=0.1187, Avg Loss=0.1191, Time Left=13.69\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1790/3393 [14:02<13:08,  2.03batch/s, Batch Loss=0.1187, Avg Loss=0.1191, Time Left=13.69\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1790/3393 [14:03<13:08,  2.03batch/s, Batch Loss=0.0808, Avg Loss=0.1191, Time Left=13.68\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1791/3393 [14:03<13:11,  2.02batch/s, Batch Loss=0.0808, Avg Loss=0.1191, Time Left=13.68\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1791/3393 [14:03<13:11,  2.02batch/s, Batch Loss=0.2086, Avg Loss=0.1191, Time Left=13.67\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1792/3393 [14:03<13:14,  2.02batch/s, Batch Loss=0.2086, Avg Loss=0.1191, Time Left=13.67\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1792/3393 [14:04<13:14,  2.02batch/s, Batch Loss=0.4960, Avg Loss=0.1193, Time Left=13.67\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1793/3393 [14:04<13:00,  2.05batch/s, Batch Loss=0.4960, Avg Loss=0.1193, Time Left=13.67\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1793/3393 [14:04<13:00,  2.05batch/s, Batch Loss=0.0063, Avg Loss=0.1193, Time Left=13.66\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1794/3393 [14:04<12:50,  2.08batch/s, Batch Loss=0.0063, Avg Loss=0.1193, Time Left=13.66\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1794/3393 [14:04<12:50,  2.08batch/s, Batch Loss=0.0165, Avg Loss=0.1192, Time Left=13.65\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1795/3393 [14:04<12:46,  2.08batch/s, Batch Loss=0.0165, Avg Loss=0.1192, Time Left=13.65\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1795/3393 [14:05<12:46,  2.08batch/s, Batch Loss=0.0112, Avg Loss=0.1191, Time Left=13.64\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1796/3393 [14:05<12:43,  2.09batch/s, Batch Loss=0.0112, Avg Loss=0.1191, Time Left=13.64\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1796/3393 [14:05<12:43,  2.09batch/s, Batch Loss=0.0891, Avg Loss=0.1191, Time Left=13.63\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1797/3393 [14:05<12:53,  2.06batch/s, Batch Loss=0.0891, Avg Loss=0.1191, Time Left=13.63\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1797/3393 [14:06<12:53,  2.06batch/s, Batch Loss=0.0360, Avg Loss=0.1191, Time Left=13.62\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1798/3393 [14:06<12:52,  2.06batch/s, Batch Loss=0.0360, Avg Loss=0.1191, Time Left=13.62\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1798/3393 [14:06<12:52,  2.06batch/s, Batch Loss=0.0355, Avg Loss=0.1190, Time Left=13.62\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1799/3393 [14:06<13:14,  2.01batch/s, Batch Loss=0.0355, Avg Loss=0.1190, Time Left=13.62\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1799/3393 [14:07<13:14,  2.01batch/s, Batch Loss=0.0209, Avg Loss=0.1190, Time Left=13.61\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1800/3393 [14:07<13:07,  2.02batch/s, Batch Loss=0.0209, Avg Loss=0.1190, Time Left=13.61\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1800/3393 [14:07<13:07,  2.02batch/s, Batch Loss=0.2679, Avg Loss=0.1191, Time Left=13.60\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1801/3393 [14:07<12:55,  2.05batch/s, Batch Loss=0.2679, Avg Loss=0.1191, Time Left=13.60\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1801/3393 [14:08<12:55,  2.05batch/s, Batch Loss=0.0419, Avg Loss=0.1190, Time Left=13.59\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1802/3393 [14:08<12:45,  2.08batch/s, Batch Loss=0.0419, Avg Loss=0.1190, Time Left=13.59\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1802/3393 [14:08<12:45,  2.08batch/s, Batch Loss=0.0777, Avg Loss=0.1190, Time Left=13.58\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1803/3393 [14:08<12:45,  2.08batch/s, Batch Loss=0.0777, Avg Loss=0.1190, Time Left=13.58\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1803/3393 [14:09<12:45,  2.08batch/s, Batch Loss=0.0128, Avg Loss=0.1189, Time Left=13.58\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1804/3393 [14:09<12:46,  2.07batch/s, Batch Loss=0.0128, Avg Loss=0.1189, Time Left=13.58\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1804/3393 [14:09<12:46,  2.07batch/s, Batch Loss=0.0980, Avg Loss=0.1189, Time Left=13.57\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1805/3393 [14:09<12:46,  2.07batch/s, Batch Loss=0.0980, Avg Loss=0.1189, Time Left=13.57\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1805/3393 [14:10<12:46,  2.07batch/s, Batch Loss=0.2477, Avg Loss=0.1190, Time Left=13.56\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1806/3393 [14:10<12:38,  2.09batch/s, Batch Loss=0.2477, Avg Loss=0.1190, Time Left=13.56\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1806/3393 [14:10<12:38,  2.09batch/s, Batch Loss=0.0471, Avg Loss=0.1189, Time Left=13.55\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1807/3393 [14:10<12:55,  2.05batch/s, Batch Loss=0.0471, Avg Loss=0.1189, Time Left=13.55\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1807/3393 [14:11<12:55,  2.05batch/s, Batch Loss=0.0359, Avg Loss=0.1189, Time Left=13.54\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1808/3393 [14:11<12:45,  2.07batch/s, Batch Loss=0.0359, Avg Loss=0.1189, Time Left=13.54\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1808/3393 [14:11<12:45,  2.07batch/s, Batch Loss=0.0269, Avg Loss=0.1188, Time Left=13.53\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1809/3393 [14:11<12:52,  2.05batch/s, Batch Loss=0.0269, Avg Loss=0.1188, Time Left=13.53\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1809/3393 [14:12<12:52,  2.05batch/s, Batch Loss=0.0033, Avg Loss=0.1188, Time Left=13.53\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1810/3393 [14:12<12:50,  2.06batch/s, Batch Loss=0.0033, Avg Loss=0.1188, Time Left=13.53\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1810/3393 [14:12<12:50,  2.06batch/s, Batch Loss=0.1462, Avg Loss=0.1188, Time Left=13.52\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1811/3393 [14:12<12:40,  2.08batch/s, Batch Loss=0.1462, Avg Loss=0.1188, Time Left=13.52\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1811/3393 [14:13<12:40,  2.08batch/s, Batch Loss=0.0653, Avg Loss=0.1188, Time Left=13.51\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1812/3393 [14:13<12:33,  2.10batch/s, Batch Loss=0.0653, Avg Loss=0.1188, Time Left=13.51\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1812/3393 [14:13<12:33,  2.10batch/s, Batch Loss=0.2376, Avg Loss=0.1188, Time Left=13.50\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  53%|▌| 1813/3393 [14:13<12:52,  2.05batch/s, Batch Loss=0.2376, Avg Loss=0.1188, Time Left=13.50\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1813/3393 [14:14<12:52,  2.05batch/s, Batch Loss=0.0198, Avg Loss=0.1188, Time Left=13.49\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1814/3393 [14:14<12:49,  2.05batch/s, Batch Loss=0.0198, Avg Loss=0.1188, Time Left=13.49\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1814/3393 [14:14<12:49,  2.05batch/s, Batch Loss=0.0290, Avg Loss=0.1187, Time Left=13.48\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1815/3393 [14:14<12:47,  2.06batch/s, Batch Loss=0.0290, Avg Loss=0.1187, Time Left=13.48\u001b[A\n",
      "Epoch 2/3 - Training:  53%|▌| 1815/3393 [14:15<12:47,  2.06batch/s, Batch Loss=0.1079, Avg Loss=0.1187, Time Left=13.48\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1816/3393 [14:15<12:45,  2.06batch/s, Batch Loss=0.1079, Avg Loss=0.1187, Time Left=13.48\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1816/3393 [14:15<12:45,  2.06batch/s, Batch Loss=0.0532, Avg Loss=0.1187, Time Left=13.47\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1817/3393 [14:15<12:29,  2.10batch/s, Batch Loss=0.0532, Avg Loss=0.1187, Time Left=13.47\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1817/3393 [14:16<12:29,  2.10batch/s, Batch Loss=0.0450, Avg Loss=0.1186, Time Left=13.46\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1818/3393 [14:16<12:32,  2.09batch/s, Batch Loss=0.0450, Avg Loss=0.1186, Time Left=13.46\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1818/3393 [14:16<12:32,  2.09batch/s, Batch Loss=0.1030, Avg Loss=0.1186, Time Left=13.45\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1819/3393 [14:16<12:41,  2.07batch/s, Batch Loss=0.1030, Avg Loss=0.1186, Time Left=13.45\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1819/3393 [14:17<12:41,  2.07batch/s, Batch Loss=0.1339, Avg Loss=0.1186, Time Left=13.44\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1820/3393 [14:17<12:41,  2.07batch/s, Batch Loss=0.1339, Avg Loss=0.1186, Time Left=13.44\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1820/3393 [14:17<12:41,  2.07batch/s, Batch Loss=0.0128, Avg Loss=0.1186, Time Left=13.43\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1821/3393 [14:17<12:56,  2.03batch/s, Batch Loss=0.0128, Avg Loss=0.1186, Time Left=13.43\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1821/3393 [14:18<12:56,  2.03batch/s, Batch Loss=0.0378, Avg Loss=0.1185, Time Left=13.43\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1822/3393 [14:18<12:59,  2.01batch/s, Batch Loss=0.0378, Avg Loss=0.1185, Time Left=13.43\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1822/3393 [14:18<12:59,  2.01batch/s, Batch Loss=0.0083, Avg Loss=0.1185, Time Left=13.42\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1823/3393 [14:18<13:07,  1.99batch/s, Batch Loss=0.0083, Avg Loss=0.1185, Time Left=13.42\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1823/3393 [14:19<13:07,  1.99batch/s, Batch Loss=0.0827, Avg Loss=0.1184, Time Left=13.41\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1824/3393 [14:19<13:07,  1.99batch/s, Batch Loss=0.0827, Avg Loss=0.1184, Time Left=13.41\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1824/3393 [14:19<13:07,  1.99batch/s, Batch Loss=0.1763, Avg Loss=0.1185, Time Left=13.40\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1825/3393 [14:19<12:51,  2.03batch/s, Batch Loss=0.1763, Avg Loss=0.1185, Time Left=13.40\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1825/3393 [14:20<12:51,  2.03batch/s, Batch Loss=0.0463, Avg Loss=0.1184, Time Left=13.39\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1826/3393 [14:20<12:39,  2.06batch/s, Batch Loss=0.0463, Avg Loss=0.1184, Time Left=13.39\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1826/3393 [14:20<12:39,  2.06batch/s, Batch Loss=0.3849, Avg Loss=0.1186, Time Left=13.39\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1827/3393 [14:20<12:37,  2.07batch/s, Batch Loss=0.3849, Avg Loss=0.1186, Time Left=13.39\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1827/3393 [14:20<12:37,  2.07batch/s, Batch Loss=0.0119, Avg Loss=0.1185, Time Left=13.38\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1828/3393 [14:20<12:30,  2.09batch/s, Batch Loss=0.0119, Avg Loss=0.1185, Time Left=13.38\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1828/3393 [14:21<12:30,  2.09batch/s, Batch Loss=0.0449, Avg Loss=0.1185, Time Left=13.37\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1829/3393 [14:21<12:38,  2.06batch/s, Batch Loss=0.0449, Avg Loss=0.1185, Time Left=13.37\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1829/3393 [14:21<12:38,  2.06batch/s, Batch Loss=0.0526, Avg Loss=0.1185, Time Left=13.36\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1830/3393 [14:21<12:22,  2.10batch/s, Batch Loss=0.0526, Avg Loss=0.1185, Time Left=13.36\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1830/3393 [14:22<12:22,  2.10batch/s, Batch Loss=0.0308, Avg Loss=0.1184, Time Left=13.35\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1831/3393 [14:22<12:25,  2.09batch/s, Batch Loss=0.0308, Avg Loss=0.1184, Time Left=13.35\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1831/3393 [14:22<12:25,  2.09batch/s, Batch Loss=0.0024, Avg Loss=0.1183, Time Left=13.34\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1832/3393 [14:22<12:35,  2.07batch/s, Batch Loss=0.0024, Avg Loss=0.1183, Time Left=13.34\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1832/3393 [14:23<12:35,  2.07batch/s, Batch Loss=0.2223, Avg Loss=0.1184, Time Left=13.34\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1833/3393 [14:23<12:34,  2.07batch/s, Batch Loss=0.2223, Avg Loss=0.1184, Time Left=13.34\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1833/3393 [14:23<12:34,  2.07batch/s, Batch Loss=0.2926, Avg Loss=0.1185, Time Left=13.33\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1834/3393 [14:23<12:42,  2.05batch/s, Batch Loss=0.2926, Avg Loss=0.1185, Time Left=13.33\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1834/3393 [14:24<12:42,  2.05batch/s, Batch Loss=0.0041, Avg Loss=0.1184, Time Left=13.32\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1835/3393 [14:24<12:40,  2.05batch/s, Batch Loss=0.0041, Avg Loss=0.1184, Time Left=13.32\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1835/3393 [14:24<12:40,  2.05batch/s, Batch Loss=0.0998, Avg Loss=0.1184, Time Left=13.31\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1836/3393 [14:24<12:44,  2.04batch/s, Batch Loss=0.0998, Avg Loss=0.1184, Time Left=13.31\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1836/3393 [14:25<12:44,  2.04batch/s, Batch Loss=0.1080, Avg Loss=0.1184, Time Left=13.30\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1837/3393 [14:25<12:41,  2.04batch/s, Batch Loss=0.1080, Avg Loss=0.1184, Time Left=13.30\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1837/3393 [14:25<12:41,  2.04batch/s, Batch Loss=0.0159, Avg Loss=0.1184, Time Left=13.30\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1838/3393 [14:25<12:38,  2.05batch/s, Batch Loss=0.0159, Avg Loss=0.1184, Time Left=13.30\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1838/3393 [14:26<12:38,  2.05batch/s, Batch Loss=0.1680, Avg Loss=0.1184, Time Left=13.29\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1839/3393 [14:26<12:57,  2.00batch/s, Batch Loss=0.1680, Avg Loss=0.1184, Time Left=13.29\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1839/3393 [14:26<12:57,  2.00batch/s, Batch Loss=0.0193, Avg Loss=0.1183, Time Left=13.28\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1840/3393 [14:26<12:50,  2.02batch/s, Batch Loss=0.0193, Avg Loss=0.1183, Time Left=13.28\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1840/3393 [14:27<12:50,  2.02batch/s, Batch Loss=0.0381, Avg Loss=0.1183, Time Left=13.27\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1841/3393 [14:27<12:37,  2.05batch/s, Batch Loss=0.0381, Avg Loss=0.1183, Time Left=13.27\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1841/3393 [14:27<12:37,  2.05batch/s, Batch Loss=0.0925, Avg Loss=0.1183, Time Left=13.26\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1842/3393 [14:27<12:27,  2.08batch/s, Batch Loss=0.0925, Avg Loss=0.1183, Time Left=13.26\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1842/3393 [14:28<12:27,  2.08batch/s, Batch Loss=0.0213, Avg Loss=0.1182, Time Left=13.25\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1843/3393 [14:28<12:19,  2.10batch/s, Batch Loss=0.0213, Avg Loss=0.1182, Time Left=13.25\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1843/3393 [14:28<12:19,  2.10batch/s, Batch Loss=0.0327, Avg Loss=0.1182, Time Left=13.25\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1844/3393 [14:28<12:29,  2.07batch/s, Batch Loss=0.0327, Avg Loss=0.1182, Time Left=13.25\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1844/3393 [14:29<12:29,  2.07batch/s, Batch Loss=0.3448, Avg Loss=0.1183, Time Left=13.24\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1845/3393 [14:29<12:29,  2.07batch/s, Batch Loss=0.3448, Avg Loss=0.1183, Time Left=13.24\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1845/3393 [14:29<12:29,  2.07batch/s, Batch Loss=0.1390, Avg Loss=0.1183, Time Left=13.23\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  54%|▌| 1846/3393 [14:29<12:28,  2.07batch/s, Batch Loss=0.1390, Avg Loss=0.1183, Time Left=13.23\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1846/3393 [14:30<12:28,  2.07batch/s, Batch Loss=0.0638, Avg Loss=0.1183, Time Left=13.22\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1847/3393 [14:30<12:33,  2.05batch/s, Batch Loss=0.0638, Avg Loss=0.1183, Time Left=13.22\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1847/3393 [14:30<12:33,  2.05batch/s, Batch Loss=0.0731, Avg Loss=0.1182, Time Left=13.21\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1848/3393 [14:30<12:32,  2.05batch/s, Batch Loss=0.0731, Avg Loss=0.1182, Time Left=13.21\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1848/3393 [14:31<12:32,  2.05batch/s, Batch Loss=0.1522, Avg Loss=0.1183, Time Left=13.20\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1849/3393 [14:31<12:23,  2.08batch/s, Batch Loss=0.1522, Avg Loss=0.1183, Time Left=13.20\u001b[A\n",
      "Epoch 2/3 - Training:  54%|▌| 1849/3393 [14:31<12:23,  2.08batch/s, Batch Loss=0.1787, Avg Loss=0.1183, Time Left=13.20\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1850/3393 [14:31<12:38,  2.03batch/s, Batch Loss=0.1787, Avg Loss=0.1183, Time Left=13.20\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1850/3393 [14:32<12:38,  2.03batch/s, Batch Loss=0.1805, Avg Loss=0.1183, Time Left=13.19\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1851/3393 [14:32<12:27,  2.06batch/s, Batch Loss=0.1805, Avg Loss=0.1183, Time Left=13.19\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1851/3393 [14:32<12:27,  2.06batch/s, Batch Loss=0.1213, Avg Loss=0.1183, Time Left=13.18\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1852/3393 [14:32<12:18,  2.09batch/s, Batch Loss=0.1213, Avg Loss=0.1183, Time Left=13.18\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1852/3393 [14:33<12:18,  2.09batch/s, Batch Loss=0.1347, Avg Loss=0.1183, Time Left=13.17\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1853/3393 [14:33<12:20,  2.08batch/s, Batch Loss=0.1347, Avg Loss=0.1183, Time Left=13.17\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1853/3393 [14:33<12:20,  2.08batch/s, Batch Loss=0.0963, Avg Loss=0.1183, Time Left=13.16\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1854/3393 [14:33<12:25,  2.07batch/s, Batch Loss=0.0963, Avg Loss=0.1183, Time Left=13.16\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1854/3393 [14:34<12:25,  2.07batch/s, Batch Loss=0.0099, Avg Loss=0.1183, Time Left=13.16\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1855/3393 [14:34<12:28,  2.06batch/s, Batch Loss=0.0099, Avg Loss=0.1183, Time Left=13.16\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1855/3393 [14:34<12:28,  2.06batch/s, Batch Loss=0.1860, Avg Loss=0.1183, Time Left=13.15\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1856/3393 [14:34<12:18,  2.08batch/s, Batch Loss=0.1860, Avg Loss=0.1183, Time Left=13.15\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1856/3393 [14:35<12:18,  2.08batch/s, Batch Loss=0.1603, Avg Loss=0.1183, Time Left=13.14\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1857/3393 [14:35<12:05,  2.12batch/s, Batch Loss=0.1603, Avg Loss=0.1183, Time Left=13.14\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1857/3393 [14:35<12:05,  2.12batch/s, Batch Loss=0.0622, Avg Loss=0.1183, Time Left=13.13\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1858/3393 [14:35<12:09,  2.11batch/s, Batch Loss=0.0622, Avg Loss=0.1183, Time Left=13.13\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1858/3393 [14:36<12:09,  2.11batch/s, Batch Loss=0.0569, Avg Loss=0.1183, Time Left=13.12\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1859/3393 [14:36<12:19,  2.07batch/s, Batch Loss=0.0569, Avg Loss=0.1183, Time Left=13.12\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1859/3393 [14:36<12:19,  2.07batch/s, Batch Loss=0.0915, Avg Loss=0.1183, Time Left=13.11\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1860/3393 [14:36<12:27,  2.05batch/s, Batch Loss=0.0915, Avg Loss=0.1183, Time Left=13.11\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1860/3393 [14:36<12:27,  2.05batch/s, Batch Loss=0.2721, Avg Loss=0.1183, Time Left=13.10\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1861/3393 [14:36<12:18,  2.07batch/s, Batch Loss=0.2721, Avg Loss=0.1183, Time Left=13.10\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1861/3393 [14:37<12:18,  2.07batch/s, Batch Loss=0.0503, Avg Loss=0.1183, Time Left=13.10\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1862/3393 [14:37<12:10,  2.09batch/s, Batch Loss=0.0503, Avg Loss=0.1183, Time Left=13.10\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1862/3393 [14:37<12:10,  2.09batch/s, Batch Loss=0.0624, Avg Loss=0.1183, Time Left=13.09\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1863/3393 [14:37<11:58,  2.13batch/s, Batch Loss=0.0624, Avg Loss=0.1183, Time Left=13.09\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1863/3393 [14:38<11:58,  2.13batch/s, Batch Loss=0.0580, Avg Loss=0.1182, Time Left=13.08\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1864/3393 [14:38<12:11,  2.09batch/s, Batch Loss=0.0580, Avg Loss=0.1182, Time Left=13.08\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1864/3393 [14:38<12:11,  2.09batch/s, Batch Loss=0.0337, Avg Loss=0.1182, Time Left=13.07\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1865/3393 [14:38<12:27,  2.04batch/s, Batch Loss=0.0337, Avg Loss=0.1182, Time Left=13.07\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1865/3393 [14:39<12:27,  2.04batch/s, Batch Loss=0.0021, Avg Loss=0.1181, Time Left=13.06\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1866/3393 [14:39<12:25,  2.05batch/s, Batch Loss=0.0021, Avg Loss=0.1181, Time Left=13.06\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1866/3393 [14:39<12:25,  2.05batch/s, Batch Loss=0.0656, Avg Loss=0.1181, Time Left=13.06\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1867/3393 [14:39<12:30,  2.03batch/s, Batch Loss=0.0656, Avg Loss=0.1181, Time Left=13.06\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1867/3393 [14:40<12:30,  2.03batch/s, Batch Loss=0.0703, Avg Loss=0.1181, Time Left=13.05\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1868/3393 [14:40<12:19,  2.06batch/s, Batch Loss=0.0703, Avg Loss=0.1181, Time Left=13.05\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1868/3393 [14:40<12:19,  2.06batch/s, Batch Loss=0.1822, Avg Loss=0.1181, Time Left=13.04\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1869/3393 [14:40<12:17,  2.07batch/s, Batch Loss=0.1822, Avg Loss=0.1181, Time Left=13.04\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1869/3393 [14:41<12:17,  2.07batch/s, Batch Loss=0.4268, Avg Loss=0.1183, Time Left=13.03\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1870/3393 [14:41<12:29,  2.03batch/s, Batch Loss=0.4268, Avg Loss=0.1183, Time Left=13.03\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1870/3393 [14:41<12:29,  2.03batch/s, Batch Loss=0.0248, Avg Loss=0.1182, Time Left=13.02\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1871/3393 [14:41<12:20,  2.06batch/s, Batch Loss=0.0248, Avg Loss=0.1182, Time Left=13.02\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1871/3393 [14:42<12:20,  2.06batch/s, Batch Loss=0.0282, Avg Loss=0.1182, Time Left=13.01\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1872/3393 [14:42<12:40,  2.00batch/s, Batch Loss=0.0282, Avg Loss=0.1182, Time Left=13.01\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1872/3393 [14:42<12:40,  2.00batch/s, Batch Loss=0.0052, Avg Loss=0.1181, Time Left=13.01\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1873/3393 [14:42<12:33,  2.02batch/s, Batch Loss=0.0052, Avg Loss=0.1181, Time Left=13.01\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1873/3393 [14:43<12:33,  2.02batch/s, Batch Loss=0.0022, Avg Loss=0.1180, Time Left=13.00\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1874/3393 [14:43<12:27,  2.03batch/s, Batch Loss=0.0022, Avg Loss=0.1180, Time Left=13.00\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1874/3393 [14:43<12:27,  2.03batch/s, Batch Loss=0.0323, Avg Loss=0.1180, Time Left=12.99\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1875/3393 [14:43<12:24,  2.04batch/s, Batch Loss=0.0323, Avg Loss=0.1180, Time Left=12.99\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1875/3393 [14:44<12:24,  2.04batch/s, Batch Loss=0.0402, Avg Loss=0.1180, Time Left=12.98\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1876/3393 [14:44<12:19,  2.05batch/s, Batch Loss=0.0402, Avg Loss=0.1180, Time Left=12.98\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1876/3393 [14:44<12:19,  2.05batch/s, Batch Loss=0.0039, Avg Loss=0.1179, Time Left=12.97\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1877/3393 [14:44<12:32,  2.01batch/s, Batch Loss=0.0039, Avg Loss=0.1179, Time Left=12.97\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1877/3393 [14:45<12:32,  2.01batch/s, Batch Loss=0.2165, Avg Loss=0.1179, Time Left=12.97\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1878/3393 [14:45<12:26,  2.03batch/s, Batch Loss=0.2165, Avg Loss=0.1179, Time Left=12.97\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1878/3393 [14:45<12:26,  2.03batch/s, Batch Loss=0.3198, Avg Loss=0.1181, Time Left=12.96\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  55%|▌| 1879/3393 [14:45<12:29,  2.02batch/s, Batch Loss=0.3198, Avg Loss=0.1181, Time Left=12.96\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1879/3393 [14:46<12:29,  2.02batch/s, Batch Loss=0.2579, Avg Loss=0.1181, Time Left=12.95\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1880/3393 [14:46<12:24,  2.03batch/s, Batch Loss=0.2579, Avg Loss=0.1181, Time Left=12.95\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1880/3393 [14:46<12:24,  2.03batch/s, Batch Loss=0.0059, Avg Loss=0.1181, Time Left=12.94\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1881/3393 [14:46<12:20,  2.04batch/s, Batch Loss=0.0059, Avg Loss=0.1181, Time Left=12.94\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1881/3393 [14:47<12:20,  2.04batch/s, Batch Loss=0.1530, Avg Loss=0.1181, Time Left=12.93\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1882/3393 [14:47<12:24,  2.03batch/s, Batch Loss=0.1530, Avg Loss=0.1181, Time Left=12.93\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1882/3393 [14:47<12:24,  2.03batch/s, Batch Loss=0.0978, Avg Loss=0.1181, Time Left=12.92\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1883/3393 [14:47<12:05,  2.08batch/s, Batch Loss=0.0978, Avg Loss=0.1181, Time Left=12.92\u001b[A\n",
      "Epoch 2/3 - Training:  55%|▌| 1883/3393 [14:48<12:05,  2.08batch/s, Batch Loss=0.1205, Avg Loss=0.1181, Time Left=12.92\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1884/3393 [14:48<12:13,  2.06batch/s, Batch Loss=0.1205, Avg Loss=0.1181, Time Left=12.92\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1884/3393 [14:48<12:13,  2.06batch/s, Batch Loss=0.0453, Avg Loss=0.1180, Time Left=12.91\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1885/3393 [14:48<12:04,  2.08batch/s, Batch Loss=0.0453, Avg Loss=0.1180, Time Left=12.91\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1885/3393 [14:49<12:04,  2.08batch/s, Batch Loss=0.0608, Avg Loss=0.1180, Time Left=12.90\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1886/3393 [14:49<11:58,  2.10batch/s, Batch Loss=0.0608, Avg Loss=0.1180, Time Left=12.90\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1886/3393 [14:49<11:58,  2.10batch/s, Batch Loss=0.0566, Avg Loss=0.1180, Time Left=12.89\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1887/3393 [14:49<12:00,  2.09batch/s, Batch Loss=0.0566, Avg Loss=0.1180, Time Left=12.89\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1887/3393 [14:50<12:00,  2.09batch/s, Batch Loss=0.1105, Avg Loss=0.1180, Time Left=12.88\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1888/3393 [14:50<12:09,  2.06batch/s, Batch Loss=0.1105, Avg Loss=0.1180, Time Left=12.88\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1888/3393 [14:50<12:09,  2.06batch/s, Batch Loss=0.1382, Avg Loss=0.1180, Time Left=12.87\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1889/3393 [14:50<11:54,  2.10batch/s, Batch Loss=0.1382, Avg Loss=0.1180, Time Left=12.87\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1889/3393 [14:51<11:54,  2.10batch/s, Batch Loss=0.1056, Avg Loss=0.1180, Time Left=12.87\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1890/3393 [14:51<12:00,  2.09batch/s, Batch Loss=0.1056, Avg Loss=0.1180, Time Left=12.87\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1890/3393 [14:51<12:00,  2.09batch/s, Batch Loss=0.1479, Avg Loss=0.1180, Time Left=12.86\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1891/3393 [14:51<12:12,  2.05batch/s, Batch Loss=0.1479, Avg Loss=0.1180, Time Left=12.86\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1891/3393 [14:52<12:12,  2.05batch/s, Batch Loss=0.0834, Avg Loss=0.1180, Time Left=12.85\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1892/3393 [14:52<12:17,  2.04batch/s, Batch Loss=0.0834, Avg Loss=0.1180, Time Left=12.85\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1892/3393 [14:52<12:17,  2.04batch/s, Batch Loss=0.0594, Avg Loss=0.1179, Time Left=12.84\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1893/3393 [14:52<12:21,  2.02batch/s, Batch Loss=0.0594, Avg Loss=0.1179, Time Left=12.84\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1893/3393 [14:53<12:21,  2.02batch/s, Batch Loss=0.3088, Avg Loss=0.1181, Time Left=12.83\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1894/3393 [14:53<12:24,  2.01batch/s, Batch Loss=0.3088, Avg Loss=0.1181, Time Left=12.83\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1894/3393 [14:53<12:24,  2.01batch/s, Batch Loss=0.0278, Avg Loss=0.1180, Time Left=12.83\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1895/3393 [14:53<12:10,  2.05batch/s, Batch Loss=0.0278, Avg Loss=0.1180, Time Left=12.83\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1895/3393 [14:54<12:10,  2.05batch/s, Batch Loss=0.0229, Avg Loss=0.1179, Time Left=12.82\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1896/3393 [14:54<12:08,  2.05batch/s, Batch Loss=0.0229, Avg Loss=0.1179, Time Left=12.82\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1896/3393 [14:54<12:08,  2.05batch/s, Batch Loss=0.2184, Avg Loss=0.1180, Time Left=12.81\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1897/3393 [14:54<12:13,  2.04batch/s, Batch Loss=0.2184, Avg Loss=0.1180, Time Left=12.81\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1897/3393 [14:55<12:13,  2.04batch/s, Batch Loss=0.0721, Avg Loss=0.1180, Time Left=12.80\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1898/3393 [14:55<12:26,  2.00batch/s, Batch Loss=0.0721, Avg Loss=0.1180, Time Left=12.80\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1898/3393 [14:55<12:26,  2.00batch/s, Batch Loss=0.0100, Avg Loss=0.1179, Time Left=12.79\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1899/3393 [14:55<12:16,  2.03batch/s, Batch Loss=0.0100, Avg Loss=0.1179, Time Left=12.79\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1899/3393 [14:56<12:16,  2.03batch/s, Batch Loss=0.1205, Avg Loss=0.1179, Time Left=12.79\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1900/3393 [14:56<12:06,  2.06batch/s, Batch Loss=0.1205, Avg Loss=0.1179, Time Left=12.79\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1900/3393 [14:56<12:06,  2.06batch/s, Batch Loss=0.0569, Avg Loss=0.1179, Time Left=12.78\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1901/3393 [14:56<12:04,  2.06batch/s, Batch Loss=0.0569, Avg Loss=0.1179, Time Left=12.78\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1901/3393 [14:56<12:04,  2.06batch/s, Batch Loss=0.0587, Avg Loss=0.1179, Time Left=12.77\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1902/3393 [14:56<11:56,  2.08batch/s, Batch Loss=0.0587, Avg Loss=0.1179, Time Left=12.77\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1902/3393 [14:57<11:56,  2.08batch/s, Batch Loss=0.0194, Avg Loss=0.1178, Time Left=12.76\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1903/3393 [14:57<11:56,  2.08batch/s, Batch Loss=0.0194, Avg Loss=0.1178, Time Left=12.76\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1903/3393 [14:57<11:56,  2.08batch/s, Batch Loss=0.1124, Avg Loss=0.1178, Time Left=12.75\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1904/3393 [14:57<11:57,  2.07batch/s, Batch Loss=0.1124, Avg Loss=0.1178, Time Left=12.75\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1904/3393 [14:58<11:57,  2.07batch/s, Batch Loss=0.4375, Avg Loss=0.1180, Time Left=12.74\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1905/3393 [14:58<12:04,  2.05batch/s, Batch Loss=0.4375, Avg Loss=0.1180, Time Left=12.74\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1905/3393 [14:58<12:04,  2.05batch/s, Batch Loss=0.1521, Avg Loss=0.1180, Time Left=12.74\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1906/3393 [14:58<12:16,  2.02batch/s, Batch Loss=0.1521, Avg Loss=0.1180, Time Left=12.74\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1906/3393 [14:59<12:16,  2.02batch/s, Batch Loss=0.5144, Avg Loss=0.1182, Time Left=12.73\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1907/3393 [14:59<12:19,  2.01batch/s, Batch Loss=0.5144, Avg Loss=0.1182, Time Left=12.73\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1907/3393 [14:59<12:19,  2.01batch/s, Batch Loss=0.1581, Avg Loss=0.1182, Time Left=12.72\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1908/3393 [14:59<12:27,  1.99batch/s, Batch Loss=0.1581, Avg Loss=0.1182, Time Left=12.72\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1908/3393 [15:00<12:27,  1.99batch/s, Batch Loss=0.1049, Avg Loss=0.1182, Time Left=12.71\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1909/3393 [15:00<12:26,  1.99batch/s, Batch Loss=0.1049, Avg Loss=0.1182, Time Left=12.71\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1909/3393 [15:00<12:26,  1.99batch/s, Batch Loss=0.3473, Avg Loss=0.1183, Time Left=12.70\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1910/3393 [15:00<12:31,  1.97batch/s, Batch Loss=0.3473, Avg Loss=0.1183, Time Left=12.70\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1910/3393 [15:01<12:31,  1.97batch/s, Batch Loss=0.0581, Avg Loss=0.1183, Time Left=12.70\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1911/3393 [15:01<12:14,  2.02batch/s, Batch Loss=0.0581, Avg Loss=0.1183, Time Left=12.70\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1911/3393 [15:01<12:14,  2.02batch/s, Batch Loss=0.0232, Avg Loss=0.1183, Time Left=12.69\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  56%|▌| 1912/3393 [15:01<12:08,  2.03batch/s, Batch Loss=0.0232, Avg Loss=0.1183, Time Left=12.69\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1912/3393 [15:02<12:08,  2.03batch/s, Batch Loss=0.0761, Avg Loss=0.1182, Time Left=12.68\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1913/3393 [15:02<12:18,  2.00batch/s, Batch Loss=0.0761, Avg Loss=0.1182, Time Left=12.68\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1913/3393 [15:02<12:18,  2.00batch/s, Batch Loss=0.0353, Avg Loss=0.1182, Time Left=12.67\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1914/3393 [15:02<12:11,  2.02batch/s, Batch Loss=0.0353, Avg Loss=0.1182, Time Left=12.67\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1914/3393 [15:03<12:11,  2.02batch/s, Batch Loss=0.0345, Avg Loss=0.1181, Time Left=12.66\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1915/3393 [15:03<12:20,  2.00batch/s, Batch Loss=0.0345, Avg Loss=0.1181, Time Left=12.66\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1915/3393 [15:03<12:20,  2.00batch/s, Batch Loss=0.2065, Avg Loss=0.1182, Time Left=12.66\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1916/3393 [15:03<12:19,  2.00batch/s, Batch Loss=0.2065, Avg Loss=0.1182, Time Left=12.66\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1916/3393 [15:04<12:19,  2.00batch/s, Batch Loss=0.0496, Avg Loss=0.1182, Time Left=12.65\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1917/3393 [15:04<12:26,  1.98batch/s, Batch Loss=0.0496, Avg Loss=0.1182, Time Left=12.65\u001b[A\n",
      "Epoch 2/3 - Training:  56%|▌| 1917/3393 [15:04<12:26,  1.98batch/s, Batch Loss=0.1183, Avg Loss=0.1182, Time Left=12.64\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1918/3393 [15:04<12:23,  1.98batch/s, Batch Loss=0.1183, Avg Loss=0.1182, Time Left=12.64\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1918/3393 [15:05<12:23,  1.98batch/s, Batch Loss=0.0234, Avg Loss=0.1181, Time Left=12.63\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1919/3393 [15:05<12:13,  2.01batch/s, Batch Loss=0.0234, Avg Loss=0.1181, Time Left=12.63\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1919/3393 [15:05<12:13,  2.01batch/s, Batch Loss=0.1422, Avg Loss=0.1181, Time Left=12.62\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1920/3393 [15:05<12:00,  2.04batch/s, Batch Loss=0.1422, Avg Loss=0.1181, Time Left=12.62\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1920/3393 [15:06<12:00,  2.04batch/s, Batch Loss=0.0145, Avg Loss=0.1181, Time Left=12.62\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1921/3393 [15:06<12:04,  2.03batch/s, Batch Loss=0.0145, Avg Loss=0.1181, Time Left=12.62\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1921/3393 [15:06<12:04,  2.03batch/s, Batch Loss=0.1237, Avg Loss=0.1181, Time Left=12.61\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1922/3393 [15:06<12:14,  2.00batch/s, Batch Loss=0.1237, Avg Loss=0.1181, Time Left=12.61\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1922/3393 [15:07<12:14,  2.00batch/s, Batch Loss=0.1083, Avg Loss=0.1181, Time Left=12.60\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1923/3393 [15:07<12:14,  2.00batch/s, Batch Loss=0.1083, Avg Loss=0.1181, Time Left=12.60\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1923/3393 [15:07<12:14,  2.00batch/s, Batch Loss=0.1084, Avg Loss=0.1181, Time Left=12.59\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1924/3393 [15:07<12:28,  1.96batch/s, Batch Loss=0.1084, Avg Loss=0.1181, Time Left=12.59\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1924/3393 [15:08<12:28,  1.96batch/s, Batch Loss=0.1008, Avg Loss=0.1180, Time Left=12.58\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1925/3393 [15:08<12:17,  1.99batch/s, Batch Loss=0.1008, Avg Loss=0.1180, Time Left=12.58\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1925/3393 [15:08<12:17,  1.99batch/s, Batch Loss=0.0727, Avg Loss=0.1180, Time Left=12.58\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1926/3393 [15:08<12:23,  1.97batch/s, Batch Loss=0.0727, Avg Loss=0.1180, Time Left=12.58\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1926/3393 [15:09<12:23,  1.97batch/s, Batch Loss=0.0361, Avg Loss=0.1180, Time Left=12.57\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1927/3393 [15:09<12:20,  1.98batch/s, Batch Loss=0.0361, Avg Loss=0.1180, Time Left=12.57\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1927/3393 [15:09<12:20,  1.98batch/s, Batch Loss=0.2058, Avg Loss=0.1180, Time Left=12.56\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1928/3393 [15:09<12:18,  1.98batch/s, Batch Loss=0.2058, Avg Loss=0.1180, Time Left=12.56\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1928/3393 [15:10<12:18,  1.98batch/s, Batch Loss=0.1540, Avg Loss=0.1180, Time Left=12.55\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1929/3393 [15:10<12:01,  2.03batch/s, Batch Loss=0.1540, Avg Loss=0.1180, Time Left=12.55\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1929/3393 [15:10<12:01,  2.03batch/s, Batch Loss=0.0459, Avg Loss=0.1180, Time Left=12.54\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1930/3393 [15:10<12:03,  2.02batch/s, Batch Loss=0.0459, Avg Loss=0.1180, Time Left=12.54\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1930/3393 [15:11<12:03,  2.02batch/s, Batch Loss=0.1206, Avg Loss=0.1180, Time Left=12.54\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1931/3393 [15:11<12:12,  2.00batch/s, Batch Loss=0.1206, Avg Loss=0.1180, Time Left=12.54\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1931/3393 [15:11<12:12,  2.00batch/s, Batch Loss=0.1627, Avg Loss=0.1180, Time Left=12.53\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1932/3393 [15:11<11:54,  2.04batch/s, Batch Loss=0.1627, Avg Loss=0.1180, Time Left=12.53\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1932/3393 [15:12<11:54,  2.04batch/s, Batch Loss=0.2935, Avg Loss=0.1181, Time Left=12.52\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1933/3393 [15:12<11:48,  2.06batch/s, Batch Loss=0.2935, Avg Loss=0.1181, Time Left=12.52\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1933/3393 [15:12<11:48,  2.06batch/s, Batch Loss=0.1205, Avg Loss=0.1181, Time Left=12.51\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1934/3393 [15:12<12:05,  2.01batch/s, Batch Loss=0.1205, Avg Loss=0.1181, Time Left=12.51\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1934/3393 [15:13<12:05,  2.01batch/s, Batch Loss=0.0156, Avg Loss=0.1181, Time Left=12.50\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1935/3393 [15:13<11:55,  2.04batch/s, Batch Loss=0.0156, Avg Loss=0.1181, Time Left=12.50\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1935/3393 [15:13<11:55,  2.04batch/s, Batch Loss=0.0161, Avg Loss=0.1180, Time Left=12.49\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1936/3393 [15:13<11:58,  2.03batch/s, Batch Loss=0.0161, Avg Loss=0.1180, Time Left=12.49\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1936/3393 [15:14<11:58,  2.03batch/s, Batch Loss=0.0385, Avg Loss=0.1180, Time Left=12.49\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1937/3393 [15:14<11:51,  2.05batch/s, Batch Loss=0.0385, Avg Loss=0.1180, Time Left=12.49\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1937/3393 [15:14<11:51,  2.05batch/s, Batch Loss=0.0440, Avg Loss=0.1179, Time Left=12.48\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1938/3393 [15:14<11:44,  2.06batch/s, Batch Loss=0.0440, Avg Loss=0.1179, Time Left=12.48\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1938/3393 [15:15<11:44,  2.06batch/s, Batch Loss=0.4175, Avg Loss=0.1181, Time Left=12.47\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1939/3393 [15:15<12:04,  2.01batch/s, Batch Loss=0.4175, Avg Loss=0.1181, Time Left=12.47\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1939/3393 [15:15<12:04,  2.01batch/s, Batch Loss=0.0208, Avg Loss=0.1180, Time Left=12.46\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1940/3393 [15:15<11:58,  2.02batch/s, Batch Loss=0.0208, Avg Loss=0.1180, Time Left=12.46\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1940/3393 [15:16<11:58,  2.02batch/s, Batch Loss=0.0137, Avg Loss=0.1180, Time Left=12.45\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1941/3393 [15:16<12:13,  1.98batch/s, Batch Loss=0.0137, Avg Loss=0.1180, Time Left=12.45\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1941/3393 [15:16<12:13,  1.98batch/s, Batch Loss=0.0978, Avg Loss=0.1180, Time Left=12.45\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1942/3393 [15:16<11:58,  2.02batch/s, Batch Loss=0.0978, Avg Loss=0.1180, Time Left=12.45\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1942/3393 [15:17<11:58,  2.02batch/s, Batch Loss=0.1769, Avg Loss=0.1180, Time Left=12.44\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1943/3393 [15:17<12:04,  2.00batch/s, Batch Loss=0.1769, Avg Loss=0.1180, Time Left=12.44\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1943/3393 [15:17<12:04,  2.00batch/s, Batch Loss=0.1659, Avg Loss=0.1180, Time Left=12.43\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1944/3393 [15:17<11:45,  2.05batch/s, Batch Loss=0.1659, Avg Loss=0.1180, Time Left=12.43\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1944/3393 [15:18<11:45,  2.05batch/s, Batch Loss=0.0519, Avg Loss=0.1180, Time Left=12.42\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  57%|▌| 1945/3393 [15:18<11:50,  2.04batch/s, Batch Loss=0.0519, Avg Loss=0.1180, Time Left=12.42\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1945/3393 [15:18<11:50,  2.04batch/s, Batch Loss=0.0080, Avg Loss=0.1179, Time Left=12.41\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1946/3393 [15:18<11:53,  2.03batch/s, Batch Loss=0.0080, Avg Loss=0.1179, Time Left=12.41\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1946/3393 [15:19<11:53,  2.03batch/s, Batch Loss=0.3505, Avg Loss=0.1181, Time Left=12.40\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1947/3393 [15:19<11:36,  2.08batch/s, Batch Loss=0.3505, Avg Loss=0.1181, Time Left=12.40\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1947/3393 [15:19<11:36,  2.08batch/s, Batch Loss=0.0083, Avg Loss=0.1180, Time Left=12.40\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1948/3393 [15:19<11:35,  2.08batch/s, Batch Loss=0.0083, Avg Loss=0.1180, Time Left=12.40\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1948/3393 [15:20<11:35,  2.08batch/s, Batch Loss=0.0343, Avg Loss=0.1180, Time Left=12.39\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1949/3393 [15:20<11:43,  2.05batch/s, Batch Loss=0.0343, Avg Loss=0.1180, Time Left=12.39\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1949/3393 [15:20<11:43,  2.05batch/s, Batch Loss=0.1685, Avg Loss=0.1180, Time Left=12.38\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1950/3393 [15:20<11:27,  2.10batch/s, Batch Loss=0.1685, Avg Loss=0.1180, Time Left=12.38\u001b[A\n",
      "Epoch 2/3 - Training:  57%|▌| 1950/3393 [15:21<11:27,  2.10batch/s, Batch Loss=0.1470, Avg Loss=0.1180, Time Left=12.37\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1951/3393 [15:21<11:30,  2.09batch/s, Batch Loss=0.1470, Avg Loss=0.1180, Time Left=12.37\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1951/3393 [15:21<11:30,  2.09batch/s, Batch Loss=0.1113, Avg Loss=0.1180, Time Left=12.36\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1952/3393 [15:21<11:31,  2.08batch/s, Batch Loss=0.1113, Avg Loss=0.1180, Time Left=12.36\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1952/3393 [15:22<11:31,  2.08batch/s, Batch Loss=0.0267, Avg Loss=0.1180, Time Left=12.35\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1953/3393 [15:22<11:33,  2.08batch/s, Batch Loss=0.0267, Avg Loss=0.1180, Time Left=12.35\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1953/3393 [15:22<11:33,  2.08batch/s, Batch Loss=0.0962, Avg Loss=0.1179, Time Left=12.35\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1954/3393 [15:22<11:26,  2.10batch/s, Batch Loss=0.0962, Avg Loss=0.1179, Time Left=12.35\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1954/3393 [15:23<11:26,  2.10batch/s, Batch Loss=0.1751, Avg Loss=0.1180, Time Left=12.34\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1955/3393 [15:23<11:34,  2.07batch/s, Batch Loss=0.1751, Avg Loss=0.1180, Time Left=12.34\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1955/3393 [15:23<11:34,  2.07batch/s, Batch Loss=0.0795, Avg Loss=0.1180, Time Left=12.33\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1956/3393 [15:23<11:35,  2.07batch/s, Batch Loss=0.0795, Avg Loss=0.1180, Time Left=12.33\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1956/3393 [15:24<11:35,  2.07batch/s, Batch Loss=0.1062, Avg Loss=0.1179, Time Left=12.32\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1957/3393 [15:24<11:54,  2.01batch/s, Batch Loss=0.1062, Avg Loss=0.1179, Time Left=12.32\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1957/3393 [15:24<11:54,  2.01batch/s, Batch Loss=0.0382, Avg Loss=0.1179, Time Left=12.31\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1958/3393 [15:24<11:49,  2.02batch/s, Batch Loss=0.0382, Avg Loss=0.1179, Time Left=12.31\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1958/3393 [15:25<11:49,  2.02batch/s, Batch Loss=0.0961, Avg Loss=0.1179, Time Left=12.31\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1959/3393 [15:25<12:04,  1.98batch/s, Batch Loss=0.0961, Avg Loss=0.1179, Time Left=12.31\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1959/3393 [15:25<12:04,  1.98batch/s, Batch Loss=0.0385, Avg Loss=0.1178, Time Left=12.30\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1960/3393 [15:25<11:55,  2.00batch/s, Batch Loss=0.0385, Avg Loss=0.1178, Time Left=12.30\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1960/3393 [15:26<11:55,  2.00batch/s, Batch Loss=0.1436, Avg Loss=0.1179, Time Left=12.29\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1961/3393 [15:26<11:48,  2.02batch/s, Batch Loss=0.1436, Avg Loss=0.1179, Time Left=12.29\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1961/3393 [15:26<11:48,  2.02batch/s, Batch Loss=0.0401, Avg Loss=0.1178, Time Left=12.28\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1962/3393 [15:26<11:44,  2.03batch/s, Batch Loss=0.0401, Avg Loss=0.1178, Time Left=12.28\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1962/3393 [15:27<11:44,  2.03batch/s, Batch Loss=0.0644, Avg Loss=0.1178, Time Left=12.27\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1963/3393 [15:27<11:25,  2.09batch/s, Batch Loss=0.0644, Avg Loss=0.1178, Time Left=12.27\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1963/3393 [15:27<11:25,  2.09batch/s, Batch Loss=0.0470, Avg Loss=0.1178, Time Left=12.26\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1964/3393 [15:27<11:33,  2.06batch/s, Batch Loss=0.0470, Avg Loss=0.1178, Time Left=12.26\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1964/3393 [15:28<11:33,  2.06batch/s, Batch Loss=0.0180, Avg Loss=0.1177, Time Left=12.26\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1965/3393 [15:28<11:29,  2.07batch/s, Batch Loss=0.0180, Avg Loss=0.1177, Time Left=12.26\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1965/3393 [15:28<11:29,  2.07batch/s, Batch Loss=0.2920, Avg Loss=0.1178, Time Left=12.25\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1966/3393 [15:28<11:12,  2.12batch/s, Batch Loss=0.2920, Avg Loss=0.1178, Time Left=12.25\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1966/3393 [15:28<11:12,  2.12batch/s, Batch Loss=0.0731, Avg Loss=0.1178, Time Left=12.24\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1967/3393 [15:28<11:16,  2.11batch/s, Batch Loss=0.0731, Avg Loss=0.1178, Time Left=12.24\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1967/3393 [15:29<11:16,  2.11batch/s, Batch Loss=0.3710, Avg Loss=0.1179, Time Left=12.23\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1968/3393 [15:29<11:33,  2.06batch/s, Batch Loss=0.3710, Avg Loss=0.1179, Time Left=12.23\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1968/3393 [15:29<11:33,  2.06batch/s, Batch Loss=0.0039, Avg Loss=0.1178, Time Left=12.22\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1969/3393 [15:29<11:25,  2.08batch/s, Batch Loss=0.0039, Avg Loss=0.1178, Time Left=12.22\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1969/3393 [15:30<11:25,  2.08batch/s, Batch Loss=0.0486, Avg Loss=0.1178, Time Left=12.22\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1970/3393 [15:30<11:45,  2.02batch/s, Batch Loss=0.0486, Avg Loss=0.1178, Time Left=12.22\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1970/3393 [15:30<11:45,  2.02batch/s, Batch Loss=0.0288, Avg Loss=0.1178, Time Left=12.21\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1971/3393 [15:30<11:34,  2.05batch/s, Batch Loss=0.0288, Avg Loss=0.1178, Time Left=12.21\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1971/3393 [15:31<11:34,  2.05batch/s, Batch Loss=0.0059, Avg Loss=0.1177, Time Left=12.20\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1972/3393 [15:31<11:32,  2.05batch/s, Batch Loss=0.0059, Avg Loss=0.1177, Time Left=12.20\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1972/3393 [15:31<11:32,  2.05batch/s, Batch Loss=0.0716, Avg Loss=0.1177, Time Left=12.19\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1973/3393 [15:31<11:40,  2.03batch/s, Batch Loss=0.0716, Avg Loss=0.1177, Time Left=12.19\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1973/3393 [15:32<11:40,  2.03batch/s, Batch Loss=0.0087, Avg Loss=0.1176, Time Left=12.18\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1974/3393 [15:32<11:39,  2.03batch/s, Batch Loss=0.0087, Avg Loss=0.1176, Time Left=12.18\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1974/3393 [15:32<11:39,  2.03batch/s, Batch Loss=0.0363, Avg Loss=0.1176, Time Left=12.17\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1975/3393 [15:32<11:41,  2.02batch/s, Batch Loss=0.0363, Avg Loss=0.1176, Time Left=12.17\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1975/3393 [15:33<11:41,  2.02batch/s, Batch Loss=0.0500, Avg Loss=0.1175, Time Left=12.17\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1976/3393 [15:33<11:43,  2.01batch/s, Batch Loss=0.0500, Avg Loss=0.1175, Time Left=12.17\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1976/3393 [15:33<11:43,  2.01batch/s, Batch Loss=0.0887, Avg Loss=0.1175, Time Left=12.16\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1977/3393 [15:33<11:37,  2.03batch/s, Batch Loss=0.0887, Avg Loss=0.1175, Time Left=12.16\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1977/3393 [15:34<11:37,  2.03batch/s, Batch Loss=0.0931, Avg Loss=0.1175, Time Left=12.15\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  58%|▌| 1978/3393 [15:34<11:26,  2.06batch/s, Batch Loss=0.0931, Avg Loss=0.1175, Time Left=12.15\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1978/3393 [15:34<11:26,  2.06batch/s, Batch Loss=0.0754, Avg Loss=0.1175, Time Left=12.14\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1979/3393 [15:34<11:19,  2.08batch/s, Batch Loss=0.0754, Avg Loss=0.1175, Time Left=12.14\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1979/3393 [15:35<11:19,  2.08batch/s, Batch Loss=0.0813, Avg Loss=0.1175, Time Left=12.13\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1980/3393 [15:35<11:39,  2.02batch/s, Batch Loss=0.0813, Avg Loss=0.1175, Time Left=12.13\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1980/3393 [15:35<11:39,  2.02batch/s, Batch Loss=0.0362, Avg Loss=0.1174, Time Left=12.12\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1981/3393 [15:35<11:26,  2.06batch/s, Batch Loss=0.0362, Avg Loss=0.1174, Time Left=12.12\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1981/3393 [15:36<11:26,  2.06batch/s, Batch Loss=0.0697, Avg Loss=0.1174, Time Left=12.12\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1982/3393 [15:36<11:19,  2.08batch/s, Batch Loss=0.0697, Avg Loss=0.1174, Time Left=12.12\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1982/3393 [15:36<11:19,  2.08batch/s, Batch Loss=0.0189, Avg Loss=0.1174, Time Left=12.11\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1983/3393 [15:36<11:33,  2.03batch/s, Batch Loss=0.0189, Avg Loss=0.1174, Time Left=12.11\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1983/3393 [15:37<11:33,  2.03batch/s, Batch Loss=0.4365, Avg Loss=0.1175, Time Left=12.10\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1984/3393 [15:37<11:23,  2.06batch/s, Batch Loss=0.4365, Avg Loss=0.1175, Time Left=12.10\u001b[A\n",
      "Epoch 2/3 - Training:  58%|▌| 1984/3393 [15:37<11:23,  2.06batch/s, Batch Loss=0.0023, Avg Loss=0.1175, Time Left=12.09\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1985/3393 [15:37<11:28,  2.04batch/s, Batch Loss=0.0023, Avg Loss=0.1175, Time Left=12.09\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1985/3393 [15:38<11:28,  2.04batch/s, Batch Loss=0.1494, Avg Loss=0.1175, Time Left=12.08\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1986/3393 [15:38<11:26,  2.05batch/s, Batch Loss=0.1494, Avg Loss=0.1175, Time Left=12.08\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1986/3393 [15:38<11:26,  2.05batch/s, Batch Loss=0.0249, Avg Loss=0.1174, Time Left=12.08\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1987/3393 [15:38<11:10,  2.10batch/s, Batch Loss=0.0249, Avg Loss=0.1174, Time Left=12.08\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1987/3393 [15:39<11:10,  2.10batch/s, Batch Loss=0.2189, Avg Loss=0.1175, Time Left=12.07\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1988/3393 [15:39<11:19,  2.07batch/s, Batch Loss=0.2189, Avg Loss=0.1175, Time Left=12.07\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1988/3393 [15:39<11:19,  2.07batch/s, Batch Loss=0.2105, Avg Loss=0.1175, Time Left=12.06\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1989/3393 [15:39<11:18,  2.07batch/s, Batch Loss=0.2105, Avg Loss=0.1175, Time Left=12.06\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1989/3393 [15:40<11:18,  2.07batch/s, Batch Loss=0.0194, Avg Loss=0.1175, Time Left=12.05\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1990/3393 [15:40<11:11,  2.09batch/s, Batch Loss=0.0194, Avg Loss=0.1175, Time Left=12.05\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1990/3393 [15:40<11:11,  2.09batch/s, Batch Loss=0.0818, Avg Loss=0.1175, Time Left=12.04\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1991/3393 [15:40<11:19,  2.06batch/s, Batch Loss=0.0818, Avg Loss=0.1175, Time Left=12.04\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1991/3393 [15:41<11:19,  2.06batch/s, Batch Loss=0.0267, Avg Loss=0.1174, Time Left=12.03\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1992/3393 [15:41<11:19,  2.06batch/s, Batch Loss=0.0267, Avg Loss=0.1174, Time Left=12.03\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1992/3393 [15:41<11:19,  2.06batch/s, Batch Loss=0.2008, Avg Loss=0.1175, Time Left=12.03\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1993/3393 [15:41<11:18,  2.06batch/s, Batch Loss=0.2008, Avg Loss=0.1175, Time Left=12.03\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1993/3393 [15:42<11:18,  2.06batch/s, Batch Loss=0.0988, Avg Loss=0.1174, Time Left=12.02\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1994/3393 [15:42<11:37,  2.01batch/s, Batch Loss=0.0988, Avg Loss=0.1174, Time Left=12.02\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1994/3393 [15:42<11:37,  2.01batch/s, Batch Loss=0.2009, Avg Loss=0.1175, Time Left=12.01\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1995/3393 [15:42<11:31,  2.02batch/s, Batch Loss=0.2009, Avg Loss=0.1175, Time Left=12.01\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1995/3393 [15:43<11:31,  2.02batch/s, Batch Loss=0.1321, Avg Loss=0.1175, Time Left=12.00\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1996/3393 [15:43<11:26,  2.03batch/s, Batch Loss=0.1321, Avg Loss=0.1175, Time Left=12.00\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1996/3393 [15:43<11:26,  2.03batch/s, Batch Loss=0.0868, Avg Loss=0.1175, Time Left=11.99\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1997/3393 [15:43<11:16,  2.06batch/s, Batch Loss=0.0868, Avg Loss=0.1175, Time Left=11.99\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1997/3393 [15:44<11:16,  2.06batch/s, Batch Loss=0.1360, Avg Loss=0.1175, Time Left=11.99\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1998/3393 [15:44<11:15,  2.06batch/s, Batch Loss=0.1360, Avg Loss=0.1175, Time Left=11.99\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1998/3393 [15:44<11:15,  2.06batch/s, Batch Loss=0.1711, Avg Loss=0.1175, Time Left=11.98\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1999/3393 [15:44<11:27,  2.03batch/s, Batch Loss=0.1711, Avg Loss=0.1175, Time Left=11.98\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 1999/3393 [15:45<11:27,  2.03batch/s, Batch Loss=0.1772, Avg Loss=0.1176, Time Left=11.97\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2000/3393 [15:45<11:30,  2.02batch/s, Batch Loss=0.1772, Avg Loss=0.1176, Time Left=11.97\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2000/3393 [15:45<11:30,  2.02batch/s, Batch Loss=0.0491, Avg Loss=0.1175, Time Left=11.96\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2001/3393 [15:45<11:38,  1.99batch/s, Batch Loss=0.0491, Avg Loss=0.1175, Time Left=11.96\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2001/3393 [15:46<11:38,  1.99batch/s, Batch Loss=0.2107, Avg Loss=0.1176, Time Left=11.95\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2002/3393 [15:46<11:17,  2.05batch/s, Batch Loss=0.2107, Avg Loss=0.1176, Time Left=11.95\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2002/3393 [15:46<11:17,  2.05batch/s, Batch Loss=0.0246, Avg Loss=0.1175, Time Left=11.94\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2003/3393 [15:46<11:15,  2.06batch/s, Batch Loss=0.0246, Avg Loss=0.1175, Time Left=11.94\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2003/3393 [15:47<11:15,  2.06batch/s, Batch Loss=0.1531, Avg Loss=0.1175, Time Left=11.94\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2004/3393 [15:47<11:35,  2.00batch/s, Batch Loss=0.1531, Avg Loss=0.1175, Time Left=11.94\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2004/3393 [15:47<11:35,  2.00batch/s, Batch Loss=0.0652, Avg Loss=0.1175, Time Left=11.93\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2005/3393 [15:47<11:26,  2.02batch/s, Batch Loss=0.0652, Avg Loss=0.1175, Time Left=11.93\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2005/3393 [15:48<11:26,  2.02batch/s, Batch Loss=0.0972, Avg Loss=0.1175, Time Left=11.92\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2006/3393 [15:48<11:21,  2.03batch/s, Batch Loss=0.0972, Avg Loss=0.1175, Time Left=11.92\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2006/3393 [15:48<11:21,  2.03batch/s, Batch Loss=0.1356, Avg Loss=0.1175, Time Left=11.91\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2007/3393 [15:48<11:05,  2.08batch/s, Batch Loss=0.1356, Avg Loss=0.1175, Time Left=11.91\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2007/3393 [15:49<11:05,  2.08batch/s, Batch Loss=0.1897, Avg Loss=0.1175, Time Left=11.90\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2008/3393 [15:49<11:05,  2.08batch/s, Batch Loss=0.1897, Avg Loss=0.1175, Time Left=11.90\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2008/3393 [15:49<11:05,  2.08batch/s, Batch Loss=0.2376, Avg Loss=0.1176, Time Left=11.90\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2009/3393 [15:49<11:12,  2.06batch/s, Batch Loss=0.2376, Avg Loss=0.1176, Time Left=11.90\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2009/3393 [15:49<11:12,  2.06batch/s, Batch Loss=0.0861, Avg Loss=0.1176, Time Left=11.89\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2010/3393 [15:49<11:06,  2.08batch/s, Batch Loss=0.0861, Avg Loss=0.1176, Time Left=11.89\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2010/3393 [15:50<11:06,  2.08batch/s, Batch Loss=0.0628, Avg Loss=0.1176, Time Left=11.88\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  59%|▌| 2011/3393 [15:50<11:05,  2.08batch/s, Batch Loss=0.0628, Avg Loss=0.1176, Time Left=11.88\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2011/3393 [15:51<11:05,  2.08batch/s, Batch Loss=0.0821, Avg Loss=0.1175, Time Left=11.87\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2012/3393 [15:51<11:26,  2.01batch/s, Batch Loss=0.0821, Avg Loss=0.1175, Time Left=11.87\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2012/3393 [15:51<11:26,  2.01batch/s, Batch Loss=0.0596, Avg Loss=0.1175, Time Left=11.86\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2013/3393 [15:51<11:20,  2.03batch/s, Batch Loss=0.0596, Avg Loss=0.1175, Time Left=11.86\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2013/3393 [15:52<11:20,  2.03batch/s, Batch Loss=0.0924, Avg Loss=0.1175, Time Left=11.86\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2014/3393 [15:52<11:34,  1.99batch/s, Batch Loss=0.0924, Avg Loss=0.1175, Time Left=11.86\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2014/3393 [15:52<11:34,  1.99batch/s, Batch Loss=0.0902, Avg Loss=0.1175, Time Left=11.85\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2015/3393 [15:52<11:28,  2.00batch/s, Batch Loss=0.0902, Avg Loss=0.1175, Time Left=11.85\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2015/3393 [15:53<11:28,  2.00batch/s, Batch Loss=0.0584, Avg Loss=0.1175, Time Left=11.84\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2016/3393 [15:53<11:34,  1.98batch/s, Batch Loss=0.0584, Avg Loss=0.1175, Time Left=11.84\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2016/3393 [15:53<11:34,  1.98batch/s, Batch Loss=0.0536, Avg Loss=0.1174, Time Left=11.83\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2017/3393 [15:53<11:19,  2.03batch/s, Batch Loss=0.0536, Avg Loss=0.1174, Time Left=11.83\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2017/3393 [15:54<11:19,  2.03batch/s, Batch Loss=0.3292, Avg Loss=0.1175, Time Left=11.82\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2018/3393 [15:54<11:25,  2.01batch/s, Batch Loss=0.3292, Avg Loss=0.1175, Time Left=11.82\u001b[A\n",
      "Epoch 2/3 - Training:  59%|▌| 2018/3393 [15:54<11:25,  2.01batch/s, Batch Loss=0.0080, Avg Loss=0.1175, Time Left=11.81\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2019/3393 [15:54<11:15,  2.04batch/s, Batch Loss=0.0080, Avg Loss=0.1175, Time Left=11.81\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2019/3393 [15:54<11:15,  2.04batch/s, Batch Loss=0.0930, Avg Loss=0.1175, Time Left=11.81\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2020/3393 [15:54<11:05,  2.06batch/s, Batch Loss=0.0930, Avg Loss=0.1175, Time Left=11.81\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2020/3393 [15:55<11:05,  2.06batch/s, Batch Loss=0.1288, Avg Loss=0.1175, Time Left=11.80\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2021/3393 [15:55<11:22,  2.01batch/s, Batch Loss=0.1288, Avg Loss=0.1175, Time Left=11.80\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2021/3393 [15:55<11:22,  2.01batch/s, Batch Loss=0.1126, Avg Loss=0.1175, Time Left=11.79\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2022/3393 [15:55<11:17,  2.02batch/s, Batch Loss=0.1126, Avg Loss=0.1175, Time Left=11.79\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2022/3393 [15:56<11:17,  2.02batch/s, Batch Loss=0.1873, Avg Loss=0.1175, Time Left=11.78\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2023/3393 [15:56<11:13,  2.03batch/s, Batch Loss=0.1873, Avg Loss=0.1175, Time Left=11.78\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2023/3393 [15:56<11:13,  2.03batch/s, Batch Loss=0.1044, Avg Loss=0.1175, Time Left=11.77\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2024/3393 [15:56<11:02,  2.07batch/s, Batch Loss=0.1044, Avg Loss=0.1175, Time Left=11.77\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2024/3393 [15:57<11:02,  2.07batch/s, Batch Loss=0.0181, Avg Loss=0.1174, Time Left=11.76\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2025/3393 [15:57<10:48,  2.11batch/s, Batch Loss=0.0181, Avg Loss=0.1174, Time Left=11.76\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2025/3393 [15:57<10:48,  2.11batch/s, Batch Loss=0.1170, Avg Loss=0.1174, Time Left=11.76\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2026/3393 [15:57<10:51,  2.10batch/s, Batch Loss=0.1170, Avg Loss=0.1174, Time Left=11.76\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2026/3393 [15:58<10:51,  2.10batch/s, Batch Loss=0.0262, Avg Loss=0.1174, Time Left=11.75\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2027/3393 [15:58<11:00,  2.07batch/s, Batch Loss=0.0262, Avg Loss=0.1174, Time Left=11.75\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2027/3393 [15:58<11:00,  2.07batch/s, Batch Loss=0.0678, Avg Loss=0.1174, Time Left=11.74\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2028/3393 [15:58<10:59,  2.07batch/s, Batch Loss=0.0678, Avg Loss=0.1174, Time Left=11.74\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2028/3393 [15:59<10:59,  2.07batch/s, Batch Loss=0.0667, Avg Loss=0.1173, Time Left=11.73\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2029/3393 [15:59<10:53,  2.09batch/s, Batch Loss=0.0667, Avg Loss=0.1173, Time Left=11.73\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2029/3393 [15:59<10:53,  2.09batch/s, Batch Loss=0.0601, Avg Loss=0.1173, Time Left=11.72\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2030/3393 [15:59<10:54,  2.08batch/s, Batch Loss=0.0601, Avg Loss=0.1173, Time Left=11.72\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2030/3393 [16:00<10:54,  2.08batch/s, Batch Loss=0.2256, Avg Loss=0.1174, Time Left=11.72\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2031/3393 [16:00<10:50,  2.09batch/s, Batch Loss=0.2256, Avg Loss=0.1174, Time Left=11.72\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2031/3393 [16:00<10:50,  2.09batch/s, Batch Loss=0.0427, Avg Loss=0.1173, Time Left=11.71\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2032/3393 [16:00<10:57,  2.07batch/s, Batch Loss=0.0427, Avg Loss=0.1173, Time Left=11.71\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2032/3393 [16:01<10:57,  2.07batch/s, Batch Loss=0.1201, Avg Loss=0.1173, Time Left=11.70\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2033/3393 [16:01<10:50,  2.09batch/s, Batch Loss=0.1201, Avg Loss=0.1173, Time Left=11.70\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2033/3393 [16:01<10:50,  2.09batch/s, Batch Loss=0.0229, Avg Loss=0.1173, Time Left=11.69\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2034/3393 [16:01<10:39,  2.12batch/s, Batch Loss=0.0229, Avg Loss=0.1173, Time Left=11.69\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2034/3393 [16:02<10:39,  2.12batch/s, Batch Loss=0.0455, Avg Loss=0.1173, Time Left=11.68\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2035/3393 [16:02<10:50,  2.09batch/s, Batch Loss=0.0455, Avg Loss=0.1173, Time Left=11.68\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2035/3393 [16:02<10:50,  2.09batch/s, Batch Loss=0.1067, Avg Loss=0.1172, Time Left=11.67\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2036/3393 [16:02<10:53,  2.08batch/s, Batch Loss=0.1067, Avg Loss=0.1172, Time Left=11.67\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2036/3393 [16:03<10:53,  2.08batch/s, Batch Loss=0.0230, Avg Loss=0.1172, Time Left=11.67\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2037/3393 [16:03<10:45,  2.10batch/s, Batch Loss=0.0230, Avg Loss=0.1172, Time Left=11.67\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2037/3393 [16:03<10:45,  2.10batch/s, Batch Loss=0.0103, Avg Loss=0.1171, Time Left=11.66\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2038/3393 [16:03<10:47,  2.09batch/s, Batch Loss=0.0103, Avg Loss=0.1171, Time Left=11.66\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2038/3393 [16:04<10:47,  2.09batch/s, Batch Loss=0.0693, Avg Loss=0.1171, Time Left=11.65\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2039/3393 [16:04<10:56,  2.06batch/s, Batch Loss=0.0693, Avg Loss=0.1171, Time Left=11.65\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2039/3393 [16:04<10:56,  2.06batch/s, Batch Loss=0.1000, Avg Loss=0.1171, Time Left=11.64\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2040/3393 [16:04<10:56,  2.06batch/s, Batch Loss=0.1000, Avg Loss=0.1171, Time Left=11.64\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2040/3393 [16:05<10:56,  2.06batch/s, Batch Loss=0.0177, Avg Loss=0.1171, Time Left=11.63\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2041/3393 [16:05<11:07,  2.02batch/s, Batch Loss=0.0177, Avg Loss=0.1171, Time Left=11.63\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2041/3393 [16:05<11:07,  2.02batch/s, Batch Loss=0.2459, Avg Loss=0.1171, Time Left=11.62\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2042/3393 [16:05<11:03,  2.04batch/s, Batch Loss=0.2459, Avg Loss=0.1171, Time Left=11.62\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2042/3393 [16:06<11:03,  2.04batch/s, Batch Loss=0.0852, Avg Loss=0.1171, Time Left=11.62\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2043/3393 [16:06<11:00,  2.04batch/s, Batch Loss=0.0852, Avg Loss=0.1171, Time Left=11.62\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2043/3393 [16:06<11:00,  2.04batch/s, Batch Loss=0.0463, Avg Loss=0.1171, Time Left=11.61\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  60%|▌| 2044/3393 [16:06<10:57,  2.05batch/s, Batch Loss=0.0463, Avg Loss=0.1171, Time Left=11.61\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2044/3393 [16:07<10:57,  2.05batch/s, Batch Loss=0.0298, Avg Loss=0.1170, Time Left=11.60\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2045/3393 [16:07<10:55,  2.06batch/s, Batch Loss=0.0298, Avg Loss=0.1170, Time Left=11.60\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2045/3393 [16:07<10:55,  2.06batch/s, Batch Loss=0.0189, Avg Loss=0.1170, Time Left=11.59\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2046/3393 [16:07<10:53,  2.06batch/s, Batch Loss=0.0189, Avg Loss=0.1170, Time Left=11.59\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2046/3393 [16:08<10:53,  2.06batch/s, Batch Loss=0.0116, Avg Loss=0.1169, Time Left=11.58\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2047/3393 [16:08<10:53,  2.06batch/s, Batch Loss=0.0116, Avg Loss=0.1169, Time Left=11.58\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2047/3393 [16:08<10:53,  2.06batch/s, Batch Loss=0.0086, Avg Loss=0.1169, Time Left=11.58\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2048/3393 [16:08<10:58,  2.04batch/s, Batch Loss=0.0086, Avg Loss=0.1169, Time Left=11.58\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2048/3393 [16:09<10:58,  2.04batch/s, Batch Loss=0.0327, Avg Loss=0.1168, Time Left=11.57\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2049/3393 [16:09<11:07,  2.01batch/s, Batch Loss=0.0327, Avg Loss=0.1168, Time Left=11.57\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2049/3393 [16:09<11:07,  2.01batch/s, Batch Loss=0.0032, Avg Loss=0.1168, Time Left=11.56\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2050/3393 [16:09<11:02,  2.03batch/s, Batch Loss=0.0032, Avg Loss=0.1168, Time Left=11.56\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2050/3393 [16:10<11:02,  2.03batch/s, Batch Loss=0.0031, Avg Loss=0.1167, Time Left=11.55\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2051/3393 [16:10<11:11,  2.00batch/s, Batch Loss=0.0031, Avg Loss=0.1167, Time Left=11.55\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2051/3393 [16:10<11:11,  2.00batch/s, Batch Loss=0.0020, Avg Loss=0.1167, Time Left=11.54\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2052/3393 [16:10<11:05,  2.02batch/s, Batch Loss=0.0020, Avg Loss=0.1167, Time Left=11.54\u001b[A\n",
      "Epoch 2/3 - Training:  60%|▌| 2052/3393 [16:10<11:05,  2.02batch/s, Batch Loss=0.0995, Avg Loss=0.1166, Time Left=11.53\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2053/3393 [16:10<10:54,  2.05batch/s, Batch Loss=0.0995, Avg Loss=0.1166, Time Left=11.53\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2053/3393 [16:11<10:54,  2.05batch/s, Batch Loss=0.0060, Avg Loss=0.1166, Time Left=11.53\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2054/3393 [16:11<10:50,  2.06batch/s, Batch Loss=0.0060, Avg Loss=0.1166, Time Left=11.53\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2054/3393 [16:11<10:50,  2.06batch/s, Batch Loss=0.3528, Avg Loss=0.1167, Time Left=11.52\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2055/3393 [16:11<10:42,  2.08batch/s, Batch Loss=0.3528, Avg Loss=0.1167, Time Left=11.52\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2055/3393 [16:12<10:42,  2.08batch/s, Batch Loss=0.0172, Avg Loss=0.1167, Time Left=11.51\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2056/3393 [16:12<10:43,  2.08batch/s, Batch Loss=0.0172, Avg Loss=0.1167, Time Left=11.51\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2056/3393 [16:12<10:43,  2.08batch/s, Batch Loss=0.0603, Avg Loss=0.1166, Time Left=11.50\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2057/3393 [16:12<10:37,  2.09batch/s, Batch Loss=0.0603, Avg Loss=0.1166, Time Left=11.50\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2057/3393 [16:13<10:37,  2.09batch/s, Batch Loss=0.1534, Avg Loss=0.1166, Time Left=11.49\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2058/3393 [16:13<10:41,  2.08batch/s, Batch Loss=0.1534, Avg Loss=0.1166, Time Left=11.49\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2058/3393 [16:13<10:41,  2.08batch/s, Batch Loss=0.0343, Avg Loss=0.1166, Time Left=11.48\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2059/3393 [16:13<10:40,  2.08batch/s, Batch Loss=0.0343, Avg Loss=0.1166, Time Left=11.48\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2059/3393 [16:14<10:40,  2.08batch/s, Batch Loss=0.1084, Avg Loss=0.1166, Time Left=11.48\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2060/3393 [16:14<10:41,  2.08batch/s, Batch Loss=0.1084, Avg Loss=0.1166, Time Left=11.48\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2060/3393 [16:14<10:41,  2.08batch/s, Batch Loss=0.1918, Avg Loss=0.1166, Time Left=11.47\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2061/3393 [16:14<10:35,  2.10batch/s, Batch Loss=0.1918, Avg Loss=0.1166, Time Left=11.47\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2061/3393 [16:15<10:35,  2.10batch/s, Batch Loss=0.0902, Avg Loss=0.1166, Time Left=11.46\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2062/3393 [16:15<10:49,  2.05batch/s, Batch Loss=0.0902, Avg Loss=0.1166, Time Left=11.46\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2062/3393 [16:15<10:49,  2.05batch/s, Batch Loss=0.0043, Avg Loss=0.1166, Time Left=11.45\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2063/3393 [16:15<10:48,  2.05batch/s, Batch Loss=0.0043, Avg Loss=0.1166, Time Left=11.45\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2063/3393 [16:16<10:48,  2.05batch/s, Batch Loss=0.0087, Avg Loss=0.1165, Time Left=11.44\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2064/3393 [16:16<10:43,  2.06batch/s, Batch Loss=0.0087, Avg Loss=0.1165, Time Left=11.44\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2064/3393 [16:16<10:43,  2.06batch/s, Batch Loss=0.0344, Avg Loss=0.1165, Time Left=11.44\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2065/3393 [16:16<10:45,  2.06batch/s, Batch Loss=0.0344, Avg Loss=0.1165, Time Left=11.44\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2065/3393 [16:17<10:45,  2.06batch/s, Batch Loss=0.0016, Avg Loss=0.1164, Time Left=11.43\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2066/3393 [16:17<10:38,  2.08batch/s, Batch Loss=0.0016, Avg Loss=0.1164, Time Left=11.43\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2066/3393 [16:17<10:38,  2.08batch/s, Batch Loss=0.2046, Avg Loss=0.1165, Time Left=11.42\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2067/3393 [16:17<10:31,  2.10batch/s, Batch Loss=0.2046, Avg Loss=0.1165, Time Left=11.42\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2067/3393 [16:18<10:31,  2.10batch/s, Batch Loss=0.3849, Avg Loss=0.1166, Time Left=11.41\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2068/3393 [16:18<10:58,  2.01batch/s, Batch Loss=0.3849, Avg Loss=0.1166, Time Left=11.41\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2068/3393 [16:18<10:58,  2.01batch/s, Batch Loss=0.0231, Avg Loss=0.1166, Time Left=11.40\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2069/3393 [16:18<10:48,  2.04batch/s, Batch Loss=0.0231, Avg Loss=0.1166, Time Left=11.40\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2069/3393 [16:19<10:48,  2.04batch/s, Batch Loss=0.0461, Avg Loss=0.1165, Time Left=11.39\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2070/3393 [16:19<10:51,  2.03batch/s, Batch Loss=0.0461, Avg Loss=0.1165, Time Left=11.39\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2070/3393 [16:19<10:51,  2.03batch/s, Batch Loss=0.0216, Avg Loss=0.1165, Time Left=11.39\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2071/3393 [16:19<10:42,  2.06batch/s, Batch Loss=0.0216, Avg Loss=0.1165, Time Left=11.39\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2071/3393 [16:20<10:42,  2.06batch/s, Batch Loss=0.0039, Avg Loss=0.1164, Time Left=11.38\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2072/3393 [16:20<10:39,  2.06batch/s, Batch Loss=0.0039, Avg Loss=0.1164, Time Left=11.38\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2072/3393 [16:20<10:39,  2.06batch/s, Batch Loss=0.0546, Avg Loss=0.1164, Time Left=11.37\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2073/3393 [16:20<10:39,  2.06batch/s, Batch Loss=0.0546, Avg Loss=0.1164, Time Left=11.37\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2073/3393 [16:21<10:39,  2.06batch/s, Batch Loss=0.2148, Avg Loss=0.1164, Time Left=11.36\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2074/3393 [16:21<10:32,  2.09batch/s, Batch Loss=0.2148, Avg Loss=0.1164, Time Left=11.36\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2074/3393 [16:21<10:32,  2.09batch/s, Batch Loss=0.0937, Avg Loss=0.1164, Time Left=11.35\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2075/3393 [16:21<10:32,  2.08batch/s, Batch Loss=0.0937, Avg Loss=0.1164, Time Left=11.35\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2075/3393 [16:22<10:32,  2.08batch/s, Batch Loss=0.0061, Avg Loss=0.1164, Time Left=11.34\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2076/3393 [16:22<10:33,  2.08batch/s, Batch Loss=0.0061, Avg Loss=0.1164, Time Left=11.34\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2076/3393 [16:22<10:33,  2.08batch/s, Batch Loss=0.0363, Avg Loss=0.1163, Time Left=11.34\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  61%|▌| 2077/3393 [16:22<10:34,  2.07batch/s, Batch Loss=0.0363, Avg Loss=0.1163, Time Left=11.34\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2077/3393 [16:23<10:34,  2.07batch/s, Batch Loss=0.0953, Avg Loss=0.1163, Time Left=11.33\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2078/3393 [16:23<10:33,  2.07batch/s, Batch Loss=0.0953, Avg Loss=0.1163, Time Left=11.33\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2078/3393 [16:23<10:33,  2.07batch/s, Batch Loss=0.0032, Avg Loss=0.1163, Time Left=11.32\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2079/3393 [16:23<10:40,  2.05batch/s, Batch Loss=0.0032, Avg Loss=0.1163, Time Left=11.32\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2079/3393 [16:24<10:40,  2.05batch/s, Batch Loss=0.0055, Avg Loss=0.1162, Time Left=11.31\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2080/3393 [16:24<10:39,  2.05batch/s, Batch Loss=0.0055, Avg Loss=0.1162, Time Left=11.31\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2080/3393 [16:24<10:39,  2.05batch/s, Batch Loss=0.2008, Avg Loss=0.1162, Time Left=11.30\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2081/3393 [16:24<10:50,  2.02batch/s, Batch Loss=0.2008, Avg Loss=0.1162, Time Left=11.30\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2081/3393 [16:25<10:50,  2.02batch/s, Batch Loss=0.0161, Avg Loss=0.1162, Time Left=11.30\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2082/3393 [16:25<10:51,  2.01batch/s, Batch Loss=0.0161, Avg Loss=0.1162, Time Left=11.30\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2082/3393 [16:25<10:51,  2.01batch/s, Batch Loss=0.0662, Avg Loss=0.1162, Time Left=11.29\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2083/3393 [16:25<10:39,  2.05batch/s, Batch Loss=0.0662, Avg Loss=0.1162, Time Left=11.29\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2083/3393 [16:26<10:39,  2.05batch/s, Batch Loss=0.0077, Avg Loss=0.1161, Time Left=11.28\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2084/3393 [16:26<10:37,  2.05batch/s, Batch Loss=0.0077, Avg Loss=0.1161, Time Left=11.28\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2084/3393 [16:26<10:37,  2.05batch/s, Batch Loss=0.0181, Avg Loss=0.1161, Time Left=11.27\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2085/3393 [16:26<10:29,  2.08batch/s, Batch Loss=0.0181, Avg Loss=0.1161, Time Left=11.27\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2085/3393 [16:26<10:29,  2.08batch/s, Batch Loss=0.0230, Avg Loss=0.1160, Time Left=11.26\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2086/3393 [16:26<10:29,  2.08batch/s, Batch Loss=0.0230, Avg Loss=0.1160, Time Left=11.26\u001b[A\n",
      "Epoch 2/3 - Training:  61%|▌| 2086/3393 [16:27<10:29,  2.08batch/s, Batch Loss=0.3935, Avg Loss=0.1162, Time Left=11.25\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2087/3393 [16:27<10:29,  2.07batch/s, Batch Loss=0.3935, Avg Loss=0.1162, Time Left=11.25\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2087/3393 [16:27<10:29,  2.07batch/s, Batch Loss=0.0254, Avg Loss=0.1161, Time Left=11.25\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2088/3393 [16:27<10:18,  2.11batch/s, Batch Loss=0.0254, Avg Loss=0.1161, Time Left=11.25\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2088/3393 [16:28<10:18,  2.11batch/s, Batch Loss=0.3103, Avg Loss=0.1162, Time Left=11.24\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2089/3393 [16:28<10:26,  2.08batch/s, Batch Loss=0.3103, Avg Loss=0.1162, Time Left=11.24\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2089/3393 [16:28<10:26,  2.08batch/s, Batch Loss=0.0628, Avg Loss=0.1162, Time Left=11.23\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2090/3393 [16:28<10:27,  2.08batch/s, Batch Loss=0.0628, Avg Loss=0.1162, Time Left=11.23\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2090/3393 [16:29<10:27,  2.08batch/s, Batch Loss=0.0285, Avg Loss=0.1161, Time Left=11.22\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2091/3393 [16:29<10:28,  2.07batch/s, Batch Loss=0.0285, Avg Loss=0.1161, Time Left=11.22\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2091/3393 [16:29<10:28,  2.07batch/s, Batch Loss=0.0624, Avg Loss=0.1161, Time Left=11.21\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2092/3393 [16:29<10:40,  2.03batch/s, Batch Loss=0.0624, Avg Loss=0.1161, Time Left=11.21\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2092/3393 [16:30<10:40,  2.03batch/s, Batch Loss=0.0081, Avg Loss=0.1161, Time Left=11.21\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2093/3393 [16:30<10:43,  2.02batch/s, Batch Loss=0.0081, Avg Loss=0.1161, Time Left=11.21\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2093/3393 [16:30<10:43,  2.02batch/s, Batch Loss=0.0111, Avg Loss=0.1160, Time Left=11.20\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2094/3393 [16:30<10:32,  2.05batch/s, Batch Loss=0.0111, Avg Loss=0.1160, Time Left=11.20\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2094/3393 [16:31<10:32,  2.05batch/s, Batch Loss=0.0769, Avg Loss=0.1160, Time Left=11.19\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2095/3393 [16:31<10:31,  2.06batch/s, Batch Loss=0.0769, Avg Loss=0.1160, Time Left=11.19\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2095/3393 [16:31<10:31,  2.06batch/s, Batch Loss=0.1808, Avg Loss=0.1160, Time Left=11.18\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2096/3393 [16:31<10:35,  2.04batch/s, Batch Loss=0.1808, Avg Loss=0.1160, Time Left=11.18\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2096/3393 [16:32<10:35,  2.04batch/s, Batch Loss=0.0680, Avg Loss=0.1160, Time Left=11.17\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2097/3393 [16:32<10:44,  2.01batch/s, Batch Loss=0.0680, Avg Loss=0.1160, Time Left=11.17\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2097/3393 [16:32<10:44,  2.01batch/s, Batch Loss=0.1424, Avg Loss=0.1160, Time Left=11.16\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2098/3393 [16:32<10:40,  2.02batch/s, Batch Loss=0.1424, Avg Loss=0.1160, Time Left=11.16\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2098/3393 [16:33<10:40,  2.02batch/s, Batch Loss=0.0549, Avg Loss=0.1160, Time Left=11.16\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2099/3393 [16:33<10:53,  1.98batch/s, Batch Loss=0.0549, Avg Loss=0.1160, Time Left=11.16\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2099/3393 [16:33<10:53,  1.98batch/s, Batch Loss=0.1930, Avg Loss=0.1160, Time Left=11.15\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2100/3393 [16:33<10:46,  2.00batch/s, Batch Loss=0.1930, Avg Loss=0.1160, Time Left=11.15\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2100/3393 [16:34<10:46,  2.00batch/s, Batch Loss=0.0111, Avg Loss=0.1160, Time Left=11.14\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2101/3393 [16:34<10:39,  2.02batch/s, Batch Loss=0.0111, Avg Loss=0.1160, Time Left=11.14\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2101/3393 [16:34<10:39,  2.02batch/s, Batch Loss=0.0575, Avg Loss=0.1159, Time Left=11.13\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2102/3393 [16:34<10:28,  2.05batch/s, Batch Loss=0.0575, Avg Loss=0.1159, Time Left=11.13\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2102/3393 [16:35<10:28,  2.05batch/s, Batch Loss=0.0831, Avg Loss=0.1159, Time Left=11.12\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2103/3393 [16:35<10:26,  2.06batch/s, Batch Loss=0.0831, Avg Loss=0.1159, Time Left=11.12\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2103/3393 [16:35<10:26,  2.06batch/s, Batch Loss=0.1182, Avg Loss=0.1159, Time Left=11.12\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2104/3393 [16:35<10:43,  2.00batch/s, Batch Loss=0.1182, Avg Loss=0.1159, Time Left=11.12\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2104/3393 [16:36<10:43,  2.00batch/s, Batch Loss=0.0279, Avg Loss=0.1159, Time Left=11.11\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2105/3393 [16:36<10:44,  2.00batch/s, Batch Loss=0.0279, Avg Loss=0.1159, Time Left=11.11\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2105/3393 [16:36<10:44,  2.00batch/s, Batch Loss=0.0377, Avg Loss=0.1158, Time Left=11.10\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2106/3393 [16:36<10:37,  2.02batch/s, Batch Loss=0.0377, Avg Loss=0.1158, Time Left=11.10\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2106/3393 [16:37<10:37,  2.02batch/s, Batch Loss=0.0423, Avg Loss=0.1158, Time Left=11.09\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2107/3393 [16:37<10:26,  2.05batch/s, Batch Loss=0.0423, Avg Loss=0.1158, Time Left=11.09\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2107/3393 [16:37<10:26,  2.05batch/s, Batch Loss=0.0177, Avg Loss=0.1158, Time Left=11.08\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2108/3393 [16:37<10:18,  2.08batch/s, Batch Loss=0.0177, Avg Loss=0.1158, Time Left=11.08\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2108/3393 [16:38<10:18,  2.08batch/s, Batch Loss=0.1696, Avg Loss=0.1158, Time Left=11.07\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2109/3393 [16:38<10:18,  2.08batch/s, Batch Loss=0.1696, Avg Loss=0.1158, Time Left=11.07\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2109/3393 [16:38<10:18,  2.08batch/s, Batch Loss=0.0687, Avg Loss=0.1158, Time Left=11.07\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  62%|▌| 2110/3393 [16:38<10:25,  2.05batch/s, Batch Loss=0.0687, Avg Loss=0.1158, Time Left=11.07\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2110/3393 [16:39<10:25,  2.05batch/s, Batch Loss=0.0714, Avg Loss=0.1157, Time Left=11.06\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2111/3393 [16:39<10:23,  2.06batch/s, Batch Loss=0.0714, Avg Loss=0.1157, Time Left=11.06\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2111/3393 [16:39<10:23,  2.06batch/s, Batch Loss=0.1352, Avg Loss=0.1157, Time Left=11.05\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2112/3393 [16:39<10:27,  2.04batch/s, Batch Loss=0.1352, Avg Loss=0.1157, Time Left=11.05\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2112/3393 [16:40<10:27,  2.04batch/s, Batch Loss=0.0832, Avg Loss=0.1157, Time Left=11.04\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2113/3393 [16:40<10:19,  2.07batch/s, Batch Loss=0.0832, Avg Loss=0.1157, Time Left=11.04\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2113/3393 [16:40<10:19,  2.07batch/s, Batch Loss=0.1486, Avg Loss=0.1157, Time Left=11.03\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2114/3393 [16:40<10:12,  2.09batch/s, Batch Loss=0.1486, Avg Loss=0.1157, Time Left=11.03\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2114/3393 [16:41<10:12,  2.09batch/s, Batch Loss=0.0302, Avg Loss=0.1157, Time Left=11.03\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2115/3393 [16:41<10:19,  2.06batch/s, Batch Loss=0.0302, Avg Loss=0.1157, Time Left=11.03\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2115/3393 [16:41<10:19,  2.06batch/s, Batch Loss=0.0358, Avg Loss=0.1157, Time Left=11.02\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2116/3393 [16:41<10:15,  2.07batch/s, Batch Loss=0.0358, Avg Loss=0.1157, Time Left=11.02\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2116/3393 [16:42<10:15,  2.07batch/s, Batch Loss=0.0812, Avg Loss=0.1156, Time Left=11.01\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2117/3393 [16:42<10:12,  2.08batch/s, Batch Loss=0.0812, Avg Loss=0.1156, Time Left=11.01\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2117/3393 [16:42<10:12,  2.08batch/s, Batch Loss=0.0107, Avg Loss=0.1156, Time Left=11.00\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2118/3393 [16:42<10:13,  2.08batch/s, Batch Loss=0.0107, Avg Loss=0.1156, Time Left=11.00\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2118/3393 [16:43<10:13,  2.08batch/s, Batch Loss=0.3699, Avg Loss=0.1157, Time Left=10.99\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2119/3393 [16:43<10:14,  2.07batch/s, Batch Loss=0.3699, Avg Loss=0.1157, Time Left=10.99\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2119/3393 [16:43<10:14,  2.07batch/s, Batch Loss=0.0135, Avg Loss=0.1157, Time Left=10.98\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2120/3393 [16:43<10:07,  2.09batch/s, Batch Loss=0.0135, Avg Loss=0.1157, Time Left=10.98\u001b[A\n",
      "Epoch 2/3 - Training:  62%|▌| 2120/3393 [16:44<10:07,  2.09batch/s, Batch Loss=0.1257, Avg Loss=0.1157, Time Left=10.98\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2121/3393 [16:44<10:18,  2.06batch/s, Batch Loss=0.1257, Avg Loss=0.1157, Time Left=10.98\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2121/3393 [16:44<10:18,  2.06batch/s, Batch Loss=0.1407, Avg Loss=0.1157, Time Left=10.97\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2122/3393 [16:44<10:14,  2.07batch/s, Batch Loss=0.1407, Avg Loss=0.1157, Time Left=10.97\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2122/3393 [16:44<10:14,  2.07batch/s, Batch Loss=0.0147, Avg Loss=0.1156, Time Left=10.96\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2123/3393 [16:44<10:14,  2.07batch/s, Batch Loss=0.0147, Avg Loss=0.1156, Time Left=10.96\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2123/3393 [16:45<10:14,  2.07batch/s, Batch Loss=0.0210, Avg Loss=0.1156, Time Left=10.95\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2124/3393 [16:45<10:07,  2.09batch/s, Batch Loss=0.0210, Avg Loss=0.1156, Time Left=10.95\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2124/3393 [16:45<10:07,  2.09batch/s, Batch Loss=0.0179, Avg Loss=0.1155, Time Left=10.94\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2125/3393 [16:45<10:02,  2.10batch/s, Batch Loss=0.0179, Avg Loss=0.1155, Time Left=10.94\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2125/3393 [16:46<10:02,  2.10batch/s, Batch Loss=0.0590, Avg Loss=0.1155, Time Left=10.93\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2126/3393 [16:46<10:23,  2.03batch/s, Batch Loss=0.0590, Avg Loss=0.1155, Time Left=10.93\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2126/3393 [16:46<10:23,  2.03batch/s, Batch Loss=0.0097, Avg Loss=0.1155, Time Left=10.93\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2127/3393 [16:46<10:08,  2.08batch/s, Batch Loss=0.0097, Avg Loss=0.1155, Time Left=10.93\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2127/3393 [16:47<10:08,  2.08batch/s, Batch Loss=0.0042, Avg Loss=0.1154, Time Left=10.92\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2128/3393 [16:47<10:14,  2.06batch/s, Batch Loss=0.0042, Avg Loss=0.1154, Time Left=10.92\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2128/3393 [16:47<10:14,  2.06batch/s, Batch Loss=0.0030, Avg Loss=0.1154, Time Left=10.91\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2129/3393 [16:47<10:07,  2.08batch/s, Batch Loss=0.0030, Avg Loss=0.1154, Time Left=10.91\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2129/3393 [16:48<10:07,  2.08batch/s, Batch Loss=0.0211, Avg Loss=0.1153, Time Left=10.90\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2130/3393 [16:48<10:13,  2.06batch/s, Batch Loss=0.0211, Avg Loss=0.1153, Time Left=10.90\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2130/3393 [16:48<10:13,  2.06batch/s, Batch Loss=0.1626, Avg Loss=0.1153, Time Left=10.89\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2131/3393 [16:48<10:06,  2.08batch/s, Batch Loss=0.1626, Avg Loss=0.1153, Time Left=10.89\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2131/3393 [16:49<10:06,  2.08batch/s, Batch Loss=0.0662, Avg Loss=0.1153, Time Left=10.89\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2132/3393 [16:49<10:25,  2.02batch/s, Batch Loss=0.0662, Avg Loss=0.1153, Time Left=10.89\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2132/3393 [16:49<10:25,  2.02batch/s, Batch Loss=0.4120, Avg Loss=0.1155, Time Left=10.88\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2133/3393 [16:49<10:20,  2.03batch/s, Batch Loss=0.4120, Avg Loss=0.1155, Time Left=10.88\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2133/3393 [16:50<10:20,  2.03batch/s, Batch Loss=0.0808, Avg Loss=0.1154, Time Left=10.87\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2134/3393 [16:50<10:28,  2.00batch/s, Batch Loss=0.0808, Avg Loss=0.1154, Time Left=10.87\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2134/3393 [16:50<10:28,  2.00batch/s, Batch Loss=0.0627, Avg Loss=0.1154, Time Left=10.86\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2135/3393 [16:50<10:11,  2.06batch/s, Batch Loss=0.0627, Avg Loss=0.1154, Time Left=10.86\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2135/3393 [16:51<10:11,  2.06batch/s, Batch Loss=0.0927, Avg Loss=0.1154, Time Left=10.85\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2136/3393 [16:51<10:15,  2.04batch/s, Batch Loss=0.0927, Avg Loss=0.1154, Time Left=10.85\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2136/3393 [16:51<10:15,  2.04batch/s, Batch Loss=0.0028, Avg Loss=0.1153, Time Left=10.84\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2137/3393 [16:51<10:18,  2.03batch/s, Batch Loss=0.0028, Avg Loss=0.1153, Time Left=10.84\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2137/3393 [16:52<10:18,  2.03batch/s, Batch Loss=0.0020, Avg Loss=0.1153, Time Left=10.84\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2138/3393 [16:52<10:15,  2.04batch/s, Batch Loss=0.0020, Avg Loss=0.1153, Time Left=10.84\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2138/3393 [16:52<10:15,  2.04batch/s, Batch Loss=0.0733, Avg Loss=0.1153, Time Left=10.83\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2139/3393 [16:52<10:25,  2.00batch/s, Batch Loss=0.0733, Avg Loss=0.1153, Time Left=10.83\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2139/3393 [16:53<10:25,  2.00batch/s, Batch Loss=0.1828, Avg Loss=0.1153, Time Left=10.82\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2140/3393 [16:53<10:20,  2.02batch/s, Batch Loss=0.1828, Avg Loss=0.1153, Time Left=10.82\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2140/3393 [16:53<10:20,  2.02batch/s, Batch Loss=0.3014, Avg Loss=0.1154, Time Left=10.81\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2141/3393 [16:53<10:32,  1.98batch/s, Batch Loss=0.3014, Avg Loss=0.1154, Time Left=10.81\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2141/3393 [16:54<10:32,  1.98batch/s, Batch Loss=0.1896, Avg Loss=0.1154, Time Left=10.80\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2142/3393 [16:54<10:24,  2.00batch/s, Batch Loss=0.1896, Avg Loss=0.1154, Time Left=10.80\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2142/3393 [16:54<10:24,  2.00batch/s, Batch Loss=0.2619, Avg Loss=0.1155, Time Left=10.80\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  63%|▋| 2143/3393 [16:54<10:18,  2.02batch/s, Batch Loss=0.2619, Avg Loss=0.1155, Time Left=10.80\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2143/3393 [16:55<10:18,  2.02batch/s, Batch Loss=0.1187, Avg Loss=0.1155, Time Left=10.79\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2144/3393 [16:55<10:13,  2.04batch/s, Batch Loss=0.1187, Avg Loss=0.1155, Time Left=10.79\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2144/3393 [16:55<10:13,  2.04batch/s, Batch Loss=0.0682, Avg Loss=0.1155, Time Left=10.78\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2145/3393 [16:55<10:10,  2.04batch/s, Batch Loss=0.0682, Avg Loss=0.1155, Time Left=10.78\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2145/3393 [16:56<10:10,  2.04batch/s, Batch Loss=0.1484, Avg Loss=0.1155, Time Left=10.77\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2146/3393 [16:56<10:26,  1.99batch/s, Batch Loss=0.1484, Avg Loss=0.1155, Time Left=10.77\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2146/3393 [16:56<10:26,  1.99batch/s, Batch Loss=0.0825, Avg Loss=0.1155, Time Left=10.76\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2147/3393 [16:56<10:12,  2.03batch/s, Batch Loss=0.0825, Avg Loss=0.1155, Time Left=10.76\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2147/3393 [16:57<10:12,  2.03batch/s, Batch Loss=0.0184, Avg Loss=0.1154, Time Left=10.76\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2148/3393 [16:57<10:03,  2.06batch/s, Batch Loss=0.0184, Avg Loss=0.1154, Time Left=10.76\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2148/3393 [16:57<10:03,  2.06batch/s, Batch Loss=0.1503, Avg Loss=0.1155, Time Left=10.75\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2149/3393 [16:57<10:13,  2.03batch/s, Batch Loss=0.1503, Avg Loss=0.1155, Time Left=10.75\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2149/3393 [16:58<10:13,  2.03batch/s, Batch Loss=0.0541, Avg Loss=0.1154, Time Left=10.74\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2150/3393 [16:58<10:10,  2.04batch/s, Batch Loss=0.0541, Avg Loss=0.1154, Time Left=10.74\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2150/3393 [16:58<10:10,  2.04batch/s, Batch Loss=0.2065, Avg Loss=0.1155, Time Left=10.73\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2151/3393 [16:58<10:13,  2.03batch/s, Batch Loss=0.2065, Avg Loss=0.1155, Time Left=10.73\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2151/3393 [16:59<10:13,  2.03batch/s, Batch Loss=0.0434, Avg Loss=0.1154, Time Left=10.72\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2152/3393 [16:59<09:58,  2.08batch/s, Batch Loss=0.0434, Avg Loss=0.1154, Time Left=10.72\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2152/3393 [16:59<09:58,  2.08batch/s, Batch Loss=0.0800, Avg Loss=0.1154, Time Left=10.71\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2153/3393 [16:59<09:57,  2.08batch/s, Batch Loss=0.0800, Avg Loss=0.1154, Time Left=10.71\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2153/3393 [17:00<09:57,  2.08batch/s, Batch Loss=0.0360, Avg Loss=0.1154, Time Left=10.71\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2154/3393 [17:00<10:03,  2.05batch/s, Batch Loss=0.0360, Avg Loss=0.1154, Time Left=10.71\u001b[A\n",
      "Epoch 2/3 - Training:  63%|▋| 2154/3393 [17:00<10:03,  2.05batch/s, Batch Loss=0.2049, Avg Loss=0.1154, Time Left=10.70\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2155/3393 [17:00<09:56,  2.08batch/s, Batch Loss=0.2049, Avg Loss=0.1154, Time Left=10.70\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2155/3393 [17:01<09:56,  2.08batch/s, Batch Loss=0.2388, Avg Loss=0.1155, Time Left=10.69\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2156/3393 [17:01<09:56,  2.07batch/s, Batch Loss=0.2388, Avg Loss=0.1155, Time Left=10.69\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2156/3393 [17:01<09:56,  2.07batch/s, Batch Loss=0.0892, Avg Loss=0.1155, Time Left=10.68\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2157/3393 [17:01<10:13,  2.01batch/s, Batch Loss=0.0892, Avg Loss=0.1155, Time Left=10.68\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2157/3393 [17:02<10:13,  2.01batch/s, Batch Loss=0.0537, Avg Loss=0.1154, Time Left=10.67\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2158/3393 [17:02<10:10,  2.02batch/s, Batch Loss=0.0537, Avg Loss=0.1154, Time Left=10.67\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2158/3393 [17:02<10:10,  2.02batch/s, Batch Loss=0.0349, Avg Loss=0.1154, Time Left=10.67\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2159/3393 [17:02<10:04,  2.04batch/s, Batch Loss=0.0349, Avg Loss=0.1154, Time Left=10.67\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2159/3393 [17:03<10:04,  2.04batch/s, Batch Loss=0.0364, Avg Loss=0.1154, Time Left=10.66\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2160/3393 [17:03<10:02,  2.05batch/s, Batch Loss=0.0364, Avg Loss=0.1154, Time Left=10.66\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2160/3393 [17:03<10:02,  2.05batch/s, Batch Loss=0.0850, Avg Loss=0.1153, Time Left=10.65\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2161/3393 [17:03<09:53,  2.07batch/s, Batch Loss=0.0850, Avg Loss=0.1153, Time Left=10.65\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2161/3393 [17:04<09:53,  2.07batch/s, Batch Loss=0.1758, Avg Loss=0.1154, Time Left=10.64\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2162/3393 [17:04<10:12,  2.01batch/s, Batch Loss=0.1758, Avg Loss=0.1154, Time Left=10.64\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2162/3393 [17:04<10:12,  2.01batch/s, Batch Loss=0.0176, Avg Loss=0.1153, Time Left=10.63\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2163/3393 [17:04<10:07,  2.03batch/s, Batch Loss=0.0176, Avg Loss=0.1153, Time Left=10.63\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2163/3393 [17:05<10:07,  2.03batch/s, Batch Loss=0.1420, Avg Loss=0.1153, Time Left=10.63\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2164/3393 [17:05<10:12,  2.01batch/s, Batch Loss=0.1420, Avg Loss=0.1153, Time Left=10.63\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2164/3393 [17:05<10:12,  2.01batch/s, Batch Loss=0.0655, Avg Loss=0.1153, Time Left=10.62\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2165/3393 [17:05<09:51,  2.08batch/s, Batch Loss=0.0655, Avg Loss=0.1153, Time Left=10.62\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2165/3393 [17:06<09:51,  2.08batch/s, Batch Loss=0.0657, Avg Loss=0.1153, Time Left=10.61\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2166/3393 [17:06<09:55,  2.06batch/s, Batch Loss=0.0657, Avg Loss=0.1153, Time Left=10.61\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2166/3393 [17:06<09:55,  2.06batch/s, Batch Loss=0.0167, Avg Loss=0.1152, Time Left=10.60\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2167/3393 [17:06<09:55,  2.06batch/s, Batch Loss=0.0167, Avg Loss=0.1152, Time Left=10.60\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2167/3393 [17:06<09:55,  2.06batch/s, Batch Loss=0.0537, Avg Loss=0.1152, Time Left=10.59\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2168/3393 [17:06<09:43,  2.10batch/s, Batch Loss=0.0537, Avg Loss=0.1152, Time Left=10.59\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2168/3393 [17:07<09:43,  2.10batch/s, Batch Loss=0.0801, Avg Loss=0.1152, Time Left=10.58\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2169/3393 [17:07<09:45,  2.09batch/s, Batch Loss=0.0801, Avg Loss=0.1152, Time Left=10.58\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2169/3393 [17:07<09:45,  2.09batch/s, Batch Loss=0.0543, Avg Loss=0.1152, Time Left=10.58\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2170/3393 [17:07<09:46,  2.08batch/s, Batch Loss=0.0543, Avg Loss=0.1152, Time Left=10.58\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2170/3393 [17:08<09:46,  2.08batch/s, Batch Loss=0.1452, Avg Loss=0.1152, Time Left=10.57\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2171/3393 [17:08<09:41,  2.10batch/s, Batch Loss=0.1452, Avg Loss=0.1152, Time Left=10.57\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2171/3393 [17:08<09:41,  2.10batch/s, Batch Loss=0.0533, Avg Loss=0.1152, Time Left=10.56\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2172/3393 [17:08<09:43,  2.09batch/s, Batch Loss=0.0533, Avg Loss=0.1152, Time Left=10.56\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2172/3393 [17:09<09:43,  2.09batch/s, Batch Loss=0.0596, Avg Loss=0.1151, Time Left=10.55\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2173/3393 [17:09<10:02,  2.03batch/s, Batch Loss=0.0596, Avg Loss=0.1151, Time Left=10.55\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2173/3393 [17:09<10:02,  2.03batch/s, Batch Loss=0.0151, Avg Loss=0.1151, Time Left=10.54\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2174/3393 [17:09<09:59,  2.03batch/s, Batch Loss=0.0151, Avg Loss=0.1151, Time Left=10.54\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2174/3393 [17:10<09:59,  2.03batch/s, Batch Loss=0.0073, Avg Loss=0.1150, Time Left=10.53\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2175/3393 [17:10<10:01,  2.03batch/s, Batch Loss=0.0073, Avg Loss=0.1150, Time Left=10.53\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2175/3393 [17:10<10:01,  2.03batch/s, Batch Loss=0.1821, Avg Loss=0.1151, Time Left=10.53\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  64%|▋| 2176/3393 [17:10<09:51,  2.06batch/s, Batch Loss=0.1821, Avg Loss=0.1151, Time Left=10.53\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2176/3393 [17:11<09:51,  2.06batch/s, Batch Loss=0.0238, Avg Loss=0.1150, Time Left=10.52\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2177/3393 [17:11<09:50,  2.06batch/s, Batch Loss=0.0238, Avg Loss=0.1150, Time Left=10.52\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2177/3393 [17:11<09:50,  2.06batch/s, Batch Loss=0.3232, Avg Loss=0.1151, Time Left=10.51\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2178/3393 [17:11<10:00,  2.02batch/s, Batch Loss=0.3232, Avg Loss=0.1151, Time Left=10.51\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2178/3393 [17:12<10:00,  2.02batch/s, Batch Loss=0.0093, Avg Loss=0.1151, Time Left=10.50\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2179/3393 [17:12<10:03,  2.01batch/s, Batch Loss=0.0093, Avg Loss=0.1151, Time Left=10.50\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2179/3393 [17:12<10:03,  2.01batch/s, Batch Loss=0.0473, Avg Loss=0.1150, Time Left=10.49\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2180/3393 [17:12<09:57,  2.03batch/s, Batch Loss=0.0473, Avg Loss=0.1150, Time Left=10.49\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2180/3393 [17:13<09:57,  2.03batch/s, Batch Loss=0.4609, Avg Loss=0.1152, Time Left=10.49\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2181/3393 [17:13<09:48,  2.06batch/s, Batch Loss=0.4609, Avg Loss=0.1152, Time Left=10.49\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2181/3393 [17:13<09:48,  2.06batch/s, Batch Loss=0.3225, Avg Loss=0.1153, Time Left=10.48\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2182/3393 [17:13<09:47,  2.06batch/s, Batch Loss=0.3225, Avg Loss=0.1153, Time Left=10.48\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2182/3393 [17:14<09:47,  2.06batch/s, Batch Loss=0.1242, Avg Loss=0.1153, Time Left=10.47\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2183/3393 [17:14<10:08,  1.99batch/s, Batch Loss=0.1242, Avg Loss=0.1153, Time Left=10.47\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2183/3393 [17:14<10:08,  1.99batch/s, Batch Loss=0.0239, Avg Loss=0.1153, Time Left=10.46\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2184/3393 [17:14<09:56,  2.03batch/s, Batch Loss=0.0239, Avg Loss=0.1153, Time Left=10.46\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2184/3393 [17:15<09:56,  2.03batch/s, Batch Loss=0.0941, Avg Loss=0.1152, Time Left=10.45\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2185/3393 [17:15<10:09,  1.98batch/s, Batch Loss=0.0941, Avg Loss=0.1152, Time Left=10.45\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2185/3393 [17:15<10:09,  1.98batch/s, Batch Loss=0.1857, Avg Loss=0.1153, Time Left=10.44\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2186/3393 [17:15<10:02,  2.00batch/s, Batch Loss=0.1857, Avg Loss=0.1153, Time Left=10.44\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2186/3393 [17:16<10:02,  2.00batch/s, Batch Loss=0.1045, Avg Loss=0.1153, Time Left=10.44\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2187/3393 [17:16<10:07,  1.98batch/s, Batch Loss=0.1045, Avg Loss=0.1153, Time Left=10.44\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2187/3393 [17:16<10:07,  1.98batch/s, Batch Loss=0.0410, Avg Loss=0.1152, Time Left=10.43\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2188/3393 [17:16<10:00,  2.01batch/s, Batch Loss=0.0410, Avg Loss=0.1152, Time Left=10.43\u001b[A\n",
      "Epoch 2/3 - Training:  64%|▋| 2188/3393 [17:17<10:00,  2.01batch/s, Batch Loss=0.0881, Avg Loss=0.1152, Time Left=10.42\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2189/3393 [17:17<10:06,  1.99batch/s, Batch Loss=0.0881, Avg Loss=0.1152, Time Left=10.42\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2189/3393 [17:17<10:06,  1.99batch/s, Batch Loss=0.2056, Avg Loss=0.1153, Time Left=10.41\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2190/3393 [17:17<09:53,  2.03batch/s, Batch Loss=0.2056, Avg Loss=0.1153, Time Left=10.41\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2190/3393 [17:18<09:53,  2.03batch/s, Batch Loss=0.0603, Avg Loss=0.1152, Time Left=10.40\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2191/3393 [17:18<09:49,  2.04batch/s, Batch Loss=0.0603, Avg Loss=0.1152, Time Left=10.40\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2191/3393 [17:18<09:49,  2.04batch/s, Batch Loss=0.0767, Avg Loss=0.1152, Time Left=10.40\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2192/3393 [17:18<09:46,  2.05batch/s, Batch Loss=0.0767, Avg Loss=0.1152, Time Left=10.40\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2192/3393 [17:19<09:46,  2.05batch/s, Batch Loss=0.3146, Avg Loss=0.1153, Time Left=10.39\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2193/3393 [17:19<09:41,  2.06batch/s, Batch Loss=0.3146, Avg Loss=0.1153, Time Left=10.39\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2193/3393 [17:19<09:41,  2.06batch/s, Batch Loss=0.2098, Avg Loss=0.1154, Time Left=10.38\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2194/3393 [17:19<09:32,  2.09batch/s, Batch Loss=0.2098, Avg Loss=0.1154, Time Left=10.38\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2194/3393 [17:20<09:32,  2.09batch/s, Batch Loss=0.1179, Avg Loss=0.1154, Time Left=10.37\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2195/3393 [17:20<09:39,  2.07batch/s, Batch Loss=0.1179, Avg Loss=0.1154, Time Left=10.37\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2195/3393 [17:20<09:39,  2.07batch/s, Batch Loss=0.0765, Avg Loss=0.1153, Time Left=10.36\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2196/3393 [17:20<09:27,  2.11batch/s, Batch Loss=0.0765, Avg Loss=0.1153, Time Left=10.36\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2196/3393 [17:21<09:27,  2.11batch/s, Batch Loss=0.1117, Avg Loss=0.1153, Time Left=10.35\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2197/3393 [17:21<09:35,  2.08batch/s, Batch Loss=0.1117, Avg Loss=0.1153, Time Left=10.35\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2197/3393 [17:21<09:35,  2.08batch/s, Batch Loss=0.0757, Avg Loss=0.1153, Time Left=10.35\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2198/3393 [17:21<09:36,  2.07batch/s, Batch Loss=0.0757, Avg Loss=0.1153, Time Left=10.35\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2198/3393 [17:22<09:36,  2.07batch/s, Batch Loss=0.2504, Avg Loss=0.1154, Time Left=10.34\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2199/3393 [17:22<09:30,  2.09batch/s, Batch Loss=0.2504, Avg Loss=0.1154, Time Left=10.34\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2199/3393 [17:22<09:30,  2.09batch/s, Batch Loss=0.0063, Avg Loss=0.1153, Time Left=10.33\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2200/3393 [17:22<09:26,  2.11batch/s, Batch Loss=0.0063, Avg Loss=0.1153, Time Left=10.33\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2200/3393 [17:23<09:26,  2.11batch/s, Batch Loss=0.0412, Avg Loss=0.1153, Time Left=10.32\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2201/3393 [17:23<09:39,  2.06batch/s, Batch Loss=0.0412, Avg Loss=0.1153, Time Left=10.32\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2201/3393 [17:23<09:39,  2.06batch/s, Batch Loss=0.0279, Avg Loss=0.1153, Time Left=10.31\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2202/3393 [17:23<09:34,  2.07batch/s, Batch Loss=0.0279, Avg Loss=0.1153, Time Left=10.31\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2202/3393 [17:24<09:34,  2.07batch/s, Batch Loss=0.0263, Avg Loss=0.1152, Time Left=10.31\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2203/3393 [17:24<09:28,  2.09batch/s, Batch Loss=0.0263, Avg Loss=0.1152, Time Left=10.31\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2203/3393 [17:24<09:28,  2.09batch/s, Batch Loss=0.1239, Avg Loss=0.1152, Time Left=10.30\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2204/3393 [17:24<09:40,  2.05batch/s, Batch Loss=0.1239, Avg Loss=0.1152, Time Left=10.30\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2204/3393 [17:25<09:40,  2.05batch/s, Batch Loss=0.0500, Avg Loss=0.1152, Time Left=10.29\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2205/3393 [17:25<09:39,  2.05batch/s, Batch Loss=0.0500, Avg Loss=0.1152, Time Left=10.29\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2205/3393 [17:25<09:39,  2.05batch/s, Batch Loss=0.1107, Avg Loss=0.1152, Time Left=10.28\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2206/3393 [17:25<09:48,  2.02batch/s, Batch Loss=0.1107, Avg Loss=0.1152, Time Left=10.28\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2206/3393 [17:26<09:48,  2.02batch/s, Batch Loss=0.0750, Avg Loss=0.1152, Time Left=10.27\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2207/3393 [17:26<09:44,  2.03batch/s, Batch Loss=0.0750, Avg Loss=0.1152, Time Left=10.27\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2207/3393 [17:26<09:44,  2.03batch/s, Batch Loss=0.1195, Avg Loss=0.1152, Time Left=10.26\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2208/3393 [17:26<09:40,  2.04batch/s, Batch Loss=0.1195, Avg Loss=0.1152, Time Left=10.26\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2208/3393 [17:27<09:40,  2.04batch/s, Batch Loss=0.1757, Avg Loss=0.1152, Time Left=10.26\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  65%|▋| 2209/3393 [17:27<09:35,  2.06batch/s, Batch Loss=0.1757, Avg Loss=0.1152, Time Left=10.26\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2209/3393 [17:27<09:35,  2.06batch/s, Batch Loss=0.2412, Avg Loss=0.1153, Time Left=10.25\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2210/3393 [17:27<09:30,  2.07batch/s, Batch Loss=0.2412, Avg Loss=0.1153, Time Left=10.25\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2210/3393 [17:27<09:30,  2.07batch/s, Batch Loss=0.0067, Avg Loss=0.1152, Time Left=10.24\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2211/3393 [17:27<09:30,  2.07batch/s, Batch Loss=0.0067, Avg Loss=0.1152, Time Left=10.24\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2211/3393 [17:28<09:30,  2.07batch/s, Batch Loss=0.0961, Avg Loss=0.1152, Time Left=10.23\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2212/3393 [17:28<09:23,  2.09batch/s, Batch Loss=0.0961, Avg Loss=0.1152, Time Left=10.23\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2212/3393 [17:28<09:23,  2.09batch/s, Batch Loss=0.0625, Avg Loss=0.1152, Time Left=10.22\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2213/3393 [17:28<09:26,  2.08batch/s, Batch Loss=0.0625, Avg Loss=0.1152, Time Left=10.22\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2213/3393 [17:29<09:26,  2.08batch/s, Batch Loss=0.0131, Avg Loss=0.1151, Time Left=10.22\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2214/3393 [17:29<09:32,  2.06batch/s, Batch Loss=0.0131, Avg Loss=0.1151, Time Left=10.22\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2214/3393 [17:29<09:32,  2.06batch/s, Batch Loss=0.1116, Avg Loss=0.1151, Time Left=10.21\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2215/3393 [17:29<09:32,  2.06batch/s, Batch Loss=0.1116, Avg Loss=0.1151, Time Left=10.21\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2215/3393 [17:30<09:32,  2.06batch/s, Batch Loss=0.0590, Avg Loss=0.1151, Time Left=10.20\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2216/3393 [17:30<09:25,  2.08batch/s, Batch Loss=0.0590, Avg Loss=0.1151, Time Left=10.20\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2216/3393 [17:30<09:25,  2.08batch/s, Batch Loss=0.0990, Avg Loss=0.1151, Time Left=10.19\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2217/3393 [17:30<09:42,  2.02batch/s, Batch Loss=0.0990, Avg Loss=0.1151, Time Left=10.19\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2217/3393 [17:31<09:42,  2.02batch/s, Batch Loss=0.0458, Avg Loss=0.1151, Time Left=10.18\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2218/3393 [17:31<09:38,  2.03batch/s, Batch Loss=0.0458, Avg Loss=0.1151, Time Left=10.18\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2218/3393 [17:31<09:38,  2.03batch/s, Batch Loss=0.0293, Avg Loss=0.1150, Time Left=10.17\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2219/3393 [17:31<09:51,  1.98batch/s, Batch Loss=0.0293, Avg Loss=0.1150, Time Left=10.17\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2219/3393 [17:32<09:51,  1.98batch/s, Batch Loss=0.0914, Avg Loss=0.1150, Time Left=10.17\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2220/3393 [17:32<09:44,  2.01batch/s, Batch Loss=0.0914, Avg Loss=0.1150, Time Left=10.17\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2220/3393 [17:32<09:44,  2.01batch/s, Batch Loss=0.0245, Avg Loss=0.1150, Time Left=10.16\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2221/3393 [17:32<09:44,  2.00batch/s, Batch Loss=0.0245, Avg Loss=0.1150, Time Left=10.16\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2221/3393 [17:33<09:44,  2.00batch/s, Batch Loss=0.0349, Avg Loss=0.1149, Time Left=10.15\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2222/3393 [17:33<09:39,  2.02batch/s, Batch Loss=0.0349, Avg Loss=0.1149, Time Left=10.15\u001b[A\n",
      "Epoch 2/3 - Training:  65%|▋| 2222/3393 [17:33<09:39,  2.02batch/s, Batch Loss=0.0829, Avg Loss=0.1149, Time Left=10.14\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2223/3393 [17:33<09:40,  2.02batch/s, Batch Loss=0.0829, Avg Loss=0.1149, Time Left=10.14\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2223/3393 [17:34<09:40,  2.02batch/s, Batch Loss=0.0248, Avg Loss=0.1149, Time Left=10.13\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2224/3393 [17:34<09:46,  1.99batch/s, Batch Loss=0.0248, Avg Loss=0.1149, Time Left=10.13\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2224/3393 [17:34<09:46,  1.99batch/s, Batch Loss=0.1291, Avg Loss=0.1149, Time Left=10.13\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2225/3393 [17:34<09:40,  2.01batch/s, Batch Loss=0.1291, Avg Loss=0.1149, Time Left=10.13\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2225/3393 [17:35<09:40,  2.01batch/s, Batch Loss=0.1467, Avg Loss=0.1149, Time Left=10.12\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2226/3393 [17:35<09:41,  2.01batch/s, Batch Loss=0.1467, Avg Loss=0.1149, Time Left=10.12\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2226/3393 [17:35<09:41,  2.01batch/s, Batch Loss=0.1891, Avg Loss=0.1149, Time Left=10.11\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2227/3393 [17:35<09:36,  2.02batch/s, Batch Loss=0.1891, Avg Loss=0.1149, Time Left=10.11\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2227/3393 [17:36<09:36,  2.02batch/s, Batch Loss=0.0886, Avg Loss=0.1149, Time Left=10.10\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2228/3393 [17:36<09:31,  2.04batch/s, Batch Loss=0.0886, Avg Loss=0.1149, Time Left=10.10\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2228/3393 [17:36<09:31,  2.04batch/s, Batch Loss=0.0617, Avg Loss=0.1149, Time Left=10.09\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2229/3393 [17:36<09:29,  2.04batch/s, Batch Loss=0.0617, Avg Loss=0.1149, Time Left=10.09\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2229/3393 [17:37<09:29,  2.04batch/s, Batch Loss=0.0085, Avg Loss=0.1148, Time Left=10.08\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2230/3393 [17:37<09:20,  2.07batch/s, Batch Loss=0.0085, Avg Loss=0.1148, Time Left=10.08\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2230/3393 [17:37<09:20,  2.07batch/s, Batch Loss=0.0325, Avg Loss=0.1148, Time Left=10.08\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2231/3393 [17:37<09:31,  2.03batch/s, Batch Loss=0.0325, Avg Loss=0.1148, Time Left=10.08\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2231/3393 [17:38<09:31,  2.03batch/s, Batch Loss=0.0897, Avg Loss=0.1148, Time Left=10.07\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2232/3393 [17:38<09:28,  2.04batch/s, Batch Loss=0.0897, Avg Loss=0.1148, Time Left=10.07\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2232/3393 [17:38<09:28,  2.04batch/s, Batch Loss=0.3672, Avg Loss=0.1149, Time Left=10.06\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2233/3393 [17:38<09:25,  2.05batch/s, Batch Loss=0.3672, Avg Loss=0.1149, Time Left=10.06\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2233/3393 [17:39<09:25,  2.05batch/s, Batch Loss=0.0039, Avg Loss=0.1149, Time Left=10.05\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2234/3393 [17:39<09:24,  2.05batch/s, Batch Loss=0.0039, Avg Loss=0.1149, Time Left=10.05\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2234/3393 [17:39<09:24,  2.05batch/s, Batch Loss=0.0059, Avg Loss=0.1148, Time Left=10.04\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2235/3393 [17:39<09:11,  2.10batch/s, Batch Loss=0.0059, Avg Loss=0.1148, Time Left=10.04\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2235/3393 [17:40<09:11,  2.10batch/s, Batch Loss=0.0351, Avg Loss=0.1148, Time Left=10.04\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2236/3393 [17:40<09:17,  2.08batch/s, Batch Loss=0.0351, Avg Loss=0.1148, Time Left=10.04\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2236/3393 [17:40<09:17,  2.08batch/s, Batch Loss=0.0147, Avg Loss=0.1147, Time Left=10.03\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2237/3393 [17:40<09:19,  2.07batch/s, Batch Loss=0.0147, Avg Loss=0.1147, Time Left=10.03\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2237/3393 [17:41<09:19,  2.07batch/s, Batch Loss=0.0455, Avg Loss=0.1147, Time Left=10.02\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2238/3393 [17:41<09:18,  2.07batch/s, Batch Loss=0.0455, Avg Loss=0.1147, Time Left=10.02\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2238/3393 [17:41<09:18,  2.07batch/s, Batch Loss=0.2473, Avg Loss=0.1148, Time Left=10.01\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2239/3393 [17:41<09:17,  2.07batch/s, Batch Loss=0.2473, Avg Loss=0.1148, Time Left=10.01\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2239/3393 [17:42<09:17,  2.07batch/s, Batch Loss=0.2306, Avg Loss=0.1148, Time Left=10.00\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2240/3393 [17:42<09:17,  2.07batch/s, Batch Loss=0.2306, Avg Loss=0.1148, Time Left=10.00\u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2240/3393 [17:42<09:17,  2.07batch/s, Batch Loss=0.0226, Avg Loss=0.1148, Time Left=9.99 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2241/3393 [17:42<09:22,  2.05batch/s, Batch Loss=0.0226, Avg Loss=0.1148, Time Left=9.99 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2241/3393 [17:43<09:22,  2.05batch/s, Batch Loss=0.0212, Avg Loss=0.1147, Time Left=9.99 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  66%|▋| 2242/3393 [17:43<09:25,  2.03batch/s, Batch Loss=0.0212, Avg Loss=0.1147, Time Left=9.99 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2242/3393 [17:43<09:25,  2.03batch/s, Batch Loss=0.3026, Avg Loss=0.1148, Time Left=9.98 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2243/3393 [17:43<09:17,  2.06batch/s, Batch Loss=0.3026, Avg Loss=0.1148, Time Left=9.98 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2243/3393 [17:44<09:17,  2.06batch/s, Batch Loss=0.2175, Avg Loss=0.1149, Time Left=9.97 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2244/3393 [17:44<09:11,  2.08batch/s, Batch Loss=0.2175, Avg Loss=0.1149, Time Left=9.97 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2244/3393 [17:44<09:11,  2.08batch/s, Batch Loss=0.0183, Avg Loss=0.1148, Time Left=9.96 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2245/3393 [17:44<09:17,  2.06batch/s, Batch Loss=0.0183, Avg Loss=0.1148, Time Left=9.96 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2245/3393 [17:45<09:17,  2.06batch/s, Batch Loss=0.0095, Avg Loss=0.1148, Time Left=9.95 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2246/3393 [17:45<09:10,  2.08batch/s, Batch Loss=0.0095, Avg Loss=0.1148, Time Left=9.95 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2246/3393 [17:45<09:10,  2.08batch/s, Batch Loss=0.0451, Avg Loss=0.1147, Time Left=9.94 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2247/3393 [17:45<09:00,  2.12batch/s, Batch Loss=0.0451, Avg Loss=0.1147, Time Left=9.94 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2247/3393 [17:46<09:00,  2.12batch/s, Batch Loss=0.0079, Avg Loss=0.1147, Time Left=9.94 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2248/3393 [17:46<09:03,  2.11batch/s, Batch Loss=0.0079, Avg Loss=0.1147, Time Left=9.94 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2248/3393 [17:46<09:03,  2.11batch/s, Batch Loss=0.2243, Avg Loss=0.1147, Time Left=9.93 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2249/3393 [17:46<09:01,  2.11batch/s, Batch Loss=0.2243, Avg Loss=0.1147, Time Left=9.93 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2249/3393 [17:46<09:01,  2.11batch/s, Batch Loss=0.1899, Avg Loss=0.1148, Time Left=9.92 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2250/3393 [17:46<08:58,  2.12batch/s, Batch Loss=0.1899, Avg Loss=0.1148, Time Left=9.92 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2250/3393 [17:47<08:58,  2.12batch/s, Batch Loss=0.2614, Avg Loss=0.1148, Time Left=9.91 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2251/3393 [17:47<09:12,  2.07batch/s, Batch Loss=0.2614, Avg Loss=0.1148, Time Left=9.91 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2251/3393 [17:47<09:12,  2.07batch/s, Batch Loss=0.0085, Avg Loss=0.1148, Time Left=9.90 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2252/3393 [17:47<09:12,  2.06batch/s, Batch Loss=0.0085, Avg Loss=0.1148, Time Left=9.90 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2252/3393 [17:48<09:12,  2.06batch/s, Batch Loss=0.1323, Avg Loss=0.1148, Time Left=9.90 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2253/3393 [17:48<09:14,  2.05batch/s, Batch Loss=0.1323, Avg Loss=0.1148, Time Left=9.90 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2253/3393 [17:48<09:14,  2.05batch/s, Batch Loss=0.0133, Avg Loss=0.1148, Time Left=9.89 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2254/3393 [17:48<09:21,  2.03batch/s, Batch Loss=0.0133, Avg Loss=0.1148, Time Left=9.89 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2254/3393 [17:49<09:21,  2.03batch/s, Batch Loss=0.1816, Avg Loss=0.1148, Time Left=9.88 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2255/3393 [17:49<09:23,  2.02batch/s, Batch Loss=0.1816, Avg Loss=0.1148, Time Left=9.88 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2255/3393 [17:49<09:23,  2.02batch/s, Batch Loss=0.1785, Avg Loss=0.1148, Time Left=9.87 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2256/3393 [17:49<09:24,  2.01batch/s, Batch Loss=0.1785, Avg Loss=0.1148, Time Left=9.87 \u001b[A\n",
      "Epoch 2/3 - Training:  66%|▋| 2256/3393 [17:50<09:24,  2.01batch/s, Batch Loss=0.0999, Avg Loss=0.1148, Time Left=9.86 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2257/3393 [17:50<09:15,  2.05batch/s, Batch Loss=0.0999, Avg Loss=0.1148, Time Left=9.86 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2257/3393 [17:50<09:15,  2.05batch/s, Batch Loss=0.0133, Avg Loss=0.1148, Time Left=9.85 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2258/3393 [17:50<09:12,  2.05batch/s, Batch Loss=0.0133, Avg Loss=0.1148, Time Left=9.85 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2258/3393 [17:51<09:12,  2.05batch/s, Batch Loss=0.0798, Avg Loss=0.1147, Time Left=9.85 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2259/3393 [17:51<09:12,  2.05batch/s, Batch Loss=0.0798, Avg Loss=0.1147, Time Left=9.85 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2259/3393 [17:51<09:12,  2.05batch/s, Batch Loss=0.0303, Avg Loss=0.1147, Time Left=9.84 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2260/3393 [17:51<09:07,  2.07batch/s, Batch Loss=0.0303, Avg Loss=0.1147, Time Left=9.84 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2260/3393 [17:52<09:07,  2.07batch/s, Batch Loss=0.0509, Avg Loss=0.1147, Time Left=9.83 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2261/3393 [17:52<09:09,  2.06batch/s, Batch Loss=0.0509, Avg Loss=0.1147, Time Left=9.83 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2261/3393 [17:52<09:09,  2.06batch/s, Batch Loss=0.1660, Avg Loss=0.1147, Time Left=9.82 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2262/3393 [17:52<09:02,  2.09batch/s, Batch Loss=0.1660, Avg Loss=0.1147, Time Left=9.82 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2262/3393 [17:53<09:02,  2.09batch/s, Batch Loss=0.0955, Avg Loss=0.1147, Time Left=9.81 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2263/3393 [17:53<09:03,  2.08batch/s, Batch Loss=0.0955, Avg Loss=0.1147, Time Left=9.81 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2263/3393 [17:53<09:03,  2.08batch/s, Batch Loss=0.0236, Avg Loss=0.1146, Time Left=9.81 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2264/3393 [17:53<09:24,  2.00batch/s, Batch Loss=0.0236, Avg Loss=0.1146, Time Left=9.81 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2264/3393 [17:54<09:24,  2.00batch/s, Batch Loss=0.0455, Avg Loss=0.1146, Time Left=9.80 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2265/3393 [17:54<09:21,  2.01batch/s, Batch Loss=0.0455, Avg Loss=0.1146, Time Left=9.80 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2265/3393 [17:54<09:21,  2.01batch/s, Batch Loss=0.1316, Avg Loss=0.1146, Time Left=9.79 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2266/3393 [17:54<09:31,  1.97batch/s, Batch Loss=0.1316, Avg Loss=0.1146, Time Left=9.79 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2266/3393 [17:55<09:31,  1.97batch/s, Batch Loss=0.0874, Avg Loss=0.1146, Time Left=9.78 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2267/3393 [17:55<09:23,  2.00batch/s, Batch Loss=0.0874, Avg Loss=0.1146, Time Left=9.78 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2267/3393 [17:55<09:23,  2.00batch/s, Batch Loss=0.0087, Avg Loss=0.1146, Time Left=9.77 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2268/3393 [17:55<09:12,  2.04batch/s, Batch Loss=0.0087, Avg Loss=0.1146, Time Left=9.77 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2268/3393 [17:56<09:12,  2.04batch/s, Batch Loss=0.0223, Avg Loss=0.1145, Time Left=9.77 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2269/3393 [17:56<09:10,  2.04batch/s, Batch Loss=0.0223, Avg Loss=0.1145, Time Left=9.77 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2269/3393 [17:56<09:10,  2.04batch/s, Batch Loss=0.0223, Avg Loss=0.1145, Time Left=9.76 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2270/3393 [17:56<09:06,  2.06batch/s, Batch Loss=0.0223, Avg Loss=0.1145, Time Left=9.76 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2270/3393 [17:57<09:06,  2.06batch/s, Batch Loss=0.2003, Avg Loss=0.1145, Time Left=9.75 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2271/3393 [17:57<09:00,  2.08batch/s, Batch Loss=0.2003, Avg Loss=0.1145, Time Left=9.75 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2271/3393 [17:57<09:00,  2.08batch/s, Batch Loss=0.0224, Avg Loss=0.1145, Time Left=9.74 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2272/3393 [17:57<09:00,  2.08batch/s, Batch Loss=0.0224, Avg Loss=0.1145, Time Left=9.74 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2272/3393 [17:58<09:00,  2.08batch/s, Batch Loss=0.1183, Avg Loss=0.1145, Time Left=9.73 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2273/3393 [17:58<08:54,  2.10batch/s, Batch Loss=0.1183, Avg Loss=0.1145, Time Left=9.73 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2273/3393 [17:58<08:54,  2.10batch/s, Batch Loss=0.0008, Avg Loss=0.1144, Time Left=9.72 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2274/3393 [17:58<08:56,  2.09batch/s, Batch Loss=0.0008, Avg Loss=0.1144, Time Left=9.72 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2274/3393 [17:59<08:56,  2.09batch/s, Batch Loss=0.0797, Avg Loss=0.1144, Time Left=9.72 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  67%|▋| 2275/3393 [17:59<08:57,  2.08batch/s, Batch Loss=0.0797, Avg Loss=0.1144, Time Left=9.72 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2275/3393 [17:59<08:57,  2.08batch/s, Batch Loss=0.0059, Avg Loss=0.1144, Time Left=9.71 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2276/3393 [17:59<08:52,  2.10batch/s, Batch Loss=0.0059, Avg Loss=0.1144, Time Left=9.71 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2276/3393 [18:00<08:52,  2.10batch/s, Batch Loss=0.1339, Avg Loss=0.1144, Time Left=9.70 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2277/3393 [18:00<08:53,  2.09batch/s, Batch Loss=0.1339, Avg Loss=0.1144, Time Left=9.70 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2277/3393 [18:00<08:53,  2.09batch/s, Batch Loss=0.0013, Avg Loss=0.1143, Time Left=9.69 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2278/3393 [18:00<08:55,  2.08batch/s, Batch Loss=0.0013, Avg Loss=0.1143, Time Left=9.69 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2278/3393 [18:01<08:55,  2.08batch/s, Batch Loss=0.3613, Avg Loss=0.1144, Time Left=9.68 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2279/3393 [18:01<08:50,  2.10batch/s, Batch Loss=0.3613, Avg Loss=0.1144, Time Left=9.68 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2279/3393 [18:01<08:50,  2.10batch/s, Batch Loss=0.1833, Avg Loss=0.1145, Time Left=9.67 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2280/3393 [18:01<08:46,  2.11batch/s, Batch Loss=0.1833, Avg Loss=0.1145, Time Left=9.67 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2280/3393 [18:02<08:46,  2.11batch/s, Batch Loss=0.1761, Avg Loss=0.1145, Time Left=9.67 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2281/3393 [18:02<08:54,  2.08batch/s, Batch Loss=0.1761, Avg Loss=0.1145, Time Left=9.67 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2281/3393 [18:02<08:54,  2.08batch/s, Batch Loss=0.0021, Avg Loss=0.1144, Time Left=9.66 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2282/3393 [18:02<09:01,  2.05batch/s, Batch Loss=0.0021, Avg Loss=0.1144, Time Left=9.66 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2282/3393 [18:03<09:01,  2.05batch/s, Batch Loss=0.1691, Avg Loss=0.1145, Time Left=9.65 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2283/3393 [18:03<09:04,  2.04batch/s, Batch Loss=0.1691, Avg Loss=0.1145, Time Left=9.65 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2283/3393 [18:03<09:04,  2.04batch/s, Batch Loss=0.0953, Avg Loss=0.1145, Time Left=9.64 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2284/3393 [18:03<09:07,  2.03batch/s, Batch Loss=0.0953, Avg Loss=0.1145, Time Left=9.64 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2284/3393 [18:04<09:07,  2.03batch/s, Batch Loss=0.1205, Avg Loss=0.1145, Time Left=9.63 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2285/3393 [18:04<09:03,  2.04batch/s, Batch Loss=0.1205, Avg Loss=0.1145, Time Left=9.63 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2285/3393 [18:04<09:03,  2.04batch/s, Batch Loss=0.0313, Avg Loss=0.1144, Time Left=9.63 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2286/3393 [18:04<08:55,  2.07batch/s, Batch Loss=0.0313, Avg Loss=0.1144, Time Left=9.63 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2286/3393 [18:04<08:55,  2.07batch/s, Batch Loss=0.0057, Avg Loss=0.1144, Time Left=9.62 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2287/3393 [18:04<08:49,  2.09batch/s, Batch Loss=0.0057, Avg Loss=0.1144, Time Left=9.62 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2287/3393 [18:05<08:49,  2.09batch/s, Batch Loss=0.2901, Avg Loss=0.1145, Time Left=9.61 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2288/3393 [18:05<08:52,  2.08batch/s, Batch Loss=0.2901, Avg Loss=0.1145, Time Left=9.61 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2288/3393 [18:05<08:52,  2.08batch/s, Batch Loss=0.0673, Avg Loss=0.1144, Time Left=9.60 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2289/3393 [18:05<08:55,  2.06batch/s, Batch Loss=0.0673, Avg Loss=0.1144, Time Left=9.60 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2289/3393 [18:06<08:55,  2.06batch/s, Batch Loss=0.0793, Avg Loss=0.1144, Time Left=9.59 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2290/3393 [18:06<08:55,  2.06batch/s, Batch Loss=0.0793, Avg Loss=0.1144, Time Left=9.59 \u001b[A\n",
      "Epoch 2/3 - Training:  67%|▋| 2290/3393 [18:06<08:55,  2.06batch/s, Batch Loss=0.0218, Avg Loss=0.1144, Time Left=9.58 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2291/3393 [18:06<09:04,  2.02batch/s, Batch Loss=0.0218, Avg Loss=0.1144, Time Left=9.58 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2291/3393 [18:07<09:04,  2.02batch/s, Batch Loss=0.1284, Avg Loss=0.1144, Time Left=9.58 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2292/3393 [18:07<09:06,  2.01batch/s, Batch Loss=0.1284, Avg Loss=0.1144, Time Left=9.58 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2292/3393 [18:07<09:06,  2.01batch/s, Batch Loss=0.3274, Avg Loss=0.1145, Time Left=9.57 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2293/3393 [18:07<09:12,  1.99batch/s, Batch Loss=0.3274, Avg Loss=0.1145, Time Left=9.57 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2293/3393 [18:08<09:12,  1.99batch/s, Batch Loss=0.0484, Avg Loss=0.1144, Time Left=9.56 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2294/3393 [18:08<09:01,  2.03batch/s, Batch Loss=0.0484, Avg Loss=0.1144, Time Left=9.56 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2294/3393 [18:08<09:01,  2.03batch/s, Batch Loss=0.0280, Avg Loss=0.1144, Time Left=9.55 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2295/3393 [18:08<08:52,  2.06batch/s, Batch Loss=0.0280, Avg Loss=0.1144, Time Left=9.55 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2295/3393 [18:09<08:52,  2.06batch/s, Batch Loss=0.0657, Avg Loss=0.1144, Time Left=9.54 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2296/3393 [18:09<08:51,  2.06batch/s, Batch Loss=0.0657, Avg Loss=0.1144, Time Left=9.54 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2296/3393 [18:09<08:51,  2.06batch/s, Batch Loss=0.1544, Avg Loss=0.1144, Time Left=9.54 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2297/3393 [18:09<08:50,  2.06batch/s, Batch Loss=0.1544, Avg Loss=0.1144, Time Left=9.54 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2297/3393 [18:10<08:50,  2.06batch/s, Batch Loss=0.0789, Avg Loss=0.1144, Time Left=9.53 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2298/3393 [18:10<08:50,  2.07batch/s, Batch Loss=0.0789, Avg Loss=0.1144, Time Left=9.53 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2298/3393 [18:10<08:50,  2.07batch/s, Batch Loss=0.0507, Avg Loss=0.1144, Time Left=9.52 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2299/3393 [18:10<08:59,  2.03batch/s, Batch Loss=0.0507, Avg Loss=0.1144, Time Left=9.52 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2299/3393 [18:11<08:59,  2.03batch/s, Batch Loss=0.0205, Avg Loss=0.1143, Time Left=9.51 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2300/3393 [18:11<08:56,  2.04batch/s, Batch Loss=0.0205, Avg Loss=0.1143, Time Left=9.51 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2300/3393 [18:11<08:56,  2.04batch/s, Batch Loss=0.2632, Avg Loss=0.1144, Time Left=9.50 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2301/3393 [18:11<08:58,  2.03batch/s, Batch Loss=0.2632, Avg Loss=0.1144, Time Left=9.50 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2301/3393 [18:12<08:58,  2.03batch/s, Batch Loss=0.0570, Avg Loss=0.1144, Time Left=9.49 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2302/3393 [18:12<08:55,  2.04batch/s, Batch Loss=0.0570, Avg Loss=0.1144, Time Left=9.49 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2302/3393 [18:12<08:55,  2.04batch/s, Batch Loss=0.2631, Avg Loss=0.1144, Time Left=9.49 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2303/3393 [18:12<08:55,  2.04batch/s, Batch Loss=0.2631, Avg Loss=0.1144, Time Left=9.49 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2303/3393 [18:13<08:55,  2.04batch/s, Batch Loss=0.1664, Avg Loss=0.1144, Time Left=9.48 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2304/3393 [18:13<08:50,  2.05batch/s, Batch Loss=0.1664, Avg Loss=0.1144, Time Left=9.48 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2304/3393 [18:13<08:50,  2.05batch/s, Batch Loss=0.1984, Avg Loss=0.1145, Time Left=9.47 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2305/3393 [18:13<08:53,  2.04batch/s, Batch Loss=0.1984, Avg Loss=0.1145, Time Left=9.47 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2305/3393 [18:14<08:53,  2.04batch/s, Batch Loss=0.0961, Avg Loss=0.1145, Time Left=9.46 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2306/3393 [18:14<08:50,  2.05batch/s, Batch Loss=0.0961, Avg Loss=0.1145, Time Left=9.46 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2306/3393 [18:14<08:50,  2.05batch/s, Batch Loss=0.0481, Avg Loss=0.1144, Time Left=9.45 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2307/3393 [18:14<08:49,  2.05batch/s, Batch Loss=0.0481, Avg Loss=0.1144, Time Left=9.45 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2307/3393 [18:15<08:49,  2.05batch/s, Batch Loss=0.1023, Avg Loss=0.1144, Time Left=9.45 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  68%|▋| 2308/3393 [18:15<08:47,  2.06batch/s, Batch Loss=0.1023, Avg Loss=0.1144, Time Left=9.45 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2308/3393 [18:15<08:47,  2.06batch/s, Batch Loss=0.1791, Avg Loss=0.1145, Time Left=9.44 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2309/3393 [18:15<08:46,  2.06batch/s, Batch Loss=0.1791, Avg Loss=0.1145, Time Left=9.44 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2309/3393 [18:16<08:46,  2.06batch/s, Batch Loss=0.0679, Avg Loss=0.1144, Time Left=9.43 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2310/3393 [18:16<08:47,  2.05batch/s, Batch Loss=0.0679, Avg Loss=0.1144, Time Left=9.43 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2310/3393 [18:16<08:47,  2.05batch/s, Batch Loss=0.0063, Avg Loss=0.1144, Time Left=9.42 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2311/3393 [18:16<08:43,  2.07batch/s, Batch Loss=0.0063, Avg Loss=0.1144, Time Left=9.42 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2311/3393 [18:17<08:43,  2.07batch/s, Batch Loss=0.0102, Avg Loss=0.1144, Time Left=9.41 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2312/3393 [18:17<08:49,  2.04batch/s, Batch Loss=0.0102, Avg Loss=0.1144, Time Left=9.41 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2312/3393 [18:17<08:49,  2.04batch/s, Batch Loss=0.0230, Avg Loss=0.1143, Time Left=9.40 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2313/3393 [18:17<08:46,  2.05batch/s, Batch Loss=0.0230, Avg Loss=0.1143, Time Left=9.40 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2313/3393 [18:18<08:46,  2.05batch/s, Batch Loss=0.0319, Avg Loss=0.1143, Time Left=9.40 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2314/3393 [18:18<08:54,  2.02batch/s, Batch Loss=0.0319, Avg Loss=0.1143, Time Left=9.40 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2314/3393 [18:18<08:54,  2.02batch/s, Batch Loss=0.0375, Avg Loss=0.1142, Time Left=9.39 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2315/3393 [18:18<08:51,  2.03batch/s, Batch Loss=0.0375, Avg Loss=0.1142, Time Left=9.39 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2315/3393 [18:19<08:51,  2.03batch/s, Batch Loss=0.3139, Avg Loss=0.1143, Time Left=9.38 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2316/3393 [18:19<08:47,  2.04batch/s, Batch Loss=0.3139, Avg Loss=0.1143, Time Left=9.38 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2316/3393 [18:19<08:47,  2.04batch/s, Batch Loss=0.3744, Avg Loss=0.1144, Time Left=9.37 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2317/3393 [18:19<08:44,  2.05batch/s, Batch Loss=0.3744, Avg Loss=0.1144, Time Left=9.37 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2317/3393 [18:20<08:44,  2.05batch/s, Batch Loss=0.1595, Avg Loss=0.1145, Time Left=9.36 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2318/3393 [18:20<08:43,  2.06batch/s, Batch Loss=0.1595, Avg Loss=0.1145, Time Left=9.36 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2318/3393 [18:20<08:43,  2.06batch/s, Batch Loss=0.0410, Avg Loss=0.1144, Time Left=9.36 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2319/3393 [18:20<08:56,  2.00batch/s, Batch Loss=0.0410, Avg Loss=0.1144, Time Left=9.36 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2319/3393 [18:21<08:56,  2.00batch/s, Batch Loss=0.0607, Avg Loss=0.1144, Time Left=9.35 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2320/3393 [18:21<08:51,  2.02batch/s, Batch Loss=0.0607, Avg Loss=0.1144, Time Left=9.35 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2320/3393 [18:21<08:51,  2.02batch/s, Batch Loss=0.0368, Avg Loss=0.1144, Time Left=9.34 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2321/3393 [18:21<08:42,  2.05batch/s, Batch Loss=0.0368, Avg Loss=0.1144, Time Left=9.34 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2321/3393 [18:22<08:42,  2.05batch/s, Batch Loss=0.1315, Avg Loss=0.1144, Time Left=9.33 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2322/3393 [18:22<08:40,  2.06batch/s, Batch Loss=0.1315, Avg Loss=0.1144, Time Left=9.33 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2322/3393 [18:22<08:40,  2.06batch/s, Batch Loss=0.0949, Avg Loss=0.1144, Time Left=9.32 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2323/3393 [18:22<08:34,  2.08batch/s, Batch Loss=0.0949, Avg Loss=0.1144, Time Left=9.32 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2323/3393 [18:23<08:34,  2.08batch/s, Batch Loss=0.1945, Avg Loss=0.1144, Time Left=9.31 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2324/3393 [18:23<08:44,  2.04batch/s, Batch Loss=0.1945, Avg Loss=0.1144, Time Left=9.31 \u001b[A\n",
      "Epoch 2/3 - Training:  68%|▋| 2324/3393 [18:23<08:44,  2.04batch/s, Batch Loss=0.0573, Avg Loss=0.1144, Time Left=9.31 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2325/3393 [18:23<08:37,  2.07batch/s, Batch Loss=0.0573, Avg Loss=0.1144, Time Left=9.31 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2325/3393 [18:24<08:37,  2.07batch/s, Batch Loss=0.0192, Avg Loss=0.1143, Time Left=9.30 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2326/3393 [18:24<08:26,  2.11batch/s, Batch Loss=0.0192, Avg Loss=0.1143, Time Left=9.30 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2326/3393 [18:24<08:26,  2.11batch/s, Batch Loss=0.0514, Avg Loss=0.1143, Time Left=9.29 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2327/3393 [18:24<08:48,  2.02batch/s, Batch Loss=0.0514, Avg Loss=0.1143, Time Left=9.29 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2327/3393 [18:25<08:48,  2.02batch/s, Batch Loss=0.0600, Avg Loss=0.1143, Time Left=9.28 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2328/3393 [18:25<08:34,  2.07batch/s, Batch Loss=0.0600, Avg Loss=0.1143, Time Left=9.28 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2328/3393 [18:25<08:34,  2.07batch/s, Batch Loss=0.0207, Avg Loss=0.1142, Time Left=9.27 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2329/3393 [18:25<08:34,  2.07batch/s, Batch Loss=0.0207, Avg Loss=0.1142, Time Left=9.27 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2329/3393 [18:26<08:34,  2.07batch/s, Batch Loss=0.0684, Avg Loss=0.1142, Time Left=9.27 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2330/3393 [18:26<08:48,  2.01batch/s, Batch Loss=0.0684, Avg Loss=0.1142, Time Left=9.27 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2330/3393 [18:26<08:48,  2.01batch/s, Batch Loss=0.0486, Avg Loss=0.1142, Time Left=9.26 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2331/3393 [18:26<08:45,  2.02batch/s, Batch Loss=0.0486, Avg Loss=0.1142, Time Left=9.26 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2331/3393 [18:27<08:45,  2.02batch/s, Batch Loss=0.0963, Avg Loss=0.1142, Time Left=9.25 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2332/3393 [18:27<08:51,  2.00batch/s, Batch Loss=0.0963, Avg Loss=0.1142, Time Left=9.25 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2332/3393 [18:27<08:51,  2.00batch/s, Batch Loss=0.0209, Avg Loss=0.1141, Time Left=9.24 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2333/3393 [18:27<08:40,  2.04batch/s, Batch Loss=0.0209, Avg Loss=0.1141, Time Left=9.24 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2333/3393 [18:27<08:40,  2.04batch/s, Batch Loss=0.0387, Avg Loss=0.1141, Time Left=9.23 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2334/3393 [18:27<08:37,  2.04batch/s, Batch Loss=0.0387, Avg Loss=0.1141, Time Left=9.23 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2334/3393 [18:28<08:37,  2.04batch/s, Batch Loss=0.1253, Avg Loss=0.1141, Time Left=9.22 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2335/3393 [18:28<08:35,  2.05batch/s, Batch Loss=0.1253, Avg Loss=0.1141, Time Left=9.22 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2335/3393 [18:28<08:35,  2.05batch/s, Batch Loss=0.0192, Avg Loss=0.1141, Time Left=9.22 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2336/3393 [18:28<08:28,  2.08batch/s, Batch Loss=0.0192, Avg Loss=0.1141, Time Left=9.22 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2336/3393 [18:29<08:28,  2.08batch/s, Batch Loss=0.0035, Avg Loss=0.1140, Time Left=9.21 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2337/3393 [18:29<08:23,  2.10batch/s, Batch Loss=0.0035, Avg Loss=0.1140, Time Left=9.21 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2337/3393 [18:29<08:23,  2.10batch/s, Batch Loss=0.0727, Avg Loss=0.1140, Time Left=9.20 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2338/3393 [18:29<08:30,  2.06batch/s, Batch Loss=0.0727, Avg Loss=0.1140, Time Left=9.20 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2338/3393 [18:30<08:30,  2.06batch/s, Batch Loss=0.0235, Avg Loss=0.1140, Time Left=9.19 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2339/3393 [18:30<08:24,  2.09batch/s, Batch Loss=0.0235, Avg Loss=0.1140, Time Left=9.19 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2339/3393 [18:30<08:24,  2.09batch/s, Batch Loss=0.1371, Avg Loss=0.1140, Time Left=9.18 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2340/3393 [18:30<08:26,  2.08batch/s, Batch Loss=0.1371, Avg Loss=0.1140, Time Left=9.18 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2340/3393 [18:31<08:26,  2.08batch/s, Batch Loss=0.0682, Avg Loss=0.1140, Time Left=9.18 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  69%|▋| 2341/3393 [18:31<08:26,  2.08batch/s, Batch Loss=0.0682, Avg Loss=0.1140, Time Left=9.18 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2341/3393 [18:31<08:26,  2.08batch/s, Batch Loss=0.4494, Avg Loss=0.1141, Time Left=9.17 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2342/3393 [18:31<08:31,  2.05batch/s, Batch Loss=0.4494, Avg Loss=0.1141, Time Left=9.17 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2342/3393 [18:32<08:31,  2.05batch/s, Batch Loss=0.0014, Avg Loss=0.1141, Time Left=9.16 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2343/3393 [18:32<08:34,  2.04batch/s, Batch Loss=0.0014, Avg Loss=0.1141, Time Left=9.16 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2343/3393 [18:32<08:34,  2.04batch/s, Batch Loss=0.1542, Avg Loss=0.1141, Time Left=9.15 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2344/3393 [18:32<08:33,  2.04batch/s, Batch Loss=0.1542, Avg Loss=0.1141, Time Left=9.15 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2344/3393 [18:33<08:33,  2.04batch/s, Batch Loss=0.1148, Avg Loss=0.1141, Time Left=9.14 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2345/3393 [18:33<08:35,  2.03batch/s, Batch Loss=0.1148, Avg Loss=0.1141, Time Left=9.14 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2345/3393 [18:33<08:35,  2.03batch/s, Batch Loss=0.0287, Avg Loss=0.1140, Time Left=9.13 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2346/3393 [18:33<08:32,  2.04batch/s, Batch Loss=0.0287, Avg Loss=0.1140, Time Left=9.13 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2346/3393 [18:34<08:32,  2.04batch/s, Batch Loss=0.0104, Avg Loss=0.1140, Time Left=9.13 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2347/3393 [18:34<08:20,  2.09batch/s, Batch Loss=0.0104, Avg Loss=0.1140, Time Left=9.13 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2347/3393 [18:34<08:20,  2.09batch/s, Batch Loss=0.3763, Avg Loss=0.1141, Time Left=9.12 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2348/3393 [18:34<08:21,  2.09batch/s, Batch Loss=0.3763, Avg Loss=0.1141, Time Left=9.12 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2348/3393 [18:35<08:21,  2.09batch/s, Batch Loss=0.1248, Avg Loss=0.1141, Time Left=9.11 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2349/3393 [18:35<08:31,  2.04batch/s, Batch Loss=0.1248, Avg Loss=0.1141, Time Left=9.11 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2349/3393 [18:35<08:31,  2.04batch/s, Batch Loss=0.0600, Avg Loss=0.1141, Time Left=9.10 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2350/3393 [18:35<08:24,  2.07batch/s, Batch Loss=0.0600, Avg Loss=0.1141, Time Left=9.10 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2350/3393 [18:36<08:24,  2.07batch/s, Batch Loss=0.0263, Avg Loss=0.1141, Time Left=9.09 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2351/3393 [18:36<08:34,  2.03batch/s, Batch Loss=0.0263, Avg Loss=0.1141, Time Left=9.09 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2351/3393 [18:36<08:34,  2.03batch/s, Batch Loss=0.1379, Avg Loss=0.1141, Time Left=9.09 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2352/3393 [18:36<08:30,  2.04batch/s, Batch Loss=0.1379, Avg Loss=0.1141, Time Left=9.09 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2352/3393 [18:37<08:30,  2.04batch/s, Batch Loss=0.1230, Avg Loss=0.1141, Time Left=9.08 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2353/3393 [18:37<08:28,  2.05batch/s, Batch Loss=0.1230, Avg Loss=0.1141, Time Left=9.08 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2353/3393 [18:37<08:28,  2.05batch/s, Batch Loss=0.2791, Avg Loss=0.1141, Time Left=9.07 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2354/3393 [18:37<08:21,  2.07batch/s, Batch Loss=0.2791, Avg Loss=0.1141, Time Left=9.07 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2354/3393 [18:38<08:21,  2.07batch/s, Batch Loss=0.0135, Avg Loss=0.1141, Time Left=9.06 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2355/3393 [18:38<08:20,  2.07batch/s, Batch Loss=0.0135, Avg Loss=0.1141, Time Left=9.06 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2355/3393 [18:38<08:20,  2.07batch/s, Batch Loss=0.4393, Avg Loss=0.1142, Time Left=9.05 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2356/3393 [18:38<08:21,  2.07batch/s, Batch Loss=0.4393, Avg Loss=0.1142, Time Left=9.05 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2356/3393 [18:39<08:21,  2.07batch/s, Batch Loss=0.0072, Avg Loss=0.1142, Time Left=9.04 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2357/3393 [18:39<08:20,  2.07batch/s, Batch Loss=0.0072, Avg Loss=0.1142, Time Left=9.04 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2357/3393 [18:39<08:20,  2.07batch/s, Batch Loss=0.0467, Avg Loss=0.1142, Time Left=9.04 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2358/3393 [18:39<08:15,  2.09batch/s, Batch Loss=0.0467, Avg Loss=0.1142, Time Left=9.04 \u001b[A\n",
      "Epoch 2/3 - Training:  69%|▋| 2358/3393 [18:40<08:15,  2.09batch/s, Batch Loss=0.2263, Avg Loss=0.1142, Time Left=9.03 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2359/3393 [18:40<08:16,  2.08batch/s, Batch Loss=0.2263, Avg Loss=0.1142, Time Left=9.03 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2359/3393 [18:40<08:16,  2.08batch/s, Batch Loss=0.0133, Avg Loss=0.1142, Time Left=9.02 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2360/3393 [18:40<08:12,  2.10batch/s, Batch Loss=0.0133, Avg Loss=0.1142, Time Left=9.02 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2360/3393 [18:41<08:12,  2.10batch/s, Batch Loss=0.0496, Avg Loss=0.1141, Time Left=9.01 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2361/3393 [18:41<08:14,  2.09batch/s, Batch Loss=0.0496, Avg Loss=0.1141, Time Left=9.01 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2361/3393 [18:41<08:14,  2.09batch/s, Batch Loss=0.0128, Avg Loss=0.1141, Time Left=9.00 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2362/3393 [18:41<08:33,  2.01batch/s, Batch Loss=0.0128, Avg Loss=0.1141, Time Left=9.00 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2362/3393 [18:42<08:33,  2.01batch/s, Batch Loss=0.0949, Avg Loss=0.1141, Time Left=9.00 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2363/3393 [18:42<08:29,  2.02batch/s, Batch Loss=0.0949, Avg Loss=0.1141, Time Left=9.00 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2363/3393 [18:42<08:29,  2.02batch/s, Batch Loss=0.0050, Avg Loss=0.1140, Time Left=8.99 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2364/3393 [18:42<08:25,  2.03batch/s, Batch Loss=0.0050, Avg Loss=0.1140, Time Left=8.99 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2364/3393 [18:43<08:25,  2.03batch/s, Batch Loss=0.0247, Avg Loss=0.1140, Time Left=8.98 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2365/3393 [18:43<08:23,  2.04batch/s, Batch Loss=0.0247, Avg Loss=0.1140, Time Left=8.98 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2365/3393 [18:43<08:23,  2.04batch/s, Batch Loss=0.0498, Avg Loss=0.1140, Time Left=8.97 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2366/3393 [18:43<08:20,  2.05batch/s, Batch Loss=0.0498, Avg Loss=0.1140, Time Left=8.97 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2366/3393 [18:44<08:20,  2.05batch/s, Batch Loss=0.0820, Avg Loss=0.1140, Time Left=8.96 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2367/3393 [18:44<08:23,  2.04batch/s, Batch Loss=0.0820, Avg Loss=0.1140, Time Left=8.96 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2367/3393 [18:44<08:23,  2.04batch/s, Batch Loss=0.0891, Avg Loss=0.1139, Time Left=8.95 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2368/3393 [18:44<08:21,  2.04batch/s, Batch Loss=0.0891, Avg Loss=0.1139, Time Left=8.95 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2368/3393 [18:44<08:21,  2.04batch/s, Batch Loss=0.0683, Avg Loss=0.1139, Time Left=8.95 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2369/3393 [18:44<08:19,  2.05batch/s, Batch Loss=0.0683, Avg Loss=0.1139, Time Left=8.95 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2369/3393 [18:45<08:19,  2.05batch/s, Batch Loss=0.0675, Avg Loss=0.1139, Time Left=8.94 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2370/3393 [18:45<08:18,  2.05batch/s, Batch Loss=0.0675, Avg Loss=0.1139, Time Left=8.94 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2370/3393 [18:45<08:18,  2.05batch/s, Batch Loss=0.0680, Avg Loss=0.1139, Time Left=8.93 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2371/3393 [18:45<08:07,  2.10batch/s, Batch Loss=0.0680, Avg Loss=0.1139, Time Left=8.93 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2371/3393 [18:46<08:07,  2.10batch/s, Batch Loss=0.0302, Avg Loss=0.1138, Time Left=8.92 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2372/3393 [18:46<08:12,  2.07batch/s, Batch Loss=0.0302, Avg Loss=0.1138, Time Left=8.92 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2372/3393 [18:46<08:12,  2.07batch/s, Batch Loss=0.0540, Avg Loss=0.1138, Time Left=8.91 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2373/3393 [18:46<08:12,  2.07batch/s, Batch Loss=0.0540, Avg Loss=0.1138, Time Left=8.91 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2373/3393 [18:47<08:12,  2.07batch/s, Batch Loss=0.2079, Avg Loss=0.1139, Time Left=8.90 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  70%|▋| 2374/3393 [18:47<08:07,  2.09batch/s, Batch Loss=0.2079, Avg Loss=0.1139, Time Left=8.90 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2374/3393 [18:47<08:07,  2.09batch/s, Batch Loss=0.0118, Avg Loss=0.1138, Time Left=8.90 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2375/3393 [18:47<08:13,  2.06batch/s, Batch Loss=0.0118, Avg Loss=0.1138, Time Left=8.90 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2375/3393 [18:48<08:13,  2.06batch/s, Batch Loss=0.2403, Avg Loss=0.1139, Time Left=8.89 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2376/3393 [18:48<08:20,  2.03batch/s, Batch Loss=0.2403, Avg Loss=0.1139, Time Left=8.89 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2376/3393 [18:48<08:20,  2.03batch/s, Batch Loss=0.1497, Avg Loss=0.1139, Time Left=8.88 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2377/3393 [18:48<08:19,  2.03batch/s, Batch Loss=0.1497, Avg Loss=0.1139, Time Left=8.88 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2377/3393 [18:49<08:19,  2.03batch/s, Batch Loss=0.1196, Avg Loss=0.1139, Time Left=8.87 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2378/3393 [18:49<08:24,  2.01batch/s, Batch Loss=0.1196, Avg Loss=0.1139, Time Left=8.87 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2378/3393 [18:49<08:24,  2.01batch/s, Batch Loss=0.0189, Avg Loss=0.1139, Time Left=8.86 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2379/3393 [18:49<08:17,  2.04batch/s, Batch Loss=0.0189, Avg Loss=0.1139, Time Left=8.86 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2379/3393 [18:50<08:17,  2.04batch/s, Batch Loss=0.0216, Avg Loss=0.1138, Time Left=8.86 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2380/3393 [18:50<08:24,  2.01batch/s, Batch Loss=0.0216, Avg Loss=0.1138, Time Left=8.86 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2380/3393 [18:50<08:24,  2.01batch/s, Batch Loss=0.0624, Avg Loss=0.1138, Time Left=8.85 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2381/3393 [18:50<08:24,  2.00batch/s, Batch Loss=0.0624, Avg Loss=0.1138, Time Left=8.85 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2381/3393 [18:51<08:24,  2.00batch/s, Batch Loss=0.0154, Avg Loss=0.1137, Time Left=8.84 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2382/3393 [18:51<08:31,  1.98batch/s, Batch Loss=0.0154, Avg Loss=0.1137, Time Left=8.84 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2382/3393 [18:51<08:31,  1.98batch/s, Batch Loss=0.0717, Avg Loss=0.1137, Time Left=8.83 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2383/3393 [18:51<08:23,  2.01batch/s, Batch Loss=0.0717, Avg Loss=0.1137, Time Left=8.83 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2383/3393 [18:52<08:23,  2.01batch/s, Batch Loss=0.0557, Avg Loss=0.1137, Time Left=8.82 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2384/3393 [18:52<08:18,  2.02batch/s, Batch Loss=0.0557, Avg Loss=0.1137, Time Left=8.82 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2384/3393 [18:52<08:18,  2.02batch/s, Batch Loss=0.1766, Avg Loss=0.1137, Time Left=8.82 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2385/3393 [18:52<08:18,  2.02batch/s, Batch Loss=0.1766, Avg Loss=0.1137, Time Left=8.82 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2385/3393 [18:53<08:18,  2.02batch/s, Batch Loss=0.2336, Avg Loss=0.1138, Time Left=8.81 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2386/3393 [18:53<08:10,  2.05batch/s, Batch Loss=0.2336, Avg Loss=0.1138, Time Left=8.81 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2386/3393 [18:53<08:10,  2.05batch/s, Batch Loss=0.0328, Avg Loss=0.1137, Time Left=8.80 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2387/3393 [18:53<08:13,  2.04batch/s, Batch Loss=0.0328, Avg Loss=0.1137, Time Left=8.80 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2387/3393 [18:54<08:13,  2.04batch/s, Batch Loss=0.0143, Avg Loss=0.1137, Time Left=8.79 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2388/3393 [18:54<08:11,  2.04batch/s, Batch Loss=0.0143, Avg Loss=0.1137, Time Left=8.79 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2388/3393 [18:54<08:11,  2.04batch/s, Batch Loss=0.1307, Avg Loss=0.1137, Time Left=8.78 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2389/3393 [18:54<08:09,  2.05batch/s, Batch Loss=0.1307, Avg Loss=0.1137, Time Left=8.78 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2389/3393 [18:55<08:09,  2.05batch/s, Batch Loss=0.1428, Avg Loss=0.1137, Time Left=8.78 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2390/3393 [18:55<08:13,  2.03batch/s, Batch Loss=0.1428, Avg Loss=0.1137, Time Left=8.78 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2390/3393 [18:55<08:13,  2.03batch/s, Batch Loss=0.1246, Avg Loss=0.1137, Time Left=8.77 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2391/3393 [18:55<08:09,  2.05batch/s, Batch Loss=0.1246, Avg Loss=0.1137, Time Left=8.77 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2391/3393 [18:56<08:09,  2.05batch/s, Batch Loss=0.0151, Avg Loss=0.1137, Time Left=8.76 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2392/3393 [18:56<08:03,  2.07batch/s, Batch Loss=0.0151, Avg Loss=0.1137, Time Left=8.76 \u001b[A\n",
      "Epoch 2/3 - Training:  70%|▋| 2392/3393 [18:56<08:03,  2.07batch/s, Batch Loss=0.1367, Avg Loss=0.1137, Time Left=8.75 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2393/3393 [18:56<08:02,  2.07batch/s, Batch Loss=0.1367, Avg Loss=0.1137, Time Left=8.75 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2393/3393 [18:57<08:02,  2.07batch/s, Batch Loss=0.0875, Avg Loss=0.1137, Time Left=8.74 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2394/3393 [18:57<07:57,  2.09batch/s, Batch Loss=0.0875, Avg Loss=0.1137, Time Left=8.74 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2394/3393 [18:57<07:57,  2.09batch/s, Batch Loss=0.1000, Avg Loss=0.1137, Time Left=8.73 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2395/3393 [18:57<07:58,  2.08batch/s, Batch Loss=0.1000, Avg Loss=0.1137, Time Left=8.73 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2395/3393 [18:58<07:58,  2.08batch/s, Batch Loss=0.0262, Avg Loss=0.1136, Time Left=8.73 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2396/3393 [18:58<08:01,  2.07batch/s, Batch Loss=0.0262, Avg Loss=0.1136, Time Left=8.73 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2396/3393 [18:58<08:01,  2.07batch/s, Batch Loss=0.1047, Avg Loss=0.1136, Time Left=8.72 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2397/3393 [18:58<07:54,  2.10batch/s, Batch Loss=0.1047, Avg Loss=0.1136, Time Left=8.72 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2397/3393 [18:59<07:54,  2.10batch/s, Batch Loss=0.1571, Avg Loss=0.1137, Time Left=8.71 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2398/3393 [18:59<07:55,  2.09batch/s, Batch Loss=0.1571, Avg Loss=0.1137, Time Left=8.71 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2398/3393 [18:59<07:55,  2.09batch/s, Batch Loss=0.0363, Avg Loss=0.1136, Time Left=8.70 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2399/3393 [18:59<07:52,  2.10batch/s, Batch Loss=0.0363, Avg Loss=0.1136, Time Left=8.70 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2399/3393 [19:00<07:52,  2.10batch/s, Batch Loss=0.0107, Avg Loss=0.1136, Time Left=8.69 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2400/3393 [19:00<07:54,  2.09batch/s, Batch Loss=0.0107, Avg Loss=0.1136, Time Left=8.69 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2400/3393 [19:00<07:54,  2.09batch/s, Batch Loss=0.0854, Avg Loss=0.1136, Time Left=8.68 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2401/3393 [19:00<07:55,  2.09batch/s, Batch Loss=0.0854, Avg Loss=0.1136, Time Left=8.68 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2401/3393 [19:01<07:55,  2.09batch/s, Batch Loss=0.0526, Avg Loss=0.1135, Time Left=8.68 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2402/3393 [19:01<07:56,  2.08batch/s, Batch Loss=0.0526, Avg Loss=0.1135, Time Left=8.68 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2402/3393 [19:01<07:56,  2.08batch/s, Batch Loss=0.0499, Avg Loss=0.1135, Time Left=8.67 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2403/3393 [19:01<07:51,  2.10batch/s, Batch Loss=0.0499, Avg Loss=0.1135, Time Left=8.67 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2403/3393 [19:01<07:51,  2.10batch/s, Batch Loss=0.1621, Avg Loss=0.1135, Time Left=8.66 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2404/3393 [19:01<07:50,  2.10batch/s, Batch Loss=0.1621, Avg Loss=0.1135, Time Left=8.66 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2404/3393 [19:02<07:50,  2.10batch/s, Batch Loss=0.0181, Avg Loss=0.1135, Time Left=8.65 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2405/3393 [19:02<07:55,  2.08batch/s, Batch Loss=0.0181, Avg Loss=0.1135, Time Left=8.65 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2405/3393 [19:02<07:55,  2.08batch/s, Batch Loss=0.0713, Avg Loss=0.1135, Time Left=8.64 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2406/3393 [19:02<07:55,  2.08batch/s, Batch Loss=0.0713, Avg Loss=0.1135, Time Left=8.64 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2406/3393 [19:03<07:55,  2.08batch/s, Batch Loss=0.1561, Avg Loss=0.1135, Time Left=8.63 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  71%|▋| 2407/3393 [19:03<08:00,  2.05batch/s, Batch Loss=0.1561, Avg Loss=0.1135, Time Left=8.63 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2407/3393 [19:03<08:00,  2.05batch/s, Batch Loss=0.1344, Avg Loss=0.1135, Time Left=8.63 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2408/3393 [19:03<07:59,  2.05batch/s, Batch Loss=0.1344, Avg Loss=0.1135, Time Left=8.63 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2408/3393 [19:04<07:59,  2.05batch/s, Batch Loss=0.2065, Avg Loss=0.1135, Time Left=8.62 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2409/3393 [19:04<07:57,  2.06batch/s, Batch Loss=0.2065, Avg Loss=0.1135, Time Left=8.62 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2409/3393 [19:04<07:57,  2.06batch/s, Batch Loss=0.1185, Avg Loss=0.1135, Time Left=8.61 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2410/3393 [19:04<08:01,  2.04batch/s, Batch Loss=0.1185, Avg Loss=0.1135, Time Left=8.61 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2410/3393 [19:05<08:01,  2.04batch/s, Batch Loss=0.0050, Avg Loss=0.1135, Time Left=8.60 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2411/3393 [19:05<07:59,  2.05batch/s, Batch Loss=0.0050, Avg Loss=0.1135, Time Left=8.60 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2411/3393 [19:05<07:59,  2.05batch/s, Batch Loss=0.0027, Avg Loss=0.1135, Time Left=8.59 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2412/3393 [19:05<07:57,  2.05batch/s, Batch Loss=0.0027, Avg Loss=0.1135, Time Left=8.59 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2412/3393 [19:06<07:57,  2.05batch/s, Batch Loss=0.0213, Avg Loss=0.1134, Time Left=8.59 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2413/3393 [19:06<07:55,  2.06batch/s, Batch Loss=0.0213, Avg Loss=0.1134, Time Left=8.59 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2413/3393 [19:06<07:55,  2.06batch/s, Batch Loss=0.0179, Avg Loss=0.1134, Time Left=8.58 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2414/3393 [19:06<07:50,  2.08batch/s, Batch Loss=0.0179, Avg Loss=0.1134, Time Left=8.58 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2414/3393 [19:07<07:50,  2.08batch/s, Batch Loss=0.1442, Avg Loss=0.1134, Time Left=8.57 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2415/3393 [19:07<08:04,  2.02batch/s, Batch Loss=0.1442, Avg Loss=0.1134, Time Left=8.57 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2415/3393 [19:07<08:04,  2.02batch/s, Batch Loss=0.2311, Avg Loss=0.1134, Time Left=8.56 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2416/3393 [19:07<07:52,  2.07batch/s, Batch Loss=0.2311, Avg Loss=0.1134, Time Left=8.56 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2416/3393 [19:08<07:52,  2.07batch/s, Batch Loss=0.0731, Avg Loss=0.1134, Time Left=8.55 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2417/3393 [19:08<07:51,  2.07batch/s, Batch Loss=0.0731, Avg Loss=0.1134, Time Left=8.55 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2417/3393 [19:08<07:51,  2.07batch/s, Batch Loss=0.0520, Avg Loss=0.1134, Time Left=8.54 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2418/3393 [19:08<07:55,  2.05batch/s, Batch Loss=0.0520, Avg Loss=0.1134, Time Left=8.54 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2418/3393 [19:09<07:55,  2.05batch/s, Batch Loss=0.0379, Avg Loss=0.1134, Time Left=8.54 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2419/3393 [19:09<07:45,  2.09batch/s, Batch Loss=0.0379, Avg Loss=0.1134, Time Left=8.54 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2419/3393 [19:09<07:45,  2.09batch/s, Batch Loss=0.0537, Avg Loss=0.1133, Time Left=8.53 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2420/3393 [19:09<07:46,  2.09batch/s, Batch Loss=0.0537, Avg Loss=0.1133, Time Left=8.53 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2420/3393 [19:10<07:46,  2.09batch/s, Batch Loss=0.0283, Avg Loss=0.1133, Time Left=8.52 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2421/3393 [19:10<07:46,  2.08batch/s, Batch Loss=0.0283, Avg Loss=0.1133, Time Left=8.52 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2421/3393 [19:10<07:46,  2.08batch/s, Batch Loss=0.0096, Avg Loss=0.1133, Time Left=8.51 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2422/3393 [19:10<07:47,  2.08batch/s, Batch Loss=0.0096, Avg Loss=0.1133, Time Left=8.51 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2422/3393 [19:11<07:47,  2.08batch/s, Batch Loss=0.0243, Avg Loss=0.1132, Time Left=8.50 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2423/3393 [19:11<07:47,  2.08batch/s, Batch Loss=0.0243, Avg Loss=0.1132, Time Left=8.50 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2423/3393 [19:11<07:47,  2.08batch/s, Batch Loss=0.0120, Avg Loss=0.1132, Time Left=8.50 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2424/3393 [19:11<07:47,  2.07batch/s, Batch Loss=0.0120, Avg Loss=0.1132, Time Left=8.50 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2424/3393 [19:12<07:47,  2.07batch/s, Batch Loss=0.0533, Avg Loss=0.1131, Time Left=8.49 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2425/3393 [19:12<07:47,  2.07batch/s, Batch Loss=0.0533, Avg Loss=0.1131, Time Left=8.49 \u001b[A\n",
      "Epoch 2/3 - Training:  71%|▋| 2425/3393 [19:12<07:47,  2.07batch/s, Batch Loss=0.0215, Avg Loss=0.1131, Time Left=8.48 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2426/3393 [19:12<07:47,  2.07batch/s, Batch Loss=0.0215, Avg Loss=0.1131, Time Left=8.48 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2426/3393 [19:13<07:47,  2.07batch/s, Batch Loss=0.1737, Avg Loss=0.1131, Time Left=8.47 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2427/3393 [19:13<07:42,  2.09batch/s, Batch Loss=0.1737, Avg Loss=0.1131, Time Left=8.47 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2427/3393 [19:13<07:42,  2.09batch/s, Batch Loss=0.4761, Avg Loss=0.1133, Time Left=8.46 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2428/3393 [19:13<07:45,  2.07batch/s, Batch Loss=0.4761, Avg Loss=0.1133, Time Left=8.46 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2428/3393 [19:14<07:45,  2.07batch/s, Batch Loss=0.0246, Avg Loss=0.1133, Time Left=8.45 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2429/3393 [19:14<07:42,  2.08batch/s, Batch Loss=0.0246, Avg Loss=0.1133, Time Left=8.45 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2429/3393 [19:14<07:42,  2.08batch/s, Batch Loss=0.0947, Avg Loss=0.1132, Time Left=8.45 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2430/3393 [19:14<07:43,  2.08batch/s, Batch Loss=0.0947, Avg Loss=0.1132, Time Left=8.45 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2430/3393 [19:15<07:43,  2.08batch/s, Batch Loss=0.0809, Avg Loss=0.1132, Time Left=8.44 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2431/3393 [19:15<07:43,  2.08batch/s, Batch Loss=0.0809, Avg Loss=0.1132, Time Left=8.44 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2431/3393 [19:15<07:43,  2.08batch/s, Batch Loss=0.0019, Avg Loss=0.1132, Time Left=8.43 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2432/3393 [19:15<07:56,  2.02batch/s, Batch Loss=0.0019, Avg Loss=0.1132, Time Left=8.43 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2432/3393 [19:16<07:56,  2.02batch/s, Batch Loss=0.0584, Avg Loss=0.1132, Time Left=8.42 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2433/3393 [19:16<07:49,  2.04batch/s, Batch Loss=0.0584, Avg Loss=0.1132, Time Left=8.42 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2433/3393 [19:16<07:49,  2.04batch/s, Batch Loss=0.0700, Avg Loss=0.1131, Time Left=8.41 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2434/3393 [19:16<07:51,  2.03batch/s, Batch Loss=0.0700, Avg Loss=0.1131, Time Left=8.41 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2434/3393 [19:17<07:51,  2.03batch/s, Batch Loss=0.0506, Avg Loss=0.1131, Time Left=8.41 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2435/3393 [19:17<07:45,  2.06batch/s, Batch Loss=0.0506, Avg Loss=0.1131, Time Left=8.41 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2435/3393 [19:17<07:45,  2.06batch/s, Batch Loss=0.2315, Avg Loss=0.1132, Time Left=8.40 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2436/3393 [19:17<07:47,  2.05batch/s, Batch Loss=0.2315, Avg Loss=0.1132, Time Left=8.40 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2436/3393 [19:18<07:47,  2.05batch/s, Batch Loss=0.0309, Avg Loss=0.1131, Time Left=8.39 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2437/3393 [19:18<07:51,  2.03batch/s, Batch Loss=0.0309, Avg Loss=0.1131, Time Left=8.39 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2437/3393 [19:18<07:51,  2.03batch/s, Batch Loss=0.0934, Avg Loss=0.1131, Time Left=8.38 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2438/3393 [19:18<07:52,  2.02batch/s, Batch Loss=0.0934, Avg Loss=0.1131, Time Left=8.38 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2438/3393 [19:19<07:52,  2.02batch/s, Batch Loss=0.0076, Avg Loss=0.1131, Time Left=8.37 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2439/3393 [19:19<07:58,  2.00batch/s, Batch Loss=0.0076, Avg Loss=0.1131, Time Left=8.37 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2439/3393 [19:19<07:58,  2.00batch/s, Batch Loss=0.0379, Avg Loss=0.1130, Time Left=8.36 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  72%|▋| 2440/3393 [19:19<07:49,  2.03batch/s, Batch Loss=0.0379, Avg Loss=0.1130, Time Left=8.36 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2440/3393 [19:19<07:49,  2.03batch/s, Batch Loss=0.0502, Avg Loss=0.1130, Time Left=8.36 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2441/3393 [19:19<07:40,  2.07batch/s, Batch Loss=0.0502, Avg Loss=0.1130, Time Left=8.36 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2441/3393 [19:20<07:40,  2.07batch/s, Batch Loss=0.1566, Avg Loss=0.1130, Time Left=8.35 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2442/3393 [19:20<07:49,  2.03batch/s, Batch Loss=0.1566, Avg Loss=0.1130, Time Left=8.35 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2442/3393 [19:20<07:49,  2.03batch/s, Batch Loss=0.0240, Avg Loss=0.1130, Time Left=8.34 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2443/3393 [19:20<07:37,  2.08batch/s, Batch Loss=0.0240, Avg Loss=0.1130, Time Left=8.34 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2443/3393 [19:21<07:37,  2.08batch/s, Batch Loss=0.1181, Avg Loss=0.1130, Time Left=8.33 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2444/3393 [19:21<07:37,  2.08batch/s, Batch Loss=0.1181, Avg Loss=0.1130, Time Left=8.33 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2444/3393 [19:21<07:37,  2.08batch/s, Batch Loss=0.0009, Avg Loss=0.1130, Time Left=8.32 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2445/3393 [19:21<07:37,  2.07batch/s, Batch Loss=0.0009, Avg Loss=0.1130, Time Left=8.32 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2445/3393 [19:22<07:37,  2.07batch/s, Batch Loss=0.0153, Avg Loss=0.1129, Time Left=8.32 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2446/3393 [19:22<07:37,  2.07batch/s, Batch Loss=0.0153, Avg Loss=0.1129, Time Left=8.32 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2446/3393 [19:22<07:37,  2.07batch/s, Batch Loss=0.0625, Avg Loss=0.1129, Time Left=8.31 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2447/3393 [19:22<07:32,  2.09batch/s, Batch Loss=0.0625, Avg Loss=0.1129, Time Left=8.31 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2447/3393 [19:23<07:32,  2.09batch/s, Batch Loss=0.0576, Avg Loss=0.1129, Time Left=8.30 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2448/3393 [19:23<07:42,  2.05batch/s, Batch Loss=0.0576, Avg Loss=0.1129, Time Left=8.30 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2448/3393 [19:23<07:42,  2.05batch/s, Batch Loss=0.0364, Avg Loss=0.1128, Time Left=8.29 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2449/3393 [19:23<07:43,  2.04batch/s, Batch Loss=0.0364, Avg Loss=0.1128, Time Left=8.29 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2449/3393 [19:24<07:43,  2.04batch/s, Batch Loss=0.0357, Avg Loss=0.1128, Time Left=8.28 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2450/3393 [19:24<07:47,  2.02batch/s, Batch Loss=0.0357, Avg Loss=0.1128, Time Left=8.28 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2450/3393 [19:24<07:47,  2.02batch/s, Batch Loss=0.2544, Avg Loss=0.1129, Time Left=8.27 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2451/3393 [19:24<07:48,  2.01batch/s, Batch Loss=0.2544, Avg Loss=0.1129, Time Left=8.27 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2451/3393 [19:25<07:48,  2.01batch/s, Batch Loss=0.0660, Avg Loss=0.1128, Time Left=8.27 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2452/3393 [19:25<07:53,  1.99batch/s, Batch Loss=0.0660, Avg Loss=0.1128, Time Left=8.27 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2452/3393 [19:25<07:53,  1.99batch/s, Batch Loss=0.0102, Avg Loss=0.1128, Time Left=8.26 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2453/3393 [19:25<07:38,  2.05batch/s, Batch Loss=0.0102, Avg Loss=0.1128, Time Left=8.26 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2453/3393 [19:26<07:38,  2.05batch/s, Batch Loss=0.0353, Avg Loss=0.1128, Time Left=8.25 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2454/3393 [19:26<07:36,  2.06batch/s, Batch Loss=0.0353, Avg Loss=0.1128, Time Left=8.25 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2454/3393 [19:26<07:36,  2.06batch/s, Batch Loss=0.1464, Avg Loss=0.1128, Time Left=8.24 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2455/3393 [19:26<07:40,  2.04batch/s, Batch Loss=0.1464, Avg Loss=0.1128, Time Left=8.24 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2455/3393 [19:27<07:40,  2.04batch/s, Batch Loss=0.0274, Avg Loss=0.1127, Time Left=8.23 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2456/3393 [19:27<07:28,  2.09batch/s, Batch Loss=0.0274, Avg Loss=0.1127, Time Left=8.23 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2456/3393 [19:27<07:28,  2.09batch/s, Batch Loss=0.2223, Avg Loss=0.1128, Time Left=8.23 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2457/3393 [19:27<07:29,  2.08batch/s, Batch Loss=0.2223, Avg Loss=0.1128, Time Left=8.23 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2457/3393 [19:28<07:29,  2.08batch/s, Batch Loss=0.0417, Avg Loss=0.1128, Time Left=8.22 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2458/3393 [19:28<07:34,  2.06batch/s, Batch Loss=0.0417, Avg Loss=0.1128, Time Left=8.22 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2458/3393 [19:28<07:34,  2.06batch/s, Batch Loss=0.0709, Avg Loss=0.1127, Time Left=8.21 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2459/3393 [19:28<07:33,  2.06batch/s, Batch Loss=0.0709, Avg Loss=0.1127, Time Left=8.21 \u001b[A\n",
      "Epoch 2/3 - Training:  72%|▋| 2459/3393 [19:29<07:33,  2.06batch/s, Batch Loss=0.0815, Avg Loss=0.1127, Time Left=8.20 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2460/3393 [19:29<07:36,  2.04batch/s, Batch Loss=0.0815, Avg Loss=0.1127, Time Left=8.20 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2460/3393 [19:29<07:36,  2.04batch/s, Batch Loss=0.4133, Avg Loss=0.1129, Time Left=8.19 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2461/3393 [19:29<07:30,  2.07batch/s, Batch Loss=0.4133, Avg Loss=0.1129, Time Left=8.19 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2461/3393 [19:30<07:30,  2.07batch/s, Batch Loss=0.0555, Avg Loss=0.1128, Time Left=8.18 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2462/3393 [19:30<07:29,  2.07batch/s, Batch Loss=0.0555, Avg Loss=0.1128, Time Left=8.18 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2462/3393 [19:30<07:29,  2.07batch/s, Batch Loss=0.4079, Avg Loss=0.1130, Time Left=8.18 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2463/3393 [19:30<07:34,  2.05batch/s, Batch Loss=0.4079, Avg Loss=0.1130, Time Left=8.18 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2463/3393 [19:31<07:34,  2.05batch/s, Batch Loss=0.1470, Avg Loss=0.1130, Time Left=8.17 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2464/3393 [19:31<07:32,  2.05batch/s, Batch Loss=0.1470, Avg Loss=0.1130, Time Left=8.17 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2464/3393 [19:31<07:32,  2.05batch/s, Batch Loss=0.2616, Avg Loss=0.1130, Time Left=8.16 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2465/3393 [19:31<07:26,  2.08batch/s, Batch Loss=0.2616, Avg Loss=0.1130, Time Left=8.16 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2465/3393 [19:32<07:26,  2.08batch/s, Batch Loss=0.0024, Avg Loss=0.1130, Time Left=8.15 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2466/3393 [19:32<07:38,  2.02batch/s, Batch Loss=0.0024, Avg Loss=0.1130, Time Left=8.15 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2466/3393 [19:32<07:38,  2.02batch/s, Batch Loss=0.0103, Avg Loss=0.1129, Time Left=8.14 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2467/3393 [19:32<07:30,  2.06batch/s, Batch Loss=0.0103, Avg Loss=0.1129, Time Left=8.14 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2467/3393 [19:33<07:30,  2.06batch/s, Batch Loss=0.1126, Avg Loss=0.1129, Time Left=8.14 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2468/3393 [19:33<07:26,  2.07batch/s, Batch Loss=0.1126, Avg Loss=0.1129, Time Left=8.14 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2468/3393 [19:33<07:26,  2.07batch/s, Batch Loss=0.4213, Avg Loss=0.1131, Time Left=8.13 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2469/3393 [19:33<07:26,  2.07batch/s, Batch Loss=0.4213, Avg Loss=0.1131, Time Left=8.13 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2469/3393 [19:34<07:26,  2.07batch/s, Batch Loss=0.6649, Avg Loss=0.1133, Time Left=8.12 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2470/3393 [19:34<07:21,  2.09batch/s, Batch Loss=0.6649, Avg Loss=0.1133, Time Left=8.12 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2470/3393 [19:34<07:21,  2.09batch/s, Batch Loss=0.1232, Avg Loss=0.1133, Time Left=8.11 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2471/3393 [19:34<07:22,  2.08batch/s, Batch Loss=0.1232, Avg Loss=0.1133, Time Left=8.11 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2471/3393 [19:35<07:22,  2.08batch/s, Batch Loss=0.0053, Avg Loss=0.1133, Time Left=8.10 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2472/3393 [19:35<07:22,  2.08batch/s, Batch Loss=0.0053, Avg Loss=0.1133, Time Left=8.10 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2472/3393 [19:35<07:22,  2.08batch/s, Batch Loss=0.4240, Avg Loss=0.1134, Time Left=8.09 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  73%|▋| 2473/3393 [19:35<07:23,  2.08batch/s, Batch Loss=0.4240, Avg Loss=0.1134, Time Left=8.09 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2473/3393 [19:35<07:23,  2.08batch/s, Batch Loss=0.0036, Avg Loss=0.1133, Time Left=8.09 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2474/3393 [19:35<07:19,  2.09batch/s, Batch Loss=0.0036, Avg Loss=0.1133, Time Left=8.09 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2474/3393 [19:36<07:19,  2.09batch/s, Batch Loss=0.4143, Avg Loss=0.1135, Time Left=8.08 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2475/3393 [19:36<07:15,  2.11batch/s, Batch Loss=0.4143, Avg Loss=0.1135, Time Left=8.08 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2475/3393 [19:36<07:15,  2.11batch/s, Batch Loss=0.0568, Avg Loss=0.1134, Time Left=8.07 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2476/3393 [19:36<07:12,  2.12batch/s, Batch Loss=0.0568, Avg Loss=0.1134, Time Left=8.07 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2476/3393 [19:37<07:12,  2.12batch/s, Batch Loss=0.0457, Avg Loss=0.1134, Time Left=8.06 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2477/3393 [19:37<07:15,  2.10batch/s, Batch Loss=0.0457, Avg Loss=0.1134, Time Left=8.06 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2477/3393 [19:37<07:15,  2.10batch/s, Batch Loss=0.0931, Avg Loss=0.1134, Time Left=8.05 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2478/3393 [19:37<07:25,  2.05batch/s, Batch Loss=0.0931, Avg Loss=0.1134, Time Left=8.05 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2478/3393 [19:38<07:25,  2.05batch/s, Batch Loss=0.1307, Avg Loss=0.1134, Time Left=8.05 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2479/3393 [19:38<07:25,  2.05batch/s, Batch Loss=0.1307, Avg Loss=0.1134, Time Left=8.05 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2479/3393 [19:38<07:25,  2.05batch/s, Batch Loss=0.0498, Avg Loss=0.1134, Time Left=8.04 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2480/3393 [19:38<07:32,  2.02batch/s, Batch Loss=0.0498, Avg Loss=0.1134, Time Left=8.04 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2480/3393 [19:39<07:32,  2.02batch/s, Batch Loss=0.0823, Avg Loss=0.1134, Time Left=8.03 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2481/3393 [19:39<07:29,  2.03batch/s, Batch Loss=0.0823, Avg Loss=0.1134, Time Left=8.03 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2481/3393 [19:39<07:29,  2.03batch/s, Batch Loss=0.1029, Avg Loss=0.1134, Time Left=8.02 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2482/3393 [19:39<07:21,  2.06batch/s, Batch Loss=0.1029, Avg Loss=0.1134, Time Left=8.02 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2482/3393 [19:40<07:21,  2.06batch/s, Batch Loss=0.0283, Avg Loss=0.1133, Time Left=8.01 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2483/3393 [19:40<07:20,  2.06batch/s, Batch Loss=0.0283, Avg Loss=0.1133, Time Left=8.01 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2483/3393 [19:40<07:20,  2.06batch/s, Batch Loss=0.1073, Avg Loss=0.1133, Time Left=8.00 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2484/3393 [19:40<07:15,  2.09batch/s, Batch Loss=0.1073, Avg Loss=0.1133, Time Left=8.00 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2484/3393 [19:41<07:15,  2.09batch/s, Batch Loss=0.0331, Avg Loss=0.1133, Time Left=8.00 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2485/3393 [19:41<07:20,  2.06batch/s, Batch Loss=0.0331, Avg Loss=0.1133, Time Left=8.00 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2485/3393 [19:41<07:20,  2.06batch/s, Batch Loss=0.2895, Avg Loss=0.1134, Time Left=7.99 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2486/3393 [19:41<07:15,  2.08batch/s, Batch Loss=0.2895, Avg Loss=0.1134, Time Left=7.99 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2486/3393 [19:42<07:15,  2.08batch/s, Batch Loss=0.1401, Avg Loss=0.1134, Time Left=7.98 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2487/3393 [19:42<07:11,  2.10batch/s, Batch Loss=0.1401, Avg Loss=0.1134, Time Left=7.98 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2487/3393 [19:42<07:11,  2.10batch/s, Batch Loss=0.1192, Avg Loss=0.1134, Time Left=7.97 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2488/3393 [19:42<07:13,  2.09batch/s, Batch Loss=0.1192, Avg Loss=0.1134, Time Left=7.97 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2488/3393 [19:43<07:13,  2.09batch/s, Batch Loss=0.0414, Avg Loss=0.1134, Time Left=7.96 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2489/3393 [19:43<07:14,  2.08batch/s, Batch Loss=0.0414, Avg Loss=0.1134, Time Left=7.96 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2489/3393 [19:43<07:14,  2.08batch/s, Batch Loss=0.1848, Avg Loss=0.1134, Time Left=7.95 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2490/3393 [19:43<07:09,  2.10batch/s, Batch Loss=0.1848, Avg Loss=0.1134, Time Left=7.95 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2490/3393 [19:44<07:09,  2.10batch/s, Batch Loss=0.0851, Avg Loss=0.1134, Time Left=7.95 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2491/3393 [19:44<07:17,  2.06batch/s, Batch Loss=0.0851, Avg Loss=0.1134, Time Left=7.95 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2491/3393 [19:44<07:17,  2.06batch/s, Batch Loss=0.2349, Avg Loss=0.1134, Time Left=7.94 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2492/3393 [19:44<07:15,  2.07batch/s, Batch Loss=0.2349, Avg Loss=0.1134, Time Left=7.94 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2492/3393 [19:45<07:15,  2.07batch/s, Batch Loss=0.1038, Avg Loss=0.1134, Time Left=7.93 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2493/3393 [19:45<07:14,  2.07batch/s, Batch Loss=0.1038, Avg Loss=0.1134, Time Left=7.93 \u001b[A\n",
      "Epoch 2/3 - Training:  73%|▋| 2493/3393 [19:45<07:14,  2.07batch/s, Batch Loss=0.0217, Avg Loss=0.1134, Time Left=7.92 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2494/3393 [19:45<07:22,  2.03batch/s, Batch Loss=0.0217, Avg Loss=0.1134, Time Left=7.92 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2494/3393 [19:46<07:22,  2.03batch/s, Batch Loss=0.1805, Avg Loss=0.1134, Time Left=7.91 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2495/3393 [19:46<07:24,  2.02batch/s, Batch Loss=0.1805, Avg Loss=0.1134, Time Left=7.91 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2495/3393 [19:46<07:24,  2.02batch/s, Batch Loss=0.0292, Avg Loss=0.1134, Time Left=7.91 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2496/3393 [19:46<07:29,  1.99batch/s, Batch Loss=0.0292, Avg Loss=0.1134, Time Left=7.91 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2496/3393 [19:47<07:29,  1.99batch/s, Batch Loss=0.2432, Avg Loss=0.1134, Time Left=7.90 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2497/3393 [19:47<07:25,  2.01batch/s, Batch Loss=0.2432, Avg Loss=0.1134, Time Left=7.90 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2497/3393 [19:47<07:25,  2.01batch/s, Batch Loss=0.0500, Avg Loss=0.1134, Time Left=7.89 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2498/3393 [19:47<07:20,  2.03batch/s, Batch Loss=0.0500, Avg Loss=0.1134, Time Left=7.89 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2498/3393 [19:48<07:20,  2.03batch/s, Batch Loss=0.1981, Avg Loss=0.1134, Time Left=7.88 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2499/3393 [19:48<07:21,  2.02batch/s, Batch Loss=0.1981, Avg Loss=0.1134, Time Left=7.88 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2499/3393 [19:48<07:21,  2.02batch/s, Batch Loss=0.0165, Avg Loss=0.1134, Time Left=7.87 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2500/3393 [19:48<07:19,  2.03batch/s, Batch Loss=0.0165, Avg Loss=0.1134, Time Left=7.87 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2500/3393 [19:49<07:19,  2.03batch/s, Batch Loss=0.3658, Avg Loss=0.1135, Time Left=7.87 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2501/3393 [19:49<07:25,  2.00batch/s, Batch Loss=0.3658, Avg Loss=0.1135, Time Left=7.87 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2501/3393 [19:49<07:25,  2.00batch/s, Batch Loss=0.0426, Avg Loss=0.1135, Time Left=7.86 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2502/3393 [19:49<07:16,  2.04batch/s, Batch Loss=0.0426, Avg Loss=0.1135, Time Left=7.86 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2502/3393 [19:50<07:16,  2.04batch/s, Batch Loss=0.0306, Avg Loss=0.1134, Time Left=7.85 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2503/3393 [19:50<07:18,  2.03batch/s, Batch Loss=0.0306, Avg Loss=0.1134, Time Left=7.85 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2503/3393 [19:50<07:18,  2.03batch/s, Batch Loss=0.2282, Avg Loss=0.1135, Time Left=7.84 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2504/3393 [19:50<07:11,  2.06batch/s, Batch Loss=0.2282, Avg Loss=0.1135, Time Left=7.84 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2504/3393 [19:51<07:11,  2.06batch/s, Batch Loss=0.0787, Avg Loss=0.1135, Time Left=7.83 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2505/3393 [19:51<07:08,  2.07batch/s, Batch Loss=0.0787, Avg Loss=0.1135, Time Left=7.83 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2505/3393 [19:51<07:08,  2.07batch/s, Batch Loss=0.0142, Avg Loss=0.1134, Time Left=7.82 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  74%|▋| 2506/3393 [19:51<07:10,  2.06batch/s, Batch Loss=0.0142, Avg Loss=0.1134, Time Left=7.82 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2506/3393 [19:52<07:10,  2.06batch/s, Batch Loss=0.0683, Avg Loss=0.1134, Time Left=7.82 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2507/3393 [19:52<07:23,  2.00batch/s, Batch Loss=0.0683, Avg Loss=0.1134, Time Left=7.82 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2507/3393 [19:52<07:23,  2.00batch/s, Batch Loss=0.0100, Avg Loss=0.1134, Time Left=7.81 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2508/3393 [19:52<07:41,  1.92batch/s, Batch Loss=0.0100, Avg Loss=0.1134, Time Left=7.81 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2508/3393 [19:53<07:41,  1.92batch/s, Batch Loss=0.1746, Avg Loss=0.1134, Time Left=7.80 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2509/3393 [19:53<07:33,  1.95batch/s, Batch Loss=0.1746, Avg Loss=0.1134, Time Left=7.80 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2509/3393 [19:53<07:33,  1.95batch/s, Batch Loss=0.0290, Avg Loss=0.1134, Time Left=7.79 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2510/3393 [19:53<07:25,  1.98batch/s, Batch Loss=0.0290, Avg Loss=0.1134, Time Left=7.79 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2510/3393 [19:54<07:25,  1.98batch/s, Batch Loss=0.1789, Avg Loss=0.1134, Time Left=7.78 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2511/3393 [19:54<07:28,  1.97batch/s, Batch Loss=0.1789, Avg Loss=0.1134, Time Left=7.78 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2511/3393 [19:54<07:28,  1.97batch/s, Batch Loss=0.0214, Avg Loss=0.1134, Time Left=7.78 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2512/3393 [19:54<07:33,  1.94batch/s, Batch Loss=0.0214, Avg Loss=0.1134, Time Left=7.78 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2512/3393 [19:55<07:33,  1.94batch/s, Batch Loss=0.0367, Avg Loss=0.1133, Time Left=7.77 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2513/3393 [19:55<07:25,  1.98batch/s, Batch Loss=0.0367, Avg Loss=0.1133, Time Left=7.77 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2513/3393 [19:55<07:25,  1.98batch/s, Batch Loss=0.2971, Avg Loss=0.1134, Time Left=7.76 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2514/3393 [19:55<07:27,  1.96batch/s, Batch Loss=0.2971, Avg Loss=0.1134, Time Left=7.76 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2514/3393 [19:56<07:27,  1.96batch/s, Batch Loss=0.1313, Avg Loss=0.1134, Time Left=7.75 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2515/3393 [19:56<07:25,  1.97batch/s, Batch Loss=0.1313, Avg Loss=0.1134, Time Left=7.75 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2515/3393 [19:56<07:25,  1.97batch/s, Batch Loss=0.1261, Avg Loss=0.1134, Time Left=7.74 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2516/3393 [19:56<07:14,  2.02batch/s, Batch Loss=0.1261, Avg Loss=0.1134, Time Left=7.74 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2516/3393 [19:57<07:14,  2.02batch/s, Batch Loss=0.2388, Avg Loss=0.1135, Time Left=7.74 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2517/3393 [19:57<07:15,  2.01batch/s, Batch Loss=0.2388, Avg Loss=0.1135, Time Left=7.74 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2517/3393 [19:57<07:15,  2.01batch/s, Batch Loss=0.0174, Avg Loss=0.1134, Time Left=7.73 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2518/3393 [19:57<07:06,  2.05batch/s, Batch Loss=0.0174, Avg Loss=0.1134, Time Left=7.73 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2518/3393 [19:58<07:06,  2.05batch/s, Batch Loss=0.3593, Avg Loss=0.1135, Time Left=7.72 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2519/3393 [19:58<07:13,  2.02batch/s, Batch Loss=0.3593, Avg Loss=0.1135, Time Left=7.72 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2519/3393 [19:58<07:13,  2.02batch/s, Batch Loss=0.1735, Avg Loss=0.1135, Time Left=7.71 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2520/3393 [19:58<07:05,  2.05batch/s, Batch Loss=0.1735, Avg Loss=0.1135, Time Left=7.71 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2520/3393 [19:59<07:05,  2.05batch/s, Batch Loss=0.0264, Avg Loss=0.1135, Time Left=7.70 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2521/3393 [19:59<07:04,  2.05batch/s, Batch Loss=0.0264, Avg Loss=0.1135, Time Left=7.70 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2521/3393 [19:59<07:04,  2.05batch/s, Batch Loss=0.1760, Avg Loss=0.1135, Time Left=7.70 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2522/3393 [19:59<07:02,  2.06batch/s, Batch Loss=0.1760, Avg Loss=0.1135, Time Left=7.70 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2522/3393 [20:00<07:02,  2.06batch/s, Batch Loss=0.0851, Avg Loss=0.1135, Time Left=7.69 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2523/3393 [20:00<06:58,  2.08batch/s, Batch Loss=0.0851, Avg Loss=0.1135, Time Left=7.69 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2523/3393 [20:00<06:58,  2.08batch/s, Batch Loss=0.0069, Avg Loss=0.1135, Time Left=7.68 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2524/3393 [20:00<06:58,  2.08batch/s, Batch Loss=0.0069, Avg Loss=0.1135, Time Left=7.68 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2524/3393 [20:01<06:58,  2.08batch/s, Batch Loss=0.0940, Avg Loss=0.1135, Time Left=7.67 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2525/3393 [20:01<07:02,  2.05batch/s, Batch Loss=0.0940, Avg Loss=0.1135, Time Left=7.67 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2525/3393 [20:01<07:02,  2.05batch/s, Batch Loss=0.0614, Avg Loss=0.1135, Time Left=7.66 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2526/3393 [20:01<07:03,  2.05batch/s, Batch Loss=0.0614, Avg Loss=0.1135, Time Left=7.66 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2526/3393 [20:01<07:03,  2.05batch/s, Batch Loss=0.1320, Avg Loss=0.1135, Time Left=7.65 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2527/3393 [20:01<06:55,  2.08batch/s, Batch Loss=0.1320, Avg Loss=0.1135, Time Left=7.65 \u001b[A\n",
      "Epoch 2/3 - Training:  74%|▋| 2527/3393 [20:02<06:55,  2.08batch/s, Batch Loss=0.4750, Avg Loss=0.1136, Time Left=7.65 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2528/3393 [20:02<06:55,  2.08batch/s, Batch Loss=0.4750, Avg Loss=0.1136, Time Left=7.65 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2528/3393 [20:02<06:55,  2.08batch/s, Batch Loss=0.0463, Avg Loss=0.1136, Time Left=7.64 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2529/3393 [20:02<06:52,  2.10batch/s, Batch Loss=0.0463, Avg Loss=0.1136, Time Left=7.64 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2529/3393 [20:03<06:52,  2.10batch/s, Batch Loss=0.2012, Avg Loss=0.1136, Time Left=7.63 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2530/3393 [20:03<06:53,  2.09batch/s, Batch Loss=0.2012, Avg Loss=0.1136, Time Left=7.63 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2530/3393 [20:03<06:53,  2.09batch/s, Batch Loss=0.1949, Avg Loss=0.1136, Time Left=7.62 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2531/3393 [20:03<06:53,  2.08batch/s, Batch Loss=0.1949, Avg Loss=0.1136, Time Left=7.62 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2531/3393 [20:04<06:53,  2.08batch/s, Batch Loss=0.0796, Avg Loss=0.1136, Time Left=7.61 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2532/3393 [20:04<06:50,  2.10batch/s, Batch Loss=0.0796, Avg Loss=0.1136, Time Left=7.61 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2532/3393 [20:04<06:50,  2.10batch/s, Batch Loss=0.0568, Avg Loss=0.1136, Time Left=7.60 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2533/3393 [20:04<06:51,  2.09batch/s, Batch Loss=0.0568, Avg Loss=0.1136, Time Left=7.60 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2533/3393 [20:05<06:51,  2.09batch/s, Batch Loss=0.0637, Avg Loss=0.1136, Time Left=7.60 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2534/3393 [20:05<06:52,  2.08batch/s, Batch Loss=0.0637, Avg Loss=0.1136, Time Left=7.60 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2534/3393 [20:05<06:52,  2.08batch/s, Batch Loss=0.0380, Avg Loss=0.1136, Time Left=7.59 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2535/3393 [20:05<06:49,  2.10batch/s, Batch Loss=0.0380, Avg Loss=0.1136, Time Left=7.59 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2535/3393 [20:06<06:49,  2.10batch/s, Batch Loss=0.0657, Avg Loss=0.1135, Time Left=7.58 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2536/3393 [20:06<06:49,  2.09batch/s, Batch Loss=0.0657, Avg Loss=0.1135, Time Left=7.58 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2536/3393 [20:06<06:49,  2.09batch/s, Batch Loss=0.0728, Avg Loss=0.1135, Time Left=7.57 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2537/3393 [20:06<06:50,  2.08batch/s, Batch Loss=0.0728, Avg Loss=0.1135, Time Left=7.57 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2537/3393 [20:07<06:50,  2.08batch/s, Batch Loss=0.1550, Avg Loss=0.1135, Time Left=7.56 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2538/3393 [20:07<06:47,  2.10batch/s, Batch Loss=0.1550, Avg Loss=0.1135, Time Left=7.56 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2538/3393 [20:07<06:47,  2.10batch/s, Batch Loss=0.0297, Avg Loss=0.1135, Time Left=7.56 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  75%|▋| 2539/3393 [20:07<06:48,  2.09batch/s, Batch Loss=0.0297, Avg Loss=0.1135, Time Left=7.56 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2539/3393 [20:08<06:48,  2.09batch/s, Batch Loss=0.1198, Avg Loss=0.1135, Time Left=7.55 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2540/3393 [20:08<06:52,  2.07batch/s, Batch Loss=0.1198, Avg Loss=0.1135, Time Left=7.55 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2540/3393 [20:08<06:52,  2.07batch/s, Batch Loss=0.1610, Avg Loss=0.1135, Time Left=7.54 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2541/3393 [20:08<06:52,  2.06batch/s, Batch Loss=0.1610, Avg Loss=0.1135, Time Left=7.54 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2541/3393 [20:09<06:52,  2.06batch/s, Batch Loss=0.0350, Avg Loss=0.1135, Time Left=7.53 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2542/3393 [20:09<07:00,  2.02batch/s, Batch Loss=0.0350, Avg Loss=0.1135, Time Left=7.53 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2542/3393 [20:09<07:00,  2.02batch/s, Batch Loss=0.1323, Avg Loss=0.1135, Time Left=7.52 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2543/3393 [20:09<06:58,  2.03batch/s, Batch Loss=0.1323, Avg Loss=0.1135, Time Left=7.52 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2543/3393 [20:10<06:58,  2.03batch/s, Batch Loss=0.0414, Avg Loss=0.1135, Time Left=7.51 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2544/3393 [20:10<07:02,  2.01batch/s, Batch Loss=0.0414, Avg Loss=0.1135, Time Left=7.51 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▋| 2544/3393 [20:10<07:02,  2.01batch/s, Batch Loss=0.0938, Avg Loss=0.1135, Time Left=7.51 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2545/3393 [20:10<06:59,  2.02batch/s, Batch Loss=0.0938, Avg Loss=0.1135, Time Left=7.51 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2545/3393 [20:11<06:59,  2.02batch/s, Batch Loss=0.0019, Avg Loss=0.1134, Time Left=7.50 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2546/3393 [20:11<07:00,  2.02batch/s, Batch Loss=0.0019, Avg Loss=0.1134, Time Left=7.50 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2546/3393 [20:11<07:00,  2.02batch/s, Batch Loss=0.2426, Avg Loss=0.1135, Time Left=7.49 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2547/3393 [20:11<06:56,  2.03batch/s, Batch Loss=0.2426, Avg Loss=0.1135, Time Left=7.49 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2547/3393 [20:12<06:56,  2.03batch/s, Batch Loss=0.0649, Avg Loss=0.1135, Time Left=7.48 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2548/3393 [20:12<06:49,  2.06batch/s, Batch Loss=0.0649, Avg Loss=0.1135, Time Left=7.48 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2548/3393 [20:12<06:49,  2.06batch/s, Batch Loss=0.0502, Avg Loss=0.1134, Time Left=7.47 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2549/3393 [20:12<06:49,  2.06batch/s, Batch Loss=0.0502, Avg Loss=0.1134, Time Left=7.47 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2549/3393 [20:13<06:49,  2.06batch/s, Batch Loss=0.0022, Avg Loss=0.1134, Time Left=7.47 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2550/3393 [20:13<06:48,  2.07batch/s, Batch Loss=0.0022, Avg Loss=0.1134, Time Left=7.47 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2550/3393 [20:13<06:48,  2.07batch/s, Batch Loss=0.0926, Avg Loss=0.1134, Time Left=7.46 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2551/3393 [20:13<06:44,  2.08batch/s, Batch Loss=0.0926, Avg Loss=0.1134, Time Left=7.46 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2551/3393 [20:14<06:44,  2.08batch/s, Batch Loss=0.2196, Avg Loss=0.1134, Time Left=7.45 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2552/3393 [20:14<06:43,  2.08batch/s, Batch Loss=0.2196, Avg Loss=0.1134, Time Left=7.45 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2552/3393 [20:14<06:43,  2.08batch/s, Batch Loss=0.3176, Avg Loss=0.1135, Time Left=7.44 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2553/3393 [20:14<06:40,  2.10batch/s, Batch Loss=0.3176, Avg Loss=0.1135, Time Left=7.44 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2553/3393 [20:15<06:40,  2.10batch/s, Batch Loss=0.4206, Avg Loss=0.1136, Time Left=7.43 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2554/3393 [20:15<06:37,  2.11batch/s, Batch Loss=0.4206, Avg Loss=0.1136, Time Left=7.43 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2554/3393 [20:15<06:37,  2.11batch/s, Batch Loss=0.2927, Avg Loss=0.1137, Time Left=7.42 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2555/3393 [20:15<06:45,  2.06batch/s, Batch Loss=0.2927, Avg Loss=0.1137, Time Left=7.42 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2555/3393 [20:15<06:45,  2.06batch/s, Batch Loss=0.0309, Avg Loss=0.1137, Time Left=7.42 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2556/3393 [20:15<06:43,  2.08batch/s, Batch Loss=0.0309, Avg Loss=0.1137, Time Left=7.42 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2556/3393 [20:16<06:43,  2.08batch/s, Batch Loss=0.0702, Avg Loss=0.1136, Time Left=7.41 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2557/3393 [20:16<06:39,  2.10batch/s, Batch Loss=0.0702, Avg Loss=0.1136, Time Left=7.41 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2557/3393 [20:16<06:39,  2.10batch/s, Batch Loss=0.1560, Avg Loss=0.1137, Time Left=7.40 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2558/3393 [20:16<06:40,  2.09batch/s, Batch Loss=0.1560, Avg Loss=0.1137, Time Left=7.40 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2558/3393 [20:17<06:40,  2.09batch/s, Batch Loss=0.0484, Avg Loss=0.1136, Time Left=7.39 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2559/3393 [20:17<06:41,  2.08batch/s, Batch Loss=0.0484, Avg Loss=0.1136, Time Left=7.39 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2559/3393 [20:17<06:41,  2.08batch/s, Batch Loss=0.0118, Avg Loss=0.1136, Time Left=7.38 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2560/3393 [20:17<06:36,  2.10batch/s, Batch Loss=0.0118, Avg Loss=0.1136, Time Left=7.38 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2560/3393 [20:18<06:36,  2.10batch/s, Batch Loss=0.1717, Avg Loss=0.1136, Time Left=7.38 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2561/3393 [20:18<06:38,  2.09batch/s, Batch Loss=0.1717, Avg Loss=0.1136, Time Left=7.38 \u001b[A\n",
      "Epoch 2/3 - Training:  75%|▊| 2561/3393 [20:18<06:38,  2.09batch/s, Batch Loss=0.1611, Avg Loss=0.1136, Time Left=7.37 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2562/3393 [20:18<06:34,  2.10batch/s, Batch Loss=0.1611, Avg Loss=0.1136, Time Left=7.37 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2562/3393 [20:19<06:34,  2.10batch/s, Batch Loss=0.1598, Avg Loss=0.1137, Time Left=7.36 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2563/3393 [20:19<06:40,  2.07batch/s, Batch Loss=0.1598, Avg Loss=0.1137, Time Left=7.36 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2563/3393 [20:19<06:40,  2.07batch/s, Batch Loss=0.0868, Avg Loss=0.1136, Time Left=7.35 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2564/3393 [20:19<06:47,  2.03batch/s, Batch Loss=0.0868, Avg Loss=0.1136, Time Left=7.35 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2564/3393 [20:20<06:47,  2.03batch/s, Batch Loss=0.1203, Avg Loss=0.1136, Time Left=7.34 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2565/3393 [20:20<06:46,  2.04batch/s, Batch Loss=0.1203, Avg Loss=0.1136, Time Left=7.34 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2565/3393 [20:20<06:46,  2.04batch/s, Batch Loss=0.1091, Avg Loss=0.1136, Time Left=7.33 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2566/3393 [20:20<06:47,  2.03batch/s, Batch Loss=0.1091, Avg Loss=0.1136, Time Left=7.33 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2566/3393 [20:21<06:47,  2.03batch/s, Batch Loss=0.0883, Avg Loss=0.1136, Time Left=7.33 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2567/3393 [20:21<06:41,  2.06batch/s, Batch Loss=0.0883, Avg Loss=0.1136, Time Left=7.33 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2567/3393 [20:21<06:41,  2.06batch/s, Batch Loss=0.0778, Avg Loss=0.1136, Time Left=7.32 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2568/3393 [20:21<06:40,  2.06batch/s, Batch Loss=0.0778, Avg Loss=0.1136, Time Left=7.32 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2568/3393 [20:22<06:40,  2.06batch/s, Batch Loss=0.1501, Avg Loss=0.1136, Time Left=7.31 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2569/3393 [20:22<06:50,  2.01batch/s, Batch Loss=0.1501, Avg Loss=0.1136, Time Left=7.31 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2569/3393 [20:22<06:50,  2.01batch/s, Batch Loss=0.1205, Avg Loss=0.1136, Time Left=7.30 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2570/3393 [20:22<06:51,  2.00batch/s, Batch Loss=0.1205, Avg Loss=0.1136, Time Left=7.30 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2570/3393 [20:23<06:51,  2.00batch/s, Batch Loss=0.2217, Avg Loss=0.1137, Time Left=7.29 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2571/3393 [20:23<06:54,  1.98batch/s, Batch Loss=0.2217, Avg Loss=0.1137, Time Left=7.29 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2571/3393 [20:23<06:54,  1.98batch/s, Batch Loss=0.0875, Avg Loss=0.1137, Time Left=7.29 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  76%|▊| 2572/3393 [20:23<06:53,  1.98batch/s, Batch Loss=0.0875, Avg Loss=0.1137, Time Left=7.29 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2572/3393 [20:24<06:53,  1.98batch/s, Batch Loss=0.1186, Avg Loss=0.1137, Time Left=7.28 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2573/3393 [20:24<06:52,  1.99batch/s, Batch Loss=0.1186, Avg Loss=0.1137, Time Left=7.28 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2573/3393 [20:24<06:52,  1.99batch/s, Batch Loss=0.1233, Avg Loss=0.1137, Time Left=7.27 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2574/3393 [20:24<06:43,  2.03batch/s, Batch Loss=0.1233, Avg Loss=0.1137, Time Left=7.27 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2574/3393 [20:25<06:43,  2.03batch/s, Batch Loss=0.0710, Avg Loss=0.1137, Time Left=7.26 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2575/3393 [20:25<06:44,  2.02batch/s, Batch Loss=0.0710, Avg Loss=0.1137, Time Left=7.26 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2575/3393 [20:25<06:44,  2.02batch/s, Batch Loss=0.0436, Avg Loss=0.1136, Time Left=7.25 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2576/3393 [20:25<06:48,  2.00batch/s, Batch Loss=0.0436, Avg Loss=0.1136, Time Left=7.25 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2576/3393 [20:26<06:48,  2.00batch/s, Batch Loss=0.1415, Avg Loss=0.1136, Time Left=7.25 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2577/3393 [20:26<06:45,  2.01batch/s, Batch Loss=0.1415, Avg Loss=0.1136, Time Left=7.25 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2577/3393 [20:26<06:45,  2.01batch/s, Batch Loss=0.0737, Avg Loss=0.1136, Time Left=7.24 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2578/3393 [20:26<06:45,  2.01batch/s, Batch Loss=0.0737, Avg Loss=0.1136, Time Left=7.24 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2578/3393 [20:27<06:45,  2.01batch/s, Batch Loss=0.1213, Avg Loss=0.1136, Time Left=7.23 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2579/3393 [20:27<06:41,  2.03batch/s, Batch Loss=0.1213, Avg Loss=0.1136, Time Left=7.23 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2579/3393 [20:27<06:41,  2.03batch/s, Batch Loss=0.0396, Avg Loss=0.1136, Time Left=7.22 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2580/3393 [20:27<06:34,  2.06batch/s, Batch Loss=0.0396, Avg Loss=0.1136, Time Left=7.22 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2580/3393 [20:28<06:34,  2.06batch/s, Batch Loss=0.1412, Avg Loss=0.1136, Time Left=7.21 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2581/3393 [20:28<06:45,  2.00batch/s, Batch Loss=0.1412, Avg Loss=0.1136, Time Left=7.21 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2581/3393 [20:28<06:45,  2.00batch/s, Batch Loss=0.2237, Avg Loss=0.1137, Time Left=7.20 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2582/3393 [20:28<06:37,  2.04batch/s, Batch Loss=0.2237, Avg Loss=0.1137, Time Left=7.20 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2582/3393 [20:29<06:37,  2.04batch/s, Batch Loss=0.1721, Avg Loss=0.1137, Time Left=7.20 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2583/3393 [20:29<06:46,  1.99batch/s, Batch Loss=0.1721, Avg Loss=0.1137, Time Left=7.20 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2583/3393 [20:29<06:46,  1.99batch/s, Batch Loss=0.0540, Avg Loss=0.1137, Time Left=7.19 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2584/3393 [20:29<06:42,  2.01batch/s, Batch Loss=0.0540, Avg Loss=0.1137, Time Left=7.19 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2584/3393 [20:30<06:42,  2.01batch/s, Batch Loss=0.0892, Avg Loss=0.1136, Time Left=7.18 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2585/3393 [20:30<06:50,  1.97batch/s, Batch Loss=0.0892, Avg Loss=0.1136, Time Left=7.18 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2585/3393 [20:30<06:50,  1.97batch/s, Batch Loss=0.0288, Avg Loss=0.1136, Time Left=7.17 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2586/3393 [20:30<06:44,  2.00batch/s, Batch Loss=0.0288, Avg Loss=0.1136, Time Left=7.17 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2586/3393 [20:31<06:44,  2.00batch/s, Batch Loss=0.0588, Avg Loss=0.1136, Time Left=7.16 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2587/3393 [20:31<06:35,  2.04batch/s, Batch Loss=0.0588, Avg Loss=0.1136, Time Left=7.16 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2587/3393 [20:31<06:35,  2.04batch/s, Batch Loss=0.0239, Avg Loss=0.1136, Time Left=7.16 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2588/3393 [20:31<06:37,  2.03batch/s, Batch Loss=0.0239, Avg Loss=0.1136, Time Left=7.16 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2588/3393 [20:32<06:37,  2.03batch/s, Batch Loss=0.0426, Avg Loss=0.1135, Time Left=7.15 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2589/3393 [20:32<06:34,  2.04batch/s, Batch Loss=0.0426, Avg Loss=0.1135, Time Left=7.15 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2589/3393 [20:32<06:34,  2.04batch/s, Batch Loss=0.0125, Avg Loss=0.1135, Time Left=7.14 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2590/3393 [20:32<06:39,  2.01batch/s, Batch Loss=0.0125, Avg Loss=0.1135, Time Left=7.14 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2590/3393 [20:33<06:39,  2.01batch/s, Batch Loss=0.1054, Avg Loss=0.1135, Time Left=7.13 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2591/3393 [20:33<06:36,  2.02batch/s, Batch Loss=0.1054, Avg Loss=0.1135, Time Left=7.13 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2591/3393 [20:33<06:36,  2.02batch/s, Batch Loss=0.2214, Avg Loss=0.1135, Time Left=7.12 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2592/3393 [20:33<06:29,  2.06batch/s, Batch Loss=0.2214, Avg Loss=0.1135, Time Left=7.12 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2592/3393 [20:34<06:29,  2.06batch/s, Batch Loss=0.1270, Avg Loss=0.1135, Time Left=7.12 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2593/3393 [20:34<06:28,  2.06batch/s, Batch Loss=0.1270, Avg Loss=0.1135, Time Left=7.12 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2593/3393 [20:34<06:28,  2.06batch/s, Batch Loss=0.1151, Avg Loss=0.1135, Time Left=7.11 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2594/3393 [20:34<06:26,  2.07batch/s, Batch Loss=0.1151, Avg Loss=0.1135, Time Left=7.11 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2594/3393 [20:35<06:26,  2.07batch/s, Batch Loss=0.1168, Avg Loss=0.1135, Time Left=7.10 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2595/3393 [20:35<06:23,  2.08batch/s, Batch Loss=0.1168, Avg Loss=0.1135, Time Left=7.10 \u001b[A\n",
      "Epoch 2/3 - Training:  76%|▊| 2595/3393 [20:35<06:23,  2.08batch/s, Batch Loss=0.0502, Avg Loss=0.1135, Time Left=7.09 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2596/3393 [20:35<06:23,  2.08batch/s, Batch Loss=0.0502, Avg Loss=0.1135, Time Left=7.09 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2596/3393 [20:36<06:23,  2.08batch/s, Batch Loss=0.0594, Avg Loss=0.1135, Time Left=7.08 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2597/3393 [20:36<06:19,  2.10batch/s, Batch Loss=0.0594, Avg Loss=0.1135, Time Left=7.08 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2597/3393 [20:36<06:19,  2.10batch/s, Batch Loss=0.0018, Avg Loss=0.1134, Time Left=7.07 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2598/3393 [20:36<06:16,  2.11batch/s, Batch Loss=0.0018, Avg Loss=0.1134, Time Left=7.07 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2598/3393 [20:37<06:16,  2.11batch/s, Batch Loss=0.0232, Avg Loss=0.1134, Time Left=7.07 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2599/3393 [20:37<06:29,  2.04batch/s, Batch Loss=0.0232, Avg Loss=0.1134, Time Left=7.07 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2599/3393 [20:37<06:29,  2.04batch/s, Batch Loss=0.2270, Avg Loss=0.1135, Time Left=7.06 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2600/3393 [20:37<06:24,  2.06batch/s, Batch Loss=0.2270, Avg Loss=0.1135, Time Left=7.06 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2600/3393 [20:38<06:24,  2.06batch/s, Batch Loss=0.0024, Avg Loss=0.1134, Time Left=7.05 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2601/3393 [20:38<06:34,  2.01batch/s, Batch Loss=0.0024, Avg Loss=0.1134, Time Left=7.05 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2601/3393 [20:38<06:34,  2.01batch/s, Batch Loss=0.0180, Avg Loss=0.1134, Time Left=7.04 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2602/3393 [20:38<06:27,  2.04batch/s, Batch Loss=0.0180, Avg Loss=0.1134, Time Left=7.04 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2602/3393 [20:39<06:27,  2.04batch/s, Batch Loss=0.1014, Avg Loss=0.1134, Time Left=7.03 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2603/3393 [20:39<06:29,  2.03batch/s, Batch Loss=0.1014, Avg Loss=0.1134, Time Left=7.03 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2603/3393 [20:39<06:29,  2.03batch/s, Batch Loss=0.0049, Avg Loss=0.1133, Time Left=7.03 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2604/3393 [20:39<06:24,  2.05batch/s, Batch Loss=0.0049, Avg Loss=0.1133, Time Left=7.03 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2604/3393 [20:40<06:24,  2.05batch/s, Batch Loss=0.0057, Avg Loss=0.1133, Time Left=7.02 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  77%|▊| 2605/3393 [20:40<06:17,  2.09batch/s, Batch Loss=0.0057, Avg Loss=0.1133, Time Left=7.02 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2605/3393 [20:40<06:17,  2.09batch/s, Batch Loss=0.0165, Avg Loss=0.1132, Time Left=7.01 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2606/3393 [20:40<06:18,  2.08batch/s, Batch Loss=0.0165, Avg Loss=0.1132, Time Left=7.01 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2606/3393 [20:40<06:18,  2.08batch/s, Batch Loss=0.1143, Avg Loss=0.1132, Time Left=7.00 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2607/3393 [20:40<06:14,  2.10batch/s, Batch Loss=0.1143, Avg Loss=0.1132, Time Left=7.00 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2607/3393 [20:41<06:14,  2.10batch/s, Batch Loss=0.0195, Avg Loss=0.1132, Time Left=6.99 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2608/3393 [20:41<06:14,  2.10batch/s, Batch Loss=0.0195, Avg Loss=0.1132, Time Left=6.99 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2608/3393 [20:41<06:14,  2.10batch/s, Batch Loss=0.0246, Avg Loss=0.1132, Time Left=6.98 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2609/3393 [20:41<06:12,  2.10batch/s, Batch Loss=0.0246, Avg Loss=0.1132, Time Left=6.98 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2609/3393 [20:42<06:12,  2.10batch/s, Batch Loss=0.0547, Avg Loss=0.1131, Time Left=6.98 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2610/3393 [20:42<06:21,  2.05batch/s, Batch Loss=0.0547, Avg Loss=0.1131, Time Left=6.98 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2610/3393 [20:42<06:21,  2.05batch/s, Batch Loss=0.4413, Avg Loss=0.1133, Time Left=6.97 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2611/3393 [20:42<06:16,  2.08batch/s, Batch Loss=0.4413, Avg Loss=0.1133, Time Left=6.97 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2611/3393 [20:43<06:16,  2.08batch/s, Batch Loss=0.0699, Avg Loss=0.1133, Time Left=6.96 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2612/3393 [20:43<06:24,  2.03batch/s, Batch Loss=0.0699, Avg Loss=0.1133, Time Left=6.96 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2612/3393 [20:43<06:24,  2.03batch/s, Batch Loss=0.2350, Avg Loss=0.1133, Time Left=6.95 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2613/3393 [20:43<06:22,  2.04batch/s, Batch Loss=0.2350, Avg Loss=0.1133, Time Left=6.95 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2613/3393 [20:44<06:22,  2.04batch/s, Batch Loss=0.0282, Avg Loss=0.1133, Time Left=6.94 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2614/3393 [20:44<06:20,  2.05batch/s, Batch Loss=0.0282, Avg Loss=0.1133, Time Left=6.94 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2614/3393 [20:44<06:20,  2.05batch/s, Batch Loss=0.4309, Avg Loss=0.1134, Time Left=6.93 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2615/3393 [20:44<06:18,  2.06batch/s, Batch Loss=0.4309, Avg Loss=0.1134, Time Left=6.93 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2615/3393 [20:45<06:18,  2.06batch/s, Batch Loss=0.0580, Avg Loss=0.1134, Time Left=6.93 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2616/3393 [20:45<06:17,  2.06batch/s, Batch Loss=0.0580, Avg Loss=0.1134, Time Left=6.93 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2616/3393 [20:45<06:17,  2.06batch/s, Batch Loss=0.0502, Avg Loss=0.1134, Time Left=6.92 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2617/3393 [20:45<06:24,  2.02batch/s, Batch Loss=0.0502, Avg Loss=0.1134, Time Left=6.92 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2617/3393 [20:46<06:24,  2.02batch/s, Batch Loss=0.0023, Avg Loss=0.1133, Time Left=6.91 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2618/3393 [20:46<06:17,  2.05batch/s, Batch Loss=0.0023, Avg Loss=0.1133, Time Left=6.91 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2618/3393 [20:46<06:17,  2.05batch/s, Batch Loss=0.2730, Avg Loss=0.1134, Time Left=6.90 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2619/3393 [20:46<06:12,  2.08batch/s, Batch Loss=0.2730, Avg Loss=0.1134, Time Left=6.90 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2619/3393 [20:47<06:12,  2.08batch/s, Batch Loss=0.1498, Avg Loss=0.1134, Time Left=6.89 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2620/3393 [20:47<06:16,  2.05batch/s, Batch Loss=0.1498, Avg Loss=0.1134, Time Left=6.89 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2620/3393 [20:47<06:16,  2.05batch/s, Batch Loss=0.2530, Avg Loss=0.1134, Time Left=6.89 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2621/3393 [20:47<06:15,  2.06batch/s, Batch Loss=0.2530, Avg Loss=0.1134, Time Left=6.89 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2621/3393 [20:48<06:15,  2.06batch/s, Batch Loss=0.0962, Avg Loss=0.1134, Time Left=6.88 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2622/3393 [20:48<06:10,  2.08batch/s, Batch Loss=0.0962, Avg Loss=0.1134, Time Left=6.88 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2622/3393 [20:48<06:10,  2.08batch/s, Batch Loss=0.2092, Avg Loss=0.1135, Time Left=6.87 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2623/3393 [20:48<06:14,  2.06batch/s, Batch Loss=0.2092, Avg Loss=0.1135, Time Left=6.87 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2623/3393 [20:49<06:14,  2.06batch/s, Batch Loss=0.0888, Avg Loss=0.1135, Time Left=6.86 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2624/3393 [20:49<06:16,  2.04batch/s, Batch Loss=0.0888, Avg Loss=0.1135, Time Left=6.86 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2624/3393 [20:49<06:16,  2.04batch/s, Batch Loss=0.0347, Avg Loss=0.1134, Time Left=6.85 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2625/3393 [20:49<06:22,  2.01batch/s, Batch Loss=0.0347, Avg Loss=0.1134, Time Left=6.85 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2625/3393 [20:50<06:22,  2.01batch/s, Batch Loss=0.0110, Avg Loss=0.1134, Time Left=6.85 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2626/3393 [20:50<06:18,  2.02batch/s, Batch Loss=0.0110, Avg Loss=0.1134, Time Left=6.85 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2626/3393 [20:50<06:18,  2.02batch/s, Batch Loss=0.0033, Avg Loss=0.1133, Time Left=6.84 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2627/3393 [20:50<06:16,  2.03batch/s, Batch Loss=0.0033, Avg Loss=0.1133, Time Left=6.84 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2627/3393 [20:51<06:16,  2.03batch/s, Batch Loss=0.0953, Avg Loss=0.1133, Time Left=6.83 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2628/3393 [20:51<06:14,  2.04batch/s, Batch Loss=0.0953, Avg Loss=0.1133, Time Left=6.83 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2628/3393 [20:51<06:14,  2.04batch/s, Batch Loss=0.1524, Avg Loss=0.1134, Time Left=6.82 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2629/3393 [20:51<06:04,  2.09batch/s, Batch Loss=0.1524, Avg Loss=0.1134, Time Left=6.82 \u001b[A\n",
      "Epoch 2/3 - Training:  77%|▊| 2629/3393 [20:52<06:04,  2.09batch/s, Batch Loss=0.0800, Avg Loss=0.1133, Time Left=6.81 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2630/3393 [20:52<06:09,  2.07batch/s, Batch Loss=0.0800, Avg Loss=0.1133, Time Left=6.81 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2630/3393 [20:52<06:09,  2.07batch/s, Batch Loss=0.1102, Avg Loss=0.1133, Time Left=6.80 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2631/3393 [20:52<06:05,  2.09batch/s, Batch Loss=0.1102, Avg Loss=0.1133, Time Left=6.80 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2631/3393 [20:53<06:05,  2.09batch/s, Batch Loss=0.0587, Avg Loss=0.1133, Time Left=6.80 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2632/3393 [20:53<06:05,  2.08batch/s, Batch Loss=0.0587, Avg Loss=0.1133, Time Left=6.80 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2632/3393 [20:53<06:05,  2.08batch/s, Batch Loss=0.2319, Avg Loss=0.1134, Time Left=6.79 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2633/3393 [20:53<06:05,  2.08batch/s, Batch Loss=0.2319, Avg Loss=0.1134, Time Left=6.79 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2633/3393 [20:54<06:05,  2.08batch/s, Batch Loss=0.1571, Avg Loss=0.1134, Time Left=6.78 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2634/3393 [20:54<06:06,  2.07batch/s, Batch Loss=0.1571, Avg Loss=0.1134, Time Left=6.78 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2634/3393 [20:54<06:06,  2.07batch/s, Batch Loss=0.0383, Avg Loss=0.1134, Time Left=6.77 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2635/3393 [20:54<06:05,  2.07batch/s, Batch Loss=0.0383, Avg Loss=0.1134, Time Left=6.77 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2635/3393 [20:55<06:05,  2.07batch/s, Batch Loss=0.0418, Avg Loss=0.1133, Time Left=6.76 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2636/3393 [20:55<06:16,  2.01batch/s, Batch Loss=0.0418, Avg Loss=0.1133, Time Left=6.76 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2636/3393 [20:55<06:16,  2.01batch/s, Batch Loss=0.0018, Avg Loss=0.1133, Time Left=6.76 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2637/3393 [20:55<06:15,  2.02batch/s, Batch Loss=0.0018, Avg Loss=0.1133, Time Left=6.76 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2637/3393 [20:56<06:15,  2.02batch/s, Batch Loss=0.2209, Avg Loss=0.1133, Time Left=6.75 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  78%|▊| 2638/3393 [20:56<06:20,  1.98batch/s, Batch Loss=0.2209, Avg Loss=0.1133, Time Left=6.75 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2638/3393 [20:56<06:20,  1.98batch/s, Batch Loss=0.0185, Avg Loss=0.1133, Time Left=6.74 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2639/3393 [20:56<06:16,  2.00batch/s, Batch Loss=0.0185, Avg Loss=0.1133, Time Left=6.74 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2639/3393 [20:57<06:16,  2.00batch/s, Batch Loss=0.0487, Avg Loss=0.1133, Time Left=6.73 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2640/3393 [20:57<06:12,  2.02batch/s, Batch Loss=0.0487, Avg Loss=0.1133, Time Left=6.73 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2640/3393 [20:57<06:12,  2.02batch/s, Batch Loss=0.0475, Avg Loss=0.1132, Time Left=6.72 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2641/3393 [20:57<06:06,  2.05batch/s, Batch Loss=0.0475, Avg Loss=0.1132, Time Left=6.72 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2641/3393 [20:58<06:06,  2.05batch/s, Batch Loss=0.0154, Avg Loss=0.1132, Time Left=6.71 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2642/3393 [20:58<06:00,  2.08batch/s, Batch Loss=0.0154, Avg Loss=0.1132, Time Left=6.71 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2642/3393 [20:58<06:00,  2.08batch/s, Batch Loss=0.2223, Avg Loss=0.1132, Time Left=6.71 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2643/3393 [20:58<06:01,  2.08batch/s, Batch Loss=0.2223, Avg Loss=0.1132, Time Left=6.71 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2643/3393 [20:58<06:01,  2.08batch/s, Batch Loss=0.0267, Avg Loss=0.1132, Time Left=6.70 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2644/3393 [20:58<06:00,  2.08batch/s, Batch Loss=0.0267, Avg Loss=0.1132, Time Left=6.70 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2644/3393 [20:59<06:00,  2.08batch/s, Batch Loss=0.0951, Avg Loss=0.1132, Time Left=6.69 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2645/3393 [20:59<05:53,  2.11batch/s, Batch Loss=0.0951, Avg Loss=0.1132, Time Left=6.69 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2645/3393 [20:59<05:53,  2.11batch/s, Batch Loss=0.0034, Avg Loss=0.1132, Time Left=6.68 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2646/3393 [20:59<05:55,  2.10batch/s, Batch Loss=0.0034, Avg Loss=0.1132, Time Left=6.68 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2646/3393 [21:00<05:55,  2.10batch/s, Batch Loss=0.0264, Avg Loss=0.1131, Time Left=6.67 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2647/3393 [21:00<06:00,  2.07batch/s, Batch Loss=0.0264, Avg Loss=0.1131, Time Left=6.67 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2647/3393 [21:00<06:00,  2.07batch/s, Batch Loss=0.0784, Avg Loss=0.1131, Time Left=6.67 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2648/3393 [21:00<05:56,  2.09batch/s, Batch Loss=0.0784, Avg Loss=0.1131, Time Left=6.67 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2648/3393 [21:01<05:56,  2.09batch/s, Batch Loss=0.2746, Avg Loss=0.1132, Time Left=6.66 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2649/3393 [21:01<05:56,  2.08batch/s, Batch Loss=0.2746, Avg Loss=0.1132, Time Left=6.66 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2649/3393 [21:01<05:56,  2.08batch/s, Batch Loss=0.2817, Avg Loss=0.1132, Time Left=6.65 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2650/3393 [21:01<05:54,  2.10batch/s, Batch Loss=0.2817, Avg Loss=0.1132, Time Left=6.65 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2650/3393 [21:02<05:54,  2.10batch/s, Batch Loss=0.2007, Avg Loss=0.1133, Time Left=6.64 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2651/3393 [21:02<05:51,  2.11batch/s, Batch Loss=0.2007, Avg Loss=0.1133, Time Left=6.64 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2651/3393 [21:02<05:51,  2.11batch/s, Batch Loss=0.0125, Avg Loss=0.1132, Time Left=6.63 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2652/3393 [21:02<05:52,  2.10batch/s, Batch Loss=0.0125, Avg Loss=0.1132, Time Left=6.63 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2652/3393 [21:03<05:52,  2.10batch/s, Batch Loss=0.0432, Avg Loss=0.1132, Time Left=6.62 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2653/3393 [21:03<05:57,  2.07batch/s, Batch Loss=0.0432, Avg Loss=0.1132, Time Left=6.62 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2653/3393 [21:03<05:57,  2.07batch/s, Batch Loss=0.0986, Avg Loss=0.1132, Time Left=6.62 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2654/3393 [21:03<05:53,  2.09batch/s, Batch Loss=0.0986, Avg Loss=0.1132, Time Left=6.62 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2654/3393 [21:04<05:53,  2.09batch/s, Batch Loss=0.0709, Avg Loss=0.1132, Time Left=6.61 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2655/3393 [21:04<05:57,  2.06batch/s, Batch Loss=0.0709, Avg Loss=0.1132, Time Left=6.61 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2655/3393 [21:04<05:57,  2.06batch/s, Batch Loss=0.0639, Avg Loss=0.1132, Time Left=6.60 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2656/3393 [21:04<05:56,  2.06batch/s, Batch Loss=0.0639, Avg Loss=0.1132, Time Left=6.60 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2656/3393 [21:05<05:56,  2.06batch/s, Batch Loss=0.0662, Avg Loss=0.1131, Time Left=6.59 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2657/3393 [21:05<05:49,  2.11batch/s, Batch Loss=0.0662, Avg Loss=0.1131, Time Left=6.59 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2657/3393 [21:05<05:49,  2.11batch/s, Batch Loss=0.0618, Avg Loss=0.1131, Time Left=6.58 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2658/3393 [21:05<05:50,  2.10batch/s, Batch Loss=0.0618, Avg Loss=0.1131, Time Left=6.58 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2658/3393 [21:06<05:50,  2.10batch/s, Batch Loss=0.1381, Avg Loss=0.1131, Time Left=6.57 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2659/3393 [21:06<05:51,  2.09batch/s, Batch Loss=0.1381, Avg Loss=0.1131, Time Left=6.57 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2659/3393 [21:06<05:51,  2.09batch/s, Batch Loss=0.0025, Avg Loss=0.1131, Time Left=6.57 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2660/3393 [21:06<05:48,  2.10batch/s, Batch Loss=0.0025, Avg Loss=0.1131, Time Left=6.57 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2660/3393 [21:07<05:48,  2.10batch/s, Batch Loss=0.1531, Avg Loss=0.1131, Time Left=6.56 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2661/3393 [21:07<05:49,  2.09batch/s, Batch Loss=0.1531, Avg Loss=0.1131, Time Left=6.56 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2661/3393 [21:07<05:49,  2.09batch/s, Batch Loss=0.1169, Avg Loss=0.1131, Time Left=6.55 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2662/3393 [21:07<05:47,  2.11batch/s, Batch Loss=0.1169, Avg Loss=0.1131, Time Left=6.55 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2662/3393 [21:08<05:47,  2.11batch/s, Batch Loss=0.2489, Avg Loss=0.1132, Time Left=6.54 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2663/3393 [21:08<05:44,  2.12batch/s, Batch Loss=0.2489, Avg Loss=0.1132, Time Left=6.54 \u001b[A\n",
      "Epoch 2/3 - Training:  78%|▊| 2663/3393 [21:08<05:44,  2.12batch/s, Batch Loss=0.1593, Avg Loss=0.1132, Time Left=6.53 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2664/3393 [21:08<05:46,  2.10batch/s, Batch Loss=0.1593, Avg Loss=0.1132, Time Left=6.53 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2664/3393 [21:09<05:46,  2.10batch/s, Batch Loss=0.2539, Avg Loss=0.1132, Time Left=6.53 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2665/3393 [21:09<05:55,  2.05batch/s, Batch Loss=0.2539, Avg Loss=0.1132, Time Left=6.53 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2665/3393 [21:09<05:55,  2.05batch/s, Batch Loss=0.0044, Avg Loss=0.1132, Time Left=6.52 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2666/3393 [21:09<05:53,  2.05batch/s, Batch Loss=0.0044, Avg Loss=0.1132, Time Left=6.52 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2666/3393 [21:10<05:53,  2.05batch/s, Batch Loss=0.1218, Avg Loss=0.1132, Time Left=6.51 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2667/3393 [21:10<05:55,  2.04batch/s, Batch Loss=0.1218, Avg Loss=0.1132, Time Left=6.51 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2667/3393 [21:10<05:55,  2.04batch/s, Batch Loss=0.0123, Avg Loss=0.1132, Time Left=6.50 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2668/3393 [21:10<05:54,  2.05batch/s, Batch Loss=0.0123, Avg Loss=0.1132, Time Left=6.50 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2668/3393 [21:10<05:54,  2.05batch/s, Batch Loss=0.0385, Avg Loss=0.1131, Time Left=6.49 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2669/3393 [21:10<05:52,  2.05batch/s, Batch Loss=0.0385, Avg Loss=0.1131, Time Left=6.49 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2669/3393 [21:11<05:52,  2.05batch/s, Batch Loss=0.2208, Avg Loss=0.1132, Time Left=6.48 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2670/3393 [21:11<05:54,  2.04batch/s, Batch Loss=0.2208, Avg Loss=0.1132, Time Left=6.48 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2670/3393 [21:11<05:54,  2.04batch/s, Batch Loss=0.0767, Avg Loss=0.1132, Time Left=6.48 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  79%|▊| 2671/3393 [21:11<05:49,  2.07batch/s, Batch Loss=0.0767, Avg Loss=0.1132, Time Left=6.48 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2671/3393 [21:12<05:49,  2.07batch/s, Batch Loss=0.2942, Avg Loss=0.1132, Time Left=6.47 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2672/3393 [21:12<05:59,  2.01batch/s, Batch Loss=0.2942, Avg Loss=0.1132, Time Left=6.47 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2672/3393 [21:12<05:59,  2.01batch/s, Batch Loss=0.0299, Avg Loss=0.1132, Time Left=6.46 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2673/3393 [21:12<05:56,  2.02batch/s, Batch Loss=0.0299, Avg Loss=0.1132, Time Left=6.46 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2673/3393 [21:13<05:56,  2.02batch/s, Batch Loss=0.1218, Avg Loss=0.1132, Time Left=6.45 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2674/3393 [21:13<05:53,  2.03batch/s, Batch Loss=0.1218, Avg Loss=0.1132, Time Left=6.45 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2674/3393 [21:13<05:53,  2.03batch/s, Batch Loss=0.1330, Avg Loss=0.1132, Time Left=6.44 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2675/3393 [21:13<05:44,  2.09batch/s, Batch Loss=0.1330, Avg Loss=0.1132, Time Left=6.44 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2675/3393 [21:14<05:44,  2.09batch/s, Batch Loss=0.0284, Avg Loss=0.1132, Time Left=6.44 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2676/3393 [21:14<05:41,  2.10batch/s, Batch Loss=0.0284, Avg Loss=0.1132, Time Left=6.44 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2676/3393 [21:14<05:41,  2.10batch/s, Batch Loss=0.0065, Avg Loss=0.1131, Time Left=6.43 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2677/3393 [21:14<05:38,  2.11batch/s, Batch Loss=0.0065, Avg Loss=0.1131, Time Left=6.43 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2677/3393 [21:15<05:38,  2.11batch/s, Batch Loss=0.0181, Avg Loss=0.1131, Time Left=6.42 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2678/3393 [21:15<05:47,  2.06batch/s, Batch Loss=0.0181, Avg Loss=0.1131, Time Left=6.42 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2678/3393 [21:15<05:47,  2.06batch/s, Batch Loss=0.0258, Avg Loss=0.1131, Time Left=6.41 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2679/3393 [21:15<05:46,  2.06batch/s, Batch Loss=0.0258, Avg Loss=0.1131, Time Left=6.41 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2679/3393 [21:16<05:46,  2.06batch/s, Batch Loss=0.0576, Avg Loss=0.1130, Time Left=6.40 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2680/3393 [21:16<05:49,  2.04batch/s, Batch Loss=0.0576, Avg Loss=0.1130, Time Left=6.40 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2680/3393 [21:16<05:49,  2.04batch/s, Batch Loss=0.0397, Avg Loss=0.1130, Time Left=6.39 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2681/3393 [21:16<05:43,  2.07batch/s, Batch Loss=0.0397, Avg Loss=0.1130, Time Left=6.39 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2681/3393 [21:17<05:43,  2.07batch/s, Batch Loss=0.0594, Avg Loss=0.1130, Time Left=6.39 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2682/3393 [21:17<05:47,  2.05batch/s, Batch Loss=0.0594, Avg Loss=0.1130, Time Left=6.39 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2682/3393 [21:17<05:47,  2.05batch/s, Batch Loss=0.0763, Avg Loss=0.1130, Time Left=6.38 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2683/3393 [21:17<05:47,  2.04batch/s, Batch Loss=0.0763, Avg Loss=0.1130, Time Left=6.38 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2683/3393 [21:18<05:47,  2.04batch/s, Batch Loss=0.0649, Avg Loss=0.1130, Time Left=6.37 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2684/3393 [21:18<05:44,  2.06batch/s, Batch Loss=0.0649, Avg Loss=0.1130, Time Left=6.37 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2684/3393 [21:18<05:44,  2.06batch/s, Batch Loss=0.0659, Avg Loss=0.1129, Time Left=6.36 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2685/3393 [21:18<05:39,  2.08batch/s, Batch Loss=0.0659, Avg Loss=0.1129, Time Left=6.36 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2685/3393 [21:19<05:39,  2.08batch/s, Batch Loss=0.0076, Avg Loss=0.1129, Time Left=6.35 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2686/3393 [21:19<05:39,  2.08batch/s, Batch Loss=0.0076, Avg Loss=0.1129, Time Left=6.35 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2686/3393 [21:19<05:39,  2.08batch/s, Batch Loss=0.0176, Avg Loss=0.1129, Time Left=6.35 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2687/3393 [21:19<05:36,  2.10batch/s, Batch Loss=0.0176, Avg Loss=0.1129, Time Left=6.35 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2687/3393 [21:20<05:36,  2.10batch/s, Batch Loss=0.1659, Avg Loss=0.1129, Time Left=6.34 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2688/3393 [21:20<05:37,  2.09batch/s, Batch Loss=0.1659, Avg Loss=0.1129, Time Left=6.34 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2688/3393 [21:20<05:37,  2.09batch/s, Batch Loss=0.0205, Avg Loss=0.1129, Time Left=6.33 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2689/3393 [21:20<05:38,  2.08batch/s, Batch Loss=0.0205, Avg Loss=0.1129, Time Left=6.33 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2689/3393 [21:21<05:38,  2.08batch/s, Batch Loss=0.0773, Avg Loss=0.1128, Time Left=6.32 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2690/3393 [21:21<05:35,  2.10batch/s, Batch Loss=0.0773, Avg Loss=0.1128, Time Left=6.32 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2690/3393 [21:21<05:35,  2.10batch/s, Batch Loss=0.0235, Avg Loss=0.1128, Time Left=6.31 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2691/3393 [21:21<05:35,  2.09batch/s, Batch Loss=0.0235, Avg Loss=0.1128, Time Left=6.31 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2691/3393 [21:22<05:35,  2.09batch/s, Batch Loss=0.0717, Avg Loss=0.1128, Time Left=6.30 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2692/3393 [21:22<05:36,  2.08batch/s, Batch Loss=0.0717, Avg Loss=0.1128, Time Left=6.30 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2692/3393 [21:22<05:36,  2.08batch/s, Batch Loss=0.3748, Avg Loss=0.1129, Time Left=6.30 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2693/3393 [21:22<05:33,  2.10batch/s, Batch Loss=0.3748, Avg Loss=0.1129, Time Left=6.30 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2693/3393 [21:23<05:33,  2.10batch/s, Batch Loss=0.1051, Avg Loss=0.1129, Time Left=6.29 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2694/3393 [21:23<05:27,  2.13batch/s, Batch Loss=0.1051, Avg Loss=0.1129, Time Left=6.29 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2694/3393 [21:23<05:27,  2.13batch/s, Batch Loss=0.0304, Avg Loss=0.1129, Time Left=6.28 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2695/3393 [21:23<05:36,  2.07batch/s, Batch Loss=0.0304, Avg Loss=0.1129, Time Left=6.28 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2695/3393 [21:23<05:36,  2.07batch/s, Batch Loss=0.0403, Avg Loss=0.1128, Time Left=6.27 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2696/3393 [21:23<05:30,  2.11batch/s, Batch Loss=0.0403, Avg Loss=0.1128, Time Left=6.27 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2696/3393 [21:24<05:30,  2.11batch/s, Batch Loss=0.0064, Avg Loss=0.1128, Time Left=6.26 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2697/3393 [21:24<05:31,  2.10batch/s, Batch Loss=0.0064, Avg Loss=0.1128, Time Left=6.26 \u001b[A\n",
      "Epoch 2/3 - Training:  79%|▊| 2697/3393 [21:24<05:31,  2.10batch/s, Batch Loss=0.0116, Avg Loss=0.1127, Time Left=6.26 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2698/3393 [21:24<05:35,  2.07batch/s, Batch Loss=0.0116, Avg Loss=0.1127, Time Left=6.26 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2698/3393 [21:25<05:35,  2.07batch/s, Batch Loss=0.0125, Avg Loss=0.1127, Time Left=6.25 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2699/3393 [21:25<05:38,  2.05batch/s, Batch Loss=0.0125, Avg Loss=0.1127, Time Left=6.25 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2699/3393 [21:25<05:38,  2.05batch/s, Batch Loss=0.1534, Avg Loss=0.1127, Time Left=6.24 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2700/3393 [21:25<05:34,  2.07batch/s, Batch Loss=0.1534, Avg Loss=0.1127, Time Left=6.24 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2700/3393 [21:26<05:34,  2.07batch/s, Batch Loss=0.0509, Avg Loss=0.1127, Time Left=6.23 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2701/3393 [21:26<05:34,  2.07batch/s, Batch Loss=0.0509, Avg Loss=0.1127, Time Left=6.23 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2701/3393 [21:26<05:34,  2.07batch/s, Batch Loss=0.0401, Avg Loss=0.1127, Time Left=6.22 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2702/3393 [21:26<05:35,  2.06batch/s, Batch Loss=0.0401, Avg Loss=0.1127, Time Left=6.22 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2702/3393 [21:27<05:35,  2.06batch/s, Batch Loss=0.0212, Avg Loss=0.1126, Time Left=6.21 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2703/3393 [21:27<05:32,  2.07batch/s, Batch Loss=0.0212, Avg Loss=0.1126, Time Left=6.21 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2703/3393 [21:27<05:32,  2.07batch/s, Batch Loss=0.0029, Avg Loss=0.1126, Time Left=6.21 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  80%|▊| 2704/3393 [21:27<05:29,  2.09batch/s, Batch Loss=0.0029, Avg Loss=0.1126, Time Left=6.21 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2704/3393 [21:28<05:29,  2.09batch/s, Batch Loss=0.0667, Avg Loss=0.1126, Time Left=6.20 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2705/3393 [21:28<05:29,  2.09batch/s, Batch Loss=0.0667, Avg Loss=0.1126, Time Left=6.20 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2705/3393 [21:28<05:29,  2.09batch/s, Batch Loss=0.1463, Avg Loss=0.1126, Time Left=6.19 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2706/3393 [21:28<05:30,  2.08batch/s, Batch Loss=0.1463, Avg Loss=0.1126, Time Left=6.19 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2706/3393 [21:29<05:30,  2.08batch/s, Batch Loss=0.0031, Avg Loss=0.1125, Time Left=6.18 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2707/3393 [21:29<05:27,  2.10batch/s, Batch Loss=0.0031, Avg Loss=0.1125, Time Left=6.18 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2707/3393 [21:29<05:27,  2.10batch/s, Batch Loss=0.2236, Avg Loss=0.1126, Time Left=6.17 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2708/3393 [21:29<05:25,  2.11batch/s, Batch Loss=0.2236, Avg Loss=0.1126, Time Left=6.17 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2708/3393 [21:30<05:25,  2.11batch/s, Batch Loss=0.1158, Avg Loss=0.1126, Time Left=6.17 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2709/3393 [21:30<05:29,  2.08batch/s, Batch Loss=0.1158, Avg Loss=0.1126, Time Left=6.17 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2709/3393 [21:30<05:29,  2.08batch/s, Batch Loss=0.0135, Avg Loss=0.1126, Time Left=6.16 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2710/3393 [21:30<05:26,  2.09batch/s, Batch Loss=0.0135, Avg Loss=0.1126, Time Left=6.16 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2710/3393 [21:31<05:26,  2.09batch/s, Batch Loss=0.0891, Avg Loss=0.1125, Time Left=6.15 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2711/3393 [21:31<05:29,  2.07batch/s, Batch Loss=0.0891, Avg Loss=0.1125, Time Left=6.15 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2711/3393 [21:31<05:29,  2.07batch/s, Batch Loss=0.0167, Avg Loss=0.1125, Time Left=6.14 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2712/3393 [21:31<05:23,  2.11batch/s, Batch Loss=0.0167, Avg Loss=0.1125, Time Left=6.14 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2712/3393 [21:32<05:23,  2.11batch/s, Batch Loss=0.1636, Avg Loss=0.1125, Time Left=6.13 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2713/3393 [21:32<05:21,  2.12batch/s, Batch Loss=0.1636, Avg Loss=0.1125, Time Left=6.13 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2713/3393 [21:32<05:21,  2.12batch/s, Batch Loss=0.0674, Avg Loss=0.1125, Time Left=6.12 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2714/3393 [21:32<05:19,  2.12batch/s, Batch Loss=0.0674, Avg Loss=0.1125, Time Left=6.12 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2714/3393 [21:33<05:19,  2.12batch/s, Batch Loss=0.0670, Avg Loss=0.1125, Time Left=6.12 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2715/3393 [21:33<05:18,  2.13batch/s, Batch Loss=0.0670, Avg Loss=0.1125, Time Left=6.12 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2715/3393 [21:33<05:18,  2.13batch/s, Batch Loss=0.1142, Avg Loss=0.1125, Time Left=6.11 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2716/3393 [21:33<05:20,  2.11batch/s, Batch Loss=0.1142, Avg Loss=0.1125, Time Left=6.11 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2716/3393 [21:34<05:20,  2.11batch/s, Batch Loss=0.0240, Avg Loss=0.1125, Time Left=6.10 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2717/3393 [21:34<05:22,  2.10batch/s, Batch Loss=0.0240, Avg Loss=0.1125, Time Left=6.10 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2717/3393 [21:34<05:22,  2.10batch/s, Batch Loss=0.0326, Avg Loss=0.1124, Time Left=6.09 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2718/3393 [21:34<05:20,  2.11batch/s, Batch Loss=0.0326, Avg Loss=0.1124, Time Left=6.09 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2718/3393 [21:34<05:20,  2.11batch/s, Batch Loss=0.0622, Avg Loss=0.1124, Time Left=6.08 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2719/3393 [21:34<05:20,  2.10batch/s, Batch Loss=0.0622, Avg Loss=0.1124, Time Left=6.08 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2719/3393 [21:35<05:20,  2.10batch/s, Batch Loss=0.0446, Avg Loss=0.1124, Time Left=6.07 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2720/3393 [21:35<05:22,  2.09batch/s, Batch Loss=0.0446, Avg Loss=0.1124, Time Left=6.07 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2720/3393 [21:35<05:22,  2.09batch/s, Batch Loss=0.2217, Avg Loss=0.1124, Time Left=6.07 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2721/3393 [21:35<05:19,  2.10batch/s, Batch Loss=0.2217, Avg Loss=0.1124, Time Left=6.07 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2721/3393 [21:36<05:19,  2.10batch/s, Batch Loss=0.0107, Avg Loss=0.1124, Time Left=6.06 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2722/3393 [21:36<05:29,  2.03batch/s, Batch Loss=0.0107, Avg Loss=0.1124, Time Left=6.06 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2722/3393 [21:36<05:29,  2.03batch/s, Batch Loss=0.0793, Avg Loss=0.1124, Time Left=6.05 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2723/3393 [21:36<05:30,  2.03batch/s, Batch Loss=0.0793, Avg Loss=0.1124, Time Left=6.05 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2723/3393 [21:37<05:30,  2.03batch/s, Batch Loss=0.1110, Avg Loss=0.1124, Time Left=6.04 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2724/3393 [21:37<05:29,  2.03batch/s, Batch Loss=0.1110, Avg Loss=0.1124, Time Left=6.04 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2724/3393 [21:37<05:29,  2.03batch/s, Batch Loss=0.0534, Avg Loss=0.1124, Time Left=6.03 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2725/3393 [21:37<05:23,  2.06batch/s, Batch Loss=0.0534, Avg Loss=0.1124, Time Left=6.03 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2725/3393 [21:38<05:23,  2.06batch/s, Batch Loss=0.0017, Avg Loss=0.1123, Time Left=6.03 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2726/3393 [21:38<05:26,  2.04batch/s, Batch Loss=0.0017, Avg Loss=0.1123, Time Left=6.03 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2726/3393 [21:38<05:26,  2.04batch/s, Batch Loss=0.1455, Avg Loss=0.1123, Time Left=6.02 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2727/3393 [21:38<05:24,  2.05batch/s, Batch Loss=0.1455, Avg Loss=0.1123, Time Left=6.02 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2727/3393 [21:39<05:24,  2.05batch/s, Batch Loss=0.0473, Avg Loss=0.1123, Time Left=6.01 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2728/3393 [21:39<05:26,  2.04batch/s, Batch Loss=0.0473, Avg Loss=0.1123, Time Left=6.01 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2728/3393 [21:39<05:26,  2.04batch/s, Batch Loss=0.0012, Avg Loss=0.1123, Time Left=6.00 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2729/3393 [21:39<05:30,  2.01batch/s, Batch Loss=0.0012, Avg Loss=0.1123, Time Left=6.00 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2729/3393 [21:40<05:30,  2.01batch/s, Batch Loss=0.0927, Avg Loss=0.1123, Time Left=5.99 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2730/3393 [21:40<05:27,  2.02batch/s, Batch Loss=0.0927, Avg Loss=0.1123, Time Left=5.99 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2730/3393 [21:40<05:27,  2.02batch/s, Batch Loss=0.0831, Avg Loss=0.1122, Time Left=5.99 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2731/3393 [21:40<05:25,  2.04batch/s, Batch Loss=0.0831, Avg Loss=0.1122, Time Left=5.99 \u001b[A\n",
      "Epoch 2/3 - Training:  80%|▊| 2731/3393 [21:41<05:25,  2.04batch/s, Batch Loss=0.1344, Avg Loss=0.1122, Time Left=5.98 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2732/3393 [21:41<05:23,  2.04batch/s, Batch Loss=0.1344, Avg Loss=0.1122, Time Left=5.98 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2732/3393 [21:41<05:23,  2.04batch/s, Batch Loss=0.1350, Avg Loss=0.1123, Time Left=5.97 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2733/3393 [21:41<05:19,  2.07batch/s, Batch Loss=0.1350, Avg Loss=0.1123, Time Left=5.97 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2733/3393 [21:42<05:19,  2.07batch/s, Batch Loss=0.0610, Avg Loss=0.1122, Time Left=5.96 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2734/3393 [21:42<05:15,  2.09batch/s, Batch Loss=0.0610, Avg Loss=0.1122, Time Left=5.96 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2734/3393 [21:42<05:15,  2.09batch/s, Batch Loss=0.0103, Avg Loss=0.1122, Time Left=5.95 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2735/3393 [21:42<05:16,  2.08batch/s, Batch Loss=0.0103, Avg Loss=0.1122, Time Left=5.95 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2735/3393 [21:43<05:16,  2.08batch/s, Batch Loss=0.0352, Avg Loss=0.1122, Time Left=5.94 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2736/3393 [21:43<05:12,  2.10batch/s, Batch Loss=0.0352, Avg Loss=0.1122, Time Left=5.94 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2736/3393 [21:43<05:12,  2.10batch/s, Batch Loss=0.0467, Avg Loss=0.1121, Time Left=5.94 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  81%|▊| 2737/3393 [21:43<05:10,  2.11batch/s, Batch Loss=0.0467, Avg Loss=0.1121, Time Left=5.94 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2737/3393 [21:44<05:10,  2.11batch/s, Batch Loss=0.0029, Avg Loss=0.1121, Time Left=5.93 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2738/3393 [21:44<05:21,  2.04batch/s, Batch Loss=0.0029, Avg Loss=0.1121, Time Left=5.93 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2738/3393 [21:44<05:21,  2.04batch/s, Batch Loss=0.0681, Avg Loss=0.1121, Time Left=5.92 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2739/3393 [21:44<05:16,  2.07batch/s, Batch Loss=0.0681, Avg Loss=0.1121, Time Left=5.92 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2739/3393 [21:45<05:16,  2.07batch/s, Batch Loss=0.0366, Avg Loss=0.1121, Time Left=5.91 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2740/3393 [21:45<05:22,  2.03batch/s, Batch Loss=0.0366, Avg Loss=0.1121, Time Left=5.91 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2740/3393 [21:45<05:22,  2.03batch/s, Batch Loss=0.5951, Avg Loss=0.1122, Time Left=5.90 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2741/3393 [21:45<05:20,  2.04batch/s, Batch Loss=0.5951, Avg Loss=0.1122, Time Left=5.90 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2741/3393 [21:46<05:20,  2.04batch/s, Batch Loss=0.2492, Avg Loss=0.1123, Time Left=5.90 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2742/3393 [21:46<05:18,  2.05batch/s, Batch Loss=0.2492, Avg Loss=0.1123, Time Left=5.90 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2742/3393 [21:46<05:18,  2.05batch/s, Batch Loss=0.0031, Avg Loss=0.1123, Time Left=5.89 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2743/3393 [21:46<05:13,  2.07batch/s, Batch Loss=0.0031, Avg Loss=0.1123, Time Left=5.89 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2743/3393 [21:47<05:13,  2.07batch/s, Batch Loss=0.0034, Avg Loss=0.1122, Time Left=5.88 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2744/3393 [21:47<05:13,  2.07batch/s, Batch Loss=0.0034, Avg Loss=0.1122, Time Left=5.88 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2744/3393 [21:47<05:13,  2.07batch/s, Batch Loss=0.4303, Avg Loss=0.1123, Time Left=5.87 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2745/3393 [21:47<05:09,  2.09batch/s, Batch Loss=0.4303, Avg Loss=0.1123, Time Left=5.87 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2745/3393 [21:48<05:09,  2.09batch/s, Batch Loss=0.0489, Avg Loss=0.1123, Time Left=5.86 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2746/3393 [21:48<05:10,  2.09batch/s, Batch Loss=0.0489, Avg Loss=0.1123, Time Left=5.86 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2746/3393 [21:48<05:10,  2.09batch/s, Batch Loss=0.0245, Avg Loss=0.1123, Time Left=5.85 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2747/3393 [21:48<05:13,  2.06batch/s, Batch Loss=0.0245, Avg Loss=0.1123, Time Left=5.85 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2747/3393 [21:49<05:13,  2.06batch/s, Batch Loss=0.4503, Avg Loss=0.1124, Time Left=5.85 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2748/3393 [21:49<05:11,  2.07batch/s, Batch Loss=0.4503, Avg Loss=0.1124, Time Left=5.85 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2748/3393 [21:49<05:11,  2.07batch/s, Batch Loss=0.1504, Avg Loss=0.1124, Time Left=5.84 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2749/3393 [21:49<05:06,  2.10batch/s, Batch Loss=0.1504, Avg Loss=0.1124, Time Left=5.84 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2749/3393 [21:50<05:06,  2.10batch/s, Batch Loss=0.4883, Avg Loss=0.1126, Time Left=5.83 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2750/3393 [21:50<05:07,  2.09batch/s, Batch Loss=0.4883, Avg Loss=0.1126, Time Left=5.83 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2750/3393 [21:50<05:07,  2.09batch/s, Batch Loss=0.0144, Avg Loss=0.1125, Time Left=5.82 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2751/3393 [21:50<05:04,  2.11batch/s, Batch Loss=0.0144, Avg Loss=0.1125, Time Left=5.82 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2751/3393 [21:50<05:04,  2.11batch/s, Batch Loss=0.0824, Avg Loss=0.1125, Time Left=5.81 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2752/3393 [21:50<05:06,  2.09batch/s, Batch Loss=0.0824, Avg Loss=0.1125, Time Left=5.81 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2752/3393 [21:51<05:06,  2.09batch/s, Batch Loss=0.0067, Avg Loss=0.1125, Time Left=5.80 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2753/3393 [21:51<05:07,  2.08batch/s, Batch Loss=0.0067, Avg Loss=0.1125, Time Left=5.80 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2753/3393 [21:52<05:07,  2.08batch/s, Batch Loss=0.2248, Avg Loss=0.1125, Time Left=5.80 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2754/3393 [21:52<05:14,  2.03batch/s, Batch Loss=0.2248, Avg Loss=0.1125, Time Left=5.80 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2754/3393 [21:52<05:14,  2.03batch/s, Batch Loss=0.0231, Avg Loss=0.1125, Time Left=5.79 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2755/3393 [21:52<05:11,  2.05batch/s, Batch Loss=0.0231, Avg Loss=0.1125, Time Left=5.79 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2755/3393 [21:52<05:11,  2.05batch/s, Batch Loss=0.2781, Avg Loss=0.1125, Time Left=5.78 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2756/3393 [21:52<05:06,  2.08batch/s, Batch Loss=0.2781, Avg Loss=0.1125, Time Left=5.78 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2756/3393 [21:53<05:06,  2.08batch/s, Batch Loss=0.1760, Avg Loss=0.1126, Time Left=5.77 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2757/3393 [21:53<05:14,  2.02batch/s, Batch Loss=0.1760, Avg Loss=0.1126, Time Left=5.77 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2757/3393 [21:53<05:14,  2.02batch/s, Batch Loss=0.1396, Avg Loss=0.1126, Time Left=5.76 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2758/3393 [21:53<05:17,  2.00batch/s, Batch Loss=0.1396, Avg Loss=0.1126, Time Left=5.76 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2758/3393 [21:54<05:17,  2.00batch/s, Batch Loss=0.2365, Avg Loss=0.1126, Time Left=5.76 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2759/3393 [21:54<05:12,  2.03batch/s, Batch Loss=0.2365, Avg Loss=0.1126, Time Left=5.76 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2759/3393 [21:54<05:12,  2.03batch/s, Batch Loss=0.1147, Avg Loss=0.1126, Time Left=5.75 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2760/3393 [21:54<05:04,  2.08batch/s, Batch Loss=0.1147, Avg Loss=0.1126, Time Left=5.75 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2760/3393 [21:55<05:04,  2.08batch/s, Batch Loss=0.2684, Avg Loss=0.1127, Time Left=5.74 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2761/3393 [21:55<05:07,  2.06batch/s, Batch Loss=0.2684, Avg Loss=0.1127, Time Left=5.74 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2761/3393 [21:55<05:07,  2.06batch/s, Batch Loss=0.0252, Avg Loss=0.1126, Time Left=5.73 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2762/3393 [21:55<05:03,  2.08batch/s, Batch Loss=0.0252, Avg Loss=0.1126, Time Left=5.73 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2762/3393 [21:56<05:03,  2.08batch/s, Batch Loss=0.1048, Avg Loss=0.1126, Time Left=5.72 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2763/3393 [21:56<05:03,  2.08batch/s, Batch Loss=0.1048, Avg Loss=0.1126, Time Left=5.72 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2763/3393 [21:56<05:03,  2.08batch/s, Batch Loss=0.1977, Avg Loss=0.1127, Time Left=5.72 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2764/3393 [21:56<05:00,  2.09batch/s, Batch Loss=0.1977, Avg Loss=0.1127, Time Left=5.72 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2764/3393 [21:57<05:00,  2.09batch/s, Batch Loss=0.0620, Avg Loss=0.1127, Time Left=5.71 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2765/3393 [21:57<05:01,  2.08batch/s, Batch Loss=0.0620, Avg Loss=0.1127, Time Left=5.71 \u001b[A\n",
      "Epoch 2/3 - Training:  81%|▊| 2765/3393 [21:57<05:01,  2.08batch/s, Batch Loss=0.0597, Avg Loss=0.1126, Time Left=5.70 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2766/3393 [21:57<05:00,  2.08batch/s, Batch Loss=0.0597, Avg Loss=0.1126, Time Left=5.70 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2766/3393 [21:58<05:00,  2.08batch/s, Batch Loss=0.0958, Avg Loss=0.1126, Time Left=5.69 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2767/3393 [21:58<04:58,  2.10batch/s, Batch Loss=0.0958, Avg Loss=0.1126, Time Left=5.69 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2767/3393 [21:58<04:58,  2.10batch/s, Batch Loss=0.2861, Avg Loss=0.1127, Time Left=5.68 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2768/3393 [21:58<04:59,  2.09batch/s, Batch Loss=0.2861, Avg Loss=0.1127, Time Left=5.68 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2768/3393 [21:59<04:59,  2.09batch/s, Batch Loss=0.1714, Avg Loss=0.1127, Time Left=5.67 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2769/3393 [21:59<04:59,  2.08batch/s, Batch Loss=0.1714, Avg Loss=0.1127, Time Left=5.67 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2769/3393 [21:59<04:59,  2.08batch/s, Batch Loss=0.1058, Avg Loss=0.1127, Time Left=5.67 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  82%|▊| 2770/3393 [21:59<04:59,  2.08batch/s, Batch Loss=0.1058, Avg Loss=0.1127, Time Left=5.67 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2770/3393 [22:00<04:59,  2.08batch/s, Batch Loss=0.0706, Avg Loss=0.1127, Time Left=5.66 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2771/3393 [22:00<04:57,  2.09batch/s, Batch Loss=0.0706, Avg Loss=0.1127, Time Left=5.66 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2771/3393 [22:00<04:57,  2.09batch/s, Batch Loss=0.0399, Avg Loss=0.1127, Time Left=5.65 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2772/3393 [22:00<04:57,  2.09batch/s, Batch Loss=0.0399, Avg Loss=0.1127, Time Left=5.65 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2772/3393 [22:01<04:57,  2.09batch/s, Batch Loss=0.0274, Avg Loss=0.1126, Time Left=5.64 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2773/3393 [22:01<04:54,  2.10batch/s, Batch Loss=0.0274, Avg Loss=0.1126, Time Left=5.64 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2773/3393 [22:01<04:54,  2.10batch/s, Batch Loss=0.0575, Avg Loss=0.1126, Time Left=5.63 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2774/3393 [22:01<04:59,  2.07batch/s, Batch Loss=0.0575, Avg Loss=0.1126, Time Left=5.63 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2774/3393 [22:02<04:59,  2.07batch/s, Batch Loss=0.1305, Avg Loss=0.1126, Time Left=5.62 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2775/3393 [22:02<04:52,  2.11batch/s, Batch Loss=0.1305, Avg Loss=0.1126, Time Left=5.62 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2775/3393 [22:02<04:52,  2.11batch/s, Batch Loss=0.1939, Avg Loss=0.1127, Time Left=5.62 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2776/3393 [22:02<04:53,  2.10batch/s, Batch Loss=0.1939, Avg Loss=0.1127, Time Left=5.62 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2776/3393 [22:03<04:53,  2.10batch/s, Batch Loss=0.3076, Avg Loss=0.1127, Time Left=5.61 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2777/3393 [22:03<04:54,  2.09batch/s, Batch Loss=0.3076, Avg Loss=0.1127, Time Left=5.61 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2777/3393 [22:03<04:54,  2.09batch/s, Batch Loss=0.0232, Avg Loss=0.1127, Time Left=5.60 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2778/3393 [22:03<04:55,  2.08batch/s, Batch Loss=0.0232, Avg Loss=0.1127, Time Left=5.60 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2778/3393 [22:04<04:55,  2.08batch/s, Batch Loss=0.0424, Avg Loss=0.1127, Time Left=5.59 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2779/3393 [22:04<04:52,  2.10batch/s, Batch Loss=0.0424, Avg Loss=0.1127, Time Left=5.59 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2779/3393 [22:04<04:52,  2.10batch/s, Batch Loss=0.0400, Avg Loss=0.1126, Time Left=5.58 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2780/3393 [22:04<05:01,  2.03batch/s, Batch Loss=0.0400, Avg Loss=0.1126, Time Left=5.58 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2780/3393 [22:05<05:01,  2.03batch/s, Batch Loss=0.1460, Avg Loss=0.1127, Time Left=5.58 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2781/3393 [22:05<05:00,  2.04batch/s, Batch Loss=0.1460, Avg Loss=0.1127, Time Left=5.58 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2781/3393 [22:05<05:00,  2.04batch/s, Batch Loss=0.0462, Avg Loss=0.1126, Time Left=5.57 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2782/3393 [22:05<05:04,  2.01batch/s, Batch Loss=0.0462, Avg Loss=0.1126, Time Left=5.57 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2782/3393 [22:06<05:04,  2.01batch/s, Batch Loss=0.1506, Avg Loss=0.1126, Time Left=5.56 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2783/3393 [22:06<05:01,  2.02batch/s, Batch Loss=0.1506, Avg Loss=0.1126, Time Left=5.56 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2783/3393 [22:06<05:01,  2.02batch/s, Batch Loss=0.2034, Avg Loss=0.1127, Time Left=5.55 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2784/3393 [22:06<04:56,  2.06batch/s, Batch Loss=0.2034, Avg Loss=0.1127, Time Left=5.55 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2784/3393 [22:06<04:56,  2.06batch/s, Batch Loss=0.0227, Avg Loss=0.1126, Time Left=5.54 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2785/3393 [22:06<04:58,  2.04batch/s, Batch Loss=0.0227, Avg Loss=0.1126, Time Left=5.54 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2785/3393 [22:07<04:58,  2.04batch/s, Batch Loss=0.1031, Avg Loss=0.1126, Time Left=5.54 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2786/3393 [22:07<04:56,  2.05batch/s, Batch Loss=0.1031, Avg Loss=0.1126, Time Left=5.54 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2786/3393 [22:07<04:56,  2.05batch/s, Batch Loss=0.2312, Avg Loss=0.1127, Time Left=5.53 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2787/3393 [22:07<04:57,  2.03batch/s, Batch Loss=0.2312, Avg Loss=0.1127, Time Left=5.53 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2787/3393 [22:08<04:57,  2.03batch/s, Batch Loss=0.0238, Avg Loss=0.1126, Time Left=5.52 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2788/3393 [22:08<04:56,  2.04batch/s, Batch Loss=0.0238, Avg Loss=0.1126, Time Left=5.52 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2788/3393 [22:08<04:56,  2.04batch/s, Batch Loss=0.2139, Avg Loss=0.1127, Time Left=5.51 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2789/3393 [22:08<04:54,  2.05batch/s, Batch Loss=0.2139, Avg Loss=0.1127, Time Left=5.51 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2789/3393 [22:09<04:54,  2.05batch/s, Batch Loss=0.2276, Avg Loss=0.1127, Time Left=5.50 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2790/3393 [22:09<04:56,  2.04batch/s, Batch Loss=0.2276, Avg Loss=0.1127, Time Left=5.50 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2790/3393 [22:09<04:56,  2.04batch/s, Batch Loss=0.2934, Avg Loss=0.1128, Time Left=5.49 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2791/3393 [22:09<04:54,  2.04batch/s, Batch Loss=0.2934, Avg Loss=0.1128, Time Left=5.49 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2791/3393 [22:10<04:54,  2.04batch/s, Batch Loss=0.0666, Avg Loss=0.1128, Time Left=5.49 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2792/3393 [22:10<04:58,  2.01batch/s, Batch Loss=0.0666, Avg Loss=0.1128, Time Left=5.49 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2792/3393 [22:10<04:58,  2.01batch/s, Batch Loss=0.0098, Avg Loss=0.1127, Time Left=5.48 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2793/3393 [22:10<04:58,  2.01batch/s, Batch Loss=0.0098, Avg Loss=0.1127, Time Left=5.48 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2793/3393 [22:11<04:58,  2.01batch/s, Batch Loss=0.0128, Avg Loss=0.1127, Time Left=5.47 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2794/3393 [22:11<04:58,  2.01batch/s, Batch Loss=0.0128, Avg Loss=0.1127, Time Left=5.47 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2794/3393 [22:11<04:58,  2.01batch/s, Batch Loss=0.0079, Avg Loss=0.1127, Time Left=5.46 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2795/3393 [22:11<04:52,  2.04batch/s, Batch Loss=0.0079, Avg Loss=0.1127, Time Left=5.46 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2795/3393 [22:12<04:52,  2.04batch/s, Batch Loss=0.0433, Avg Loss=0.1126, Time Left=5.45 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2796/3393 [22:12<04:51,  2.05batch/s, Batch Loss=0.0433, Avg Loss=0.1126, Time Left=5.45 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2796/3393 [22:12<04:51,  2.05batch/s, Batch Loss=0.0899, Avg Loss=0.1126, Time Left=5.45 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2797/3393 [22:12<04:55,  2.02batch/s, Batch Loss=0.0899, Avg Loss=0.1126, Time Left=5.45 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2797/3393 [22:13<04:55,  2.02batch/s, Batch Loss=0.0048, Avg Loss=0.1126, Time Left=5.44 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2798/3393 [22:13<04:56,  2.01batch/s, Batch Loss=0.0048, Avg Loss=0.1126, Time Left=5.44 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2798/3393 [22:13<04:56,  2.01batch/s, Batch Loss=0.1628, Avg Loss=0.1126, Time Left=5.43 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2799/3393 [22:13<04:59,  1.99batch/s, Batch Loss=0.1628, Avg Loss=0.1126, Time Left=5.43 \u001b[A\n",
      "Epoch 2/3 - Training:  82%|▊| 2799/3393 [22:14<04:59,  1.99batch/s, Batch Loss=0.0929, Avg Loss=0.1126, Time Left=5.42 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2800/3393 [22:14<04:52,  2.03batch/s, Batch Loss=0.0929, Avg Loss=0.1126, Time Left=5.42 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2800/3393 [22:14<04:52,  2.03batch/s, Batch Loss=0.0268, Avg Loss=0.1126, Time Left=5.41 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2801/3393 [22:14<04:50,  2.04batch/s, Batch Loss=0.0268, Avg Loss=0.1126, Time Left=5.41 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2801/3393 [22:15<04:50,  2.04batch/s, Batch Loss=0.1259, Avg Loss=0.1126, Time Left=5.41 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2802/3393 [22:15<04:51,  2.03batch/s, Batch Loss=0.1259, Avg Loss=0.1126, Time Left=5.41 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2802/3393 [22:15<04:51,  2.03batch/s, Batch Loss=0.3459, Avg Loss=0.1127, Time Left=5.40 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  83%|▊| 2803/3393 [22:15<04:43,  2.08batch/s, Batch Loss=0.3459, Avg Loss=0.1127, Time Left=5.40 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2803/3393 [22:16<04:43,  2.08batch/s, Batch Loss=0.2816, Avg Loss=0.1127, Time Left=5.39 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2804/3393 [22:16<04:46,  2.06batch/s, Batch Loss=0.2816, Avg Loss=0.1127, Time Left=5.39 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2804/3393 [22:16<04:46,  2.06batch/s, Batch Loss=0.0731, Avg Loss=0.1127, Time Left=5.38 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2805/3393 [22:16<04:45,  2.06batch/s, Batch Loss=0.0731, Avg Loss=0.1127, Time Left=5.38 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2805/3393 [22:17<04:45,  2.06batch/s, Batch Loss=0.1376, Avg Loss=0.1127, Time Left=5.37 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2806/3393 [22:17<04:41,  2.08batch/s, Batch Loss=0.1376, Avg Loss=0.1127, Time Left=5.37 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2806/3393 [22:17<04:41,  2.08batch/s, Batch Loss=0.1114, Avg Loss=0.1127, Time Left=5.36 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2807/3393 [22:17<04:41,  2.08batch/s, Batch Loss=0.1114, Avg Loss=0.1127, Time Left=5.36 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2807/3393 [22:18<04:41,  2.08batch/s, Batch Loss=0.0764, Avg Loss=0.1127, Time Left=5.36 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2808/3393 [22:18<04:42,  2.07batch/s, Batch Loss=0.0764, Avg Loss=0.1127, Time Left=5.36 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2808/3393 [22:18<04:42,  2.07batch/s, Batch Loss=0.0588, Avg Loss=0.1127, Time Left=5.35 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2809/3393 [22:18<04:36,  2.11batch/s, Batch Loss=0.0588, Avg Loss=0.1127, Time Left=5.35 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2809/3393 [22:19<04:36,  2.11batch/s, Batch Loss=0.0094, Avg Loss=0.1126, Time Left=5.34 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2810/3393 [22:19<04:37,  2.10batch/s, Batch Loss=0.0094, Avg Loss=0.1126, Time Left=5.34 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2810/3393 [22:19<04:37,  2.10batch/s, Batch Loss=0.0647, Avg Loss=0.1126, Time Left=5.33 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2811/3393 [22:19<04:38,  2.09batch/s, Batch Loss=0.0647, Avg Loss=0.1126, Time Left=5.33 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2811/3393 [22:20<04:38,  2.09batch/s, Batch Loss=0.1390, Avg Loss=0.1126, Time Left=5.32 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2812/3393 [22:20<04:41,  2.07batch/s, Batch Loss=0.1390, Avg Loss=0.1126, Time Left=5.32 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2812/3393 [22:20<04:41,  2.07batch/s, Batch Loss=0.1174, Avg Loss=0.1126, Time Left=5.31 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2813/3393 [22:20<04:40,  2.06batch/s, Batch Loss=0.1174, Avg Loss=0.1126, Time Left=5.31 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2813/3393 [22:21<04:40,  2.06batch/s, Batch Loss=0.3706, Avg Loss=0.1127, Time Left=5.31 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2814/3393 [22:21<04:40,  2.06batch/s, Batch Loss=0.3706, Avg Loss=0.1127, Time Left=5.31 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2814/3393 [22:21<04:40,  2.06batch/s, Batch Loss=0.1871, Avg Loss=0.1128, Time Left=5.30 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2815/3393 [22:21<04:39,  2.07batch/s, Batch Loss=0.1871, Avg Loss=0.1128, Time Left=5.30 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2815/3393 [22:22<04:39,  2.07batch/s, Batch Loss=0.0250, Avg Loss=0.1127, Time Left=5.29 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2816/3393 [22:22<04:41,  2.05batch/s, Batch Loss=0.0250, Avg Loss=0.1127, Time Left=5.29 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2816/3393 [22:22<04:41,  2.05batch/s, Batch Loss=0.0102, Avg Loss=0.1127, Time Left=5.28 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2817/3393 [22:22<04:40,  2.05batch/s, Batch Loss=0.0102, Avg Loss=0.1127, Time Left=5.28 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2817/3393 [22:23<04:40,  2.05batch/s, Batch Loss=0.0988, Avg Loss=0.1127, Time Left=5.27 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2818/3393 [22:23<04:39,  2.06batch/s, Batch Loss=0.0988, Avg Loss=0.1127, Time Left=5.27 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2818/3393 [22:23<04:39,  2.06batch/s, Batch Loss=0.0189, Avg Loss=0.1127, Time Left=5.27 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2819/3393 [22:23<04:36,  2.08batch/s, Batch Loss=0.0189, Avg Loss=0.1127, Time Left=5.27 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2819/3393 [22:24<04:36,  2.08batch/s, Batch Loss=0.1458, Avg Loss=0.1127, Time Left=5.26 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2820/3393 [22:24<04:33,  2.10batch/s, Batch Loss=0.1458, Avg Loss=0.1127, Time Left=5.26 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2820/3393 [22:24<04:33,  2.10batch/s, Batch Loss=0.2170, Avg Loss=0.1127, Time Left=5.25 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2821/3393 [22:24<04:33,  2.09batch/s, Batch Loss=0.2170, Avg Loss=0.1127, Time Left=5.25 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2821/3393 [22:25<04:33,  2.09batch/s, Batch Loss=0.3435, Avg Loss=0.1128, Time Left=5.24 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2822/3393 [22:25<04:38,  2.05batch/s, Batch Loss=0.3435, Avg Loss=0.1128, Time Left=5.24 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2822/3393 [22:25<04:38,  2.05batch/s, Batch Loss=0.3743, Avg Loss=0.1129, Time Left=5.23 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2823/3393 [22:25<04:35,  2.07batch/s, Batch Loss=0.3743, Avg Loss=0.1129, Time Left=5.23 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2823/3393 [22:25<04:35,  2.07batch/s, Batch Loss=0.0254, Avg Loss=0.1129, Time Left=5.23 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2824/3393 [22:25<04:37,  2.05batch/s, Batch Loss=0.0254, Avg Loss=0.1129, Time Left=5.23 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2824/3393 [22:26<04:37,  2.05batch/s, Batch Loss=0.0783, Avg Loss=0.1128, Time Left=5.22 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2825/3393 [22:26<04:36,  2.05batch/s, Batch Loss=0.0783, Avg Loss=0.1128, Time Left=5.22 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2825/3393 [22:26<04:36,  2.05batch/s, Batch Loss=0.0740, Avg Loss=0.1128, Time Left=5.21 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2826/3393 [22:26<04:32,  2.08batch/s, Batch Loss=0.0740, Avg Loss=0.1128, Time Left=5.21 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2826/3393 [22:27<04:32,  2.08batch/s, Batch Loss=0.0106, Avg Loss=0.1128, Time Left=5.20 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2827/3393 [22:27<04:35,  2.06batch/s, Batch Loss=0.0106, Avg Loss=0.1128, Time Left=5.20 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2827/3393 [22:27<04:35,  2.06batch/s, Batch Loss=0.0667, Avg Loss=0.1128, Time Left=5.19 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2828/3393 [22:27<04:31,  2.08batch/s, Batch Loss=0.0667, Avg Loss=0.1128, Time Left=5.19 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2828/3393 [22:28<04:31,  2.08batch/s, Batch Loss=0.1012, Avg Loss=0.1128, Time Left=5.18 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2829/3393 [22:28<04:31,  2.08batch/s, Batch Loss=0.1012, Avg Loss=0.1128, Time Left=5.18 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2829/3393 [22:28<04:31,  2.08batch/s, Batch Loss=0.0662, Avg Loss=0.1127, Time Left=5.18 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2830/3393 [22:28<04:31,  2.07batch/s, Batch Loss=0.0662, Avg Loss=0.1127, Time Left=5.18 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2830/3393 [22:29<04:31,  2.07batch/s, Batch Loss=0.0644, Avg Loss=0.1127, Time Left=5.17 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2831/3393 [22:29<04:31,  2.07batch/s, Batch Loss=0.0644, Avg Loss=0.1127, Time Left=5.17 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2831/3393 [22:29<04:31,  2.07batch/s, Batch Loss=0.0897, Avg Loss=0.1127, Time Left=5.16 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2832/3393 [22:29<04:30,  2.07batch/s, Batch Loss=0.0897, Avg Loss=0.1127, Time Left=5.16 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2832/3393 [22:30<04:30,  2.07batch/s, Batch Loss=0.1252, Avg Loss=0.1127, Time Left=5.15 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2833/3393 [22:30<04:33,  2.05batch/s, Batch Loss=0.1252, Avg Loss=0.1127, Time Left=5.15 \u001b[A\n",
      "Epoch 2/3 - Training:  83%|▊| 2833/3393 [22:30<04:33,  2.05batch/s, Batch Loss=0.0072, Avg Loss=0.1127, Time Left=5.14 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2834/3393 [22:30<04:32,  2.05batch/s, Batch Loss=0.0072, Avg Loss=0.1127, Time Left=5.14 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2834/3393 [22:31<04:32,  2.05batch/s, Batch Loss=0.0149, Avg Loss=0.1127, Time Left=5.14 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2835/3393 [22:31<04:36,  2.02batch/s, Batch Loss=0.0149, Avg Loss=0.1127, Time Left=5.14 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2835/3393 [22:31<04:36,  2.02batch/s, Batch Loss=0.0265, Avg Loss=0.1126, Time Left=5.13 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  84%|▊| 2836/3393 [22:31<04:35,  2.02batch/s, Batch Loss=0.0265, Avg Loss=0.1126, Time Left=5.13 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2836/3393 [22:32<04:35,  2.02batch/s, Batch Loss=0.0100, Avg Loss=0.1126, Time Left=5.12 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2837/3393 [22:32<04:37,  2.00batch/s, Batch Loss=0.0100, Avg Loss=0.1126, Time Left=5.12 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2837/3393 [22:32<04:37,  2.00batch/s, Batch Loss=0.1986, Avg Loss=0.1126, Time Left=5.11 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2838/3393 [22:32<04:31,  2.04batch/s, Batch Loss=0.1986, Avg Loss=0.1126, Time Left=5.11 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2838/3393 [22:33<04:31,  2.04batch/s, Batch Loss=0.0667, Avg Loss=0.1126, Time Left=5.10 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2839/3393 [22:33<04:31,  2.04batch/s, Batch Loss=0.0667, Avg Loss=0.1126, Time Left=5.10 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2839/3393 [22:33<04:31,  2.04batch/s, Batch Loss=0.0231, Avg Loss=0.1126, Time Left=5.09 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2840/3393 [22:33<04:31,  2.04batch/s, Batch Loss=0.0231, Avg Loss=0.1126, Time Left=5.09 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2840/3393 [22:34<04:31,  2.04batch/s, Batch Loss=0.2574, Avg Loss=0.1126, Time Left=5.09 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2841/3393 [22:34<04:33,  2.02batch/s, Batch Loss=0.2574, Avg Loss=0.1126, Time Left=5.09 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2841/3393 [22:34<04:33,  2.02batch/s, Batch Loss=0.1825, Avg Loss=0.1126, Time Left=5.08 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2842/3393 [22:34<04:38,  1.98batch/s, Batch Loss=0.1825, Avg Loss=0.1126, Time Left=5.08 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2842/3393 [22:35<04:38,  1.98batch/s, Batch Loss=0.0518, Avg Loss=0.1126, Time Left=5.07 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2843/3393 [22:35<04:34,  2.01batch/s, Batch Loss=0.0518, Avg Loss=0.1126, Time Left=5.07 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2843/3393 [22:35<04:34,  2.01batch/s, Batch Loss=0.0244, Avg Loss=0.1126, Time Left=5.06 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2844/3393 [22:35<04:39,  1.97batch/s, Batch Loss=0.0244, Avg Loss=0.1126, Time Left=5.06 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2844/3393 [22:36<04:39,  1.97batch/s, Batch Loss=0.1563, Avg Loss=0.1126, Time Left=5.05 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2845/3393 [22:36<04:32,  2.01batch/s, Batch Loss=0.1563, Avg Loss=0.1126, Time Left=5.05 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2845/3393 [22:36<04:32,  2.01batch/s, Batch Loss=0.1858, Avg Loss=0.1126, Time Left=5.05 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2846/3393 [22:36<04:29,  2.03batch/s, Batch Loss=0.1858, Avg Loss=0.1126, Time Left=5.05 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2846/3393 [22:37<04:29,  2.03batch/s, Batch Loss=0.0178, Avg Loss=0.1126, Time Left=5.04 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2847/3393 [22:37<04:30,  2.02batch/s, Batch Loss=0.0178, Avg Loss=0.1126, Time Left=5.04 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2847/3393 [22:37<04:30,  2.02batch/s, Batch Loss=0.1431, Avg Loss=0.1126, Time Left=5.03 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2848/3393 [22:37<04:28,  2.03batch/s, Batch Loss=0.1431, Avg Loss=0.1126, Time Left=5.03 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2848/3393 [22:38<04:28,  2.03batch/s, Batch Loss=0.0453, Avg Loss=0.1126, Time Left=5.02 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2849/3393 [22:38<04:28,  2.02batch/s, Batch Loss=0.0453, Avg Loss=0.1126, Time Left=5.02 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2849/3393 [22:38<04:28,  2.02batch/s, Batch Loss=0.1138, Avg Loss=0.1126, Time Left=5.01 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2850/3393 [22:38<04:24,  2.05batch/s, Batch Loss=0.1138, Avg Loss=0.1126, Time Left=5.01 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2850/3393 [22:39<04:24,  2.05batch/s, Batch Loss=0.0246, Avg Loss=0.1126, Time Left=5.01 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2851/3393 [22:39<04:23,  2.06batch/s, Batch Loss=0.0246, Avg Loss=0.1126, Time Left=5.01 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2851/3393 [22:39<04:23,  2.06batch/s, Batch Loss=0.0221, Avg Loss=0.1125, Time Left=5.00 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2852/3393 [22:39<04:22,  2.06batch/s, Batch Loss=0.0221, Avg Loss=0.1125, Time Left=5.00 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2852/3393 [22:40<04:22,  2.06batch/s, Batch Loss=0.0307, Avg Loss=0.1125, Time Left=4.99 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2853/3393 [22:40<04:21,  2.06batch/s, Batch Loss=0.0307, Avg Loss=0.1125, Time Left=4.99 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2853/3393 [22:40<04:21,  2.06batch/s, Batch Loss=0.0200, Avg Loss=0.1125, Time Left=4.98 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2854/3393 [22:40<04:18,  2.09batch/s, Batch Loss=0.0200, Avg Loss=0.1125, Time Left=4.98 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2854/3393 [22:41<04:18,  2.09batch/s, Batch Loss=0.0918, Avg Loss=0.1125, Time Left=4.97 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2855/3393 [22:41<04:26,  2.02batch/s, Batch Loss=0.0918, Avg Loss=0.1125, Time Left=4.97 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2855/3393 [22:41<04:26,  2.02batch/s, Batch Loss=0.0090, Avg Loss=0.1124, Time Left=4.96 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2856/3393 [22:41<04:27,  2.01batch/s, Batch Loss=0.0090, Avg Loss=0.1124, Time Left=4.96 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2856/3393 [22:42<04:27,  2.01batch/s, Batch Loss=0.2015, Avg Loss=0.1124, Time Left=4.96 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2857/3393 [22:42<04:29,  1.99batch/s, Batch Loss=0.2015, Avg Loss=0.1124, Time Left=4.96 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2857/3393 [22:42<04:29,  1.99batch/s, Batch Loss=0.1491, Avg Loss=0.1125, Time Left=4.95 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2858/3393 [22:42<04:25,  2.01batch/s, Batch Loss=0.1491, Avg Loss=0.1125, Time Left=4.95 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2858/3393 [22:43<04:25,  2.01batch/s, Batch Loss=0.0136, Avg Loss=0.1124, Time Left=4.94 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2859/3393 [22:43<04:28,  1.99batch/s, Batch Loss=0.0136, Avg Loss=0.1124, Time Left=4.94 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2859/3393 [22:43<04:28,  1.99batch/s, Batch Loss=0.0250, Avg Loss=0.1124, Time Left=4.93 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2860/3393 [22:43<04:25,  2.01batch/s, Batch Loss=0.0250, Avg Loss=0.1124, Time Left=4.93 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2860/3393 [22:44<04:25,  2.01batch/s, Batch Loss=0.0580, Avg Loss=0.1124, Time Left=4.92 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2861/3393 [22:44<04:24,  2.01batch/s, Batch Loss=0.0580, Avg Loss=0.1124, Time Left=4.92 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2861/3393 [22:44<04:24,  2.01batch/s, Batch Loss=0.2680, Avg Loss=0.1124, Time Left=4.92 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2862/3393 [22:44<04:21,  2.03batch/s, Batch Loss=0.2680, Avg Loss=0.1124, Time Left=4.92 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2862/3393 [22:45<04:21,  2.03batch/s, Batch Loss=0.0102, Avg Loss=0.1124, Time Left=4.91 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2863/3393 [22:45<04:17,  2.06batch/s, Batch Loss=0.0102, Avg Loss=0.1124, Time Left=4.91 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2863/3393 [22:45<04:17,  2.06batch/s, Batch Loss=0.0387, Avg Loss=0.1124, Time Left=4.90 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2864/3393 [22:45<04:21,  2.02batch/s, Batch Loss=0.0387, Avg Loss=0.1124, Time Left=4.90 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2864/3393 [22:46<04:21,  2.02batch/s, Batch Loss=0.1082, Avg Loss=0.1124, Time Left=4.89 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2865/3393 [22:46<04:22,  2.01batch/s, Batch Loss=0.1082, Avg Loss=0.1124, Time Left=4.89 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2865/3393 [22:46<04:22,  2.01batch/s, Batch Loss=0.0189, Avg Loss=0.1123, Time Left=4.88 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2866/3393 [22:46<04:20,  2.03batch/s, Batch Loss=0.0189, Avg Loss=0.1123, Time Left=4.88 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2866/3393 [22:47<04:20,  2.03batch/s, Batch Loss=0.0461, Avg Loss=0.1123, Time Left=4.88 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2867/3393 [22:47<04:18,  2.04batch/s, Batch Loss=0.0461, Avg Loss=0.1123, Time Left=4.88 \u001b[A\n",
      "Epoch 2/3 - Training:  84%|▊| 2867/3393 [22:47<04:18,  2.04batch/s, Batch Loss=0.2845, Avg Loss=0.1124, Time Left=4.87 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2868/3393 [22:47<04:16,  2.05batch/s, Batch Loss=0.2845, Avg Loss=0.1124, Time Left=4.87 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2868/3393 [22:48<04:16,  2.05batch/s, Batch Loss=0.0276, Avg Loss=0.1123, Time Left=4.86 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  85%|▊| 2869/3393 [22:48<04:17,  2.03batch/s, Batch Loss=0.0276, Avg Loss=0.1123, Time Left=4.86 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2869/3393 [22:48<04:17,  2.03batch/s, Batch Loss=0.0285, Avg Loss=0.1123, Time Left=4.85 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2870/3393 [22:48<04:13,  2.06batch/s, Batch Loss=0.0285, Avg Loss=0.1123, Time Left=4.85 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2870/3393 [22:49<04:13,  2.06batch/s, Batch Loss=0.0773, Avg Loss=0.1123, Time Left=4.84 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2871/3393 [22:49<04:12,  2.06batch/s, Batch Loss=0.0773, Avg Loss=0.1123, Time Left=4.84 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2871/3393 [22:49<04:12,  2.06batch/s, Batch Loss=0.1497, Avg Loss=0.1123, Time Left=4.83 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2872/3393 [22:49<04:16,  2.03batch/s, Batch Loss=0.1497, Avg Loss=0.1123, Time Left=4.83 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2872/3393 [22:50<04:16,  2.03batch/s, Batch Loss=0.0408, Avg Loss=0.1123, Time Left=4.83 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2873/3393 [22:50<04:15,  2.04batch/s, Batch Loss=0.0408, Avg Loss=0.1123, Time Left=4.83 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2873/3393 [22:50<04:15,  2.04batch/s, Batch Loss=0.0072, Avg Loss=0.1122, Time Left=4.82 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2874/3393 [22:50<04:18,  2.01batch/s, Batch Loss=0.0072, Avg Loss=0.1122, Time Left=4.82 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2874/3393 [22:51<04:18,  2.01batch/s, Batch Loss=0.0471, Avg Loss=0.1122, Time Left=4.81 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2875/3393 [22:51<04:17,  2.01batch/s, Batch Loss=0.0471, Avg Loss=0.1122, Time Left=4.81 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2875/3393 [22:51<04:17,  2.01batch/s, Batch Loss=0.0248, Avg Loss=0.1122, Time Left=4.80 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2876/3393 [22:51<04:15,  2.02batch/s, Batch Loss=0.0248, Avg Loss=0.1122, Time Left=4.80 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2876/3393 [22:52<04:15,  2.02batch/s, Batch Loss=0.1183, Avg Loss=0.1122, Time Left=4.79 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2877/3393 [22:52<04:14,  2.02batch/s, Batch Loss=0.1183, Avg Loss=0.1122, Time Left=4.79 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2877/3393 [22:52<04:14,  2.02batch/s, Batch Loss=0.0241, Avg Loss=0.1122, Time Left=4.79 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2878/3393 [22:52<04:11,  2.04batch/s, Batch Loss=0.0241, Avg Loss=0.1122, Time Left=4.79 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2878/3393 [22:53<04:11,  2.04batch/s, Batch Loss=0.0916, Avg Loss=0.1122, Time Left=4.78 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2879/3393 [22:53<04:14,  2.02batch/s, Batch Loss=0.0916, Avg Loss=0.1122, Time Left=4.78 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2879/3393 [22:53<04:14,  2.02batch/s, Batch Loss=0.0075, Avg Loss=0.1121, Time Left=4.77 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2880/3393 [22:53<04:12,  2.03batch/s, Batch Loss=0.0075, Avg Loss=0.1121, Time Left=4.77 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2880/3393 [22:54<04:12,  2.03batch/s, Batch Loss=0.4927, Avg Loss=0.1123, Time Left=4.76 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2881/3393 [22:54<04:09,  2.05batch/s, Batch Loss=0.4927, Avg Loss=0.1123, Time Left=4.76 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2881/3393 [22:54<04:09,  2.05batch/s, Batch Loss=0.0289, Avg Loss=0.1122, Time Left=4.75 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2882/3393 [22:54<04:07,  2.06batch/s, Batch Loss=0.0289, Avg Loss=0.1122, Time Left=4.75 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2882/3393 [22:54<04:07,  2.06batch/s, Batch Loss=0.0062, Avg Loss=0.1122, Time Left=4.74 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2883/3393 [22:54<04:04,  2.09batch/s, Batch Loss=0.0062, Avg Loss=0.1122, Time Left=4.74 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2883/3393 [22:55<04:04,  2.09batch/s, Batch Loss=0.0527, Avg Loss=0.1122, Time Left=4.74 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2884/3393 [22:55<04:04,  2.08batch/s, Batch Loss=0.0527, Avg Loss=0.1122, Time Left=4.74 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2884/3393 [22:55<04:04,  2.08batch/s, Batch Loss=0.0843, Avg Loss=0.1122, Time Left=4.73 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2885/3393 [22:55<04:04,  2.08batch/s, Batch Loss=0.0843, Avg Loss=0.1122, Time Left=4.73 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2885/3393 [22:56<04:04,  2.08batch/s, Batch Loss=0.0347, Avg Loss=0.1121, Time Left=4.72 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2886/3393 [22:56<04:06,  2.05batch/s, Batch Loss=0.0347, Avg Loss=0.1121, Time Left=4.72 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2886/3393 [22:56<04:06,  2.05batch/s, Batch Loss=0.0505, Avg Loss=0.1121, Time Left=4.71 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2887/3393 [22:56<04:08,  2.04batch/s, Batch Loss=0.0505, Avg Loss=0.1121, Time Left=4.71 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2887/3393 [22:57<04:08,  2.04batch/s, Batch Loss=0.0102, Avg Loss=0.1121, Time Left=4.70 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2888/3393 [22:57<04:09,  2.02batch/s, Batch Loss=0.0102, Avg Loss=0.1121, Time Left=4.70 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2888/3393 [22:57<04:09,  2.02batch/s, Batch Loss=0.0256, Avg Loss=0.1120, Time Left=4.70 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2889/3393 [22:57<04:04,  2.06batch/s, Batch Loss=0.0256, Avg Loss=0.1120, Time Left=4.70 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2889/3393 [22:58<04:04,  2.06batch/s, Batch Loss=0.2627, Avg Loss=0.1121, Time Left=4.69 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2890/3393 [22:58<04:04,  2.06batch/s, Batch Loss=0.2627, Avg Loss=0.1121, Time Left=4.69 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2890/3393 [22:58<04:04,  2.06batch/s, Batch Loss=0.0336, Avg Loss=0.1121, Time Left=4.68 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2891/3393 [22:58<04:01,  2.08batch/s, Batch Loss=0.0336, Avg Loss=0.1121, Time Left=4.68 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2891/3393 [22:59<04:01,  2.08batch/s, Batch Loss=0.0811, Avg Loss=0.1121, Time Left=4.67 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2892/3393 [22:59<04:05,  2.04batch/s, Batch Loss=0.0811, Avg Loss=0.1121, Time Left=4.67 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2892/3393 [22:59<04:05,  2.04batch/s, Batch Loss=0.1733, Avg Loss=0.1121, Time Left=4.66 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2893/3393 [22:59<03:59,  2.09batch/s, Batch Loss=0.1733, Avg Loss=0.1121, Time Left=4.66 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2893/3393 [23:00<03:59,  2.09batch/s, Batch Loss=0.1250, Avg Loss=0.1121, Time Left=4.65 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2894/3393 [23:00<03:57,  2.10batch/s, Batch Loss=0.1250, Avg Loss=0.1121, Time Left=4.65 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2894/3393 [23:00<03:57,  2.10batch/s, Batch Loss=0.0598, Avg Loss=0.1121, Time Left=4.65 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2895/3393 [23:00<03:57,  2.09batch/s, Batch Loss=0.0598, Avg Loss=0.1121, Time Left=4.65 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2895/3393 [23:01<03:57,  2.09batch/s, Batch Loss=0.0852, Avg Loss=0.1121, Time Left=4.64 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2896/3393 [23:01<03:56,  2.10batch/s, Batch Loss=0.0852, Avg Loss=0.1121, Time Left=4.64 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2896/3393 [23:01<03:56,  2.10batch/s, Batch Loss=0.0021, Avg Loss=0.1120, Time Left=4.63 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2897/3393 [23:01<03:54,  2.12batch/s, Batch Loss=0.0021, Avg Loss=0.1120, Time Left=4.63 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2897/3393 [23:02<03:54,  2.12batch/s, Batch Loss=0.0734, Avg Loss=0.1120, Time Left=4.62 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2898/3393 [23:02<03:57,  2.08batch/s, Batch Loss=0.0734, Avg Loss=0.1120, Time Left=4.62 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2898/3393 [23:02<03:57,  2.08batch/s, Batch Loss=0.2170, Avg Loss=0.1120, Time Left=4.61 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2899/3393 [23:02<03:57,  2.08batch/s, Batch Loss=0.2170, Avg Loss=0.1120, Time Left=4.61 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2899/3393 [23:03<03:57,  2.08batch/s, Batch Loss=0.0050, Avg Loss=0.1120, Time Left=4.61 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2900/3393 [23:03<03:56,  2.08batch/s, Batch Loss=0.0050, Avg Loss=0.1120, Time Left=4.61 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2900/3393 [23:03<03:56,  2.08batch/s, Batch Loss=0.0892, Avg Loss=0.1120, Time Left=4.60 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2901/3393 [23:03<03:55,  2.09batch/s, Batch Loss=0.0892, Avg Loss=0.1120, Time Left=4.60 \u001b[A\n",
      "Epoch 2/3 - Training:  85%|▊| 2901/3393 [23:04<03:55,  2.09batch/s, Batch Loss=0.0350, Avg Loss=0.1120, Time Left=4.59 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  86%|▊| 2902/3393 [23:04<03:55,  2.08batch/s, Batch Loss=0.0350, Avg Loss=0.1120, Time Left=4.59 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2902/3393 [23:04<03:55,  2.08batch/s, Batch Loss=0.2116, Avg Loss=0.1120, Time Left=4.58 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2903/3393 [23:04<03:55,  2.08batch/s, Batch Loss=0.2116, Avg Loss=0.1120, Time Left=4.58 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2903/3393 [23:05<03:55,  2.08batch/s, Batch Loss=0.0524, Avg Loss=0.1120, Time Left=4.57 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2904/3393 [23:05<03:53,  2.10batch/s, Batch Loss=0.0524, Avg Loss=0.1120, Time Left=4.57 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2904/3393 [23:05<03:53,  2.10batch/s, Batch Loss=0.1421, Avg Loss=0.1120, Time Left=4.56 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2905/3393 [23:05<03:51,  2.11batch/s, Batch Loss=0.1421, Avg Loss=0.1120, Time Left=4.56 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2905/3393 [23:06<03:51,  2.11batch/s, Batch Loss=0.1210, Avg Loss=0.1120, Time Left=4.56 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2906/3393 [23:06<03:51,  2.11batch/s, Batch Loss=0.1210, Avg Loss=0.1120, Time Left=4.56 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2906/3393 [23:06<03:51,  2.11batch/s, Batch Loss=0.0740, Avg Loss=0.1120, Time Left=4.55 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2907/3393 [23:06<03:50,  2.11batch/s, Batch Loss=0.0740, Avg Loss=0.1120, Time Left=4.55 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2907/3393 [23:06<03:50,  2.11batch/s, Batch Loss=0.0170, Avg Loss=0.1119, Time Left=4.54 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2908/3393 [23:06<03:53,  2.08batch/s, Batch Loss=0.0170, Avg Loss=0.1119, Time Left=4.54 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2908/3393 [23:07<03:53,  2.08batch/s, Batch Loss=0.0782, Avg Loss=0.1119, Time Left=4.53 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2909/3393 [23:07<03:48,  2.12batch/s, Batch Loss=0.0782, Avg Loss=0.1119, Time Left=4.53 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2909/3393 [23:07<03:48,  2.12batch/s, Batch Loss=0.1108, Avg Loss=0.1119, Time Left=4.52 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2910/3393 [23:07<03:51,  2.09batch/s, Batch Loss=0.1108, Avg Loss=0.1119, Time Left=4.52 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2910/3393 [23:08<03:51,  2.09batch/s, Batch Loss=0.0071, Avg Loss=0.1119, Time Left=4.52 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2911/3393 [23:08<03:55,  2.05batch/s, Batch Loss=0.0071, Avg Loss=0.1119, Time Left=4.52 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2911/3393 [23:08<03:55,  2.05batch/s, Batch Loss=0.0401, Avg Loss=0.1119, Time Left=4.51 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2912/3393 [23:08<03:53,  2.06batch/s, Batch Loss=0.0401, Avg Loss=0.1119, Time Left=4.51 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2912/3393 [23:09<03:53,  2.06batch/s, Batch Loss=0.0654, Avg Loss=0.1119, Time Left=4.50 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2913/3393 [23:09<03:55,  2.04batch/s, Batch Loss=0.0654, Avg Loss=0.1119, Time Left=4.50 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2913/3393 [23:09<03:55,  2.04batch/s, Batch Loss=0.0255, Avg Loss=0.1118, Time Left=4.49 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2914/3393 [23:09<03:51,  2.07batch/s, Batch Loss=0.0255, Avg Loss=0.1118, Time Left=4.49 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2914/3393 [23:10<03:51,  2.07batch/s, Batch Loss=0.0138, Avg Loss=0.1118, Time Left=4.48 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2915/3393 [23:10<03:49,  2.09batch/s, Batch Loss=0.0138, Avg Loss=0.1118, Time Left=4.48 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2915/3393 [23:10<03:49,  2.09batch/s, Batch Loss=0.1329, Avg Loss=0.1118, Time Left=4.47 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2916/3393 [23:10<03:53,  2.04batch/s, Batch Loss=0.1329, Avg Loss=0.1118, Time Left=4.47 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2916/3393 [23:11<03:53,  2.04batch/s, Batch Loss=0.0284, Avg Loss=0.1118, Time Left=4.47 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2917/3393 [23:11<03:52,  2.05batch/s, Batch Loss=0.0284, Avg Loss=0.1118, Time Left=4.47 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2917/3393 [23:11<03:52,  2.05batch/s, Batch Loss=0.0244, Avg Loss=0.1117, Time Left=4.46 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2918/3393 [23:11<03:51,  2.05batch/s, Batch Loss=0.0244, Avg Loss=0.1117, Time Left=4.46 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2918/3393 [23:12<03:51,  2.05batch/s, Batch Loss=0.3305, Avg Loss=0.1118, Time Left=4.45 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2919/3393 [23:12<03:50,  2.06batch/s, Batch Loss=0.3305, Avg Loss=0.1118, Time Left=4.45 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2919/3393 [23:12<03:50,  2.06batch/s, Batch Loss=0.0655, Avg Loss=0.1118, Time Left=4.44 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2920/3393 [23:12<03:49,  2.06batch/s, Batch Loss=0.0655, Avg Loss=0.1118, Time Left=4.44 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2920/3393 [23:13<03:49,  2.06batch/s, Batch Loss=0.1507, Avg Loss=0.1118, Time Left=4.43 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2921/3393 [23:13<03:53,  2.02batch/s, Batch Loss=0.1507, Avg Loss=0.1118, Time Left=4.43 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2921/3393 [23:13<03:53,  2.02batch/s, Batch Loss=0.0043, Avg Loss=0.1118, Time Left=4.43 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2922/3393 [23:13<03:51,  2.04batch/s, Batch Loss=0.0043, Avg Loss=0.1118, Time Left=4.43 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2922/3393 [23:14<03:51,  2.04batch/s, Batch Loss=0.0359, Avg Loss=0.1117, Time Left=4.42 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2923/3393 [23:14<03:47,  2.07batch/s, Batch Loss=0.0359, Avg Loss=0.1117, Time Left=4.42 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2923/3393 [23:14<03:47,  2.07batch/s, Batch Loss=0.0048, Avg Loss=0.1117, Time Left=4.41 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2924/3393 [23:14<03:49,  2.04batch/s, Batch Loss=0.0048, Avg Loss=0.1117, Time Left=4.41 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2924/3393 [23:15<03:49,  2.04batch/s, Batch Loss=0.0355, Avg Loss=0.1117, Time Left=4.40 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2925/3393 [23:15<03:48,  2.05batch/s, Batch Loss=0.0355, Avg Loss=0.1117, Time Left=4.40 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2925/3393 [23:15<03:48,  2.05batch/s, Batch Loss=0.4206, Avg Loss=0.1118, Time Left=4.39 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2926/3393 [23:15<03:49,  2.04batch/s, Batch Loss=0.4206, Avg Loss=0.1118, Time Left=4.39 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2926/3393 [23:16<03:49,  2.04batch/s, Batch Loss=0.0051, Avg Loss=0.1118, Time Left=4.39 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2927/3393 [23:16<03:45,  2.06batch/s, Batch Loss=0.0051, Avg Loss=0.1118, Time Left=4.39 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2927/3393 [23:16<03:45,  2.06batch/s, Batch Loss=0.1369, Avg Loss=0.1118, Time Left=4.38 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2928/3393 [23:16<03:44,  2.07batch/s, Batch Loss=0.1369, Avg Loss=0.1118, Time Left=4.38 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2928/3393 [23:17<03:44,  2.07batch/s, Batch Loss=0.0239, Avg Loss=0.1117, Time Left=4.37 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2929/3393 [23:17<03:44,  2.07batch/s, Batch Loss=0.0239, Avg Loss=0.1117, Time Left=4.37 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2929/3393 [23:17<03:44,  2.07batch/s, Batch Loss=0.0264, Avg Loss=0.1117, Time Left=4.36 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2930/3393 [23:17<03:41,  2.09batch/s, Batch Loss=0.0264, Avg Loss=0.1117, Time Left=4.36 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2930/3393 [23:18<03:41,  2.09batch/s, Batch Loss=0.0026, Avg Loss=0.1117, Time Left=4.35 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2931/3393 [23:18<03:43,  2.07batch/s, Batch Loss=0.0026, Avg Loss=0.1117, Time Left=4.35 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2931/3393 [23:18<03:43,  2.07batch/s, Batch Loss=0.0992, Avg Loss=0.1117, Time Left=4.34 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2932/3393 [23:18<03:43,  2.06batch/s, Batch Loss=0.0992, Avg Loss=0.1117, Time Left=4.34 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2932/3393 [23:19<03:43,  2.06batch/s, Batch Loss=0.0063, Avg Loss=0.1116, Time Left=4.34 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2933/3393 [23:19<03:45,  2.04batch/s, Batch Loss=0.0063, Avg Loss=0.1116, Time Left=4.34 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2933/3393 [23:19<03:45,  2.04batch/s, Batch Loss=0.0056, Avg Loss=0.1116, Time Left=4.33 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2934/3393 [23:19<03:46,  2.03batch/s, Batch Loss=0.0056, Avg Loss=0.1116, Time Left=4.33 \u001b[A\n",
      "Epoch 2/3 - Training:  86%|▊| 2934/3393 [23:20<03:46,  2.03batch/s, Batch Loss=0.0068, Avg Loss=0.1115, Time Left=4.32 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  87%|▊| 2935/3393 [23:20<03:44,  2.04batch/s, Batch Loss=0.0068, Avg Loss=0.1115, Time Left=4.32 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2935/3393 [23:20<03:44,  2.04batch/s, Batch Loss=0.0082, Avg Loss=0.1115, Time Left=4.31 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2936/3393 [23:20<03:43,  2.05batch/s, Batch Loss=0.0082, Avg Loss=0.1115, Time Left=4.31 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2936/3393 [23:21<03:43,  2.05batch/s, Batch Loss=0.2667, Avg Loss=0.1116, Time Left=4.30 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2937/3393 [23:21<03:44,  2.03batch/s, Batch Loss=0.2667, Avg Loss=0.1116, Time Left=4.30 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2937/3393 [23:21<03:44,  2.03batch/s, Batch Loss=0.0018, Avg Loss=0.1115, Time Left=4.30 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2938/3393 [23:21<03:40,  2.06batch/s, Batch Loss=0.0018, Avg Loss=0.1115, Time Left=4.30 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2938/3393 [23:22<03:40,  2.06batch/s, Batch Loss=0.2187, Avg Loss=0.1116, Time Left=4.29 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2939/3393 [23:22<03:41,  2.05batch/s, Batch Loss=0.2187, Avg Loss=0.1116, Time Left=4.29 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2939/3393 [23:22<03:41,  2.05batch/s, Batch Loss=0.2608, Avg Loss=0.1116, Time Left=4.28 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2940/3393 [23:22<03:38,  2.07batch/s, Batch Loss=0.2608, Avg Loss=0.1116, Time Left=4.28 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2940/3393 [23:23<03:38,  2.07batch/s, Batch Loss=0.0639, Avg Loss=0.1116, Time Left=4.27 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2941/3393 [23:23<03:38,  2.07batch/s, Batch Loss=0.0639, Avg Loss=0.1116, Time Left=4.27 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2941/3393 [23:23<03:38,  2.07batch/s, Batch Loss=0.0023, Avg Loss=0.1116, Time Left=4.26 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2942/3393 [23:23<03:37,  2.07batch/s, Batch Loss=0.0023, Avg Loss=0.1116, Time Left=4.26 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2942/3393 [23:24<03:37,  2.07batch/s, Batch Loss=0.1528, Avg Loss=0.1116, Time Left=4.25 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2943/3393 [23:24<03:37,  2.07batch/s, Batch Loss=0.1528, Avg Loss=0.1116, Time Left=4.25 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2943/3393 [23:24<03:37,  2.07batch/s, Batch Loss=0.0019, Avg Loss=0.1115, Time Left=4.25 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2944/3393 [23:24<03:37,  2.07batch/s, Batch Loss=0.0019, Avg Loss=0.1115, Time Left=4.25 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2944/3393 [23:24<03:37,  2.07batch/s, Batch Loss=0.0100, Avg Loss=0.1115, Time Left=4.24 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2945/3393 [23:24<03:36,  2.07batch/s, Batch Loss=0.0100, Avg Loss=0.1115, Time Left=4.24 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2945/3393 [23:25<03:36,  2.07batch/s, Batch Loss=0.1228, Avg Loss=0.1115, Time Left=4.23 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2946/3393 [23:25<03:36,  2.07batch/s, Batch Loss=0.1228, Avg Loss=0.1115, Time Left=4.23 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2946/3393 [23:25<03:36,  2.07batch/s, Batch Loss=0.0865, Avg Loss=0.1115, Time Left=4.22 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2947/3393 [23:25<03:33,  2.09batch/s, Batch Loss=0.0865, Avg Loss=0.1115, Time Left=4.22 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2947/3393 [23:26<03:33,  2.09batch/s, Batch Loss=0.0043, Avg Loss=0.1115, Time Left=4.21 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2948/3393 [23:26<03:33,  2.08batch/s, Batch Loss=0.0043, Avg Loss=0.1115, Time Left=4.21 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2948/3393 [23:26<03:33,  2.08batch/s, Batch Loss=0.0017, Avg Loss=0.1114, Time Left=4.21 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2949/3393 [23:26<03:33,  2.08batch/s, Batch Loss=0.0017, Avg Loss=0.1114, Time Left=4.21 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2949/3393 [23:27<03:33,  2.08batch/s, Batch Loss=0.0599, Avg Loss=0.1114, Time Left=4.20 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2950/3393 [23:27<03:35,  2.05batch/s, Batch Loss=0.0599, Avg Loss=0.1114, Time Left=4.20 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2950/3393 [23:27<03:35,  2.05batch/s, Batch Loss=0.0280, Avg Loss=0.1114, Time Left=4.19 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2951/3393 [23:27<03:36,  2.04batch/s, Batch Loss=0.0280, Avg Loss=0.1114, Time Left=4.19 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2951/3393 [23:28<03:36,  2.04batch/s, Batch Loss=0.1318, Avg Loss=0.1114, Time Left=4.18 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2952/3393 [23:28<03:33,  2.06batch/s, Batch Loss=0.1318, Avg Loss=0.1114, Time Left=4.18 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2952/3393 [23:28<03:33,  2.06batch/s, Batch Loss=0.2653, Avg Loss=0.1114, Time Left=4.17 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2953/3393 [23:28<03:30,  2.09batch/s, Batch Loss=0.2653, Avg Loss=0.1114, Time Left=4.17 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2953/3393 [23:29<03:30,  2.09batch/s, Batch Loss=0.2222, Avg Loss=0.1115, Time Left=4.16 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2954/3393 [23:29<03:30,  2.08batch/s, Batch Loss=0.2222, Avg Loss=0.1115, Time Left=4.16 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2954/3393 [23:29<03:30,  2.08batch/s, Batch Loss=0.0111, Avg Loss=0.1114, Time Left=4.16 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2955/3393 [23:29<03:31,  2.07batch/s, Batch Loss=0.0111, Avg Loss=0.1114, Time Left=4.16 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2955/3393 [23:30<03:31,  2.07batch/s, Batch Loss=0.0625, Avg Loss=0.1114, Time Left=4.15 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2956/3393 [23:30<03:30,  2.08batch/s, Batch Loss=0.0625, Avg Loss=0.1114, Time Left=4.15 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2956/3393 [23:30<03:30,  2.08batch/s, Batch Loss=0.1686, Avg Loss=0.1114, Time Left=4.14 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2957/3393 [23:30<03:28,  2.09batch/s, Batch Loss=0.1686, Avg Loss=0.1114, Time Left=4.14 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2957/3393 [23:31<03:28,  2.09batch/s, Batch Loss=0.1448, Avg Loss=0.1115, Time Left=4.13 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2958/3393 [23:31<03:26,  2.11batch/s, Batch Loss=0.1448, Avg Loss=0.1115, Time Left=4.13 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2958/3393 [23:31<03:26,  2.11batch/s, Batch Loss=0.0452, Avg Loss=0.1114, Time Left=4.12 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2959/3393 [23:31<03:26,  2.10batch/s, Batch Loss=0.0452, Avg Loss=0.1114, Time Left=4.12 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2959/3393 [23:32<03:26,  2.10batch/s, Batch Loss=0.0153, Avg Loss=0.1114, Time Left=4.12 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2960/3393 [23:32<03:25,  2.11batch/s, Batch Loss=0.0153, Avg Loss=0.1114, Time Left=4.12 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2960/3393 [23:32<03:25,  2.11batch/s, Batch Loss=0.1161, Avg Loss=0.1114, Time Left=4.11 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2961/3393 [23:32<03:27,  2.08batch/s, Batch Loss=0.1161, Avg Loss=0.1114, Time Left=4.11 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2961/3393 [23:33<03:27,  2.08batch/s, Batch Loss=0.0234, Avg Loss=0.1114, Time Left=4.10 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2962/3393 [23:33<03:25,  2.10batch/s, Batch Loss=0.0234, Avg Loss=0.1114, Time Left=4.10 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2962/3393 [23:33<03:25,  2.10batch/s, Batch Loss=0.0191, Avg Loss=0.1113, Time Left=4.09 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2963/3393 [23:33<03:24,  2.11batch/s, Batch Loss=0.0191, Avg Loss=0.1113, Time Left=4.09 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2963/3393 [23:34<03:24,  2.11batch/s, Batch Loss=0.0408, Avg Loss=0.1113, Time Left=4.08 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2964/3393 [23:34<03:22,  2.12batch/s, Batch Loss=0.0408, Avg Loss=0.1113, Time Left=4.08 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2964/3393 [23:34<03:22,  2.12batch/s, Batch Loss=0.0489, Avg Loss=0.1113, Time Left=4.07 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2965/3393 [23:34<03:23,  2.10batch/s, Batch Loss=0.0489, Avg Loss=0.1113, Time Left=4.07 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2965/3393 [23:35<03:23,  2.10batch/s, Batch Loss=0.0126, Avg Loss=0.1113, Time Left=4.07 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2966/3393 [23:35<03:28,  2.05batch/s, Batch Loss=0.0126, Avg Loss=0.1113, Time Left=4.07 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2966/3393 [23:35<03:28,  2.05batch/s, Batch Loss=0.0031, Avg Loss=0.1112, Time Left=4.06 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2967/3393 [23:35<03:27,  2.05batch/s, Batch Loss=0.0031, Avg Loss=0.1112, Time Left=4.06 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2967/3393 [23:36<03:27,  2.05batch/s, Batch Loss=0.0085, Avg Loss=0.1112, Time Left=4.05 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  87%|▊| 2968/3393 [23:36<03:28,  2.04batch/s, Batch Loss=0.0085, Avg Loss=0.1112, Time Left=4.05 \u001b[A\n",
      "Epoch 2/3 - Training:  87%|▊| 2968/3393 [23:36<03:28,  2.04batch/s, Batch Loss=0.1040, Avg Loss=0.1112, Time Left=4.04 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2969/3393 [23:36<03:23,  2.09batch/s, Batch Loss=0.1040, Avg Loss=0.1112, Time Left=4.04 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2969/3393 [23:36<03:23,  2.09batch/s, Batch Loss=0.0320, Avg Loss=0.1112, Time Left=4.03 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2970/3393 [23:36<03:24,  2.06batch/s, Batch Loss=0.0320, Avg Loss=0.1112, Time Left=4.03 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2970/3393 [23:37<03:24,  2.06batch/s, Batch Loss=0.1259, Avg Loss=0.1112, Time Left=4.03 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2971/3393 [23:37<03:22,  2.09batch/s, Batch Loss=0.1259, Avg Loss=0.1112, Time Left=4.03 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2971/3393 [23:37<03:22,  2.09batch/s, Batch Loss=0.0871, Avg Loss=0.1112, Time Left=4.02 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2972/3393 [23:37<03:20,  2.10batch/s, Batch Loss=0.0871, Avg Loss=0.1112, Time Left=4.02 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2972/3393 [23:38<03:20,  2.10batch/s, Batch Loss=0.0385, Avg Loss=0.1111, Time Left=4.01 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2973/3393 [23:38<03:22,  2.07batch/s, Batch Loss=0.0385, Avg Loss=0.1111, Time Left=4.01 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2973/3393 [23:38<03:22,  2.07batch/s, Batch Loss=0.0875, Avg Loss=0.1111, Time Left=4.00 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2974/3393 [23:38<03:26,  2.03batch/s, Batch Loss=0.0875, Avg Loss=0.1111, Time Left=4.00 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2974/3393 [23:39<03:26,  2.03batch/s, Batch Loss=0.0393, Avg Loss=0.1111, Time Left=3.99 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2975/3393 [23:39<03:25,  2.04batch/s, Batch Loss=0.0393, Avg Loss=0.1111, Time Left=3.99 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2975/3393 [23:39<03:25,  2.04batch/s, Batch Loss=0.0278, Avg Loss=0.1111, Time Left=3.98 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2976/3393 [23:39<03:23,  2.05batch/s, Batch Loss=0.0278, Avg Loss=0.1111, Time Left=3.98 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2976/3393 [23:40<03:23,  2.05batch/s, Batch Loss=0.3956, Avg Loss=0.1112, Time Left=3.98 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2977/3393 [23:40<03:22,  2.05batch/s, Batch Loss=0.3956, Avg Loss=0.1112, Time Left=3.98 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2977/3393 [23:40<03:22,  2.05batch/s, Batch Loss=0.0929, Avg Loss=0.1112, Time Left=3.97 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2978/3393 [23:40<03:19,  2.08batch/s, Batch Loss=0.0929, Avg Loss=0.1112, Time Left=3.97 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2978/3393 [23:41<03:19,  2.08batch/s, Batch Loss=0.5328, Avg Loss=0.1113, Time Left=3.96 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2979/3393 [23:41<03:21,  2.05batch/s, Batch Loss=0.5328, Avg Loss=0.1113, Time Left=3.96 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2979/3393 [23:41<03:21,  2.05batch/s, Batch Loss=0.2331, Avg Loss=0.1113, Time Left=3.95 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2980/3393 [23:41<03:18,  2.08batch/s, Batch Loss=0.2331, Avg Loss=0.1113, Time Left=3.95 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2980/3393 [23:42<03:18,  2.08batch/s, Batch Loss=0.0887, Avg Loss=0.1113, Time Left=3.94 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2981/3393 [23:42<03:18,  2.08batch/s, Batch Loss=0.0887, Avg Loss=0.1113, Time Left=3.94 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2981/3393 [23:42<03:18,  2.08batch/s, Batch Loss=0.0796, Avg Loss=0.1113, Time Left=3.94 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2982/3393 [23:42<03:16,  2.10batch/s, Batch Loss=0.0796, Avg Loss=0.1113, Time Left=3.94 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2982/3393 [23:43<03:16,  2.10batch/s, Batch Loss=0.3390, Avg Loss=0.1114, Time Left=3.93 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2983/3393 [23:43<03:16,  2.09batch/s, Batch Loss=0.3390, Avg Loss=0.1114, Time Left=3.93 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2983/3393 [23:43<03:16,  2.09batch/s, Batch Loss=0.0885, Avg Loss=0.1114, Time Left=3.92 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2984/3393 [23:43<03:14,  2.10batch/s, Batch Loss=0.0885, Avg Loss=0.1114, Time Left=3.92 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2984/3393 [23:44<03:14,  2.10batch/s, Batch Loss=0.0372, Avg Loss=0.1114, Time Left=3.91 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2985/3393 [23:44<03:16,  2.07batch/s, Batch Loss=0.0372, Avg Loss=0.1114, Time Left=3.91 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2985/3393 [23:44<03:16,  2.07batch/s, Batch Loss=0.2536, Avg Loss=0.1114, Time Left=3.90 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2986/3393 [23:44<03:12,  2.11batch/s, Batch Loss=0.2536, Avg Loss=0.1114, Time Left=3.90 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2986/3393 [23:45<03:12,  2.11batch/s, Batch Loss=0.0864, Avg Loss=0.1114, Time Left=3.90 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2987/3393 [23:45<03:15,  2.08batch/s, Batch Loss=0.0864, Avg Loss=0.1114, Time Left=3.90 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2987/3393 [23:45<03:15,  2.08batch/s, Batch Loss=0.0776, Avg Loss=0.1114, Time Left=3.89 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2988/3393 [23:45<03:16,  2.07batch/s, Batch Loss=0.0776, Avg Loss=0.1114, Time Left=3.89 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2988/3393 [23:46<03:16,  2.07batch/s, Batch Loss=0.0147, Avg Loss=0.1114, Time Left=3.88 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2989/3393 [23:46<03:12,  2.09batch/s, Batch Loss=0.0147, Avg Loss=0.1114, Time Left=3.88 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2989/3393 [23:46<03:12,  2.09batch/s, Batch Loss=0.0752, Avg Loss=0.1114, Time Left=3.87 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2990/3393 [23:46<03:12,  2.09batch/s, Batch Loss=0.0752, Avg Loss=0.1114, Time Left=3.87 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2990/3393 [23:47<03:12,  2.09batch/s, Batch Loss=0.1052, Avg Loss=0.1114, Time Left=3.86 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2991/3393 [23:47<03:13,  2.08batch/s, Batch Loss=0.1052, Avg Loss=0.1114, Time Left=3.86 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2991/3393 [23:47<03:13,  2.08batch/s, Batch Loss=0.0167, Avg Loss=0.1113, Time Left=3.85 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2992/3393 [23:47<03:09,  2.12batch/s, Batch Loss=0.0167, Avg Loss=0.1113, Time Left=3.85 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2992/3393 [23:48<03:09,  2.12batch/s, Batch Loss=0.2014, Avg Loss=0.1114, Time Left=3.85 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2993/3393 [23:48<03:12,  2.08batch/s, Batch Loss=0.2014, Avg Loss=0.1114, Time Left=3.85 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2993/3393 [23:48<03:12,  2.08batch/s, Batch Loss=0.2171, Avg Loss=0.1114, Time Left=3.84 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2994/3393 [23:48<03:09,  2.10batch/s, Batch Loss=0.2171, Avg Loss=0.1114, Time Left=3.84 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2994/3393 [23:48<03:09,  2.10batch/s, Batch Loss=0.0263, Avg Loss=0.1114, Time Left=3.83 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2995/3393 [23:49<03:10,  2.09batch/s, Batch Loss=0.0263, Avg Loss=0.1114, Time Left=3.83 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2995/3393 [23:49<03:10,  2.09batch/s, Batch Loss=0.0861, Avg Loss=0.1113, Time Left=3.82 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2996/3393 [23:49<03:08,  2.11batch/s, Batch Loss=0.0861, Avg Loss=0.1113, Time Left=3.82 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2996/3393 [23:49<03:08,  2.11batch/s, Batch Loss=0.1084, Avg Loss=0.1113, Time Left=3.81 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2997/3393 [23:49<03:11,  2.07batch/s, Batch Loss=0.1084, Avg Loss=0.1113, Time Left=3.81 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2997/3393 [23:50<03:11,  2.07batch/s, Batch Loss=0.2014, Avg Loss=0.1114, Time Left=3.81 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2998/3393 [23:50<03:06,  2.11batch/s, Batch Loss=0.2014, Avg Loss=0.1114, Time Left=3.81 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2998/3393 [23:50<03:06,  2.11batch/s, Batch Loss=0.1876, Avg Loss=0.1114, Time Left=3.80 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2999/3393 [23:50<03:07,  2.10batch/s, Batch Loss=0.1876, Avg Loss=0.1114, Time Left=3.80 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 2999/3393 [23:51<03:07,  2.10batch/s, Batch Loss=0.2082, Avg Loss=0.1114, Time Left=3.79 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 3000/3393 [23:51<03:07,  2.09batch/s, Batch Loss=0.2082, Avg Loss=0.1114, Time Left=3.79 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 3000/3393 [23:51<03:07,  2.09batch/s, Batch Loss=0.0147, Avg Loss=0.1114, Time Left=3.78 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  88%|▉| 3001/3393 [23:51<03:06,  2.10batch/s, Batch Loss=0.0147, Avg Loss=0.1114, Time Left=3.78 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 3001/3393 [23:52<03:06,  2.10batch/s, Batch Loss=0.0995, Avg Loss=0.1114, Time Left=3.77 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 3002/3393 [23:52<03:11,  2.04batch/s, Batch Loss=0.0995, Avg Loss=0.1114, Time Left=3.77 \u001b[A\n",
      "Epoch 2/3 - Training:  88%|▉| 3002/3393 [23:52<03:11,  2.04batch/s, Batch Loss=0.0934, Avg Loss=0.1114, Time Left=3.76 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3003/3393 [23:52<03:11,  2.04batch/s, Batch Loss=0.0934, Avg Loss=0.1114, Time Left=3.76 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3003/3393 [23:53<03:11,  2.04batch/s, Batch Loss=0.0945, Avg Loss=0.1114, Time Left=3.76 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3004/3393 [23:53<03:10,  2.05batch/s, Batch Loss=0.0945, Avg Loss=0.1114, Time Left=3.76 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3004/3393 [23:53<03:10,  2.05batch/s, Batch Loss=0.1846, Avg Loss=0.1114, Time Left=3.75 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3005/3393 [23:53<03:10,  2.03batch/s, Batch Loss=0.1846, Avg Loss=0.1114, Time Left=3.75 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3005/3393 [23:54<03:10,  2.03batch/s, Batch Loss=0.0523, Avg Loss=0.1114, Time Left=3.74 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3006/3393 [23:54<03:14,  1.99batch/s, Batch Loss=0.0523, Avg Loss=0.1114, Time Left=3.74 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3006/3393 [23:54<03:14,  1.99batch/s, Batch Loss=0.1710, Avg Loss=0.1114, Time Left=3.73 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3007/3393 [23:54<03:14,  1.99batch/s, Batch Loss=0.1710, Avg Loss=0.1114, Time Left=3.73 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3007/3393 [23:55<03:14,  1.99batch/s, Batch Loss=0.0995, Avg Loss=0.1114, Time Left=3.72 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3008/3393 [23:55<03:13,  1.99batch/s, Batch Loss=0.0995, Avg Loss=0.1114, Time Left=3.72 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3008/3393 [23:55<03:13,  1.99batch/s, Batch Loss=0.1199, Avg Loss=0.1114, Time Left=3.72 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3009/3393 [23:55<03:14,  1.97batch/s, Batch Loss=0.1199, Avg Loss=0.1114, Time Left=3.72 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3009/3393 [23:56<03:14,  1.97batch/s, Batch Loss=0.0818, Avg Loss=0.1114, Time Left=3.71 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3010/3393 [23:56<03:13,  1.98batch/s, Batch Loss=0.0818, Avg Loss=0.1114, Time Left=3.71 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3010/3393 [23:56<03:13,  1.98batch/s, Batch Loss=0.0945, Avg Loss=0.1114, Time Left=3.70 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3011/3393 [23:56<03:10,  2.00batch/s, Batch Loss=0.0945, Avg Loss=0.1114, Time Left=3.70 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3011/3393 [23:57<03:10,  2.00batch/s, Batch Loss=0.0444, Avg Loss=0.1114, Time Left=3.69 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3012/3393 [23:57<03:06,  2.04batch/s, Batch Loss=0.0444, Avg Loss=0.1114, Time Left=3.69 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3012/3393 [23:57<03:06,  2.04batch/s, Batch Loss=0.0390, Avg Loss=0.1114, Time Left=3.68 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3013/3393 [23:57<03:03,  2.07batch/s, Batch Loss=0.0390, Avg Loss=0.1114, Time Left=3.68 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3013/3393 [23:58<03:03,  2.07batch/s, Batch Loss=0.0291, Avg Loss=0.1113, Time Left=3.67 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3014/3393 [23:58<03:02,  2.07batch/s, Batch Loss=0.0291, Avg Loss=0.1113, Time Left=3.67 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3014/3393 [23:58<03:02,  2.07batch/s, Batch Loss=0.0564, Avg Loss=0.1113, Time Left=3.67 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3015/3393 [23:58<03:00,  2.09batch/s, Batch Loss=0.0564, Avg Loss=0.1113, Time Left=3.67 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3015/3393 [23:59<03:00,  2.09batch/s, Batch Loss=0.0830, Avg Loss=0.1113, Time Left=3.66 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3016/3393 [23:59<03:00,  2.09batch/s, Batch Loss=0.0830, Avg Loss=0.1113, Time Left=3.66 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3016/3393 [23:59<03:00,  2.09batch/s, Batch Loss=0.1203, Avg Loss=0.1113, Time Left=3.65 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3017/3393 [23:59<03:00,  2.08batch/s, Batch Loss=0.1203, Avg Loss=0.1113, Time Left=3.65 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3017/3393 [24:00<03:00,  2.08batch/s, Batch Loss=0.3913, Avg Loss=0.1114, Time Left=3.64 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3018/3393 [24:00<03:00,  2.07batch/s, Batch Loss=0.3913, Avg Loss=0.1114, Time Left=3.64 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3018/3393 [24:00<03:00,  2.07batch/s, Batch Loss=0.0330, Avg Loss=0.1114, Time Left=3.63 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3019/3393 [24:00<02:56,  2.11batch/s, Batch Loss=0.0330, Avg Loss=0.1114, Time Left=3.63 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3019/3393 [24:01<02:56,  2.11batch/s, Batch Loss=0.1086, Avg Loss=0.1114, Time Left=3.63 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3020/3393 [24:01<02:57,  2.10batch/s, Batch Loss=0.1086, Avg Loss=0.1114, Time Left=3.63 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3020/3393 [24:01<02:57,  2.10batch/s, Batch Loss=0.0062, Avg Loss=0.1113, Time Left=3.62 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3021/3393 [24:01<02:59,  2.07batch/s, Batch Loss=0.0062, Avg Loss=0.1113, Time Left=3.62 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3021/3393 [24:02<02:59,  2.07batch/s, Batch Loss=0.0553, Avg Loss=0.1113, Time Left=3.61 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3022/3393 [24:02<03:01,  2.05batch/s, Batch Loss=0.0553, Avg Loss=0.1113, Time Left=3.61 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3022/3393 [24:02<03:01,  2.05batch/s, Batch Loss=0.0986, Avg Loss=0.1113, Time Left=3.60 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3023/3393 [24:02<03:03,  2.02batch/s, Batch Loss=0.0986, Avg Loss=0.1113, Time Left=3.60 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3023/3393 [24:03<03:03,  2.02batch/s, Batch Loss=0.0524, Avg Loss=0.1113, Time Left=3.59 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3024/3393 [24:03<03:02,  2.03batch/s, Batch Loss=0.0524, Avg Loss=0.1113, Time Left=3.59 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3024/3393 [24:03<03:02,  2.03batch/s, Batch Loss=0.0716, Avg Loss=0.1113, Time Left=3.59 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3025/3393 [24:03<03:00,  2.04batch/s, Batch Loss=0.0716, Avg Loss=0.1113, Time Left=3.59 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3025/3393 [24:04<03:00,  2.04batch/s, Batch Loss=0.1436, Avg Loss=0.1113, Time Left=3.58 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3026/3393 [24:04<02:59,  2.05batch/s, Batch Loss=0.1436, Avg Loss=0.1113, Time Left=3.58 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3026/3393 [24:04<02:59,  2.05batch/s, Batch Loss=0.0591, Avg Loss=0.1113, Time Left=3.57 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3027/3393 [24:04<02:56,  2.07batch/s, Batch Loss=0.0591, Avg Loss=0.1113, Time Left=3.57 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3027/3393 [24:05<02:56,  2.07batch/s, Batch Loss=0.0173, Avg Loss=0.1112, Time Left=3.56 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3028/3393 [24:05<02:57,  2.05batch/s, Batch Loss=0.0173, Avg Loss=0.1112, Time Left=3.56 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3028/3393 [24:05<02:57,  2.05batch/s, Batch Loss=0.2430, Avg Loss=0.1113, Time Left=3.55 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3029/3393 [24:05<02:57,  2.06batch/s, Batch Loss=0.2430, Avg Loss=0.1113, Time Left=3.55 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3029/3393 [24:06<02:57,  2.06batch/s, Batch Loss=0.0060, Avg Loss=0.1112, Time Left=3.54 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3030/3393 [24:06<02:57,  2.05batch/s, Batch Loss=0.0060, Avg Loss=0.1112, Time Left=3.54 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3030/3393 [24:06<02:57,  2.05batch/s, Batch Loss=0.0471, Avg Loss=0.1112, Time Left=3.54 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3031/3393 [24:06<02:55,  2.06batch/s, Batch Loss=0.0471, Avg Loss=0.1112, Time Left=3.54 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3031/3393 [24:07<02:55,  2.06batch/s, Batch Loss=0.0115, Avg Loss=0.1112, Time Left=3.53 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3032/3393 [24:07<02:53,  2.09batch/s, Batch Loss=0.0115, Avg Loss=0.1112, Time Left=3.53 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3032/3393 [24:07<02:53,  2.09batch/s, Batch Loss=0.0487, Avg Loss=0.1112, Time Left=3.52 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3033/3393 [24:07<02:52,  2.08batch/s, Batch Loss=0.0487, Avg Loss=0.1112, Time Left=3.52 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3033/3393 [24:07<02:52,  2.08batch/s, Batch Loss=0.0043, Avg Loss=0.1111, Time Left=3.51 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  89%|▉| 3034/3393 [24:07<02:52,  2.08batch/s, Batch Loss=0.0043, Avg Loss=0.1111, Time Left=3.51 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3034/3393 [24:08<02:52,  2.08batch/s, Batch Loss=0.0739, Avg Loss=0.1111, Time Left=3.50 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3035/3393 [24:08<02:50,  2.10batch/s, Batch Loss=0.0739, Avg Loss=0.1111, Time Left=3.50 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3035/3393 [24:08<02:50,  2.10batch/s, Batch Loss=0.1039, Avg Loss=0.1111, Time Left=3.50 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3036/3393 [24:08<02:50,  2.09batch/s, Batch Loss=0.1039, Avg Loss=0.1111, Time Left=3.50 \u001b[A\n",
      "Epoch 2/3 - Training:  89%|▉| 3036/3393 [24:09<02:50,  2.09batch/s, Batch Loss=0.0355, Avg Loss=0.1111, Time Left=3.49 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3037/3393 [24:09<02:50,  2.08batch/s, Batch Loss=0.0355, Avg Loss=0.1111, Time Left=3.49 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3037/3393 [24:09<02:50,  2.08batch/s, Batch Loss=0.0135, Avg Loss=0.1111, Time Left=3.48 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3038/3393 [24:09<02:49,  2.10batch/s, Batch Loss=0.0135, Avg Loss=0.1111, Time Left=3.48 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3038/3393 [24:10<02:49,  2.10batch/s, Batch Loss=0.0607, Avg Loss=0.1110, Time Left=3.47 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3039/3393 [24:10<02:49,  2.09batch/s, Batch Loss=0.0607, Avg Loss=0.1110, Time Left=3.47 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3039/3393 [24:10<02:49,  2.09batch/s, Batch Loss=0.1961, Avg Loss=0.1111, Time Left=3.46 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3040/3393 [24:10<02:54,  2.02batch/s, Batch Loss=0.1961, Avg Loss=0.1111, Time Left=3.46 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3040/3393 [24:11<02:54,  2.02batch/s, Batch Loss=0.0286, Avg Loss=0.1110, Time Left=3.45 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3041/3393 [24:11<02:52,  2.04batch/s, Batch Loss=0.0286, Avg Loss=0.1110, Time Left=3.45 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3041/3393 [24:11<02:52,  2.04batch/s, Batch Loss=0.0680, Avg Loss=0.1110, Time Left=3.45 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3042/3393 [24:11<02:53,  2.03batch/s, Batch Loss=0.0680, Avg Loss=0.1110, Time Left=3.45 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3042/3393 [24:12<02:53,  2.03batch/s, Batch Loss=0.0683, Avg Loss=0.1110, Time Left=3.44 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3043/3393 [24:12<02:48,  2.08batch/s, Batch Loss=0.0683, Avg Loss=0.1110, Time Left=3.44 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3043/3393 [24:12<02:48,  2.08batch/s, Batch Loss=0.0347, Avg Loss=0.1110, Time Left=3.43 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3044/3393 [24:12<02:48,  2.07batch/s, Batch Loss=0.0347, Avg Loss=0.1110, Time Left=3.43 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3044/3393 [24:13<02:48,  2.07batch/s, Batch Loss=0.0560, Avg Loss=0.1110, Time Left=3.42 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3045/3393 [24:13<02:51,  2.03batch/s, Batch Loss=0.0560, Avg Loss=0.1110, Time Left=3.42 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3045/3393 [24:13<02:51,  2.03batch/s, Batch Loss=0.0045, Avg Loss=0.1109, Time Left=3.41 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3046/3393 [24:13<02:50,  2.04batch/s, Batch Loss=0.0045, Avg Loss=0.1109, Time Left=3.41 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3046/3393 [24:14<02:50,  2.04batch/s, Batch Loss=0.0043, Avg Loss=0.1109, Time Left=3.41 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3047/3393 [24:14<02:53,  1.99batch/s, Batch Loss=0.0043, Avg Loss=0.1109, Time Left=3.41 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3047/3393 [24:14<02:53,  1.99batch/s, Batch Loss=0.3536, Avg Loss=0.1110, Time Left=3.40 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3048/3393 [24:14<02:51,  2.01batch/s, Batch Loss=0.3536, Avg Loss=0.1110, Time Left=3.40 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3048/3393 [24:15<02:51,  2.01batch/s, Batch Loss=0.0237, Avg Loss=0.1109, Time Left=3.39 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3049/3393 [24:15<02:48,  2.05batch/s, Batch Loss=0.0237, Avg Loss=0.1109, Time Left=3.39 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3049/3393 [24:15<02:48,  2.05batch/s, Batch Loss=0.0728, Avg Loss=0.1109, Time Left=3.38 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3050/3393 [24:15<02:48,  2.03batch/s, Batch Loss=0.0728, Avg Loss=0.1109, Time Left=3.38 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3050/3393 [24:16<02:48,  2.03batch/s, Batch Loss=0.0024, Avg Loss=0.1109, Time Left=3.37 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3051/3393 [24:16<02:47,  2.04batch/s, Batch Loss=0.0024, Avg Loss=0.1109, Time Left=3.37 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3051/3393 [24:16<02:47,  2.04batch/s, Batch Loss=0.1200, Avg Loss=0.1109, Time Left=3.36 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3052/3393 [24:16<02:50,  2.00batch/s, Batch Loss=0.1200, Avg Loss=0.1109, Time Left=3.36 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3052/3393 [24:17<02:50,  2.00batch/s, Batch Loss=0.8334, Avg Loss=0.1111, Time Left=3.36 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3053/3393 [24:17<02:47,  2.03batch/s, Batch Loss=0.8334, Avg Loss=0.1111, Time Left=3.36 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3053/3393 [24:17<02:47,  2.03batch/s, Batch Loss=0.0292, Avg Loss=0.1111, Time Left=3.35 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3054/3393 [24:17<02:44,  2.06batch/s, Batch Loss=0.0292, Avg Loss=0.1111, Time Left=3.35 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3054/3393 [24:18<02:44,  2.06batch/s, Batch Loss=0.3064, Avg Loss=0.1112, Time Left=3.34 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3055/3393 [24:18<02:43,  2.06batch/s, Batch Loss=0.3064, Avg Loss=0.1112, Time Left=3.34 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3055/3393 [24:18<02:43,  2.06batch/s, Batch Loss=0.0111, Avg Loss=0.1111, Time Left=3.33 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3056/3393 [24:18<02:43,  2.07batch/s, Batch Loss=0.0111, Avg Loss=0.1111, Time Left=3.33 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3056/3393 [24:19<02:43,  2.07batch/s, Batch Loss=0.0218, Avg Loss=0.1111, Time Left=3.32 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3057/3393 [24:19<02:42,  2.07batch/s, Batch Loss=0.0218, Avg Loss=0.1111, Time Left=3.32 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3057/3393 [24:19<02:42,  2.07batch/s, Batch Loss=0.1144, Avg Loss=0.1111, Time Left=3.32 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3058/3393 [24:19<02:42,  2.06batch/s, Batch Loss=0.1144, Avg Loss=0.1111, Time Left=3.32 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3058/3393 [24:20<02:42,  2.06batch/s, Batch Loss=0.1587, Avg Loss=0.1111, Time Left=3.31 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3059/3393 [24:20<02:40,  2.09batch/s, Batch Loss=0.1587, Avg Loss=0.1111, Time Left=3.31 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3059/3393 [24:20<02:40,  2.09batch/s, Batch Loss=0.0500, Avg Loss=0.1111, Time Left=3.30 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3060/3393 [24:20<02:39,  2.08batch/s, Batch Loss=0.0500, Avg Loss=0.1111, Time Left=3.30 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3060/3393 [24:21<02:39,  2.08batch/s, Batch Loss=0.0587, Avg Loss=0.1111, Time Left=3.29 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3061/3393 [24:21<02:38,  2.10batch/s, Batch Loss=0.0587, Avg Loss=0.1111, Time Left=3.29 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3061/3393 [24:21<02:38,  2.10batch/s, Batch Loss=0.0120, Avg Loss=0.1111, Time Left=3.28 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3062/3393 [24:21<02:39,  2.08batch/s, Batch Loss=0.0120, Avg Loss=0.1111, Time Left=3.28 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3062/3393 [24:22<02:39,  2.08batch/s, Batch Loss=0.1984, Avg Loss=0.1111, Time Left=3.28 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3063/3393 [24:22<02:38,  2.09batch/s, Batch Loss=0.1984, Avg Loss=0.1111, Time Left=3.28 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3063/3393 [24:22<02:38,  2.09batch/s, Batch Loss=0.2071, Avg Loss=0.1111, Time Left=3.27 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3064/3393 [24:22<02:38,  2.08batch/s, Batch Loss=0.2071, Avg Loss=0.1111, Time Left=3.27 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3064/3393 [24:23<02:38,  2.08batch/s, Batch Loss=0.0150, Avg Loss=0.1111, Time Left=3.26 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3065/3393 [24:23<02:36,  2.10batch/s, Batch Loss=0.0150, Avg Loss=0.1111, Time Left=3.26 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3065/3393 [24:23<02:36,  2.10batch/s, Batch Loss=0.1127, Avg Loss=0.1111, Time Left=3.25 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3066/3393 [24:23<02:39,  2.05batch/s, Batch Loss=0.1127, Avg Loss=0.1111, Time Left=3.25 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3066/3393 [24:24<02:39,  2.05batch/s, Batch Loss=0.3536, Avg Loss=0.1112, Time Left=3.24 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  90%|▉| 3067/3393 [24:24<02:35,  2.09batch/s, Batch Loss=0.3536, Avg Loss=0.1112, Time Left=3.24 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3067/3393 [24:24<02:35,  2.09batch/s, Batch Loss=0.0276, Avg Loss=0.1111, Time Left=3.23 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3068/3393 [24:24<02:37,  2.07batch/s, Batch Loss=0.0276, Avg Loss=0.1111, Time Left=3.23 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3068/3393 [24:24<02:37,  2.07batch/s, Batch Loss=0.1303, Avg Loss=0.1112, Time Left=3.23 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3069/3393 [24:24<02:36,  2.07batch/s, Batch Loss=0.1303, Avg Loss=0.1112, Time Left=3.23 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3069/3393 [24:25<02:36,  2.07batch/s, Batch Loss=0.0570, Avg Loss=0.1111, Time Left=3.22 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3070/3393 [24:25<02:34,  2.09batch/s, Batch Loss=0.0570, Avg Loss=0.1111, Time Left=3.22 \u001b[A\n",
      "Epoch 2/3 - Training:  90%|▉| 3070/3393 [24:25<02:34,  2.09batch/s, Batch Loss=0.0487, Avg Loss=0.1111, Time Left=3.21 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3071/3393 [24:25<02:33,  2.10batch/s, Batch Loss=0.0487, Avg Loss=0.1111, Time Left=3.21 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3071/3393 [24:26<02:33,  2.10batch/s, Batch Loss=0.0178, Avg Loss=0.1111, Time Left=3.20 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3072/3393 [24:26<02:33,  2.09batch/s, Batch Loss=0.0178, Avg Loss=0.1111, Time Left=3.20 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3072/3393 [24:26<02:33,  2.09batch/s, Batch Loss=0.0485, Avg Loss=0.1111, Time Left=3.19 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3073/3393 [24:26<02:31,  2.11batch/s, Batch Loss=0.0485, Avg Loss=0.1111, Time Left=3.19 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3073/3393 [24:27<02:31,  2.11batch/s, Batch Loss=0.1101, Avg Loss=0.1111, Time Left=3.19 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3074/3393 [24:27<02:33,  2.08batch/s, Batch Loss=0.1101, Avg Loss=0.1111, Time Left=3.19 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3074/3393 [24:27<02:33,  2.08batch/s, Batch Loss=0.0266, Avg Loss=0.1110, Time Left=3.18 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3075/3393 [24:27<02:33,  2.07batch/s, Batch Loss=0.0266, Avg Loss=0.1110, Time Left=3.18 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3075/3393 [24:28<02:33,  2.07batch/s, Batch Loss=0.1539, Avg Loss=0.1110, Time Left=3.17 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3076/3393 [24:28<02:31,  2.09batch/s, Batch Loss=0.1539, Avg Loss=0.1110, Time Left=3.17 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3076/3393 [24:28<02:31,  2.09batch/s, Batch Loss=0.0235, Avg Loss=0.1110, Time Left=3.16 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3077/3393 [24:28<02:31,  2.09batch/s, Batch Loss=0.0235, Avg Loss=0.1110, Time Left=3.16 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3077/3393 [24:29<02:31,  2.09batch/s, Batch Loss=0.1817, Avg Loss=0.1110, Time Left=3.15 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3078/3393 [24:29<02:32,  2.06batch/s, Batch Loss=0.1817, Avg Loss=0.1110, Time Left=3.15 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3078/3393 [24:29<02:32,  2.06batch/s, Batch Loss=0.0124, Avg Loss=0.1110, Time Left=3.14 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3079/3393 [24:29<02:30,  2.08batch/s, Batch Loss=0.0124, Avg Loss=0.1110, Time Left=3.14 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3079/3393 [24:30<02:30,  2.08batch/s, Batch Loss=0.0531, Avg Loss=0.1110, Time Left=3.14 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3080/3393 [24:30<02:30,  2.08batch/s, Batch Loss=0.0531, Avg Loss=0.1110, Time Left=3.14 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3080/3393 [24:30<02:30,  2.08batch/s, Batch Loss=0.2456, Avg Loss=0.1110, Time Left=3.13 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3081/3393 [24:30<02:30,  2.07batch/s, Batch Loss=0.2456, Avg Loss=0.1110, Time Left=3.13 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3081/3393 [24:31<02:30,  2.07batch/s, Batch Loss=0.0493, Avg Loss=0.1110, Time Left=3.12 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3082/3393 [24:31<02:27,  2.11batch/s, Batch Loss=0.0493, Avg Loss=0.1110, Time Left=3.12 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3082/3393 [24:31<02:27,  2.11batch/s, Batch Loss=0.0053, Avg Loss=0.1110, Time Left=3.11 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3083/3393 [24:31<02:28,  2.08batch/s, Batch Loss=0.0053, Avg Loss=0.1110, Time Left=3.11 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3083/3393 [24:32<02:28,  2.08batch/s, Batch Loss=0.2110, Avg Loss=0.1110, Time Left=3.10 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3084/3393 [24:32<02:28,  2.08batch/s, Batch Loss=0.2110, Avg Loss=0.1110, Time Left=3.10 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3084/3393 [24:32<02:28,  2.08batch/s, Batch Loss=0.0210, Avg Loss=0.1110, Time Left=3.10 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3085/3393 [24:32<02:25,  2.12batch/s, Batch Loss=0.0210, Avg Loss=0.1110, Time Left=3.10 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3085/3393 [24:33<02:25,  2.12batch/s, Batch Loss=0.0749, Avg Loss=0.1110, Time Left=3.09 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3086/3393 [24:33<02:25,  2.10batch/s, Batch Loss=0.0749, Avg Loss=0.1110, Time Left=3.09 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3086/3393 [24:33<02:25,  2.10batch/s, Batch Loss=0.0486, Avg Loss=0.1110, Time Left=3.08 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3087/3393 [24:33<02:27,  2.08batch/s, Batch Loss=0.0486, Avg Loss=0.1110, Time Left=3.08 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3087/3393 [24:34<02:27,  2.08batch/s, Batch Loss=0.0968, Avg Loss=0.1109, Time Left=3.07 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3088/3393 [24:34<02:23,  2.13batch/s, Batch Loss=0.0968, Avg Loss=0.1109, Time Left=3.07 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3088/3393 [24:34<02:23,  2.13batch/s, Batch Loss=0.0734, Avg Loss=0.1109, Time Left=3.06 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3089/3393 [24:34<02:23,  2.11batch/s, Batch Loss=0.0734, Avg Loss=0.1109, Time Left=3.06 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3089/3393 [24:35<02:23,  2.11batch/s, Batch Loss=0.3505, Avg Loss=0.1110, Time Left=3.05 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3090/3393 [24:35<02:25,  2.08batch/s, Batch Loss=0.3505, Avg Loss=0.1110, Time Left=3.05 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3090/3393 [24:35<02:25,  2.08batch/s, Batch Loss=0.0828, Avg Loss=0.1110, Time Left=3.05 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3091/3393 [24:35<02:25,  2.07batch/s, Batch Loss=0.0828, Avg Loss=0.1110, Time Left=3.05 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3091/3393 [24:35<02:25,  2.07batch/s, Batch Loss=0.0249, Avg Loss=0.1110, Time Left=3.04 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3092/3393 [24:35<02:25,  2.07batch/s, Batch Loss=0.0249, Avg Loss=0.1110, Time Left=3.04 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3092/3393 [24:36<02:25,  2.07batch/s, Batch Loss=0.1017, Avg Loss=0.1110, Time Left=3.03 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3093/3393 [24:36<02:25,  2.06batch/s, Batch Loss=0.1017, Avg Loss=0.1110, Time Left=3.03 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3093/3393 [24:36<02:25,  2.06batch/s, Batch Loss=0.0151, Avg Loss=0.1109, Time Left=3.02 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3094/3393 [24:36<02:22,  2.09batch/s, Batch Loss=0.0151, Avg Loss=0.1109, Time Left=3.02 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3094/3393 [24:37<02:22,  2.09batch/s, Batch Loss=0.0265, Avg Loss=0.1109, Time Left=3.01 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3095/3393 [24:37<02:24,  2.06batch/s, Batch Loss=0.0265, Avg Loss=0.1109, Time Left=3.01 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3095/3393 [24:37<02:24,  2.06batch/s, Batch Loss=0.0033, Avg Loss=0.1109, Time Left=3.01 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3096/3393 [24:37<02:22,  2.09batch/s, Batch Loss=0.0033, Avg Loss=0.1109, Time Left=3.01 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3096/3393 [24:38<02:22,  2.09batch/s, Batch Loss=0.0809, Avg Loss=0.1109, Time Left=3.00 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3097/3393 [24:38<02:20,  2.10batch/s, Batch Loss=0.0809, Avg Loss=0.1109, Time Left=3.00 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3097/3393 [24:38<02:20,  2.10batch/s, Batch Loss=0.2119, Avg Loss=0.1109, Time Left=2.99 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3098/3393 [24:38<02:19,  2.11batch/s, Batch Loss=0.2119, Avg Loss=0.1109, Time Left=2.99 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3098/3393 [24:39<02:19,  2.11batch/s, Batch Loss=0.0220, Avg Loss=0.1109, Time Left=2.98 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3099/3393 [24:39<02:22,  2.06batch/s, Batch Loss=0.0220, Avg Loss=0.1109, Time Left=2.98 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3099/3393 [24:39<02:22,  2.06batch/s, Batch Loss=0.0744, Avg Loss=0.1109, Time Left=2.97 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  91%|▉| 3100/3393 [24:39<02:22,  2.06batch/s, Batch Loss=0.0744, Avg Loss=0.1109, Time Left=2.97 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3100/3393 [24:40<02:22,  2.06batch/s, Batch Loss=0.2027, Avg Loss=0.1109, Time Left=2.96 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3101/3393 [24:40<02:24,  2.02batch/s, Batch Loss=0.2027, Avg Loss=0.1109, Time Left=2.96 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3101/3393 [24:40<02:24,  2.02batch/s, Batch Loss=0.1465, Avg Loss=0.1109, Time Left=2.96 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3102/3393 [24:40<02:23,  2.03batch/s, Batch Loss=0.1465, Avg Loss=0.1109, Time Left=2.96 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3102/3393 [24:41<02:23,  2.03batch/s, Batch Loss=0.0102, Avg Loss=0.1109, Time Left=2.95 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3103/3393 [24:41<02:23,  2.02batch/s, Batch Loss=0.0102, Avg Loss=0.1109, Time Left=2.95 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3103/3393 [24:41<02:23,  2.02batch/s, Batch Loss=0.1255, Avg Loss=0.1109, Time Left=2.94 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3104/3393 [24:41<02:21,  2.04batch/s, Batch Loss=0.1255, Avg Loss=0.1109, Time Left=2.94 \u001b[A\n",
      "Epoch 2/3 - Training:  91%|▉| 3104/3393 [24:42<02:21,  2.04batch/s, Batch Loss=0.0987, Avg Loss=0.1109, Time Left=2.93 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3105/3393 [24:42<02:21,  2.04batch/s, Batch Loss=0.0987, Avg Loss=0.1109, Time Left=2.93 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3105/3393 [24:42<02:21,  2.04batch/s, Batch Loss=0.0547, Avg Loss=0.1109, Time Left=2.92 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3106/3393 [24:42<02:21,  2.03batch/s, Batch Loss=0.0547, Avg Loss=0.1109, Time Left=2.92 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3106/3393 [24:43<02:21,  2.03batch/s, Batch Loss=0.0781, Avg Loss=0.1108, Time Left=2.92 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3107/3393 [24:43<02:19,  2.04batch/s, Batch Loss=0.0781, Avg Loss=0.1108, Time Left=2.92 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3107/3393 [24:43<02:19,  2.04batch/s, Batch Loss=0.0429, Avg Loss=0.1108, Time Left=2.91 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3108/3393 [24:43<02:19,  2.05batch/s, Batch Loss=0.0429, Avg Loss=0.1108, Time Left=2.91 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3108/3393 [24:44<02:19,  2.05batch/s, Batch Loss=0.0425, Avg Loss=0.1108, Time Left=2.90 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3109/3393 [24:44<02:18,  2.05batch/s, Batch Loss=0.0425, Avg Loss=0.1108, Time Left=2.90 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3109/3393 [24:44<02:18,  2.05batch/s, Batch Loss=0.1126, Avg Loss=0.1108, Time Left=2.89 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3110/3393 [24:44<02:14,  2.10batch/s, Batch Loss=0.1126, Avg Loss=0.1108, Time Left=2.89 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3110/3393 [24:45<02:14,  2.10batch/s, Batch Loss=0.0403, Avg Loss=0.1108, Time Left=2.88 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3111/3393 [24:45<02:16,  2.07batch/s, Batch Loss=0.0403, Avg Loss=0.1108, Time Left=2.88 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3111/3393 [24:45<02:16,  2.07batch/s, Batch Loss=0.0885, Avg Loss=0.1108, Time Left=2.88 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3112/3393 [24:45<02:15,  2.07batch/s, Batch Loss=0.0885, Avg Loss=0.1108, Time Left=2.88 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3112/3393 [24:46<02:15,  2.07batch/s, Batch Loss=0.0101, Avg Loss=0.1107, Time Left=2.87 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3113/3393 [24:46<02:12,  2.11batch/s, Batch Loss=0.0101, Avg Loss=0.1107, Time Left=2.87 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3113/3393 [24:46<02:12,  2.11batch/s, Batch Loss=0.0406, Avg Loss=0.1107, Time Left=2.86 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3114/3393 [24:46<02:12,  2.10batch/s, Batch Loss=0.0406, Avg Loss=0.1107, Time Left=2.86 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3114/3393 [24:47<02:12,  2.10batch/s, Batch Loss=0.3220, Avg Loss=0.1108, Time Left=2.85 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3115/3393 [24:47<02:15,  2.05batch/s, Batch Loss=0.3220, Avg Loss=0.1108, Time Left=2.85 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3115/3393 [24:47<02:15,  2.05batch/s, Batch Loss=0.0025, Avg Loss=0.1107, Time Left=2.84 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3116/3393 [24:47<02:14,  2.05batch/s, Batch Loss=0.0025, Avg Loss=0.1107, Time Left=2.84 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3116/3393 [24:48<02:14,  2.05batch/s, Batch Loss=0.0909, Avg Loss=0.1107, Time Left=2.83 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3117/3393 [24:48<02:16,  2.02batch/s, Batch Loss=0.0909, Avg Loss=0.1107, Time Left=2.83 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3117/3393 [24:48<02:16,  2.02batch/s, Batch Loss=0.0488, Avg Loss=0.1107, Time Left=2.83 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3118/3393 [24:48<02:14,  2.05batch/s, Batch Loss=0.0488, Avg Loss=0.1107, Time Left=2.83 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3118/3393 [24:49<02:14,  2.05batch/s, Batch Loss=0.1319, Avg Loss=0.1107, Time Left=2.82 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3119/3393 [24:49<02:13,  2.06batch/s, Batch Loss=0.1319, Avg Loss=0.1107, Time Left=2.82 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3119/3393 [24:49<02:13,  2.06batch/s, Batch Loss=0.2300, Avg Loss=0.1108, Time Left=2.81 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3120/3393 [24:49<02:12,  2.06batch/s, Batch Loss=0.2300, Avg Loss=0.1108, Time Left=2.81 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3120/3393 [24:50<02:12,  2.06batch/s, Batch Loss=0.0187, Avg Loss=0.1107, Time Left=2.80 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3121/3393 [24:50<02:10,  2.08batch/s, Batch Loss=0.0187, Avg Loss=0.1107, Time Left=2.80 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3121/3393 [24:50<02:10,  2.08batch/s, Batch Loss=0.4198, Avg Loss=0.1108, Time Left=2.79 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3122/3393 [24:50<02:11,  2.06batch/s, Batch Loss=0.4198, Avg Loss=0.1108, Time Left=2.79 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3122/3393 [24:51<02:11,  2.06batch/s, Batch Loss=0.0805, Avg Loss=0.1108, Time Left=2.79 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3123/3393 [24:51<02:09,  2.08batch/s, Batch Loss=0.0805, Avg Loss=0.1108, Time Left=2.79 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3123/3393 [24:51<02:09,  2.08batch/s, Batch Loss=0.0086, Avg Loss=0.1108, Time Left=2.78 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3124/3393 [24:51<02:08,  2.10batch/s, Batch Loss=0.0086, Avg Loss=0.1108, Time Left=2.78 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3124/3393 [24:51<02:08,  2.10batch/s, Batch Loss=0.1713, Avg Loss=0.1108, Time Left=2.77 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3125/3393 [24:51<02:08,  2.09batch/s, Batch Loss=0.1713, Avg Loss=0.1108, Time Left=2.77 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3125/3393 [24:52<02:08,  2.09batch/s, Batch Loss=0.2107, Avg Loss=0.1108, Time Left=2.76 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3126/3393 [24:52<02:08,  2.08batch/s, Batch Loss=0.2107, Avg Loss=0.1108, Time Left=2.76 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3126/3393 [24:52<02:08,  2.08batch/s, Batch Loss=0.0473, Avg Loss=0.1108, Time Left=2.75 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3127/3393 [24:52<02:06,  2.10batch/s, Batch Loss=0.0473, Avg Loss=0.1108, Time Left=2.75 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3127/3393 [24:53<02:06,  2.10batch/s, Batch Loss=0.0408, Avg Loss=0.1108, Time Left=2.74 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3128/3393 [24:53<02:07,  2.07batch/s, Batch Loss=0.0408, Avg Loss=0.1108, Time Left=2.74 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3128/3393 [24:53<02:07,  2.07batch/s, Batch Loss=0.2904, Avg Loss=0.1109, Time Left=2.74 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3129/3393 [24:53<02:06,  2.09batch/s, Batch Loss=0.2904, Avg Loss=0.1109, Time Left=2.74 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3129/3393 [24:54<02:06,  2.09batch/s, Batch Loss=0.1312, Avg Loss=0.1109, Time Left=2.73 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3130/3393 [24:54<02:06,  2.08batch/s, Batch Loss=0.1312, Avg Loss=0.1109, Time Left=2.73 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3130/3393 [24:54<02:06,  2.08batch/s, Batch Loss=0.0405, Avg Loss=0.1108, Time Left=2.72 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3131/3393 [24:54<02:09,  2.02batch/s, Batch Loss=0.0405, Avg Loss=0.1108, Time Left=2.72 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3131/3393 [24:55<02:09,  2.02batch/s, Batch Loss=0.1565, Avg Loss=0.1109, Time Left=2.71 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3132/3393 [24:55<02:09,  2.01batch/s, Batch Loss=0.1565, Avg Loss=0.1109, Time Left=2.71 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3132/3393 [24:55<02:09,  2.01batch/s, Batch Loss=0.0201, Avg Loss=0.1108, Time Left=2.70 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  92%|▉| 3133/3393 [24:55<02:10,  1.99batch/s, Batch Loss=0.0201, Avg Loss=0.1108, Time Left=2.70 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3133/3393 [24:56<02:10,  1.99batch/s, Batch Loss=0.1049, Avg Loss=0.1108, Time Left=2.70 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3134/3393 [24:56<02:10,  1.99batch/s, Batch Loss=0.1049, Avg Loss=0.1108, Time Left=2.70 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3134/3393 [24:56<02:10,  1.99batch/s, Batch Loss=0.0626, Avg Loss=0.1108, Time Left=2.69 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3135/3393 [24:56<02:08,  2.01batch/s, Batch Loss=0.0626, Avg Loss=0.1108, Time Left=2.69 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3135/3393 [24:57<02:08,  2.01batch/s, Batch Loss=0.0581, Avg Loss=0.1108, Time Left=2.68 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3136/3393 [24:57<02:05,  2.05batch/s, Batch Loss=0.0581, Avg Loss=0.1108, Time Left=2.68 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3136/3393 [24:57<02:05,  2.05batch/s, Batch Loss=0.0233, Avg Loss=0.1108, Time Left=2.67 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3137/3393 [24:57<02:03,  2.07batch/s, Batch Loss=0.0233, Avg Loss=0.1108, Time Left=2.67 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3137/3393 [24:58<02:03,  2.07batch/s, Batch Loss=0.0094, Avg Loss=0.1107, Time Left=2.66 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3138/3393 [24:58<02:04,  2.06batch/s, Batch Loss=0.0094, Avg Loss=0.1107, Time Left=2.66 \u001b[A\n",
      "Epoch 2/3 - Training:  92%|▉| 3138/3393 [24:58<02:04,  2.06batch/s, Batch Loss=0.1800, Avg Loss=0.1108, Time Left=2.66 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3139/3393 [24:58<02:02,  2.08batch/s, Batch Loss=0.1800, Avg Loss=0.1108, Time Left=2.66 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3139/3393 [24:59<02:02,  2.08batch/s, Batch Loss=0.0296, Avg Loss=0.1107, Time Left=2.65 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3140/3393 [24:59<01:59,  2.12batch/s, Batch Loss=0.0296, Avg Loss=0.1107, Time Left=2.65 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3140/3393 [24:59<01:59,  2.12batch/s, Batch Loss=0.0706, Avg Loss=0.1107, Time Left=2.64 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3141/3393 [24:59<01:58,  2.12batch/s, Batch Loss=0.0706, Avg Loss=0.1107, Time Left=2.64 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3141/3393 [25:00<01:58,  2.12batch/s, Batch Loss=0.1222, Avg Loss=0.1107, Time Left=2.63 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3142/3393 [25:00<01:59,  2.11batch/s, Batch Loss=0.1222, Avg Loss=0.1107, Time Left=2.63 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3142/3393 [25:00<01:59,  2.11batch/s, Batch Loss=0.0744, Avg Loss=0.1107, Time Left=2.62 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3143/3393 [25:00<01:59,  2.10batch/s, Batch Loss=0.0744, Avg Loss=0.1107, Time Left=2.62 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3143/3393 [25:01<01:59,  2.10batch/s, Batch Loss=0.0123, Avg Loss=0.1107, Time Left=2.61 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3144/3393 [25:01<01:59,  2.09batch/s, Batch Loss=0.0123, Avg Loss=0.1107, Time Left=2.61 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3144/3393 [25:01<01:59,  2.09batch/s, Batch Loss=0.0390, Avg Loss=0.1106, Time Left=2.61 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3145/3393 [25:01<01:59,  2.08batch/s, Batch Loss=0.0390, Avg Loss=0.1106, Time Left=2.61 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3145/3393 [25:02<01:59,  2.08batch/s, Batch Loss=0.1750, Avg Loss=0.1107, Time Left=2.60 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3146/3393 [25:02<01:58,  2.09batch/s, Batch Loss=0.1750, Avg Loss=0.1107, Time Left=2.60 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3146/3393 [25:02<01:58,  2.09batch/s, Batch Loss=0.0077, Avg Loss=0.1106, Time Left=2.59 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3147/3393 [25:02<01:58,  2.08batch/s, Batch Loss=0.0077, Avg Loss=0.1106, Time Left=2.59 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3147/3393 [25:03<01:58,  2.08batch/s, Batch Loss=0.1556, Avg Loss=0.1107, Time Left=2.58 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3148/3393 [25:03<01:59,  2.05batch/s, Batch Loss=0.1556, Avg Loss=0.1107, Time Left=2.58 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3148/3393 [25:03<01:59,  2.05batch/s, Batch Loss=0.0329, Avg Loss=0.1106, Time Left=2.57 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3149/3393 [25:03<01:58,  2.05batch/s, Batch Loss=0.0329, Avg Loss=0.1106, Time Left=2.57 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3149/3393 [25:04<01:58,  2.05batch/s, Batch Loss=0.1698, Avg Loss=0.1106, Time Left=2.57 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3150/3393 [25:04<01:58,  2.06batch/s, Batch Loss=0.1698, Avg Loss=0.1106, Time Left=2.57 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3150/3393 [25:04<01:58,  2.06batch/s, Batch Loss=0.0483, Avg Loss=0.1106, Time Left=2.56 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3151/3393 [25:04<01:57,  2.06batch/s, Batch Loss=0.0483, Avg Loss=0.1106, Time Left=2.56 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3151/3393 [25:05<01:57,  2.06batch/s, Batch Loss=0.0652, Avg Loss=0.1106, Time Left=2.55 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3152/3393 [25:05<01:55,  2.08batch/s, Batch Loss=0.0652, Avg Loss=0.1106, Time Left=2.55 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3152/3393 [25:05<01:55,  2.08batch/s, Batch Loss=0.0028, Avg Loss=0.1106, Time Left=2.54 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3153/3393 [25:05<01:58,  2.02batch/s, Batch Loss=0.0028, Avg Loss=0.1106, Time Left=2.54 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3153/3393 [25:06<01:58,  2.02batch/s, Batch Loss=0.0084, Avg Loss=0.1105, Time Left=2.53 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3154/3393 [25:06<01:57,  2.03batch/s, Batch Loss=0.0084, Avg Loss=0.1105, Time Left=2.53 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3154/3393 [25:06<01:57,  2.03batch/s, Batch Loss=0.0095, Avg Loss=0.1105, Time Left=2.52 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3155/3393 [25:06<01:57,  2.02batch/s, Batch Loss=0.0095, Avg Loss=0.1105, Time Left=2.52 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3155/3393 [25:07<01:57,  2.02batch/s, Batch Loss=0.0462, Avg Loss=0.1105, Time Left=2.52 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3156/3393 [25:07<01:56,  2.03batch/s, Batch Loss=0.0462, Avg Loss=0.1105, Time Left=2.52 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3156/3393 [25:07<01:56,  2.03batch/s, Batch Loss=0.0398, Avg Loss=0.1105, Time Left=2.51 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3157/3393 [25:07<01:55,  2.04batch/s, Batch Loss=0.0398, Avg Loss=0.1105, Time Left=2.51 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3157/3393 [25:08<01:55,  2.04batch/s, Batch Loss=0.4145, Avg Loss=0.1106, Time Left=2.50 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3158/3393 [25:08<01:54,  2.05batch/s, Batch Loss=0.4145, Avg Loss=0.1106, Time Left=2.50 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3158/3393 [25:08<01:54,  2.05batch/s, Batch Loss=0.1027, Avg Loss=0.1106, Time Left=2.49 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3159/3393 [25:08<01:52,  2.08batch/s, Batch Loss=0.1027, Avg Loss=0.1106, Time Left=2.49 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3159/3393 [25:08<01:52,  2.08batch/s, Batch Loss=0.0585, Avg Loss=0.1105, Time Left=2.48 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3160/3393 [25:08<01:52,  2.07batch/s, Batch Loss=0.0585, Avg Loss=0.1105, Time Left=2.48 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3160/3393 [25:09<01:52,  2.07batch/s, Batch Loss=0.0020, Avg Loss=0.1105, Time Left=2.48 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3161/3393 [25:09<01:51,  2.07batch/s, Batch Loss=0.0020, Avg Loss=0.1105, Time Left=2.48 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3161/3393 [25:09<01:51,  2.07batch/s, Batch Loss=0.0262, Avg Loss=0.1105, Time Left=2.47 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3162/3393 [25:09<01:50,  2.09batch/s, Batch Loss=0.0262, Avg Loss=0.1105, Time Left=2.47 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3162/3393 [25:10<01:50,  2.09batch/s, Batch Loss=0.0583, Avg Loss=0.1105, Time Left=2.46 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3163/3393 [25:10<01:53,  2.03batch/s, Batch Loss=0.0583, Avg Loss=0.1105, Time Left=2.46 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3163/3393 [25:10<01:53,  2.03batch/s, Batch Loss=0.0288, Avg Loss=0.1104, Time Left=2.45 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3164/3393 [25:10<01:51,  2.05batch/s, Batch Loss=0.0288, Avg Loss=0.1104, Time Left=2.45 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3164/3393 [25:11<01:51,  2.05batch/s, Batch Loss=0.0125, Avg Loss=0.1104, Time Left=2.44 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3165/3393 [25:11<01:50,  2.06batch/s, Batch Loss=0.0125, Avg Loss=0.1104, Time Left=2.44 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3165/3393 [25:11<01:50,  2.06batch/s, Batch Loss=0.0523, Avg Loss=0.1104, Time Left=2.44 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  93%|▉| 3166/3393 [25:11<01:53,  2.00batch/s, Batch Loss=0.0523, Avg Loss=0.1104, Time Left=2.44 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3166/3393 [25:12<01:53,  2.00batch/s, Batch Loss=0.0319, Avg Loss=0.1104, Time Left=2.43 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3167/3393 [25:12<01:51,  2.02batch/s, Batch Loss=0.0319, Avg Loss=0.1104, Time Left=2.43 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3167/3393 [25:12<01:51,  2.02batch/s, Batch Loss=0.3885, Avg Loss=0.1105, Time Left=2.42 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3168/3393 [25:12<01:51,  2.01batch/s, Batch Loss=0.3885, Avg Loss=0.1105, Time Left=2.42 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3168/3393 [25:13<01:51,  2.01batch/s, Batch Loss=0.0106, Avg Loss=0.1104, Time Left=2.41 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3169/3393 [25:13<01:51,  2.01batch/s, Batch Loss=0.0106, Avg Loss=0.1104, Time Left=2.41 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3169/3393 [25:13<01:51,  2.01batch/s, Batch Loss=0.0201, Avg Loss=0.1104, Time Left=2.40 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3170/3393 [25:13<01:48,  2.05batch/s, Batch Loss=0.0201, Avg Loss=0.1104, Time Left=2.40 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3170/3393 [25:14<01:48,  2.05batch/s, Batch Loss=0.0302, Avg Loss=0.1104, Time Left=2.39 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3171/3393 [25:14<01:48,  2.06batch/s, Batch Loss=0.0302, Avg Loss=0.1104, Time Left=2.39 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3171/3393 [25:14<01:48,  2.06batch/s, Batch Loss=0.0053, Avg Loss=0.1103, Time Left=2.39 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3172/3393 [25:14<01:46,  2.08batch/s, Batch Loss=0.0053, Avg Loss=0.1103, Time Left=2.39 \u001b[A\n",
      "Epoch 2/3 - Training:  93%|▉| 3172/3393 [25:15<01:46,  2.08batch/s, Batch Loss=0.0797, Avg Loss=0.1103, Time Left=2.38 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3173/3393 [25:15<01:47,  2.05batch/s, Batch Loss=0.0797, Avg Loss=0.1103, Time Left=2.38 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3173/3393 [25:15<01:47,  2.05batch/s, Batch Loss=0.0035, Avg Loss=0.1103, Time Left=2.37 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3174/3393 [25:15<01:45,  2.08batch/s, Batch Loss=0.0035, Avg Loss=0.1103, Time Left=2.37 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3174/3393 [25:16<01:45,  2.08batch/s, Batch Loss=0.3300, Avg Loss=0.1104, Time Left=2.36 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3175/3393 [25:16<01:46,  2.06batch/s, Batch Loss=0.3300, Avg Loss=0.1104, Time Left=2.36 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3175/3393 [25:16<01:46,  2.06batch/s, Batch Loss=0.0017, Avg Loss=0.1103, Time Left=2.35 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3176/3393 [25:16<01:46,  2.04batch/s, Batch Loss=0.0017, Avg Loss=0.1103, Time Left=2.35 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3176/3393 [25:17<01:46,  2.04batch/s, Batch Loss=0.3110, Avg Loss=0.1104, Time Left=2.35 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3177/3393 [25:17<01:44,  2.07batch/s, Batch Loss=0.3110, Avg Loss=0.1104, Time Left=2.35 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3177/3393 [25:17<01:44,  2.07batch/s, Batch Loss=0.0896, Avg Loss=0.1104, Time Left=2.34 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3178/3393 [25:17<01:45,  2.05batch/s, Batch Loss=0.0896, Avg Loss=0.1104, Time Left=2.34 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3178/3393 [25:18<01:45,  2.05batch/s, Batch Loss=0.2269, Avg Loss=0.1104, Time Left=2.33 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3179/3393 [25:18<01:44,  2.05batch/s, Batch Loss=0.2269, Avg Loss=0.1104, Time Left=2.33 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3179/3393 [25:18<01:44,  2.05batch/s, Batch Loss=0.0780, Avg Loss=0.1104, Time Left=2.32 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3180/3393 [25:18<01:41,  2.10batch/s, Batch Loss=0.0780, Avg Loss=0.1104, Time Left=2.32 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3180/3393 [25:19<01:41,  2.10batch/s, Batch Loss=0.0056, Avg Loss=0.1104, Time Left=2.31 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3181/3393 [25:19<01:41,  2.09batch/s, Batch Loss=0.0056, Avg Loss=0.1104, Time Left=2.31 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3181/3393 [25:19<01:41,  2.09batch/s, Batch Loss=0.1241, Avg Loss=0.1104, Time Left=2.30 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3182/3393 [25:19<01:44,  2.02batch/s, Batch Loss=0.1241, Avg Loss=0.1104, Time Left=2.30 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3182/3393 [25:20<01:44,  2.02batch/s, Batch Loss=0.0563, Avg Loss=0.1104, Time Left=2.30 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3183/3393 [25:20<01:42,  2.05batch/s, Batch Loss=0.0563, Avg Loss=0.1104, Time Left=2.30 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3183/3393 [25:20<01:42,  2.05batch/s, Batch Loss=0.4237, Avg Loss=0.1105, Time Left=2.29 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3184/3393 [25:20<01:43,  2.02batch/s, Batch Loss=0.4237, Avg Loss=0.1105, Time Left=2.29 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3184/3393 [25:21<01:43,  2.02batch/s, Batch Loss=0.2923, Avg Loss=0.1105, Time Left=2.28 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3185/3393 [25:21<01:42,  2.03batch/s, Batch Loss=0.2923, Avg Loss=0.1105, Time Left=2.28 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3185/3393 [25:21<01:42,  2.03batch/s, Batch Loss=0.2355, Avg Loss=0.1106, Time Left=2.27 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3186/3393 [25:21<01:40,  2.06batch/s, Batch Loss=0.2355, Avg Loss=0.1106, Time Left=2.27 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3186/3393 [25:22<01:40,  2.06batch/s, Batch Loss=0.1373, Avg Loss=0.1106, Time Left=2.26 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3187/3393 [25:22<01:42,  2.02batch/s, Batch Loss=0.1373, Avg Loss=0.1106, Time Left=2.26 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3187/3393 [25:22<01:42,  2.02batch/s, Batch Loss=0.0429, Avg Loss=0.1105, Time Left=2.26 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3188/3393 [25:22<01:39,  2.06batch/s, Batch Loss=0.0429, Avg Loss=0.1105, Time Left=2.26 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3188/3393 [25:23<01:39,  2.06batch/s, Batch Loss=0.2161, Avg Loss=0.1106, Time Left=2.25 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3189/3393 [25:23<01:40,  2.02batch/s, Batch Loss=0.2161, Avg Loss=0.1106, Time Left=2.25 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3189/3393 [25:23<01:40,  2.02batch/s, Batch Loss=0.1404, Avg Loss=0.1106, Time Left=2.24 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3190/3393 [25:23<01:37,  2.07batch/s, Batch Loss=0.1404, Avg Loss=0.1106, Time Left=2.24 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3190/3393 [25:24<01:37,  2.07batch/s, Batch Loss=0.4756, Avg Loss=0.1107, Time Left=2.23 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3191/3393 [25:24<01:37,  2.07batch/s, Batch Loss=0.4756, Avg Loss=0.1107, Time Left=2.23 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3191/3393 [25:24<01:37,  2.07batch/s, Batch Loss=0.1625, Avg Loss=0.1107, Time Left=2.22 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3192/3393 [25:24<01:36,  2.07batch/s, Batch Loss=0.1625, Avg Loss=0.1107, Time Left=2.22 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3192/3393 [25:25<01:36,  2.07batch/s, Batch Loss=0.1226, Avg Loss=0.1107, Time Left=2.22 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3193/3393 [25:25<01:37,  2.05batch/s, Batch Loss=0.1226, Avg Loss=0.1107, Time Left=2.22 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3193/3393 [25:25<01:37,  2.05batch/s, Batch Loss=0.0126, Avg Loss=0.1107, Time Left=2.21 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3194/3393 [25:25<01:35,  2.07batch/s, Batch Loss=0.0126, Avg Loss=0.1107, Time Left=2.21 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3194/3393 [25:26<01:35,  2.07batch/s, Batch Loss=0.0486, Avg Loss=0.1107, Time Left=2.20 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3195/3393 [25:26<01:36,  2.06batch/s, Batch Loss=0.0486, Avg Loss=0.1107, Time Left=2.20 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3195/3393 [25:26<01:36,  2.06batch/s, Batch Loss=0.0200, Avg Loss=0.1106, Time Left=2.19 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3196/3393 [25:26<01:33,  2.10batch/s, Batch Loss=0.0200, Avg Loss=0.1106, Time Left=2.19 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3196/3393 [25:26<01:33,  2.10batch/s, Batch Loss=0.0263, Avg Loss=0.1106, Time Left=2.18 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3197/3393 [25:26<01:33,  2.09batch/s, Batch Loss=0.0263, Avg Loss=0.1106, Time Left=2.18 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3197/3393 [25:27<01:33,  2.09batch/s, Batch Loss=0.0871, Avg Loss=0.1106, Time Left=2.17 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3198/3393 [25:27<01:33,  2.08batch/s, Batch Loss=0.0871, Avg Loss=0.1106, Time Left=2.17 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3198/3393 [25:27<01:33,  2.08batch/s, Batch Loss=0.0157, Avg Loss=0.1106, Time Left=2.17 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  94%|▉| 3199/3393 [25:27<01:33,  2.08batch/s, Batch Loss=0.0157, Avg Loss=0.1106, Time Left=2.17 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3199/3393 [25:28<01:33,  2.08batch/s, Batch Loss=0.0693, Avg Loss=0.1106, Time Left=2.16 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3200/3393 [25:28<01:36,  2.00batch/s, Batch Loss=0.0693, Avg Loss=0.1106, Time Left=2.16 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3200/3393 [25:28<01:36,  2.00batch/s, Batch Loss=0.0355, Avg Loss=0.1105, Time Left=2.15 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3201/3393 [25:28<01:35,  2.01batch/s, Batch Loss=0.0355, Avg Loss=0.1105, Time Left=2.15 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3201/3393 [25:29<01:35,  2.01batch/s, Batch Loss=0.0256, Avg Loss=0.1105, Time Left=2.14 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3202/3393 [25:29<01:35,  2.01batch/s, Batch Loss=0.0256, Avg Loss=0.1105, Time Left=2.14 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3202/3393 [25:29<01:35,  2.01batch/s, Batch Loss=0.0097, Avg Loss=0.1105, Time Left=2.13 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3203/3393 [25:29<01:32,  2.05batch/s, Batch Loss=0.0097, Avg Loss=0.1105, Time Left=2.13 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3203/3393 [25:30<01:32,  2.05batch/s, Batch Loss=0.1407, Avg Loss=0.1105, Time Left=2.13 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3204/3393 [25:30<01:32,  2.05batch/s, Batch Loss=0.1407, Avg Loss=0.1105, Time Left=2.13 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3204/3393 [25:30<01:32,  2.05batch/s, Batch Loss=0.1788, Avg Loss=0.1105, Time Left=2.12 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3205/3393 [25:30<01:32,  2.04batch/s, Batch Loss=0.1788, Avg Loss=0.1105, Time Left=2.12 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3205/3393 [25:31<01:32,  2.04batch/s, Batch Loss=0.2226, Avg Loss=0.1106, Time Left=2.11 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3206/3393 [25:31<01:29,  2.09batch/s, Batch Loss=0.2226, Avg Loss=0.1106, Time Left=2.11 \u001b[A\n",
      "Epoch 2/3 - Training:  94%|▉| 3206/3393 [25:31<01:29,  2.09batch/s, Batch Loss=0.1446, Avg Loss=0.1106, Time Left=2.10 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3207/3393 [25:31<01:29,  2.08batch/s, Batch Loss=0.1446, Avg Loss=0.1106, Time Left=2.10 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3207/3393 [25:32<01:29,  2.08batch/s, Batch Loss=0.0873, Avg Loss=0.1106, Time Left=2.09 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3208/3393 [25:32<01:30,  2.04batch/s, Batch Loss=0.0873, Avg Loss=0.1106, Time Left=2.09 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3208/3393 [25:32<01:30,  2.04batch/s, Batch Loss=0.0673, Avg Loss=0.1105, Time Left=2.08 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3209/3393 [25:32<01:29,  2.05batch/s, Batch Loss=0.0673, Avg Loss=0.1105, Time Left=2.08 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3209/3393 [25:33<01:29,  2.05batch/s, Batch Loss=0.0449, Avg Loss=0.1105, Time Left=2.08 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3210/3393 [25:33<01:31,  1.99batch/s, Batch Loss=0.0449, Avg Loss=0.1105, Time Left=2.08 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3210/3393 [25:33<01:31,  1.99batch/s, Batch Loss=0.0178, Avg Loss=0.1105, Time Left=2.07 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3211/3393 [25:33<01:31,  1.99batch/s, Batch Loss=0.0178, Avg Loss=0.1105, Time Left=2.07 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3211/3393 [25:34<01:31,  1.99batch/s, Batch Loss=0.2123, Avg Loss=0.1105, Time Left=2.06 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3212/3393 [25:34<01:31,  1.97batch/s, Batch Loss=0.2123, Avg Loss=0.1105, Time Left=2.06 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3212/3393 [25:34<01:31,  1.97batch/s, Batch Loss=0.1979, Avg Loss=0.1106, Time Left=2.05 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3213/3393 [25:34<01:28,  2.04batch/s, Batch Loss=0.1979, Avg Loss=0.1106, Time Left=2.05 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3213/3393 [25:35<01:28,  2.04batch/s, Batch Loss=0.0203, Avg Loss=0.1105, Time Left=2.04 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3214/3393 [25:35<01:28,  2.03batch/s, Batch Loss=0.0203, Avg Loss=0.1105, Time Left=2.04 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3214/3393 [25:35<01:28,  2.03batch/s, Batch Loss=0.1888, Avg Loss=0.1105, Time Left=2.04 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3215/3393 [25:35<01:28,  2.00batch/s, Batch Loss=0.1888, Avg Loss=0.1105, Time Left=2.04 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3215/3393 [25:36<01:28,  2.00batch/s, Batch Loss=0.0990, Avg Loss=0.1105, Time Left=2.03 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3216/3393 [25:36<01:27,  2.02batch/s, Batch Loss=0.0990, Avg Loss=0.1105, Time Left=2.03 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3216/3393 [25:36<01:27,  2.02batch/s, Batch Loss=0.0126, Avg Loss=0.1105, Time Left=2.02 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3217/3393 [25:36<01:28,  1.99batch/s, Batch Loss=0.0126, Avg Loss=0.1105, Time Left=2.02 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3217/3393 [25:37<01:28,  1.99batch/s, Batch Loss=0.0194, Avg Loss=0.1105, Time Left=2.01 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3218/3393 [25:37<01:26,  2.02batch/s, Batch Loss=0.0194, Avg Loss=0.1105, Time Left=2.01 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3218/3393 [25:37<01:26,  2.02batch/s, Batch Loss=0.1022, Avg Loss=0.1105, Time Left=2.00 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3219/3393 [25:37<01:24,  2.05batch/s, Batch Loss=0.1022, Avg Loss=0.1105, Time Left=2.00 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3219/3393 [25:38<01:24,  2.05batch/s, Batch Loss=0.0082, Avg Loss=0.1105, Time Left=2.00 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3220/3393 [25:38<01:24,  2.04batch/s, Batch Loss=0.0082, Avg Loss=0.1105, Time Left=2.00 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3220/3393 [25:38<01:24,  2.04batch/s, Batch Loss=0.1014, Avg Loss=0.1104, Time Left=1.99 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3221/3393 [25:38<01:24,  2.04batch/s, Batch Loss=0.1014, Avg Loss=0.1104, Time Left=1.99 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3221/3393 [25:39<01:24,  2.04batch/s, Batch Loss=0.1043, Avg Loss=0.1104, Time Left=1.98 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3222/3393 [25:39<01:24,  2.01batch/s, Batch Loss=0.1043, Avg Loss=0.1104, Time Left=1.98 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3222/3393 [25:39<01:24,  2.01batch/s, Batch Loss=0.1753, Avg Loss=0.1105, Time Left=1.97 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3223/3393 [25:39<01:23,  2.02batch/s, Batch Loss=0.1753, Avg Loss=0.1105, Time Left=1.97 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3223/3393 [25:40<01:23,  2.02batch/s, Batch Loss=0.0411, Avg Loss=0.1104, Time Left=1.96 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3224/3393 [25:40<01:24,  2.00batch/s, Batch Loss=0.0411, Avg Loss=0.1104, Time Left=1.96 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3224/3393 [25:40<01:24,  2.00batch/s, Batch Loss=0.1405, Avg Loss=0.1105, Time Left=1.95 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3225/3393 [25:40<01:23,  2.02batch/s, Batch Loss=0.1405, Avg Loss=0.1105, Time Left=1.95 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3225/3393 [25:41<01:23,  2.02batch/s, Batch Loss=0.3665, Avg Loss=0.1105, Time Left=1.95 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3226/3393 [25:41<01:21,  2.05batch/s, Batch Loss=0.3665, Avg Loss=0.1105, Time Left=1.95 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3226/3393 [25:41<01:21,  2.05batch/s, Batch Loss=0.1974, Avg Loss=0.1106, Time Left=1.94 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3227/3393 [25:41<01:22,  2.02batch/s, Batch Loss=0.1974, Avg Loss=0.1106, Time Left=1.94 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3227/3393 [25:42<01:22,  2.02batch/s, Batch Loss=0.0646, Avg Loss=0.1105, Time Left=1.93 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3228/3393 [25:42<01:21,  2.03batch/s, Batch Loss=0.0646, Avg Loss=0.1105, Time Left=1.93 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3228/3393 [25:42<01:21,  2.03batch/s, Batch Loss=0.0881, Avg Loss=0.1105, Time Left=1.92 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3229/3393 [25:42<01:22,  1.98batch/s, Batch Loss=0.0881, Avg Loss=0.1105, Time Left=1.92 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3229/3393 [25:43<01:22,  1.98batch/s, Batch Loss=0.0860, Avg Loss=0.1105, Time Left=1.91 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3230/3393 [25:43<01:22,  1.98batch/s, Batch Loss=0.0860, Avg Loss=0.1105, Time Left=1.91 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3230/3393 [25:43<01:22,  1.98batch/s, Batch Loss=0.0345, Avg Loss=0.1105, Time Left=1.91 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3231/3393 [25:43<01:22,  1.97batch/s, Batch Loss=0.0345, Avg Loss=0.1105, Time Left=1.91 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3231/3393 [25:44<01:22,  1.97batch/s, Batch Loss=0.0696, Avg Loss=0.1105, Time Left=1.90 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  95%|▉| 3232/3393 [25:44<01:21,  1.98batch/s, Batch Loss=0.0696, Avg Loss=0.1105, Time Left=1.90 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3232/3393 [25:44<01:21,  1.98batch/s, Batch Loss=0.0842, Avg Loss=0.1105, Time Left=1.89 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3233/3393 [25:44<01:19,  2.02batch/s, Batch Loss=0.0842, Avg Loss=0.1105, Time Left=1.89 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3233/3393 [25:45<01:19,  2.02batch/s, Batch Loss=0.0274, Avg Loss=0.1105, Time Left=1.88 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3234/3393 [25:45<01:17,  2.06batch/s, Batch Loss=0.0274, Avg Loss=0.1105, Time Left=1.88 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3234/3393 [25:45<01:17,  2.06batch/s, Batch Loss=0.1099, Avg Loss=0.1105, Time Left=1.87 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3235/3393 [25:45<01:15,  2.10batch/s, Batch Loss=0.1099, Avg Loss=0.1105, Time Left=1.87 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3235/3393 [25:46<01:15,  2.10batch/s, Batch Loss=0.0131, Avg Loss=0.1104, Time Left=1.87 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3236/3393 [25:46<01:14,  2.09batch/s, Batch Loss=0.0131, Avg Loss=0.1104, Time Left=1.87 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3236/3393 [25:46<01:14,  2.09batch/s, Batch Loss=0.0144, Avg Loss=0.1104, Time Left=1.86 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3237/3393 [25:46<01:14,  2.09batch/s, Batch Loss=0.0144, Avg Loss=0.1104, Time Left=1.86 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3237/3393 [25:47<01:14,  2.09batch/s, Batch Loss=0.0681, Avg Loss=0.1104, Time Left=1.85 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3238/3393 [25:47<01:13,  2.10batch/s, Batch Loss=0.0681, Avg Loss=0.1104, Time Left=1.85 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3238/3393 [25:47<01:13,  2.10batch/s, Batch Loss=0.0537, Avg Loss=0.1104, Time Left=1.84 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3239/3393 [25:47<01:12,  2.13batch/s, Batch Loss=0.0537, Avg Loss=0.1104, Time Left=1.84 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3239/3393 [25:48<01:12,  2.13batch/s, Batch Loss=0.0322, Avg Loss=0.1103, Time Left=1.83 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3240/3393 [25:48<01:14,  2.05batch/s, Batch Loss=0.0322, Avg Loss=0.1103, Time Left=1.83 \u001b[A\n",
      "Epoch 2/3 - Training:  95%|▉| 3240/3393 [25:48<01:14,  2.05batch/s, Batch Loss=0.0140, Avg Loss=0.1103, Time Left=1.82 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3241/3393 [25:48<01:14,  2.05batch/s, Batch Loss=0.0140, Avg Loss=0.1103, Time Left=1.82 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3241/3393 [25:49<01:14,  2.05batch/s, Batch Loss=0.1441, Avg Loss=0.1103, Time Left=1.82 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3242/3393 [25:49<01:13,  2.06batch/s, Batch Loss=0.1441, Avg Loss=0.1103, Time Left=1.82 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3242/3393 [25:49<01:13,  2.06batch/s, Batch Loss=0.0797, Avg Loss=0.1103, Time Left=1.81 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3243/3393 [25:49<01:12,  2.08batch/s, Batch Loss=0.0797, Avg Loss=0.1103, Time Left=1.81 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3243/3393 [25:50<01:12,  2.08batch/s, Batch Loss=0.2038, Avg Loss=0.1103, Time Left=1.80 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3244/3393 [25:50<01:10,  2.10batch/s, Batch Loss=0.2038, Avg Loss=0.1103, Time Left=1.80 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3244/3393 [25:50<01:10,  2.10batch/s, Batch Loss=0.0040, Avg Loss=0.1103, Time Left=1.79 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3245/3393 [25:50<01:10,  2.09batch/s, Batch Loss=0.0040, Avg Loss=0.1103, Time Left=1.79 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3245/3393 [25:51<01:10,  2.09batch/s, Batch Loss=0.1738, Avg Loss=0.1103, Time Left=1.78 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3246/3393 [25:51<01:10,  2.08batch/s, Batch Loss=0.1738, Avg Loss=0.1103, Time Left=1.78 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3246/3393 [25:51<01:10,  2.08batch/s, Batch Loss=0.1493, Avg Loss=0.1103, Time Left=1.78 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3247/3393 [25:51<01:10,  2.08batch/s, Batch Loss=0.1493, Avg Loss=0.1103, Time Left=1.78 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3247/3393 [25:52<01:10,  2.08batch/s, Batch Loss=0.0209, Avg Loss=0.1103, Time Left=1.77 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3248/3393 [25:52<01:13,  1.97batch/s, Batch Loss=0.0209, Avg Loss=0.1103, Time Left=1.77 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3248/3393 [25:52<01:13,  1.97batch/s, Batch Loss=0.0083, Avg Loss=0.1103, Time Left=1.76 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3249/3393 [25:52<01:13,  1.96batch/s, Batch Loss=0.0083, Avg Loss=0.1103, Time Left=1.76 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3249/3393 [25:53<01:13,  1.96batch/s, Batch Loss=0.0179, Avg Loss=0.1103, Time Left=1.75 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3250/3393 [25:53<01:11,  1.99batch/s, Batch Loss=0.0179, Avg Loss=0.1103, Time Left=1.75 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3250/3393 [25:53<01:11,  1.99batch/s, Batch Loss=0.2485, Avg Loss=0.1103, Time Left=1.74 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3251/3393 [25:53<01:10,  2.03batch/s, Batch Loss=0.2485, Avg Loss=0.1103, Time Left=1.74 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3251/3393 [25:54<01:10,  2.03batch/s, Batch Loss=0.2247, Avg Loss=0.1103, Time Left=1.73 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3252/3393 [25:54<01:09,  2.02batch/s, Batch Loss=0.2247, Avg Loss=0.1103, Time Left=1.73 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3252/3393 [25:54<01:09,  2.02batch/s, Batch Loss=0.0817, Avg Loss=0.1103, Time Left=1.73 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3253/3393 [25:54<01:08,  2.03batch/s, Batch Loss=0.0817, Avg Loss=0.1103, Time Left=1.73 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3253/3393 [25:55<01:08,  2.03batch/s, Batch Loss=0.0075, Avg Loss=0.1103, Time Left=1.72 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3254/3393 [25:55<01:07,  2.05batch/s, Batch Loss=0.0075, Avg Loss=0.1103, Time Left=1.72 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3254/3393 [25:55<01:07,  2.05batch/s, Batch Loss=0.0737, Avg Loss=0.1103, Time Left=1.71 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3255/3393 [25:55<01:08,  2.02batch/s, Batch Loss=0.0737, Avg Loss=0.1103, Time Left=1.71 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3255/3393 [25:56<01:08,  2.02batch/s, Batch Loss=0.2615, Avg Loss=0.1103, Time Left=1.70 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3256/3393 [25:56<01:07,  2.03batch/s, Batch Loss=0.2615, Avg Loss=0.1103, Time Left=1.70 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3256/3393 [25:56<01:07,  2.03batch/s, Batch Loss=0.0798, Avg Loss=0.1103, Time Left=1.69 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3257/3393 [25:56<01:05,  2.06batch/s, Batch Loss=0.0798, Avg Loss=0.1103, Time Left=1.69 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3257/3393 [25:56<01:05,  2.06batch/s, Batch Loss=0.0543, Avg Loss=0.1103, Time Left=1.69 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3258/3393 [25:56<01:05,  2.06batch/s, Batch Loss=0.0543, Avg Loss=0.1103, Time Left=1.69 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3258/3393 [25:57<01:05,  2.06batch/s, Batch Loss=0.1441, Avg Loss=0.1103, Time Left=1.68 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3259/3393 [25:57<01:05,  2.04batch/s, Batch Loss=0.1441, Avg Loss=0.1103, Time Left=1.68 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3259/3393 [25:57<01:05,  2.04batch/s, Batch Loss=0.0733, Avg Loss=0.1103, Time Left=1.67 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3260/3393 [25:57<01:06,  2.01batch/s, Batch Loss=0.0733, Avg Loss=0.1103, Time Left=1.67 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3260/3393 [25:58<01:06,  2.01batch/s, Batch Loss=0.0420, Avg Loss=0.1103, Time Left=1.66 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3261/3393 [25:58<01:05,  2.01batch/s, Batch Loss=0.0420, Avg Loss=0.1103, Time Left=1.66 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3261/3393 [25:58<01:05,  2.01batch/s, Batch Loss=0.0119, Avg Loss=0.1102, Time Left=1.65 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3262/3393 [25:58<01:04,  2.04batch/s, Batch Loss=0.0119, Avg Loss=0.1102, Time Left=1.65 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3262/3393 [25:59<01:04,  2.04batch/s, Batch Loss=0.0043, Avg Loss=0.1102, Time Left=1.65 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3263/3393 [25:59<01:03,  2.05batch/s, Batch Loss=0.0043, Avg Loss=0.1102, Time Left=1.65 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3263/3393 [25:59<01:03,  2.05batch/s, Batch Loss=0.0154, Avg Loss=0.1102, Time Left=1.64 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3264/3393 [25:59<01:02,  2.08batch/s, Batch Loss=0.0154, Avg Loss=0.1102, Time Left=1.64 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3264/3393 [26:00<01:02,  2.08batch/s, Batch Loss=0.0930, Avg Loss=0.1102, Time Left=1.63 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  96%|▉| 3265/3393 [26:00<01:01,  2.08batch/s, Batch Loss=0.0930, Avg Loss=0.1102, Time Left=1.63 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3265/3393 [26:00<01:01,  2.08batch/s, Batch Loss=0.1866, Avg Loss=0.1102, Time Left=1.62 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3266/3393 [26:00<01:01,  2.07batch/s, Batch Loss=0.1866, Avg Loss=0.1102, Time Left=1.62 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3266/3393 [26:01<01:01,  2.07batch/s, Batch Loss=0.0048, Avg Loss=0.1102, Time Left=1.61 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3267/3393 [26:01<01:00,  2.09batch/s, Batch Loss=0.0048, Avg Loss=0.1102, Time Left=1.61 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3267/3393 [26:01<01:00,  2.09batch/s, Batch Loss=0.4739, Avg Loss=0.1103, Time Left=1.60 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3268/3393 [26:01<00:59,  2.08batch/s, Batch Loss=0.4739, Avg Loss=0.1103, Time Left=1.60 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3268/3393 [26:02<00:59,  2.08batch/s, Batch Loss=0.0951, Avg Loss=0.1103, Time Left=1.60 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3269/3393 [26:02<01:00,  2.05batch/s, Batch Loss=0.0951, Avg Loss=0.1103, Time Left=1.60 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3269/3393 [26:02<01:00,  2.05batch/s, Batch Loss=0.1080, Avg Loss=0.1103, Time Left=1.59 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3270/3393 [26:02<00:59,  2.06batch/s, Batch Loss=0.1080, Avg Loss=0.1103, Time Left=1.59 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3270/3393 [26:03<00:59,  2.06batch/s, Batch Loss=0.0062, Avg Loss=0.1102, Time Left=1.58 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3271/3393 [26:03<01:00,  2.02batch/s, Batch Loss=0.0062, Avg Loss=0.1102, Time Left=1.58 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3271/3393 [26:03<01:00,  2.02batch/s, Batch Loss=0.4676, Avg Loss=0.1104, Time Left=1.57 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3272/3393 [26:03<00:58,  2.06batch/s, Batch Loss=0.4676, Avg Loss=0.1104, Time Left=1.57 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3272/3393 [26:04<00:58,  2.06batch/s, Batch Loss=0.0310, Avg Loss=0.1103, Time Left=1.56 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3273/3393 [26:04<00:58,  2.06batch/s, Batch Loss=0.0310, Avg Loss=0.1103, Time Left=1.56 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3273/3393 [26:04<00:58,  2.06batch/s, Batch Loss=0.0381, Avg Loss=0.1103, Time Left=1.56 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3274/3393 [26:04<00:57,  2.06batch/s, Batch Loss=0.0381, Avg Loss=0.1103, Time Left=1.56 \u001b[A\n",
      "Epoch 2/3 - Training:  96%|▉| 3274/3393 [26:05<00:57,  2.06batch/s, Batch Loss=0.1123, Avg Loss=0.1103, Time Left=1.55 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3275/3393 [26:05<00:56,  2.08batch/s, Batch Loss=0.1123, Avg Loss=0.1103, Time Left=1.55 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3275/3393 [26:05<00:56,  2.08batch/s, Batch Loss=0.0070, Avg Loss=0.1103, Time Left=1.54 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3276/3393 [26:05<00:56,  2.08batch/s, Batch Loss=0.0070, Avg Loss=0.1103, Time Left=1.54 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3276/3393 [26:06<00:56,  2.08batch/s, Batch Loss=0.2965, Avg Loss=0.1103, Time Left=1.53 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3277/3393 [26:06<00:56,  2.05batch/s, Batch Loss=0.2965, Avg Loss=0.1103, Time Left=1.53 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3277/3393 [26:06<00:56,  2.05batch/s, Batch Loss=0.0494, Avg Loss=0.1103, Time Left=1.52 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3278/3393 [26:06<00:55,  2.06batch/s, Batch Loss=0.0494, Avg Loss=0.1103, Time Left=1.52 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3278/3393 [26:07<00:55,  2.06batch/s, Batch Loss=0.3033, Avg Loss=0.1104, Time Left=1.51 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3279/3393 [26:07<00:56,  2.02batch/s, Batch Loss=0.3033, Avg Loss=0.1104, Time Left=1.51 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3279/3393 [26:07<00:56,  2.02batch/s, Batch Loss=0.0402, Avg Loss=0.1104, Time Left=1.51 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3280/3393 [26:07<00:55,  2.04batch/s, Batch Loss=0.0402, Avg Loss=0.1104, Time Left=1.51 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3280/3393 [26:08<00:55,  2.04batch/s, Batch Loss=0.0066, Avg Loss=0.1103, Time Left=1.50 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3281/3393 [26:08<00:55,  2.03batch/s, Batch Loss=0.0066, Avg Loss=0.1103, Time Left=1.50 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3281/3393 [26:08<00:55,  2.03batch/s, Batch Loss=0.0079, Avg Loss=0.1103, Time Left=1.49 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3282/3393 [26:08<00:54,  2.04batch/s, Batch Loss=0.0079, Avg Loss=0.1103, Time Left=1.49 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3282/3393 [26:09<00:54,  2.04batch/s, Batch Loss=0.1605, Avg Loss=0.1103, Time Left=1.48 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3283/3393 [26:09<00:53,  2.07batch/s, Batch Loss=0.1605, Avg Loss=0.1103, Time Left=1.48 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3283/3393 [26:09<00:53,  2.07batch/s, Batch Loss=0.0844, Avg Loss=0.1103, Time Left=1.47 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3284/3393 [26:09<00:53,  2.03batch/s, Batch Loss=0.0844, Avg Loss=0.1103, Time Left=1.47 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3284/3393 [26:10<00:53,  2.03batch/s, Batch Loss=0.1306, Avg Loss=0.1103, Time Left=1.47 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3285/3393 [26:10<00:53,  2.04batch/s, Batch Loss=0.1306, Avg Loss=0.1103, Time Left=1.47 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3285/3393 [26:10<00:53,  2.04batch/s, Batch Loss=0.0212, Avg Loss=0.1103, Time Left=1.46 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3286/3393 [26:10<00:52,  2.05batch/s, Batch Loss=0.0212, Avg Loss=0.1103, Time Left=1.46 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3286/3393 [26:11<00:52,  2.05batch/s, Batch Loss=0.0788, Avg Loss=0.1103, Time Left=1.45 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3287/3393 [26:11<00:51,  2.05batch/s, Batch Loss=0.0788, Avg Loss=0.1103, Time Left=1.45 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3287/3393 [26:11<00:51,  2.05batch/s, Batch Loss=0.0100, Avg Loss=0.1102, Time Left=1.44 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3288/3393 [26:11<00:51,  2.06batch/s, Batch Loss=0.0100, Avg Loss=0.1102, Time Left=1.44 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3288/3393 [26:12<00:51,  2.06batch/s, Batch Loss=0.0220, Avg Loss=0.1102, Time Left=1.43 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3289/3393 [26:12<00:51,  2.02batch/s, Batch Loss=0.0220, Avg Loss=0.1102, Time Left=1.43 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3289/3393 [26:12<00:51,  2.02batch/s, Batch Loss=0.0643, Avg Loss=0.1102, Time Left=1.43 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3290/3393 [26:12<00:50,  2.03batch/s, Batch Loss=0.0643, Avg Loss=0.1102, Time Left=1.43 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3290/3393 [26:13<00:50,  2.03batch/s, Batch Loss=0.0054, Avg Loss=0.1102, Time Left=1.42 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3291/3393 [26:13<00:49,  2.04batch/s, Batch Loss=0.0054, Avg Loss=0.1102, Time Left=1.42 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3291/3393 [26:13<00:49,  2.04batch/s, Batch Loss=0.0647, Avg Loss=0.1101, Time Left=1.41 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3292/3393 [26:13<00:49,  2.03batch/s, Batch Loss=0.0647, Avg Loss=0.1101, Time Left=1.41 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3292/3393 [26:14<00:49,  2.03batch/s, Batch Loss=0.0233, Avg Loss=0.1101, Time Left=1.40 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3293/3393 [26:14<00:48,  2.04batch/s, Batch Loss=0.0233, Avg Loss=0.1101, Time Left=1.40 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3293/3393 [26:14<00:48,  2.04batch/s, Batch Loss=0.0278, Avg Loss=0.1101, Time Left=1.39 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3294/3393 [26:14<00:48,  2.03batch/s, Batch Loss=0.0278, Avg Loss=0.1101, Time Left=1.39 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3294/3393 [26:15<00:48,  2.03batch/s, Batch Loss=0.0366, Avg Loss=0.1101, Time Left=1.38 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3295/3393 [26:15<00:47,  2.08batch/s, Batch Loss=0.0366, Avg Loss=0.1101, Time Left=1.38 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3295/3393 [26:15<00:47,  2.08batch/s, Batch Loss=0.1549, Avg Loss=0.1101, Time Left=1.38 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3296/3393 [26:15<00:46,  2.08batch/s, Batch Loss=0.1549, Avg Loss=0.1101, Time Left=1.38 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3296/3393 [26:15<00:46,  2.08batch/s, Batch Loss=0.0073, Avg Loss=0.1101, Time Left=1.37 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3297/3393 [26:15<00:46,  2.05batch/s, Batch Loss=0.0073, Avg Loss=0.1101, Time Left=1.37 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3297/3393 [26:16<00:46,  2.05batch/s, Batch Loss=0.0368, Avg Loss=0.1100, Time Left=1.36 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  97%|▉| 3298/3393 [26:16<00:45,  2.08batch/s, Batch Loss=0.0368, Avg Loss=0.1100, Time Left=1.36 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3298/3393 [26:16<00:45,  2.08batch/s, Batch Loss=0.1389, Avg Loss=0.1100, Time Left=1.35 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3299/3393 [26:16<00:45,  2.08batch/s, Batch Loss=0.1389, Avg Loss=0.1100, Time Left=1.35 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3299/3393 [26:17<00:45,  2.08batch/s, Batch Loss=0.0933, Avg Loss=0.1100, Time Left=1.34 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3300/3393 [26:17<00:44,  2.09batch/s, Batch Loss=0.0933, Avg Loss=0.1100, Time Left=1.34 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3300/3393 [26:17<00:44,  2.09batch/s, Batch Loss=0.0689, Avg Loss=0.1100, Time Left=1.34 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3301/3393 [26:17<00:44,  2.08batch/s, Batch Loss=0.0689, Avg Loss=0.1100, Time Left=1.34 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3301/3393 [26:18<00:44,  2.08batch/s, Batch Loss=0.0144, Avg Loss=0.1100, Time Left=1.33 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3302/3393 [26:18<00:44,  2.06batch/s, Batch Loss=0.0144, Avg Loss=0.1100, Time Left=1.33 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3302/3393 [26:18<00:44,  2.06batch/s, Batch Loss=0.0242, Avg Loss=0.1100, Time Left=1.32 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3303/3393 [26:18<00:43,  2.05batch/s, Batch Loss=0.0242, Avg Loss=0.1100, Time Left=1.32 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3303/3393 [26:19<00:43,  2.05batch/s, Batch Loss=0.0836, Avg Loss=0.1100, Time Left=1.31 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3304/3393 [26:19<00:43,  2.05batch/s, Batch Loss=0.0836, Avg Loss=0.1100, Time Left=1.31 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3304/3393 [26:19<00:43,  2.05batch/s, Batch Loss=0.0198, Avg Loss=0.1099, Time Left=1.30 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3305/3393 [26:19<00:43,  2.01batch/s, Batch Loss=0.0198, Avg Loss=0.1099, Time Left=1.30 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3305/3393 [26:20<00:43,  2.01batch/s, Batch Loss=0.0276, Avg Loss=0.1099, Time Left=1.29 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3306/3393 [26:20<00:42,  2.03batch/s, Batch Loss=0.0276, Avg Loss=0.1099, Time Left=1.29 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3306/3393 [26:20<00:42,  2.03batch/s, Batch Loss=0.1132, Avg Loss=0.1099, Time Left=1.29 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3307/3393 [26:20<00:42,  2.04batch/s, Batch Loss=0.1132, Avg Loss=0.1099, Time Left=1.29 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3307/3393 [26:21<00:42,  2.04batch/s, Batch Loss=0.0118, Avg Loss=0.1099, Time Left=1.28 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3308/3393 [26:21<00:41,  2.03batch/s, Batch Loss=0.0118, Avg Loss=0.1099, Time Left=1.28 \u001b[A\n",
      "Epoch 2/3 - Training:  97%|▉| 3308/3393 [26:21<00:41,  2.03batch/s, Batch Loss=0.2257, Avg Loss=0.1099, Time Left=1.27 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3309/3393 [26:21<00:40,  2.10batch/s, Batch Loss=0.2257, Avg Loss=0.1099, Time Left=1.27 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3309/3393 [26:22<00:40,  2.10batch/s, Batch Loss=0.0245, Avg Loss=0.1099, Time Left=1.26 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3310/3393 [26:22<00:39,  2.09batch/s, Batch Loss=0.0245, Avg Loss=0.1099, Time Left=1.26 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3310/3393 [26:22<00:39,  2.09batch/s, Batch Loss=0.0304, Avg Loss=0.1099, Time Left=1.25 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3311/3393 [26:22<00:39,  2.08batch/s, Batch Loss=0.0304, Avg Loss=0.1099, Time Left=1.25 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3311/3393 [26:23<00:39,  2.08batch/s, Batch Loss=0.0111, Avg Loss=0.1098, Time Left=1.25 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3312/3393 [26:23<00:38,  2.10batch/s, Batch Loss=0.0111, Avg Loss=0.1098, Time Left=1.25 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3312/3393 [26:23<00:38,  2.10batch/s, Batch Loss=0.3606, Avg Loss=0.1099, Time Left=1.24 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3313/3393 [26:23<00:38,  2.09batch/s, Batch Loss=0.3606, Avg Loss=0.1099, Time Left=1.24 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3313/3393 [26:24<00:38,  2.09batch/s, Batch Loss=0.0037, Avg Loss=0.1099, Time Left=1.23 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3314/3393 [26:24<00:38,  2.06batch/s, Batch Loss=0.0037, Avg Loss=0.1099, Time Left=1.23 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3314/3393 [26:24<00:38,  2.06batch/s, Batch Loss=0.1595, Avg Loss=0.1099, Time Left=1.22 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3315/3393 [26:24<00:37,  2.07batch/s, Batch Loss=0.1595, Avg Loss=0.1099, Time Left=1.22 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3315/3393 [26:25<00:37,  2.07batch/s, Batch Loss=0.0106, Avg Loss=0.1099, Time Left=1.21 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3316/3393 [26:25<00:37,  2.07batch/s, Batch Loss=0.0106, Avg Loss=0.1099, Time Left=1.21 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3316/3393 [26:25<00:37,  2.07batch/s, Batch Loss=0.0441, Avg Loss=0.1098, Time Left=1.21 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3317/3393 [26:25<00:36,  2.06batch/s, Batch Loss=0.0441, Avg Loss=0.1098, Time Left=1.21 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3317/3393 [26:26<00:36,  2.06batch/s, Batch Loss=0.0206, Avg Loss=0.1098, Time Left=1.20 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3318/3393 [26:26<00:36,  2.07batch/s, Batch Loss=0.0206, Avg Loss=0.1098, Time Left=1.20 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3318/3393 [26:26<00:36,  2.07batch/s, Batch Loss=0.0435, Avg Loss=0.1098, Time Left=1.19 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3319/3393 [26:26<00:36,  2.05batch/s, Batch Loss=0.0435, Avg Loss=0.1098, Time Left=1.19 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3319/3393 [26:27<00:36,  2.05batch/s, Batch Loss=0.0353, Avg Loss=0.1098, Time Left=1.18 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3320/3393 [26:27<00:34,  2.09batch/s, Batch Loss=0.0353, Avg Loss=0.1098, Time Left=1.18 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3320/3393 [26:27<00:34,  2.09batch/s, Batch Loss=0.0528, Avg Loss=0.1098, Time Left=1.17 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3321/3393 [26:27<00:34,  2.11batch/s, Batch Loss=0.0528, Avg Loss=0.1098, Time Left=1.17 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3321/3393 [26:28<00:34,  2.11batch/s, Batch Loss=0.1371, Avg Loss=0.1098, Time Left=1.16 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3322/3393 [26:28<00:34,  2.08batch/s, Batch Loss=0.1371, Avg Loss=0.1098, Time Left=1.16 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3322/3393 [26:28<00:34,  2.08batch/s, Batch Loss=0.0215, Avg Loss=0.1097, Time Left=1.16 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3323/3393 [26:28<00:33,  2.09batch/s, Batch Loss=0.0215, Avg Loss=0.1097, Time Left=1.16 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3323/3393 [26:28<00:33,  2.09batch/s, Batch Loss=0.4420, Avg Loss=0.1098, Time Left=1.15 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3324/3393 [26:29<00:32,  2.11batch/s, Batch Loss=0.4420, Avg Loss=0.1098, Time Left=1.15 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3324/3393 [26:29<00:32,  2.11batch/s, Batch Loss=0.1379, Avg Loss=0.1098, Time Left=1.14 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3325/3393 [26:29<00:32,  2.08batch/s, Batch Loss=0.1379, Avg Loss=0.1098, Time Left=1.14 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3325/3393 [26:29<00:32,  2.08batch/s, Batch Loss=0.0216, Avg Loss=0.1098, Time Left=1.13 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3326/3393 [26:29<00:32,  2.09batch/s, Batch Loss=0.0216, Avg Loss=0.1098, Time Left=1.13 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3326/3393 [26:30<00:32,  2.09batch/s, Batch Loss=0.0757, Avg Loss=0.1098, Time Left=1.12 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3327/3393 [26:30<00:31,  2.09batch/s, Batch Loss=0.0757, Avg Loss=0.1098, Time Left=1.12 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3327/3393 [26:30<00:31,  2.09batch/s, Batch Loss=0.0410, Avg Loss=0.1098, Time Left=1.12 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3328/3393 [26:30<00:32,  2.02batch/s, Batch Loss=0.0410, Avg Loss=0.1098, Time Left=1.12 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3328/3393 [26:31<00:32,  2.02batch/s, Batch Loss=0.0446, Avg Loss=0.1098, Time Left=1.11 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3329/3393 [26:31<00:31,  2.03batch/s, Batch Loss=0.0446, Avg Loss=0.1098, Time Left=1.11 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3329/3393 [26:31<00:31,  2.03batch/s, Batch Loss=0.0450, Avg Loss=0.1097, Time Left=1.10 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3330/3393 [26:31<00:30,  2.04batch/s, Batch Loss=0.0450, Avg Loss=0.1097, Time Left=1.10 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3330/3393 [26:32<00:30,  2.04batch/s, Batch Loss=0.0408, Avg Loss=0.1097, Time Left=1.09 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  98%|▉| 3331/3393 [26:32<00:30,  2.05batch/s, Batch Loss=0.0408, Avg Loss=0.1097, Time Left=1.09 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3331/3393 [26:32<00:30,  2.05batch/s, Batch Loss=0.0525, Avg Loss=0.1097, Time Left=1.08 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3332/3393 [26:32<00:30,  2.03batch/s, Batch Loss=0.0525, Avg Loss=0.1097, Time Left=1.08 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3332/3393 [26:33<00:30,  2.03batch/s, Batch Loss=0.0815, Avg Loss=0.1097, Time Left=1.07 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3333/3393 [26:33<00:29,  2.06batch/s, Batch Loss=0.0815, Avg Loss=0.1097, Time Left=1.07 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3333/3393 [26:33<00:29,  2.06batch/s, Batch Loss=0.0061, Avg Loss=0.1097, Time Left=1.07 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3334/3393 [26:33<00:28,  2.07batch/s, Batch Loss=0.0061, Avg Loss=0.1097, Time Left=1.07 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3334/3393 [26:34<00:28,  2.07batch/s, Batch Loss=0.0387, Avg Loss=0.1096, Time Left=1.06 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3335/3393 [26:34<00:28,  2.05batch/s, Batch Loss=0.0387, Avg Loss=0.1096, Time Left=1.06 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3335/3393 [26:34<00:28,  2.05batch/s, Batch Loss=0.1633, Avg Loss=0.1097, Time Left=1.05 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3336/3393 [26:34<00:27,  2.07batch/s, Batch Loss=0.1633, Avg Loss=0.1097, Time Left=1.05 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3336/3393 [26:35<00:27,  2.07batch/s, Batch Loss=0.0320, Avg Loss=0.1096, Time Left=1.04 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3337/3393 [26:35<00:26,  2.07batch/s, Batch Loss=0.0320, Avg Loss=0.1096, Time Left=1.04 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3337/3393 [26:35<00:26,  2.07batch/s, Batch Loss=0.0055, Avg Loss=0.1096, Time Left=1.03 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3338/3393 [26:35<00:26,  2.09batch/s, Batch Loss=0.0055, Avg Loss=0.1096, Time Left=1.03 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3338/3393 [26:36<00:26,  2.09batch/s, Batch Loss=0.0151, Avg Loss=0.1096, Time Left=1.03 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3339/3393 [26:36<00:25,  2.08batch/s, Batch Loss=0.0151, Avg Loss=0.1096, Time Left=1.03 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3339/3393 [26:36<00:25,  2.08batch/s, Batch Loss=0.0170, Avg Loss=0.1095, Time Left=1.02 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3340/3393 [26:36<00:25,  2.10batch/s, Batch Loss=0.0170, Avg Loss=0.1095, Time Left=1.02 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3340/3393 [26:37<00:25,  2.10batch/s, Batch Loss=0.1083, Avg Loss=0.1095, Time Left=1.01 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3341/3393 [26:37<00:24,  2.09batch/s, Batch Loss=0.1083, Avg Loss=0.1095, Time Left=1.01 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3341/3393 [26:37<00:24,  2.09batch/s, Batch Loss=0.0474, Avg Loss=0.1095, Time Left=1.00 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3342/3393 [26:37<00:24,  2.10batch/s, Batch Loss=0.0474, Avg Loss=0.1095, Time Left=1.00 \u001b[A\n",
      "Epoch 2/3 - Training:  98%|▉| 3342/3393 [26:38<00:24,  2.10batch/s, Batch Loss=0.0399, Avg Loss=0.1095, Time Left=0.99 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3343/3393 [26:38<00:23,  2.09batch/s, Batch Loss=0.0399, Avg Loss=0.1095, Time Left=0.99 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3343/3393 [26:38<00:23,  2.09batch/s, Batch Loss=0.0106, Avg Loss=0.1095, Time Left=0.99 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3344/3393 [26:38<00:23,  2.11batch/s, Batch Loss=0.0106, Avg Loss=0.1095, Time Left=0.99 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3344/3393 [26:39<00:23,  2.11batch/s, Batch Loss=0.0781, Avg Loss=0.1095, Time Left=0.98 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3345/3393 [26:39<00:23,  2.07batch/s, Batch Loss=0.0781, Avg Loss=0.1095, Time Left=0.98 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3345/3393 [26:39<00:23,  2.07batch/s, Batch Loss=0.1005, Avg Loss=0.1095, Time Left=0.97 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3346/3393 [26:39<00:22,  2.11batch/s, Batch Loss=0.1005, Avg Loss=0.1095, Time Left=0.97 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3346/3393 [26:40<00:22,  2.11batch/s, Batch Loss=0.0933, Avg Loss=0.1095, Time Left=0.96 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3347/3393 [26:40<00:21,  2.10batch/s, Batch Loss=0.0933, Avg Loss=0.1095, Time Left=0.96 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3347/3393 [26:40<00:21,  2.10batch/s, Batch Loss=0.0695, Avg Loss=0.1094, Time Left=0.95 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3348/3393 [26:40<00:21,  2.05batch/s, Batch Loss=0.0695, Avg Loss=0.1094, Time Left=0.95 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3348/3393 [26:41<00:21,  2.05batch/s, Batch Loss=0.1368, Avg Loss=0.1095, Time Left=0.94 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3349/3393 [26:41<00:21,  2.05batch/s, Batch Loss=0.1368, Avg Loss=0.1095, Time Left=0.94 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3349/3393 [26:41<00:21,  2.05batch/s, Batch Loss=0.1070, Avg Loss=0.1095, Time Left=0.94 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3350/3393 [26:41<00:21,  2.05batch/s, Batch Loss=0.1070, Avg Loss=0.1095, Time Left=0.94 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3350/3393 [26:42<00:21,  2.05batch/s, Batch Loss=0.2178, Avg Loss=0.1095, Time Left=0.93 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3351/3393 [26:42<00:20,  2.06batch/s, Batch Loss=0.2178, Avg Loss=0.1095, Time Left=0.93 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3351/3393 [26:42<00:20,  2.06batch/s, Batch Loss=0.1748, Avg Loss=0.1095, Time Left=0.92 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3352/3393 [26:42<00:19,  2.09batch/s, Batch Loss=0.1748, Avg Loss=0.1095, Time Left=0.92 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3352/3393 [26:42<00:19,  2.09batch/s, Batch Loss=0.0047, Avg Loss=0.1095, Time Left=0.91 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3353/3393 [26:43<00:19,  2.08batch/s, Batch Loss=0.0047, Avg Loss=0.1095, Time Left=0.91 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3353/3393 [26:43<00:19,  2.08batch/s, Batch Loss=0.0568, Avg Loss=0.1095, Time Left=0.90 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3354/3393 [26:43<00:18,  2.10batch/s, Batch Loss=0.0568, Avg Loss=0.1095, Time Left=0.90 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3354/3393 [26:43<00:18,  2.10batch/s, Batch Loss=0.0032, Avg Loss=0.1094, Time Left=0.90 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3355/3393 [26:43<00:18,  2.09batch/s, Batch Loss=0.0032, Avg Loss=0.1094, Time Left=0.90 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3355/3393 [26:44<00:18,  2.09batch/s, Batch Loss=0.0315, Avg Loss=0.1094, Time Left=0.89 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3356/3393 [26:44<00:17,  2.10batch/s, Batch Loss=0.0315, Avg Loss=0.1094, Time Left=0.89 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3356/3393 [26:44<00:17,  2.10batch/s, Batch Loss=0.0579, Avg Loss=0.1094, Time Left=0.88 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3357/3393 [26:44<00:17,  2.09batch/s, Batch Loss=0.0579, Avg Loss=0.1094, Time Left=0.88 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3357/3393 [26:45<00:17,  2.09batch/s, Batch Loss=0.0317, Avg Loss=0.1094, Time Left=0.87 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3358/3393 [26:45<00:16,  2.08batch/s, Batch Loss=0.0317, Avg Loss=0.1094, Time Left=0.87 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3358/3393 [26:45<00:16,  2.08batch/s, Batch Loss=0.1228, Avg Loss=0.1094, Time Left=0.86 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3359/3393 [26:45<00:16,  2.08batch/s, Batch Loss=0.1228, Avg Loss=0.1094, Time Left=0.86 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3359/3393 [26:46<00:16,  2.08batch/s, Batch Loss=0.0017, Avg Loss=0.1093, Time Left=0.85 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3360/3393 [26:46<00:15,  2.08batch/s, Batch Loss=0.0017, Avg Loss=0.1093, Time Left=0.85 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3360/3393 [26:46<00:15,  2.08batch/s, Batch Loss=0.0592, Avg Loss=0.1093, Time Left=0.85 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3361/3393 [26:46<00:15,  2.10batch/s, Batch Loss=0.0592, Avg Loss=0.1093, Time Left=0.85 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3361/3393 [26:47<00:15,  2.10batch/s, Batch Loss=0.0389, Avg Loss=0.1093, Time Left=0.84 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3362/3393 [26:47<00:15,  2.03batch/s, Batch Loss=0.0389, Avg Loss=0.1093, Time Left=0.84 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3362/3393 [26:47<00:15,  2.03batch/s, Batch Loss=0.0587, Avg Loss=0.1093, Time Left=0.83 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3363/3393 [26:47<00:14,  2.06batch/s, Batch Loss=0.0587, Avg Loss=0.1093, Time Left=0.83 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3363/3393 [26:48<00:14,  2.06batch/s, Batch Loss=0.0996, Avg Loss=0.1093, Time Left=0.82 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training:  99%|▉| 3364/3393 [26:48<00:14,  2.00batch/s, Batch Loss=0.0996, Avg Loss=0.1093, Time Left=0.82 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3364/3393 [26:48<00:14,  2.00batch/s, Batch Loss=0.0138, Avg Loss=0.1093, Time Left=0.81 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3365/3393 [26:48<00:13,  2.02batch/s, Batch Loss=0.0138, Avg Loss=0.1093, Time Left=0.81 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3365/3393 [26:49<00:13,  2.02batch/s, Batch Loss=0.0144, Avg Loss=0.1092, Time Left=0.81 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3366/3393 [26:49<00:13,  1.98batch/s, Batch Loss=0.0144, Avg Loss=0.1092, Time Left=0.81 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3366/3393 [26:49<00:13,  1.98batch/s, Batch Loss=0.0201, Avg Loss=0.1092, Time Left=0.80 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3367/3393 [26:49<00:12,  2.00batch/s, Batch Loss=0.0201, Avg Loss=0.1092, Time Left=0.80 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3367/3393 [26:50<00:12,  2.00batch/s, Batch Loss=0.0382, Avg Loss=0.1092, Time Left=0.79 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3368/3393 [26:50<00:12,  2.04batch/s, Batch Loss=0.0382, Avg Loss=0.1092, Time Left=0.79 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3368/3393 [26:50<00:12,  2.04batch/s, Batch Loss=0.2044, Avg Loss=0.1092, Time Left=0.78 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3369/3393 [26:50<00:11,  2.07batch/s, Batch Loss=0.2044, Avg Loss=0.1092, Time Left=0.78 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3369/3393 [26:51<00:11,  2.07batch/s, Batch Loss=0.0942, Avg Loss=0.1092, Time Left=0.77 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3370/3393 [26:51<00:11,  2.09batch/s, Batch Loss=0.0942, Avg Loss=0.1092, Time Left=0.77 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3370/3393 [26:51<00:11,  2.09batch/s, Batch Loss=0.0029, Avg Loss=0.1092, Time Left=0.77 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3371/3393 [26:51<00:10,  2.08batch/s, Batch Loss=0.0029, Avg Loss=0.1092, Time Left=0.77 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3371/3393 [26:52<00:10,  2.08batch/s, Batch Loss=0.0397, Avg Loss=0.1091, Time Left=0.76 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3372/3393 [26:52<00:10,  2.08batch/s, Batch Loss=0.0397, Avg Loss=0.1091, Time Left=0.76 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3372/3393 [26:52<00:10,  2.08batch/s, Batch Loss=0.0515, Avg Loss=0.1091, Time Left=0.75 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3373/3393 [26:52<00:09,  2.09batch/s, Batch Loss=0.0515, Avg Loss=0.1091, Time Left=0.75 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3373/3393 [26:53<00:09,  2.09batch/s, Batch Loss=0.1328, Avg Loss=0.1091, Time Left=0.74 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3374/3393 [26:53<00:09,  2.09batch/s, Batch Loss=0.1328, Avg Loss=0.1091, Time Left=0.74 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3374/3393 [26:53<00:09,  2.09batch/s, Batch Loss=0.0035, Avg Loss=0.1091, Time Left=0.73 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3375/3393 [26:53<00:08,  2.08batch/s, Batch Loss=0.0035, Avg Loss=0.1091, Time Left=0.73 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3375/3393 [26:54<00:08,  2.08batch/s, Batch Loss=0.3421, Avg Loss=0.1092, Time Left=0.72 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3376/3393 [26:54<00:08,  2.10batch/s, Batch Loss=0.3421, Avg Loss=0.1092, Time Left=0.72 \u001b[A\n",
      "Epoch 2/3 - Training:  99%|▉| 3376/3393 [26:54<00:08,  2.10batch/s, Batch Loss=0.0031, Avg Loss=0.1091, Time Left=0.72 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3377/3393 [26:54<00:07,  2.07batch/s, Batch Loss=0.0031, Avg Loss=0.1091, Time Left=0.72 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3377/3393 [26:55<00:07,  2.07batch/s, Batch Loss=0.1662, Avg Loss=0.1092, Time Left=0.71 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3378/3393 [26:55<00:07,  2.05batch/s, Batch Loss=0.1662, Avg Loss=0.1092, Time Left=0.71 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3378/3393 [26:55<00:07,  2.05batch/s, Batch Loss=0.0033, Avg Loss=0.1091, Time Left=0.70 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3379/3393 [26:55<00:06,  2.05batch/s, Batch Loss=0.0033, Avg Loss=0.1091, Time Left=0.70 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3379/3393 [26:56<00:06,  2.05batch/s, Batch Loss=0.0726, Avg Loss=0.1091, Time Left=0.69 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3380/3393 [26:56<00:06,  2.04batch/s, Batch Loss=0.0726, Avg Loss=0.1091, Time Left=0.69 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3380/3393 [26:56<00:06,  2.04batch/s, Batch Loss=0.1012, Avg Loss=0.1091, Time Left=0.68 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3381/3393 [26:56<00:05,  2.04batch/s, Batch Loss=0.1012, Avg Loss=0.1091, Time Left=0.68 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3381/3393 [26:57<00:05,  2.04batch/s, Batch Loss=0.0055, Avg Loss=0.1091, Time Left=0.68 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3382/3393 [26:57<00:05,  2.05batch/s, Batch Loss=0.0055, Avg Loss=0.1091, Time Left=0.68 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3382/3393 [26:57<00:05,  2.05batch/s, Batch Loss=0.0390, Avg Loss=0.1091, Time Left=0.67 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3383/3393 [26:57<00:04,  2.04batch/s, Batch Loss=0.0390, Avg Loss=0.1091, Time Left=0.67 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3383/3393 [26:58<00:04,  2.04batch/s, Batch Loss=0.1187, Avg Loss=0.1091, Time Left=0.66 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3384/3393 [26:58<00:04,  2.09batch/s, Batch Loss=0.1187, Avg Loss=0.1091, Time Left=0.66 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3384/3393 [26:58<00:04,  2.09batch/s, Batch Loss=0.0148, Avg Loss=0.1090, Time Left=0.65 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3385/3393 [26:58<00:03,  2.08batch/s, Batch Loss=0.0148, Avg Loss=0.1090, Time Left=0.65 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3385/3393 [26:59<00:03,  2.08batch/s, Batch Loss=0.0781, Avg Loss=0.1090, Time Left=0.64 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3386/3393 [26:59<00:03,  2.06batch/s, Batch Loss=0.0781, Avg Loss=0.1090, Time Left=0.64 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3386/3393 [26:59<00:03,  2.06batch/s, Batch Loss=0.0073, Avg Loss=0.1090, Time Left=0.64 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3387/3393 [26:59<00:02,  2.06batch/s, Batch Loss=0.0073, Avg Loss=0.1090, Time Left=0.64 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3387/3393 [27:00<00:02,  2.06batch/s, Batch Loss=0.0023, Avg Loss=0.1090, Time Left=0.63 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3388/3393 [27:00<00:02,  2.02batch/s, Batch Loss=0.0023, Avg Loss=0.1090, Time Left=0.63 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3388/3393 [27:00<00:02,  2.02batch/s, Batch Loss=0.0375, Avg Loss=0.1089, Time Left=0.62 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3389/3393 [27:00<00:01,  2.03batch/s, Batch Loss=0.0375, Avg Loss=0.1089, Time Left=0.62 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3389/3393 [27:00<00:01,  2.03batch/s, Batch Loss=0.0037, Avg Loss=0.1089, Time Left=0.61 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3390/3393 [27:01<00:01,  2.02batch/s, Batch Loss=0.0037, Avg Loss=0.1089, Time Left=0.61 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3390/3393 [27:01<00:01,  2.02batch/s, Batch Loss=0.1967, Avg Loss=0.1089, Time Left=0.60 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3391/3393 [27:01<00:00,  2.06batch/s, Batch Loss=0.1967, Avg Loss=0.1089, Time Left=0.60 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3391/3393 [27:01<00:00,  2.06batch/s, Batch Loss=0.0241, Avg Loss=0.1089, Time Left=0.59 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3392/3393 [27:01<00:00,  2.08batch/s, Batch Loss=0.0241, Avg Loss=0.1089, Time Left=0.59 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|▉| 3392/3393 [27:02<00:00,  2.08batch/s, Batch Loss=0.2958, Avg Loss=0.1090, Time Left=0.59 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|█| 3393/3393 [27:02<00:00,  2.05batch/s, Batch Loss=0.2958, Avg Loss=0.1090, Time Left=0.59 \u001b[A\n",
      "Epoch 2/3 - Training: 100%|█| 3393/3393 [27:02<00:00,  2.05batch/s, Batch Loss=0.0809, Avg Loss=0.1090, Time Left=0.58 \u001b[A\n",
      "Epoch 2/3 - Training: 3394batch [27:02,  2.08batch/s, Batch Loss=0.0809, Avg Loss=0.1090, Time Left=0.58 min]          \u001b[A\n",
      "Epoch 2/3 - Training: 3394batch [27:03,  2.08batch/s, Batch Loss=0.1497, Avg Loss=0.1090, Time Left=0.57 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3395batch [27:03,  2.08batch/s, Batch Loss=0.1497, Avg Loss=0.1090, Time Left=0.57 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3395batch [27:03,  2.08batch/s, Batch Loss=0.0121, Avg Loss=0.1089, Time Left=0.56 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3396batch [27:03,  2.02batch/s, Batch Loss=0.0121, Avg Loss=0.1089, Time Left=0.56 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3396batch [27:04,  2.02batch/s, Batch Loss=0.0075, Avg Loss=0.1089, Time Left=0.55 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3397batch [27:04,  2.01batch/s, Batch Loss=0.0075, Avg Loss=0.1089, Time Left=0.55 min]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training: 3397batch [27:04,  2.01batch/s, Batch Loss=0.0644, Avg Loss=0.1089, Time Left=0.55 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3398batch [27:04,  1.99batch/s, Batch Loss=0.0644, Avg Loss=0.1089, Time Left=0.55 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3398batch [27:05,  1.99batch/s, Batch Loss=0.1289, Avg Loss=0.1089, Time Left=0.54 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3399batch [27:05,  1.99batch/s, Batch Loss=0.1289, Avg Loss=0.1089, Time Left=0.54 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3399batch [27:05,  1.99batch/s, Batch Loss=0.1253, Avg Loss=0.1089, Time Left=0.53 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3400batch [27:05,  1.97batch/s, Batch Loss=0.1253, Avg Loss=0.1089, Time Left=0.53 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3400batch [27:06,  1.97batch/s, Batch Loss=0.4285, Avg Loss=0.1090, Time Left=0.52 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3401batch [27:06,  1.98batch/s, Batch Loss=0.4285, Avg Loss=0.1090, Time Left=0.52 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3401batch [27:06,  1.98batch/s, Batch Loss=0.1613, Avg Loss=0.1090, Time Left=0.51 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3402batch [27:06,  2.00batch/s, Batch Loss=0.1613, Avg Loss=0.1090, Time Left=0.51 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3402batch [27:07,  2.00batch/s, Batch Loss=0.2949, Avg Loss=0.1091, Time Left=0.50 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3403batch [27:07,  2.02batch/s, Batch Loss=0.2949, Avg Loss=0.1091, Time Left=0.50 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3403batch [27:07,  2.02batch/s, Batch Loss=0.2867, Avg Loss=0.1091, Time Left=0.50 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3404batch [27:07,  2.04batch/s, Batch Loss=0.2867, Avg Loss=0.1091, Time Left=0.50 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3404batch [27:08,  2.04batch/s, Batch Loss=0.0755, Avg Loss=0.1091, Time Left=0.49 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3405batch [27:08,  2.03batch/s, Batch Loss=0.0755, Avg Loss=0.1091, Time Left=0.49 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3405batch [27:08,  2.03batch/s, Batch Loss=0.0694, Avg Loss=0.1091, Time Left=0.48 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3406batch [27:08,  2.06batch/s, Batch Loss=0.0694, Avg Loss=0.1091, Time Left=0.48 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3406batch [27:09,  2.06batch/s, Batch Loss=0.0304, Avg Loss=0.1091, Time Left=0.47 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3407batch [27:09,  2.06batch/s, Batch Loss=0.0304, Avg Loss=0.1091, Time Left=0.47 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3407batch [27:09,  2.06batch/s, Batch Loss=0.0920, Avg Loss=0.1091, Time Left=0.46 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3408batch [27:09,  2.02batch/s, Batch Loss=0.0920, Avg Loss=0.1091, Time Left=0.46 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3408batch [27:10,  2.02batch/s, Batch Loss=0.0118, Avg Loss=0.1090, Time Left=0.46 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3409batch [27:10,  2.06batch/s, Batch Loss=0.0118, Avg Loss=0.1090, Time Left=0.46 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3409batch [27:10,  2.06batch/s, Batch Loss=0.0476, Avg Loss=0.1090, Time Left=0.45 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3410batch [27:10,  2.08batch/s, Batch Loss=0.0476, Avg Loss=0.1090, Time Left=0.45 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3410batch [27:11,  2.08batch/s, Batch Loss=0.0553, Avg Loss=0.1090, Time Left=0.44 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3411batch [27:11,  2.08batch/s, Batch Loss=0.0553, Avg Loss=0.1090, Time Left=0.44 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3411batch [27:11,  2.08batch/s, Batch Loss=0.0046, Avg Loss=0.1090, Time Left=0.43 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3412batch [27:11,  2.09batch/s, Batch Loss=0.0046, Avg Loss=0.1090, Time Left=0.43 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3412batch [27:12,  2.09batch/s, Batch Loss=0.1239, Avg Loss=0.1090, Time Left=0.42 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3413batch [27:12,  2.09batch/s, Batch Loss=0.1239, Avg Loss=0.1090, Time Left=0.42 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3413batch [27:12,  2.09batch/s, Batch Loss=0.0414, Avg Loss=0.1090, Time Left=0.42 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3414batch [27:12,  2.08batch/s, Batch Loss=0.0414, Avg Loss=0.1090, Time Left=0.42 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3414batch [27:13,  2.08batch/s, Batch Loss=0.1991, Avg Loss=0.1090, Time Left=0.41 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3415batch [27:13,  2.10batch/s, Batch Loss=0.1991, Avg Loss=0.1090, Time Left=0.41 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3415batch [27:13,  2.10batch/s, Batch Loss=0.0132, Avg Loss=0.1090, Time Left=0.40 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3416batch [27:13,  2.09batch/s, Batch Loss=0.0132, Avg Loss=0.1090, Time Left=0.40 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3416batch [27:14,  2.09batch/s, Batch Loss=0.4312, Avg Loss=0.1091, Time Left=0.39 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3417batch [27:14,  2.08batch/s, Batch Loss=0.4312, Avg Loss=0.1091, Time Left=0.39 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3417batch [27:14,  2.08batch/s, Batch Loss=0.1754, Avg Loss=0.1091, Time Left=0.38 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3418batch [27:14,  2.10batch/s, Batch Loss=0.1754, Avg Loss=0.1091, Time Left=0.38 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3418batch [27:15,  2.10batch/s, Batch Loss=0.0510, Avg Loss=0.1091, Time Left=0.37 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3419batch [27:15,  2.07batch/s, Batch Loss=0.0510, Avg Loss=0.1091, Time Left=0.37 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3419batch [27:15,  2.07batch/s, Batch Loss=0.1065, Avg Loss=0.1091, Time Left=0.37 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3420batch [27:15,  2.07batch/s, Batch Loss=0.1065, Avg Loss=0.1091, Time Left=0.37 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3420batch [27:16,  2.07batch/s, Batch Loss=0.0252, Avg Loss=0.1090, Time Left=0.36 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3421batch [27:16,  2.07batch/s, Batch Loss=0.0252, Avg Loss=0.1090, Time Left=0.36 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3421batch [27:16,  2.07batch/s, Batch Loss=0.1034, Avg Loss=0.1090, Time Left=0.35 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3422batch [27:16,  2.03batch/s, Batch Loss=0.1034, Avg Loss=0.1090, Time Left=0.35 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3422batch [27:17,  2.03batch/s, Batch Loss=0.1102, Avg Loss=0.1090, Time Left=0.34 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3423batch [27:17,  2.04batch/s, Batch Loss=0.1102, Avg Loss=0.1090, Time Left=0.34 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3423batch [27:17,  2.04batch/s, Batch Loss=0.0098, Avg Loss=0.1090, Time Left=0.33 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3424batch [27:17,  2.05batch/s, Batch Loss=0.0098, Avg Loss=0.1090, Time Left=0.33 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3424batch [27:18,  2.05batch/s, Batch Loss=0.3219, Avg Loss=0.1091, Time Left=0.33 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3425batch [27:18,  2.05batch/s, Batch Loss=0.3219, Avg Loss=0.1091, Time Left=0.33 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3425batch [27:18,  2.05batch/s, Batch Loss=0.0529, Avg Loss=0.1091, Time Left=0.32 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3426batch [27:18,  2.06batch/s, Batch Loss=0.0529, Avg Loss=0.1091, Time Left=0.32 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3426batch [27:19,  2.06batch/s, Batch Loss=0.1533, Avg Loss=0.1091, Time Left=0.31 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3427batch [27:19,  2.02batch/s, Batch Loss=0.1533, Avg Loss=0.1091, Time Left=0.31 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3427batch [27:19,  2.02batch/s, Batch Loss=0.2389, Avg Loss=0.1091, Time Left=0.30 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3428batch [27:19,  2.07batch/s, Batch Loss=0.2389, Avg Loss=0.1091, Time Left=0.30 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3428batch [27:20,  2.07batch/s, Batch Loss=0.0109, Avg Loss=0.1091, Time Left=0.29 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3429batch [27:20,  2.07batch/s, Batch Loss=0.0109, Avg Loss=0.1091, Time Left=0.29 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3429batch [27:20,  2.07batch/s, Batch Loss=0.0679, Avg Loss=0.1091, Time Left=0.28 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3430batch [27:20,  2.07batch/s, Batch Loss=0.0679, Avg Loss=0.1091, Time Left=0.28 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3430batch [27:20,  2.07batch/s, Batch Loss=0.0174, Avg Loss=0.1090, Time Left=0.28 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3431batch [27:20,  2.09batch/s, Batch Loss=0.0174, Avg Loss=0.1090, Time Left=0.28 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3431batch [27:21,  2.09batch/s, Batch Loss=0.2844, Avg Loss=0.1091, Time Left=0.27 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3432batch [27:21,  2.07batch/s, Batch Loss=0.2844, Avg Loss=0.1091, Time Left=0.27 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3432batch [27:21,  2.07batch/s, Batch Loss=0.1133, Avg Loss=0.1091, Time Left=0.26 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3433batch [27:21,  2.07batch/s, Batch Loss=0.1133, Avg Loss=0.1091, Time Left=0.26 min]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training: 3433batch [27:22,  2.07batch/s, Batch Loss=0.0378, Avg Loss=0.1091, Time Left=0.25 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3434batch [27:22,  2.05batch/s, Batch Loss=0.0378, Avg Loss=0.1091, Time Left=0.25 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3434batch [27:22,  2.05batch/s, Batch Loss=0.1832, Avg Loss=0.1091, Time Left=0.24 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3435batch [27:22,  2.07batch/s, Batch Loss=0.1832, Avg Loss=0.1091, Time Left=0.24 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3435batch [27:23,  2.07batch/s, Batch Loss=0.0415, Avg Loss=0.1091, Time Left=0.24 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3436batch [27:23,  2.09batch/s, Batch Loss=0.0415, Avg Loss=0.1091, Time Left=0.24 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3436batch [27:23,  2.09batch/s, Batch Loss=0.0422, Avg Loss=0.1091, Time Left=0.23 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3437batch [27:23,  2.08batch/s, Batch Loss=0.0422, Avg Loss=0.1091, Time Left=0.23 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3437batch [27:24,  2.08batch/s, Batch Loss=0.0212, Avg Loss=0.1090, Time Left=0.22 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3438batch [27:24,  2.08batch/s, Batch Loss=0.0212, Avg Loss=0.1090, Time Left=0.22 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3438batch [27:24,  2.08batch/s, Batch Loss=0.0123, Avg Loss=0.1090, Time Left=0.21 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3439batch [27:24,  2.10batch/s, Batch Loss=0.0123, Avg Loss=0.1090, Time Left=0.21 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3439batch [27:25,  2.10batch/s, Batch Loss=0.0730, Avg Loss=0.1090, Time Left=0.20 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3440batch [27:25,  2.11batch/s, Batch Loss=0.0730, Avg Loss=0.1090, Time Left=0.20 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3440batch [27:25,  2.11batch/s, Batch Loss=0.0226, Avg Loss=0.1090, Time Left=0.20 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3441batch [27:25,  2.10batch/s, Batch Loss=0.0226, Avg Loss=0.1090, Time Left=0.20 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3441batch [27:26,  2.10batch/s, Batch Loss=0.0067, Avg Loss=0.1089, Time Left=0.19 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3442batch [27:26,  2.07batch/s, Batch Loss=0.0067, Avg Loss=0.1089, Time Left=0.19 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3442batch [27:26,  2.07batch/s, Batch Loss=0.0075, Avg Loss=0.1089, Time Left=0.18 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3443batch [27:26,  2.07batch/s, Batch Loss=0.0075, Avg Loss=0.1089, Time Left=0.18 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3443batch [27:27,  2.07batch/s, Batch Loss=0.0457, Avg Loss=0.1089, Time Left=0.17 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3444batch [27:27,  2.01batch/s, Batch Loss=0.0457, Avg Loss=0.1089, Time Left=0.17 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3444batch [27:27,  2.01batch/s, Batch Loss=0.3075, Avg Loss=0.1089, Time Left=0.16 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3445batch [27:27,  2.02batch/s, Batch Loss=0.3075, Avg Loss=0.1089, Time Left=0.16 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3445batch [27:28,  2.02batch/s, Batch Loss=0.0401, Avg Loss=0.1089, Time Left=0.15 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3446batch [27:28,  2.06batch/s, Batch Loss=0.0401, Avg Loss=0.1089, Time Left=0.15 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3446batch [27:28,  2.06batch/s, Batch Loss=0.0051, Avg Loss=0.1089, Time Left=0.15 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3447batch [27:28,  2.08batch/s, Batch Loss=0.0051, Avg Loss=0.1089, Time Left=0.15 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3447batch [27:29,  2.08batch/s, Batch Loss=0.0456, Avg Loss=0.1089, Time Left=0.14 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3448batch [27:29,  2.10batch/s, Batch Loss=0.0456, Avg Loss=0.1089, Time Left=0.14 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3448batch [27:29,  2.10batch/s, Batch Loss=0.0532, Avg Loss=0.1089, Time Left=0.13 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3449batch [27:29,  2.07batch/s, Batch Loss=0.0532, Avg Loss=0.1089, Time Left=0.13 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3449batch [27:30,  2.07batch/s, Batch Loss=0.0258, Avg Loss=0.1088, Time Left=0.12 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3450batch [27:30,  2.07batch/s, Batch Loss=0.0258, Avg Loss=0.1088, Time Left=0.12 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3450batch [27:30,  2.07batch/s, Batch Loss=0.1029, Avg Loss=0.1088, Time Left=0.11 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3451batch [27:30,  2.07batch/s, Batch Loss=0.1029, Avg Loss=0.1088, Time Left=0.11 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3451batch [27:31,  2.07batch/s, Batch Loss=0.0123, Avg Loss=0.1088, Time Left=0.11 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3452batch [27:31,  2.07batch/s, Batch Loss=0.0123, Avg Loss=0.1088, Time Left=0.11 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3452batch [27:31,  2.07batch/s, Batch Loss=0.0808, Avg Loss=0.1088, Time Left=0.10 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3453batch [27:31,  2.09batch/s, Batch Loss=0.0808, Avg Loss=0.1088, Time Left=0.10 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3453batch [27:32,  2.09batch/s, Batch Loss=0.1416, Avg Loss=0.1088, Time Left=0.09 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3454batch [27:32,  2.08batch/s, Batch Loss=0.1416, Avg Loss=0.1088, Time Left=0.09 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3454batch [27:32,  2.08batch/s, Batch Loss=0.2399, Avg Loss=0.1088, Time Left=0.08 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3455batch [27:32,  2.02batch/s, Batch Loss=0.2399, Avg Loss=0.1088, Time Left=0.08 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3455batch [27:33,  2.02batch/s, Batch Loss=0.0094, Avg Loss=0.1088, Time Left=0.07 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3456batch [27:33,  2.05batch/s, Batch Loss=0.0094, Avg Loss=0.1088, Time Left=0.07 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3456batch [27:33,  2.05batch/s, Batch Loss=0.1622, Avg Loss=0.1088, Time Left=0.07 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3457batch [27:33,  2.04batch/s, Batch Loss=0.1622, Avg Loss=0.1088, Time Left=0.07 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3457batch [27:34,  2.04batch/s, Batch Loss=0.0208, Avg Loss=0.1088, Time Left=0.06 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3458batch [27:34,  2.04batch/s, Batch Loss=0.0208, Avg Loss=0.1088, Time Left=0.06 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3458batch [27:34,  2.04batch/s, Batch Loss=0.0012, Avg Loss=0.1088, Time Left=0.05 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3459batch [27:34,  2.07batch/s, Batch Loss=0.0012, Avg Loss=0.1088, Time Left=0.05 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3459batch [27:34,  2.07batch/s, Batch Loss=0.1776, Avg Loss=0.1088, Time Left=0.04 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3460batch [27:35,  2.07batch/s, Batch Loss=0.1776, Avg Loss=0.1088, Time Left=0.04 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3460batch [27:35,  2.07batch/s, Batch Loss=0.3894, Avg Loss=0.1089, Time Left=0.03 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3461batch [27:35,  2.07batch/s, Batch Loss=0.3894, Avg Loss=0.1089, Time Left=0.03 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3461batch [27:35,  2.07batch/s, Batch Loss=0.0552, Avg Loss=0.1089, Time Left=0.02 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3462batch [27:35,  2.09batch/s, Batch Loss=0.0552, Avg Loss=0.1089, Time Left=0.02 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3462batch [27:36,  2.09batch/s, Batch Loss=0.0179, Avg Loss=0.1088, Time Left=0.02 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3463batch [27:36,  2.02batch/s, Batch Loss=0.0179, Avg Loss=0.1088, Time Left=0.02 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3463batch [27:36,  2.02batch/s, Batch Loss=0.0734, Avg Loss=0.1088, Time Left=0.01 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3464batch [27:36,  2.03batch/s, Batch Loss=0.0734, Avg Loss=0.1088, Time Left=0.01 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3464batch [27:37,  2.03batch/s, Batch Loss=0.0112, Avg Loss=0.1088, Time Left=0.00 min]\u001b[A\n",
      "Epoch 2/3 - Training: 3465batch [27:37,  2.47batch/s, Batch Loss=0.0112, Avg Loss=0.1088, Time Left=0.00 min]\u001b[A\n",
      "                                                                                                             \u001b[A\n",
      "Epoch 2/3 - Evaluating:   0%|                                                               | 0/849 [00:00<?, ?batch/s]\u001b[A\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3 Results:\n",
      "Train Loss: 0.1088\n",
      "Validation Loss: 0.0997, Accuracy: 0.9614\n",
      "\n",
      "Starting Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3 - Training:   0%|                                                                | 0/3393 [00:00<?, ?batch/s]\u001b[AC:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14248\\3382488418.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "\n",
      "Epoch 3/3 - Training:   0%|       | 0/3393 [00:00<?, ?batch/s, Batch Loss=0.1308, Avg Loss=0.1308, Time Left=30.41 min]\u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 100/3393 [00:00<00:17, 185.93batch/s, Batch Loss=0.1308, Avg Loss=0.1308, Time Left=30.41\u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 100/3393 [00:16<00:17, 185.93batch/s, Batch Loss=0.1308, Avg Loss=0.1308, Time Left=30.41\u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 100/3393 [00:16<00:17, 185.93batch/s, Batch Loss=0.0069, Avg Loss=0.0694, Time Left=27.04\u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 101/3393 [00:16<12:57,  4.24batch/s, Batch Loss=0.0069, Avg Loss=0.0694, Time Left=27.04 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 101/3393 [00:17<12:57,  4.24batch/s, Batch Loss=0.0763, Avg Loss=0.0696, Time Left=27.03 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 102/3393 [00:17<13:12,  4.15batch/s, Batch Loss=0.0763, Avg Loss=0.0696, Time Left=27.03 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 102/3393 [00:17<13:12,  4.15batch/s, Batch Loss=0.1048, Avg Loss=0.0706, Time Left=27.07 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 103/3393 [00:17<13:36,  4.03batch/s, Batch Loss=0.1048, Avg Loss=0.0706, Time Left=27.07 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 103/3393 [00:18<13:36,  4.03batch/s, Batch Loss=0.0024, Avg Loss=0.0688, Time Left=27.07 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 104/3393 [00:18<14:07,  3.88batch/s, Batch Loss=0.0024, Avg Loss=0.0688, Time Left=27.07 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 104/3393 [00:18<14:07,  3.88batch/s, Batch Loss=0.0130, Avg Loss=0.0673, Time Left=27.06 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 105/3393 [00:18<14:43,  3.72batch/s, Batch Loss=0.0130, Avg Loss=0.0673, Time Left=27.06 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 105/3393 [00:19<14:43,  3.72batch/s, Batch Loss=0.0326, Avg Loss=0.0665, Time Left=27.03 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 106/3393 [00:19<15:31,  3.53batch/s, Batch Loss=0.0326, Avg Loss=0.0665, Time Left=27.03 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 106/3393 [00:19<15:31,  3.53batch/s, Batch Loss=0.0030, Avg Loss=0.0649, Time Left=27.02 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 107/3393 [00:19<16:24,  3.34batch/s, Batch Loss=0.0030, Avg Loss=0.0649, Time Left=27.02 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 107/3393 [00:20<16:24,  3.34batch/s, Batch Loss=0.0415, Avg Loss=0.0644, Time Left=26.99 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 108/3393 [00:20<17:31,  3.12batch/s, Batch Loss=0.0415, Avg Loss=0.0644, Time Left=26.99 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 108/3393 [00:20<17:31,  3.12batch/s, Batch Loss=0.0186, Avg Loss=0.0633, Time Left=26.98 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 109/3393 [00:20<18:45,  2.92batch/s, Batch Loss=0.0186, Avg Loss=0.0633, Time Left=26.98 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 109/3393 [00:21<18:45,  2.92batch/s, Batch Loss=0.0017, Avg Loss=0.0619, Time Left=26.96 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 110/3393 [00:21<19:44,  2.77batch/s, Batch Loss=0.0017, Avg Loss=0.0619, Time Left=26.96 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 110/3393 [00:21<19:44,  2.77batch/s, Batch Loss=0.0086, Avg Loss=0.0607, Time Left=26.93 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 111/3393 [00:21<20:51,  2.62batch/s, Batch Loss=0.0086, Avg Loss=0.0607, Time Left=26.93 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 111/3393 [00:22<20:51,  2.62batch/s, Batch Loss=0.0296, Avg Loss=0.0600, Time Left=26.98 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 112/3393 [00:22<22:34,  2.42batch/s, Batch Loss=0.0296, Avg Loss=0.0600, Time Left=26.98 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 112/3393 [00:22<22:34,  2.42batch/s, Batch Loss=0.0163, Avg Loss=0.0591, Time Left=26.95 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 113/3393 [00:22<23:17,  2.35batch/s, Batch Loss=0.0163, Avg Loss=0.0591, Time Left=26.95 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 113/3393 [00:23<23:17,  2.35batch/s, Batch Loss=0.0045, Avg Loss=0.0580, Time Left=26.98 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 114/3393 [00:23<24:30,  2.23batch/s, Batch Loss=0.0045, Avg Loss=0.0580, Time Left=26.98 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 114/3393 [00:23<24:30,  2.23batch/s, Batch Loss=0.0753, Avg Loss=0.0583, Time Left=26.96 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 115/3393 [00:23<24:47,  2.20batch/s, Batch Loss=0.0753, Avg Loss=0.0583, Time Left=26.96 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 115/3393 [00:24<24:47,  2.20batch/s, Batch Loss=0.0111, Avg Loss=0.0574, Time Left=26.95 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 116/3393 [00:24<25:15,  2.16batch/s, Batch Loss=0.0111, Avg Loss=0.0574, Time Left=26.95 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 116/3393 [00:24<25:15,  2.16batch/s, Batch Loss=0.0223, Avg Loss=0.0567, Time Left=26.96 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 117/3393 [00:24<25:47,  2.12batch/s, Batch Loss=0.0223, Avg Loss=0.0567, Time Left=26.96 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 117/3393 [00:25<25:47,  2.12batch/s, Batch Loss=0.0034, Avg Loss=0.0557, Time Left=26.95 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 118/3393 [00:25<25:58,  2.10batch/s, Batch Loss=0.0034, Avg Loss=0.0557, Time Left=26.95 \u001b[A\n",
      "Epoch 3/3 - Training:   3%| | 118/3393 [00:25<25:58,  2.10batch/s, Batch Loss=0.0173, Avg Loss=0.0549, Time Left=26.97 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 119/3393 [00:25<26:35,  2.05batch/s, Batch Loss=0.0173, Avg Loss=0.0549, Time Left=26.97 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 119/3393 [00:26<26:35,  2.05batch/s, Batch Loss=0.7972, Avg Loss=0.0687, Time Left=26.98 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 120/3393 [00:26<26:48,  2.03batch/s, Batch Loss=0.7972, Avg Loss=0.0687, Time Left=26.98 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 120/3393 [00:26<26:48,  2.03batch/s, Batch Loss=0.2269, Avg Loss=0.0716, Time Left=26.96 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 121/3393 [00:26<26:25,  2.06batch/s, Batch Loss=0.2269, Avg Loss=0.0716, Time Left=26.96 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 121/3393 [00:27<26:25,  2.06batch/s, Batch Loss=0.0291, Avg Loss=0.0708, Time Left=26.94 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 122/3393 [00:27<26:11,  2.08batch/s, Batch Loss=0.0291, Avg Loss=0.0708, Time Left=26.94 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 122/3393 [00:27<26:11,  2.08batch/s, Batch Loss=0.0066, Avg Loss=0.0697, Time Left=26.91 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 123/3393 [00:27<25:55,  2.10batch/s, Batch Loss=0.0066, Avg Loss=0.0697, Time Left=26.91 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 123/3393 [00:28<25:55,  2.10batch/s, Batch Loss=0.0957, Avg Loss=0.0701, Time Left=26.90 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 124/3393 [00:28<26:17,  2.07batch/s, Batch Loss=0.0957, Avg Loss=0.0701, Time Left=26.90 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 124/3393 [00:28<26:17,  2.07batch/s, Batch Loss=0.0470, Avg Loss=0.0697, Time Left=26.91 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 125/3393 [00:28<26:20,  2.07batch/s, Batch Loss=0.0470, Avg Loss=0.0697, Time Left=26.91 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 125/3393 [00:29<26:20,  2.07batch/s, Batch Loss=0.1703, Avg Loss=0.0714, Time Left=26.87 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 126/3393 [00:29<25:46,  2.11batch/s, Batch Loss=0.1703, Avg Loss=0.0714, Time Left=26.87 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 126/3393 [00:29<25:46,  2.11batch/s, Batch Loss=0.1001, Avg Loss=0.0719, Time Left=26.86 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 127/3393 [00:29<26:00,  2.09batch/s, Batch Loss=0.1001, Avg Loss=0.0719, Time Left=26.86 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 127/3393 [00:30<26:00,  2.09batch/s, Batch Loss=0.0156, Avg Loss=0.0710, Time Left=26.89 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 128/3393 [00:30<26:47,  2.03batch/s, Batch Loss=0.0156, Avg Loss=0.0710, Time Left=26.89 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 128/3393 [00:30<26:47,  2.03batch/s, Batch Loss=0.1093, Avg Loss=0.0716, Time Left=26.89 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 129/3393 [00:30<26:39,  2.04batch/s, Batch Loss=0.1093, Avg Loss=0.0716, Time Left=26.89 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 129/3393 [00:31<26:39,  2.04batch/s, Batch Loss=0.0046, Avg Loss=0.0705, Time Left=26.88 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:   4%| | 130/3393 [00:31<26:34,  2.05batch/s, Batch Loss=0.0046, Avg Loss=0.0705, Time Left=26.88 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 130/3393 [00:31<26:34,  2.05batch/s, Batch Loss=0.0054, Avg Loss=0.0695, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 131/3393 [00:31<25:58,  2.09batch/s, Batch Loss=0.0054, Avg Loss=0.0695, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 131/3393 [00:31<25:58,  2.09batch/s, Batch Loss=0.0093, Avg Loss=0.0686, Time Left=26.85 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 132/3393 [00:31<26:16,  2.07batch/s, Batch Loss=0.0093, Avg Loss=0.0686, Time Left=26.85 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 132/3393 [00:32<26:16,  2.07batch/s, Batch Loss=0.0169, Avg Loss=0.0679, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 133/3393 [00:32<26:18,  2.07batch/s, Batch Loss=0.0169, Avg Loss=0.0679, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 133/3393 [00:32<26:18,  2.07batch/s, Batch Loss=0.0061, Avg Loss=0.0669, Time Left=26.82 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 134/3393 [00:32<26:16,  2.07batch/s, Batch Loss=0.0061, Avg Loss=0.0669, Time Left=26.82 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 134/3393 [00:33<26:16,  2.07batch/s, Batch Loss=0.1221, Avg Loss=0.0677, Time Left=26.82 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 135/3393 [00:33<26:14,  2.07batch/s, Batch Loss=0.1221, Avg Loss=0.0677, Time Left=26.82 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 135/3393 [00:33<26:14,  2.07batch/s, Batch Loss=0.0058, Avg Loss=0.0669, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 136/3393 [00:33<26:45,  2.03batch/s, Batch Loss=0.0058, Avg Loss=0.0669, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 136/3393 [00:34<26:45,  2.03batch/s, Batch Loss=0.1200, Avg Loss=0.0676, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 137/3393 [00:34<26:38,  2.04batch/s, Batch Loss=0.1200, Avg Loss=0.0676, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 137/3393 [00:34<26:38,  2.04batch/s, Batch Loss=0.0665, Avg Loss=0.0676, Time Left=26.85 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 138/3393 [00:34<27:16,  1.99batch/s, Batch Loss=0.0665, Avg Loss=0.0676, Time Left=26.85 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 138/3393 [00:35<27:16,  1.99batch/s, Batch Loss=0.0810, Avg Loss=0.0678, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 139/3393 [00:35<26:59,  2.01batch/s, Batch Loss=0.0810, Avg Loss=0.0678, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 139/3393 [00:35<26:59,  2.01batch/s, Batch Loss=0.1488, Avg Loss=0.0689, Time Left=26.87 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 140/3393 [00:35<27:16,  1.99batch/s, Batch Loss=0.1488, Avg Loss=0.0689, Time Left=26.87 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 140/3393 [00:36<27:16,  1.99batch/s, Batch Loss=0.0082, Avg Loss=0.0681, Time Left=26.85 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 141/3393 [00:36<26:44,  2.03batch/s, Batch Loss=0.0082, Avg Loss=0.0681, Time Left=26.85 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 141/3393 [00:36<26:44,  2.03batch/s, Batch Loss=0.0083, Avg Loss=0.0673, Time Left=26.86 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 142/3393 [00:36<27:04,  2.00batch/s, Batch Loss=0.0083, Avg Loss=0.0673, Time Left=26.86 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 142/3393 [00:37<27:04,  2.00batch/s, Batch Loss=0.0334, Avg Loss=0.0668, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 143/3393 [00:37<26:36,  2.04batch/s, Batch Loss=0.0334, Avg Loss=0.0668, Time Left=26.84 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 143/3393 [00:37<26:36,  2.04batch/s, Batch Loss=0.5526, Avg Loss=0.0731, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 144/3393 [00:37<26:27,  2.05batch/s, Batch Loss=0.5526, Avg Loss=0.0731, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 144/3393 [00:38<26:27,  2.05batch/s, Batch Loss=0.0062, Avg Loss=0.0722, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 145/3393 [00:38<26:52,  2.01batch/s, Batch Loss=0.0062, Avg Loss=0.0722, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 145/3393 [00:38<26:52,  2.01batch/s, Batch Loss=0.0038, Avg Loss=0.0714, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 146/3393 [00:38<26:39,  2.03batch/s, Batch Loss=0.0038, Avg Loss=0.0714, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 146/3393 [00:39<26:39,  2.03batch/s, Batch Loss=0.2538, Avg Loss=0.0736, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 147/3393 [00:39<26:32,  2.04batch/s, Batch Loss=0.2538, Avg Loss=0.0736, Time Left=26.83 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 147/3393 [00:39<26:32,  2.04batch/s, Batch Loss=0.0978, Avg Loss=0.0739, Time Left=26.81 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 148/3393 [00:39<26:10,  2.07batch/s, Batch Loss=0.0978, Avg Loss=0.0739, Time Left=26.81 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 148/3393 [00:40<26:10,  2.07batch/s, Batch Loss=0.1246, Avg Loss=0.0745, Time Left=26.79 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 149/3393 [00:40<25:54,  2.09batch/s, Batch Loss=0.1246, Avg Loss=0.0745, Time Left=26.79 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 149/3393 [00:40<25:54,  2.09batch/s, Batch Loss=0.0909, Avg Loss=0.0747, Time Left=26.77 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 150/3393 [00:40<25:55,  2.09batch/s, Batch Loss=0.0909, Avg Loss=0.0747, Time Left=26.77 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 150/3393 [00:41<25:55,  2.09batch/s, Batch Loss=0.0647, Avg Loss=0.0746, Time Left=26.78 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 151/3393 [00:41<26:29,  2.04batch/s, Batch Loss=0.0647, Avg Loss=0.0746, Time Left=26.78 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 151/3393 [00:41<26:29,  2.04batch/s, Batch Loss=0.0654, Avg Loss=0.0745, Time Left=26.77 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 152/3393 [00:41<26:24,  2.05batch/s, Batch Loss=0.0654, Avg Loss=0.0745, Time Left=26.77 \u001b[A\n",
      "Epoch 3/3 - Training:   4%| | 152/3393 [00:42<26:24,  2.05batch/s, Batch Loss=0.0780, Avg Loss=0.0745, Time Left=26.78 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 153/3393 [00:42<26:33,  2.03batch/s, Batch Loss=0.0780, Avg Loss=0.0745, Time Left=26.78 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 153/3393 [00:42<26:33,  2.03batch/s, Batch Loss=0.2895, Avg Loss=0.0770, Time Left=26.77 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 154/3393 [00:42<26:26,  2.04batch/s, Batch Loss=0.2895, Avg Loss=0.0770, Time Left=26.77 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 154/3393 [00:43<26:26,  2.04batch/s, Batch Loss=0.0555, Avg Loss=0.0767, Time Left=26.75 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 155/3393 [00:43<26:06,  2.07batch/s, Batch Loss=0.0555, Avg Loss=0.0767, Time Left=26.75 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 155/3393 [00:43<26:06,  2.07batch/s, Batch Loss=0.2299, Avg Loss=0.0784, Time Left=26.75 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 156/3393 [00:43<26:18,  2.05batch/s, Batch Loss=0.2299, Avg Loss=0.0784, Time Left=26.75 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 156/3393 [00:44<26:18,  2.05batch/s, Batch Loss=0.1303, Avg Loss=0.0790, Time Left=26.73 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 157/3393 [00:44<25:59,  2.08batch/s, Batch Loss=0.1303, Avg Loss=0.0790, Time Left=26.73 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 157/3393 [00:44<25:59,  2.08batch/s, Batch Loss=0.2963, Avg Loss=0.0814, Time Left=26.72 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 158/3393 [00:44<26:01,  2.07batch/s, Batch Loss=0.2963, Avg Loss=0.0814, Time Left=26.72 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 158/3393 [00:45<26:01,  2.07batch/s, Batch Loss=0.0071, Avg Loss=0.0806, Time Left=26.70 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 159/3393 [00:45<25:46,  2.09batch/s, Batch Loss=0.0071, Avg Loss=0.0806, Time Left=26.70 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 159/3393 [00:45<25:46,  2.09batch/s, Batch Loss=0.0191, Avg Loss=0.0799, Time Left=26.68 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 160/3393 [00:45<25:45,  2.09batch/s, Batch Loss=0.0191, Avg Loss=0.0799, Time Left=26.68 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 160/3393 [00:46<25:45,  2.09batch/s, Batch Loss=0.0648, Avg Loss=0.0798, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 161/3393 [00:46<25:38,  2.10batch/s, Batch Loss=0.0648, Avg Loss=0.0798, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 161/3393 [00:46<25:38,  2.10batch/s, Batch Loss=0.0386, Avg Loss=0.0793, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 162/3393 [00:46<26:00,  2.07batch/s, Batch Loss=0.0386, Avg Loss=0.0793, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 162/3393 [00:47<26:00,  2.07batch/s, Batch Loss=0.1909, Avg Loss=0.0805, Time Left=26.67 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:   5%| | 163/3393 [00:47<26:01,  2.07batch/s, Batch Loss=0.1909, Avg Loss=0.0805, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 163/3393 [00:47<26:01,  2.07batch/s, Batch Loss=0.0553, Avg Loss=0.0802, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 164/3393 [00:47<26:31,  2.03batch/s, Batch Loss=0.0553, Avg Loss=0.0802, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 164/3393 [00:48<26:31,  2.03batch/s, Batch Loss=0.0257, Avg Loss=0.0797, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 165/3393 [00:48<26:39,  2.02batch/s, Batch Loss=0.0257, Avg Loss=0.0797, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 165/3393 [00:48<26:39,  2.02batch/s, Batch Loss=0.1840, Avg Loss=0.0807, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 166/3393 [00:48<26:29,  2.03batch/s, Batch Loss=0.1840, Avg Loss=0.0807, Time Left=26.67 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 166/3393 [00:49<26:29,  2.03batch/s, Batch Loss=0.1461, Avg Loss=0.0814, Time Left=26.66 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 167/3393 [00:49<26:21,  2.04batch/s, Batch Loss=0.1461, Avg Loss=0.0814, Time Left=26.66 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 167/3393 [00:49<26:21,  2.04batch/s, Batch Loss=0.2412, Avg Loss=0.0829, Time Left=26.65 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 168/3393 [00:49<26:13,  2.05batch/s, Batch Loss=0.2412, Avg Loss=0.0829, Time Left=26.65 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 168/3393 [00:50<26:13,  2.05batch/s, Batch Loss=0.0666, Avg Loss=0.0828, Time Left=26.65 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 169/3393 [00:50<26:38,  2.02batch/s, Batch Loss=0.0666, Avg Loss=0.0828, Time Left=26.65 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 169/3393 [00:50<26:38,  2.02batch/s, Batch Loss=0.0382, Avg Loss=0.0823, Time Left=26.64 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 170/3393 [00:50<26:12,  2.05batch/s, Batch Loss=0.0382, Avg Loss=0.0823, Time Left=26.64 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 170/3393 [00:51<26:12,  2.05batch/s, Batch Loss=0.0150, Avg Loss=0.0817, Time Left=26.63 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 171/3393 [00:51<26:08,  2.05batch/s, Batch Loss=0.0150, Avg Loss=0.0817, Time Left=26.63 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 171/3393 [00:51<26:08,  2.05batch/s, Batch Loss=0.1530, Avg Loss=0.0824, Time Left=26.62 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 172/3393 [00:51<26:04,  2.06batch/s, Batch Loss=0.1530, Avg Loss=0.0824, Time Left=26.62 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 172/3393 [00:51<26:04,  2.06batch/s, Batch Loss=0.0096, Avg Loss=0.0817, Time Left=26.61 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 173/3393 [00:51<26:01,  2.06batch/s, Batch Loss=0.0096, Avg Loss=0.0817, Time Left=26.61 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 173/3393 [00:52<26:01,  2.06batch/s, Batch Loss=0.0178, Avg Loss=0.0811, Time Left=26.62 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 174/3393 [00:52<26:45,  2.01batch/s, Batch Loss=0.0178, Avg Loss=0.0811, Time Left=26.62 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 174/3393 [00:53<26:45,  2.01batch/s, Batch Loss=0.0823, Avg Loss=0.0811, Time Left=26.62 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 175/3393 [00:53<26:48,  2.00batch/s, Batch Loss=0.0823, Avg Loss=0.0811, Time Left=26.62 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 175/3393 [00:53<26:48,  2.00batch/s, Batch Loss=0.0417, Avg Loss=0.0808, Time Left=26.62 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 176/3393 [00:53<26:47,  2.00batch/s, Batch Loss=0.0417, Avg Loss=0.0808, Time Left=26.62 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 176/3393 [00:53<26:47,  2.00batch/s, Batch Loss=0.0471, Avg Loss=0.0804, Time Left=26.61 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 177/3393 [00:53<26:19,  2.04batch/s, Batch Loss=0.0471, Avg Loss=0.0804, Time Left=26.61 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 177/3393 [00:54<26:19,  2.04batch/s, Batch Loss=0.0670, Avg Loss=0.0803, Time Left=26.60 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 178/3393 [00:54<26:15,  2.04batch/s, Batch Loss=0.0670, Avg Loss=0.0803, Time Left=26.60 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 178/3393 [00:54<26:15,  2.04batch/s, Batch Loss=0.0150, Avg Loss=0.0798, Time Left=26.60 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 179/3393 [00:54<26:19,  2.03batch/s, Batch Loss=0.0150, Avg Loss=0.0798, Time Left=26.60 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 179/3393 [00:55<26:19,  2.03batch/s, Batch Loss=0.0686, Avg Loss=0.0797, Time Left=26.58 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 180/3393 [00:55<25:55,  2.07batch/s, Batch Loss=0.0686, Avg Loss=0.0797, Time Left=26.58 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 180/3393 [00:55<25:55,  2.07batch/s, Batch Loss=0.0473, Avg Loss=0.0794, Time Left=26.57 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 181/3393 [00:55<25:55,  2.07batch/s, Batch Loss=0.0473, Avg Loss=0.0794, Time Left=26.57 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 181/3393 [00:56<25:55,  2.07batch/s, Batch Loss=0.2203, Avg Loss=0.0806, Time Left=26.56 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 182/3393 [00:56<25:54,  2.07batch/s, Batch Loss=0.2203, Avg Loss=0.0806, Time Left=26.56 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 182/3393 [00:56<25:54,  2.07batch/s, Batch Loss=0.0966, Avg Loss=0.0807, Time Left=26.55 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 183/3393 [00:56<25:52,  2.07batch/s, Batch Loss=0.0966, Avg Loss=0.0807, Time Left=26.55 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 183/3393 [00:57<25:52,  2.07batch/s, Batch Loss=0.0093, Avg Loss=0.0801, Time Left=26.53 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 184/3393 [00:57<25:36,  2.09batch/s, Batch Loss=0.0093, Avg Loss=0.0801, Time Left=26.53 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 184/3393 [00:57<25:36,  2.09batch/s, Batch Loss=0.0740, Avg Loss=0.0801, Time Left=26.53 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 185/3393 [00:57<25:55,  2.06batch/s, Batch Loss=0.0740, Avg Loss=0.0801, Time Left=26.53 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 185/3393 [00:58<25:55,  2.06batch/s, Batch Loss=0.2555, Avg Loss=0.0815, Time Left=26.51 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 186/3393 [00:58<25:23,  2.10batch/s, Batch Loss=0.2555, Avg Loss=0.0815, Time Left=26.51 \u001b[A\n",
      "Epoch 3/3 - Training:   5%| | 186/3393 [00:58<25:23,  2.10batch/s, Batch Loss=0.1273, Avg Loss=0.0819, Time Left=26.50 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 187/3393 [00:58<25:16,  2.11batch/s, Batch Loss=0.1273, Avg Loss=0.0819, Time Left=26.50 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 187/3393 [00:59<25:16,  2.11batch/s, Batch Loss=0.0532, Avg Loss=0.0817, Time Left=26.50 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 188/3393 [00:59<25:49,  2.07batch/s, Batch Loss=0.0532, Avg Loss=0.0817, Time Left=26.50 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 188/3393 [00:59<25:49,  2.07batch/s, Batch Loss=0.0581, Avg Loss=0.0815, Time Left=26.49 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 189/3393 [00:59<25:41,  2.08batch/s, Batch Loss=0.0581, Avg Loss=0.0815, Time Left=26.49 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 189/3393 [01:00<25:41,  2.08batch/s, Batch Loss=0.0037, Avg Loss=0.0809, Time Left=26.49 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 190/3393 [01:00<26:14,  2.03batch/s, Batch Loss=0.0037, Avg Loss=0.0809, Time Left=26.49 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 190/3393 [01:00<26:14,  2.03batch/s, Batch Loss=0.0094, Avg Loss=0.0803, Time Left=26.47 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 191/3393 [01:00<25:36,  2.08batch/s, Batch Loss=0.0094, Avg Loss=0.0803, Time Left=26.47 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 191/3393 [01:01<25:36,  2.08batch/s, Batch Loss=0.0057, Avg Loss=0.0797, Time Left=26.46 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 192/3393 [01:01<25:39,  2.08batch/s, Batch Loss=0.0057, Avg Loss=0.0797, Time Left=26.46 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 192/3393 [01:01<25:39,  2.08batch/s, Batch Loss=0.0584, Avg Loss=0.0795, Time Left=26.46 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 193/3393 [01:01<26:12,  2.03batch/s, Batch Loss=0.0584, Avg Loss=0.0795, Time Left=26.46 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 193/3393 [01:02<26:12,  2.03batch/s, Batch Loss=0.0019, Avg Loss=0.0789, Time Left=26.46 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 194/3393 [01:02<26:03,  2.05batch/s, Batch Loss=0.0019, Avg Loss=0.0789, Time Left=26.46 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 194/3393 [01:02<26:03,  2.05batch/s, Batch Loss=0.2203, Avg Loss=0.0800, Time Left=26.45 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 195/3393 [01:02<26:15,  2.03batch/s, Batch Loss=0.2203, Avg Loss=0.0800, Time Left=26.45 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 195/3393 [01:03<26:15,  2.03batch/s, Batch Loss=0.0023, Avg Loss=0.0794, Time Left=26.44 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:   6%| | 196/3393 [01:03<25:53,  2.06batch/s, Batch Loss=0.0023, Avg Loss=0.0794, Time Left=26.44 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 196/3393 [01:03<25:53,  2.06batch/s, Batch Loss=0.0100, Avg Loss=0.0789, Time Left=26.43 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 197/3393 [01:03<25:48,  2.06batch/s, Batch Loss=0.0100, Avg Loss=0.0789, Time Left=26.43 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 197/3393 [01:04<25:48,  2.06batch/s, Batch Loss=0.0921, Avg Loss=0.0790, Time Left=26.43 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 198/3393 [01:04<26:01,  2.05batch/s, Batch Loss=0.0921, Avg Loss=0.0790, Time Left=26.43 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 198/3393 [01:04<26:01,  2.05batch/s, Batch Loss=0.0107, Avg Loss=0.0785, Time Left=26.41 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 199/3393 [01:04<25:42,  2.07batch/s, Batch Loss=0.0107, Avg Loss=0.0785, Time Left=26.41 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 199/3393 [01:05<25:42,  2.07batch/s, Batch Loss=0.0015, Avg Loss=0.0779, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 200/3393 [01:05<25:42,  2.07batch/s, Batch Loss=0.0015, Avg Loss=0.0779, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 200/3393 [01:05<25:42,  2.07batch/s, Batch Loss=0.0389, Avg Loss=0.0776, Time Left=26.41 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 201/3393 [01:05<26:26,  2.01batch/s, Batch Loss=0.0389, Avg Loss=0.0776, Time Left=26.41 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 201/3393 [01:06<26:26,  2.01batch/s, Batch Loss=0.0839, Avg Loss=0.0777, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 202/3393 [01:06<26:25,  2.01batch/s, Batch Loss=0.0839, Avg Loss=0.0777, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 202/3393 [01:06<26:25,  2.01batch/s, Batch Loss=0.0111, Avg Loss=0.0772, Time Left=26.41 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 203/3393 [01:06<26:35,  2.00batch/s, Batch Loss=0.0111, Avg Loss=0.0772, Time Left=26.41 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 203/3393 [01:07<26:35,  2.00batch/s, Batch Loss=0.0028, Avg Loss=0.0766, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 204/3393 [01:07<26:20,  2.02batch/s, Batch Loss=0.0028, Avg Loss=0.0766, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 204/3393 [01:07<26:20,  2.02batch/s, Batch Loss=0.0176, Avg Loss=0.0762, Time Left=26.41 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 205/3393 [01:07<26:52,  1.98batch/s, Batch Loss=0.0176, Avg Loss=0.0762, Time Left=26.41 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 205/3393 [01:08<26:52,  1.98batch/s, Batch Loss=0.0196, Avg Loss=0.0758, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 206/3393 [01:08<26:48,  1.98batch/s, Batch Loss=0.0196, Avg Loss=0.0758, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 206/3393 [01:08<26:48,  1.98batch/s, Batch Loss=0.0327, Avg Loss=0.0755, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 207/3393 [01:08<26:42,  1.99batch/s, Batch Loss=0.0327, Avg Loss=0.0755, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 207/3393 [01:09<26:42,  1.99batch/s, Batch Loss=0.0096, Avg Loss=0.0750, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 208/3393 [01:09<26:40,  1.99batch/s, Batch Loss=0.0096, Avg Loss=0.0750, Time Left=26.40 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 208/3393 [01:09<26:40,  1.99batch/s, Batch Loss=0.1541, Avg Loss=0.0756, Time Left=26.38 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 209/3393 [01:09<26:07,  2.03batch/s, Batch Loss=0.1541, Avg Loss=0.0756, Time Left=26.38 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 209/3393 [01:10<26:07,  2.03batch/s, Batch Loss=0.0090, Avg Loss=0.0751, Time Left=26.37 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 210/3393 [01:10<25:58,  2.04batch/s, Batch Loss=0.0090, Avg Loss=0.0751, Time Left=26.37 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 210/3393 [01:10<25:58,  2.04batch/s, Batch Loss=0.0060, Avg Loss=0.0746, Time Left=26.36 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 211/3393 [01:10<25:38,  2.07batch/s, Batch Loss=0.0060, Avg Loss=0.0746, Time Left=26.36 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 211/3393 [01:11<25:38,  2.07batch/s, Batch Loss=0.1399, Avg Loss=0.0751, Time Left=26.35 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 212/3393 [01:11<25:37,  2.07batch/s, Batch Loss=0.1399, Avg Loss=0.0751, Time Left=26.35 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 212/3393 [01:11<25:37,  2.07batch/s, Batch Loss=0.0305, Avg Loss=0.0748, Time Left=26.34 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 213/3393 [01:11<25:52,  2.05batch/s, Batch Loss=0.0305, Avg Loss=0.0748, Time Left=26.34 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 213/3393 [01:12<25:52,  2.05batch/s, Batch Loss=0.0031, Avg Loss=0.0743, Time Left=26.34 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 214/3393 [01:12<25:46,  2.06batch/s, Batch Loss=0.0031, Avg Loss=0.0743, Time Left=26.34 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 214/3393 [01:12<25:46,  2.06batch/s, Batch Loss=0.0099, Avg Loss=0.0739, Time Left=26.34 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 215/3393 [01:12<26:15,  2.02batch/s, Batch Loss=0.0099, Avg Loss=0.0739, Time Left=26.34 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 215/3393 [01:13<26:15,  2.02batch/s, Batch Loss=0.0236, Avg Loss=0.0735, Time Left=26.33 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 216/3393 [01:13<26:04,  2.03batch/s, Batch Loss=0.0236, Avg Loss=0.0735, Time Left=26.33 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 216/3393 [01:13<26:04,  2.03batch/s, Batch Loss=0.0969, Avg Loss=0.0737, Time Left=26.32 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 217/3393 [01:13<25:55,  2.04batch/s, Batch Loss=0.0969, Avg Loss=0.0737, Time Left=26.32 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 217/3393 [01:14<25:55,  2.04batch/s, Batch Loss=0.0042, Avg Loss=0.0732, Time Left=26.31 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 218/3393 [01:14<25:51,  2.05batch/s, Batch Loss=0.0042, Avg Loss=0.0732, Time Left=26.31 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 218/3393 [01:14<25:51,  2.05batch/s, Batch Loss=0.0218, Avg Loss=0.0729, Time Left=26.30 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 219/3393 [01:14<25:34,  2.07batch/s, Batch Loss=0.0218, Avg Loss=0.0729, Time Left=26.30 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 219/3393 [01:14<25:34,  2.07batch/s, Batch Loss=0.0039, Avg Loss=0.0724, Time Left=26.29 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 220/3393 [01:14<25:29,  2.07batch/s, Batch Loss=0.0039, Avg Loss=0.0724, Time Left=26.29 \u001b[A\n",
      "Epoch 3/3 - Training:   6%| | 220/3393 [01:15<25:29,  2.07batch/s, Batch Loss=0.0430, Avg Loss=0.0723, Time Left=26.28 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 221/3393 [01:15<25:31,  2.07batch/s, Batch Loss=0.0430, Avg Loss=0.0723, Time Left=26.28 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 221/3393 [01:15<25:31,  2.07batch/s, Batch Loss=0.2848, Avg Loss=0.0736, Time Left=26.27 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 222/3393 [01:15<25:44,  2.05batch/s, Batch Loss=0.2848, Avg Loss=0.0736, Time Left=26.27 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 222/3393 [01:16<25:44,  2.05batch/s, Batch Loss=0.5029, Avg Loss=0.0764, Time Left=26.28 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 223/3393 [01:16<26:10,  2.02batch/s, Batch Loss=0.5029, Avg Loss=0.0764, Time Left=26.28 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 223/3393 [01:16<26:10,  2.02batch/s, Batch Loss=0.0526, Avg Loss=0.0762, Time Left=26.27 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 224/3393 [01:16<26:01,  2.03batch/s, Batch Loss=0.0526, Avg Loss=0.0762, Time Left=26.27 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 224/3393 [01:17<26:01,  2.03batch/s, Batch Loss=0.0017, Avg Loss=0.0757, Time Left=26.26 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 225/3393 [01:17<25:54,  2.04batch/s, Batch Loss=0.0017, Avg Loss=0.0757, Time Left=26.26 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 225/3393 [01:17<25:54,  2.04batch/s, Batch Loss=0.0685, Avg Loss=0.0757, Time Left=26.25 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 226/3393 [01:17<25:46,  2.05batch/s, Batch Loss=0.0685, Avg Loss=0.0757, Time Left=26.25 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 226/3393 [01:18<25:46,  2.05batch/s, Batch Loss=0.0194, Avg Loss=0.0753, Time Left=26.23 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 227/3393 [01:18<25:12,  2.09batch/s, Batch Loss=0.0194, Avg Loss=0.0753, Time Left=26.23 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 227/3393 [01:18<25:12,  2.09batch/s, Batch Loss=0.0194, Avg Loss=0.0750, Time Left=26.22 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 228/3393 [01:18<25:28,  2.07batch/s, Batch Loss=0.0194, Avg Loss=0.0750, Time Left=26.22 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 228/3393 [01:19<25:28,  2.07batch/s, Batch Loss=0.0772, Avg Loss=0.0750, Time Left=26.21 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:   7%| | 229/3393 [01:19<25:15,  2.09batch/s, Batch Loss=0.0772, Avg Loss=0.0750, Time Left=26.21 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 229/3393 [01:19<25:15,  2.09batch/s, Batch Loss=0.2372, Avg Loss=0.0760, Time Left=26.20 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 230/3393 [01:19<25:19,  2.08batch/s, Batch Loss=0.2372, Avg Loss=0.0760, Time Left=26.20 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 230/3393 [01:20<25:19,  2.08batch/s, Batch Loss=0.1815, Avg Loss=0.0766, Time Left=26.19 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 231/3393 [01:20<25:36,  2.06batch/s, Batch Loss=0.1815, Avg Loss=0.0766, Time Left=26.19 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 231/3393 [01:20<25:36,  2.06batch/s, Batch Loss=0.0556, Avg Loss=0.0765, Time Left=26.18 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 232/3393 [01:20<25:19,  2.08batch/s, Batch Loss=0.0556, Avg Loss=0.0765, Time Left=26.18 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 232/3393 [01:21<25:19,  2.08batch/s, Batch Loss=0.0015, Avg Loss=0.0761, Time Left=26.16 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 233/3393 [01:21<25:06,  2.10batch/s, Batch Loss=0.0015, Avg Loss=0.0761, Time Left=26.16 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 233/3393 [01:21<25:06,  2.10batch/s, Batch Loss=0.3378, Avg Loss=0.0776, Time Left=26.15 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 234/3393 [01:21<24:57,  2.11batch/s, Batch Loss=0.3378, Avg Loss=0.0776, Time Left=26.15 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 234/3393 [01:22<24:57,  2.11batch/s, Batch Loss=0.0521, Avg Loss=0.0775, Time Left=26.15 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 235/3393 [01:22<25:19,  2.08batch/s, Batch Loss=0.0521, Avg Loss=0.0775, Time Left=26.15 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 235/3393 [01:22<25:19,  2.08batch/s, Batch Loss=0.0227, Avg Loss=0.0771, Time Left=26.14 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 236/3393 [01:22<25:22,  2.07batch/s, Batch Loss=0.0227, Avg Loss=0.0771, Time Left=26.14 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 236/3393 [01:23<25:22,  2.07batch/s, Batch Loss=0.0676, Avg Loss=0.0771, Time Left=26.14 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 237/3393 [01:23<25:37,  2.05batch/s, Batch Loss=0.0676, Avg Loss=0.0771, Time Left=26.14 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 237/3393 [01:23<25:37,  2.05batch/s, Batch Loss=0.0024, Avg Loss=0.0767, Time Left=26.12 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 238/3393 [01:23<25:05,  2.10batch/s, Batch Loss=0.0024, Avg Loss=0.0767, Time Left=26.12 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 238/3393 [01:24<25:05,  2.10batch/s, Batch Loss=0.0853, Avg Loss=0.0767, Time Left=26.11 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 239/3393 [01:24<25:10,  2.09batch/s, Batch Loss=0.0853, Avg Loss=0.0767, Time Left=26.11 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 239/3393 [01:24<25:10,  2.09batch/s, Batch Loss=0.0107, Avg Loss=0.0763, Time Left=26.10 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 240/3393 [01:24<25:28,  2.06batch/s, Batch Loss=0.0107, Avg Loss=0.0763, Time Left=26.10 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 240/3393 [01:25<25:28,  2.06batch/s, Batch Loss=0.0750, Avg Loss=0.0763, Time Left=26.09 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 241/3393 [01:25<25:12,  2.08batch/s, Batch Loss=0.0750, Avg Loss=0.0763, Time Left=26.09 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 241/3393 [01:25<25:12,  2.08batch/s, Batch Loss=0.0910, Avg Loss=0.0764, Time Left=26.08 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 242/3393 [01:25<25:17,  2.08batch/s, Batch Loss=0.0910, Avg Loss=0.0764, Time Left=26.08 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 242/3393 [01:26<25:17,  2.08batch/s, Batch Loss=0.1226, Avg Loss=0.0767, Time Left=26.08 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 243/3393 [01:26<26:00,  2.02batch/s, Batch Loss=0.1226, Avg Loss=0.0767, Time Left=26.08 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 243/3393 [01:26<26:00,  2.02batch/s, Batch Loss=0.0105, Avg Loss=0.0763, Time Left=26.07 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 244/3393 [01:26<25:37,  2.05batch/s, Batch Loss=0.0105, Avg Loss=0.0763, Time Left=26.07 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 244/3393 [01:27<25:37,  2.05batch/s, Batch Loss=0.0039, Avg Loss=0.0759, Time Left=26.07 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 245/3393 [01:27<26:03,  2.01batch/s, Batch Loss=0.0039, Avg Loss=0.0759, Time Left=26.07 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 245/3393 [01:27<26:03,  2.01batch/s, Batch Loss=0.0738, Avg Loss=0.0759, Time Left=26.07 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 246/3393 [01:27<25:52,  2.03batch/s, Batch Loss=0.0738, Avg Loss=0.0759, Time Left=26.07 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 246/3393 [01:28<25:52,  2.03batch/s, Batch Loss=0.0058, Avg Loss=0.0755, Time Left=26.07 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 247/3393 [01:28<26:11,  2.00batch/s, Batch Loss=0.0058, Avg Loss=0.0755, Time Left=26.07 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 247/3393 [01:28<26:11,  2.00batch/s, Batch Loss=0.0826, Avg Loss=0.0755, Time Left=26.05 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 248/3393 [01:28<25:43,  2.04batch/s, Batch Loss=0.0826, Avg Loss=0.0755, Time Left=26.05 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 248/3393 [01:29<25:43,  2.04batch/s, Batch Loss=0.0858, Avg Loss=0.0756, Time Left=26.04 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 249/3393 [01:29<25:21,  2.07batch/s, Batch Loss=0.0858, Avg Loss=0.0756, Time Left=26.04 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 249/3393 [01:29<25:21,  2.07batch/s, Batch Loss=0.0841, Avg Loss=0.0756, Time Left=26.04 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 250/3393 [01:29<26:01,  2.01batch/s, Batch Loss=0.0841, Avg Loss=0.0756, Time Left=26.04 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 250/3393 [01:30<26:01,  2.01batch/s, Batch Loss=0.1517, Avg Loss=0.0760, Time Left=26.03 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 251/3393 [01:30<25:22,  2.06batch/s, Batch Loss=0.1517, Avg Loss=0.0760, Time Left=26.03 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 251/3393 [01:30<25:22,  2.06batch/s, Batch Loss=0.0374, Avg Loss=0.0758, Time Left=26.02 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 252/3393 [01:30<25:20,  2.07batch/s, Batch Loss=0.0374, Avg Loss=0.0758, Time Left=26.02 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 252/3393 [01:31<25:20,  2.07batch/s, Batch Loss=0.3415, Avg Loss=0.0773, Time Left=26.02 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 253/3393 [01:31<25:49,  2.03batch/s, Batch Loss=0.3415, Avg Loss=0.0773, Time Left=26.02 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 253/3393 [01:31<25:49,  2.03batch/s, Batch Loss=0.2131, Avg Loss=0.0780, Time Left=26.01 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 254/3393 [01:31<25:41,  2.04batch/s, Batch Loss=0.2131, Avg Loss=0.0780, Time Left=26.01 \u001b[A\n",
      "Epoch 3/3 - Training:   7%| | 254/3393 [01:32<25:41,  2.04batch/s, Batch Loss=0.0943, Avg Loss=0.0781, Time Left=26.00 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 255/3393 [01:32<25:50,  2.02batch/s, Batch Loss=0.0943, Avg Loss=0.0781, Time Left=26.00 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 255/3393 [01:32<25:50,  2.02batch/s, Batch Loss=0.0270, Avg Loss=0.0778, Time Left=26.00 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 256/3393 [01:32<25:54,  2.02batch/s, Batch Loss=0.0270, Avg Loss=0.0778, Time Left=26.00 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 256/3393 [01:33<25:54,  2.02batch/s, Batch Loss=0.0518, Avg Loss=0.0777, Time Left=25.99 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 257/3393 [01:33<25:57,  2.01batch/s, Batch Loss=0.0518, Avg Loss=0.0777, Time Left=25.99 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 257/3393 [01:33<25:57,  2.01batch/s, Batch Loss=0.0138, Avg Loss=0.0773, Time Left=25.98 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 258/3393 [01:33<25:46,  2.03batch/s, Batch Loss=0.0138, Avg Loss=0.0773, Time Left=25.98 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 258/3393 [01:33<25:46,  2.03batch/s, Batch Loss=0.0069, Avg Loss=0.0770, Time Left=25.97 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 259/3393 [01:33<25:21,  2.06batch/s, Batch Loss=0.0069, Avg Loss=0.0770, Time Left=25.97 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 259/3393 [01:34<25:21,  2.06batch/s, Batch Loss=0.0339, Avg Loss=0.0767, Time Left=25.97 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 260/3393 [01:34<25:47,  2.02batch/s, Batch Loss=0.0339, Avg Loss=0.0767, Time Left=25.97 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 260/3393 [01:34<25:47,  2.02batch/s, Batch Loss=0.1565, Avg Loss=0.0771, Time Left=25.96 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 261/3393 [01:34<25:39,  2.03batch/s, Batch Loss=0.1565, Avg Loss=0.0771, Time Left=25.96 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 261/3393 [01:35<25:39,  2.03batch/s, Batch Loss=0.0705, Avg Loss=0.0771, Time Left=25.96 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:   8%| | 262/3393 [01:35<25:48,  2.02batch/s, Batch Loss=0.0705, Avg Loss=0.0771, Time Left=25.96 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 262/3393 [01:35<25:48,  2.02batch/s, Batch Loss=0.0397, Avg Loss=0.0769, Time Left=25.95 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 263/3393 [01:35<25:22,  2.06batch/s, Batch Loss=0.0397, Avg Loss=0.0769, Time Left=25.95 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 263/3393 [01:36<25:22,  2.06batch/s, Batch Loss=0.0693, Avg Loss=0.0769, Time Left=25.93 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 264/3393 [01:36<24:49,  2.10batch/s, Batch Loss=0.0693, Avg Loss=0.0769, Time Left=25.93 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 264/3393 [01:36<24:49,  2.10batch/s, Batch Loss=0.0347, Avg Loss=0.0767, Time Left=25.92 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 265/3393 [01:36<24:55,  2.09batch/s, Batch Loss=0.0347, Avg Loss=0.0767, Time Left=25.92 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 265/3393 [01:37<24:55,  2.09batch/s, Batch Loss=0.0026, Avg Loss=0.0763, Time Left=25.91 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 266/3393 [01:37<24:59,  2.09batch/s, Batch Loss=0.0026, Avg Loss=0.0763, Time Left=25.91 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 266/3393 [01:37<24:59,  2.09batch/s, Batch Loss=0.5031, Avg Loss=0.0784, Time Left=25.90 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 267/3393 [01:37<24:48,  2.10batch/s, Batch Loss=0.5031, Avg Loss=0.0784, Time Left=25.90 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 267/3393 [01:38<24:48,  2.10batch/s, Batch Loss=0.1467, Avg Loss=0.0788, Time Left=25.88 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 268/3393 [01:38<24:39,  2.11batch/s, Batch Loss=0.1467, Avg Loss=0.0788, Time Left=25.88 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 268/3393 [01:38<24:39,  2.11batch/s, Batch Loss=0.0185, Avg Loss=0.0785, Time Left=25.89 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 269/3393 [01:38<25:31,  2.04batch/s, Batch Loss=0.0185, Avg Loss=0.0785, Time Left=25.89 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 269/3393 [01:39<25:31,  2.04batch/s, Batch Loss=0.0435, Avg Loss=0.0783, Time Left=25.87 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 270/3393 [01:39<25:23,  2.05batch/s, Batch Loss=0.0435, Avg Loss=0.0783, Time Left=25.87 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 270/3393 [01:39<25:23,  2.05batch/s, Batch Loss=0.0598, Avg Loss=0.0782, Time Left=25.87 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 271/3393 [01:39<25:37,  2.03batch/s, Batch Loss=0.0598, Avg Loss=0.0782, Time Left=25.87 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 271/3393 [01:40<25:37,  2.03batch/s, Batch Loss=0.0337, Avg Loss=0.0780, Time Left=25.86 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 272/3393 [01:40<25:30,  2.04batch/s, Batch Loss=0.0337, Avg Loss=0.0780, Time Left=25.86 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 272/3393 [01:40<25:30,  2.04batch/s, Batch Loss=0.0213, Avg Loss=0.0777, Time Left=25.85 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 273/3393 [01:40<25:10,  2.07batch/s, Batch Loss=0.0213, Avg Loss=0.0777, Time Left=25.85 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 273/3393 [01:41<25:10,  2.07batch/s, Batch Loss=0.0017, Avg Loss=0.0774, Time Left=25.85 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 274/3393 [01:41<25:37,  2.03batch/s, Batch Loss=0.0017, Avg Loss=0.0774, Time Left=25.85 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 274/3393 [01:41<25:37,  2.03batch/s, Batch Loss=0.0263, Avg Loss=0.0771, Time Left=25.84 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 275/3393 [01:41<25:28,  2.04batch/s, Batch Loss=0.0263, Avg Loss=0.0771, Time Left=25.84 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 275/3393 [01:42<25:28,  2.04batch/s, Batch Loss=0.0106, Avg Loss=0.0768, Time Left=25.84 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 276/3393 [01:42<25:36,  2.03batch/s, Batch Loss=0.0106, Avg Loss=0.0768, Time Left=25.84 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 276/3393 [01:42<25:36,  2.03batch/s, Batch Loss=0.2721, Avg Loss=0.0777, Time Left=25.82 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 277/3393 [01:42<25:14,  2.06batch/s, Batch Loss=0.2721, Avg Loss=0.0777, Time Left=25.82 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 277/3393 [01:43<25:14,  2.06batch/s, Batch Loss=0.0399, Avg Loss=0.0775, Time Left=25.82 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 278/3393 [01:43<25:10,  2.06batch/s, Batch Loss=0.0399, Avg Loss=0.0775, Time Left=25.82 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 278/3393 [01:43<25:10,  2.06batch/s, Batch Loss=0.2249, Avg Loss=0.0782, Time Left=25.81 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 279/3393 [01:43<25:39,  2.02batch/s, Batch Loss=0.2249, Avg Loss=0.0782, Time Left=25.81 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 279/3393 [01:44<25:39,  2.02batch/s, Batch Loss=0.2543, Avg Loss=0.0791, Time Left=25.81 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 280/3393 [01:44<25:44,  2.02batch/s, Batch Loss=0.2543, Avg Loss=0.0791, Time Left=25.81 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 280/3393 [01:44<25:44,  2.02batch/s, Batch Loss=0.0024, Avg Loss=0.0787, Time Left=25.80 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 281/3393 [01:44<25:32,  2.03batch/s, Batch Loss=0.0024, Avg Loss=0.0787, Time Left=25.80 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 281/3393 [01:45<25:32,  2.03batch/s, Batch Loss=0.0025, Avg Loss=0.0783, Time Left=25.79 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 282/3393 [01:45<25:14,  2.05batch/s, Batch Loss=0.0025, Avg Loss=0.0783, Time Left=25.79 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 282/3393 [01:45<25:14,  2.05batch/s, Batch Loss=0.3318, Avg Loss=0.0795, Time Left=25.78 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 283/3393 [01:45<24:52,  2.08batch/s, Batch Loss=0.3318, Avg Loss=0.0795, Time Left=25.78 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 283/3393 [01:46<24:52,  2.08batch/s, Batch Loss=0.0101, Avg Loss=0.0792, Time Left=25.78 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 284/3393 [01:46<25:37,  2.02batch/s, Batch Loss=0.0101, Avg Loss=0.0792, Time Left=25.78 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 284/3393 [01:46<25:37,  2.02batch/s, Batch Loss=0.0031, Avg Loss=0.0788, Time Left=25.77 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 285/3393 [01:46<25:29,  2.03batch/s, Batch Loss=0.0031, Avg Loss=0.0788, Time Left=25.77 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 285/3393 [01:47<25:29,  2.03batch/s, Batch Loss=0.0620, Avg Loss=0.0788, Time Left=25.77 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 286/3393 [01:47<25:50,  2.00batch/s, Batch Loss=0.0620, Avg Loss=0.0788, Time Left=25.77 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 286/3393 [01:47<25:50,  2.00batch/s, Batch Loss=0.0152, Avg Loss=0.0785, Time Left=25.76 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 287/3393 [01:47<25:38,  2.02batch/s, Batch Loss=0.0152, Avg Loss=0.0785, Time Left=25.76 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 287/3393 [01:48<25:38,  2.02batch/s, Batch Loss=0.0116, Avg Loss=0.0782, Time Left=25.75 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 288/3393 [01:48<25:40,  2.02batch/s, Batch Loss=0.0116, Avg Loss=0.0782, Time Left=25.75 \u001b[A\n",
      "Epoch 3/3 - Training:   8%| | 288/3393 [01:48<25:40,  2.02batch/s, Batch Loss=0.0170, Avg Loss=0.0779, Time Left=25.75 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 289/3393 [01:48<25:29,  2.03batch/s, Batch Loss=0.0170, Avg Loss=0.0779, Time Left=25.75 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 289/3393 [01:49<25:29,  2.03batch/s, Batch Loss=0.2581, Avg Loss=0.0787, Time Left=25.74 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 290/3393 [01:49<25:20,  2.04batch/s, Batch Loss=0.2581, Avg Loss=0.0787, Time Left=25.74 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 290/3393 [01:49<25:20,  2.04batch/s, Batch Loss=0.1674, Avg Loss=0.0791, Time Left=25.73 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 291/3393 [01:49<25:14,  2.05batch/s, Batch Loss=0.1674, Avg Loss=0.0791, Time Left=25.73 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 291/3393 [01:50<25:14,  2.05batch/s, Batch Loss=0.0708, Avg Loss=0.0791, Time Left=25.72 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 292/3393 [01:50<25:09,  2.05batch/s, Batch Loss=0.0708, Avg Loss=0.0791, Time Left=25.72 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 292/3393 [01:50<25:09,  2.05batch/s, Batch Loss=0.0118, Avg Loss=0.0788, Time Left=25.70 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 293/3393 [01:50<24:50,  2.08batch/s, Batch Loss=0.0118, Avg Loss=0.0788, Time Left=25.70 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 293/3393 [01:51<24:50,  2.08batch/s, Batch Loss=0.0174, Avg Loss=0.0785, Time Left=25.70 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 294/3393 [01:51<25:06,  2.06batch/s, Batch Loss=0.0174, Avg Loss=0.0785, Time Left=25.70 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 294/3393 [01:51<25:06,  2.06batch/s, Batch Loss=0.0339, Avg Loss=0.0783, Time Left=25.69 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:   9%| | 295/3393 [01:51<25:11,  2.05batch/s, Batch Loss=0.0339, Avg Loss=0.0783, Time Left=25.69 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 295/3393 [01:52<25:11,  2.05batch/s, Batch Loss=0.0177, Avg Loss=0.0780, Time Left=25.69 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 296/3393 [01:52<25:16,  2.04batch/s, Batch Loss=0.0177, Avg Loss=0.0780, Time Left=25.69 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 296/3393 [01:52<25:16,  2.04batch/s, Batch Loss=0.1311, Avg Loss=0.0783, Time Left=25.68 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 297/3393 [01:52<25:17,  2.04batch/s, Batch Loss=0.1311, Avg Loss=0.0783, Time Left=25.68 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 297/3393 [01:53<25:17,  2.04batch/s, Batch Loss=0.0609, Avg Loss=0.0782, Time Left=25.67 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 298/3393 [01:53<25:04,  2.06batch/s, Batch Loss=0.0609, Avg Loss=0.0782, Time Left=25.67 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 298/3393 [01:53<25:04,  2.06batch/s, Batch Loss=0.1445, Avg Loss=0.0785, Time Left=25.67 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 299/3393 [01:53<25:31,  2.02batch/s, Batch Loss=0.1445, Avg Loss=0.0785, Time Left=25.67 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 299/3393 [01:54<25:31,  2.02batch/s, Batch Loss=0.0443, Avg Loss=0.0783, Time Left=25.66 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 300/3393 [01:54<25:21,  2.03batch/s, Batch Loss=0.0443, Avg Loss=0.0783, Time Left=25.66 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 300/3393 [01:54<25:21,  2.03batch/s, Batch Loss=0.0173, Avg Loss=0.0781, Time Left=25.65 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 301/3393 [01:54<24:59,  2.06batch/s, Batch Loss=0.0173, Avg Loss=0.0781, Time Left=25.65 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 301/3393 [01:55<24:59,  2.06batch/s, Batch Loss=0.0311, Avg Loss=0.0779, Time Left=25.64 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 302/3393 [01:55<25:25,  2.03batch/s, Batch Loss=0.0311, Avg Loss=0.0779, Time Left=25.64 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 302/3393 [01:55<25:25,  2.03batch/s, Batch Loss=0.0431, Avg Loss=0.0777, Time Left=25.64 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 303/3393 [01:55<25:17,  2.04batch/s, Batch Loss=0.0431, Avg Loss=0.0777, Time Left=25.64 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 303/3393 [01:56<25:17,  2.04batch/s, Batch Loss=0.1240, Avg Loss=0.0779, Time Left=25.63 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 304/3393 [01:56<25:23,  2.03batch/s, Batch Loss=0.1240, Avg Loss=0.0779, Time Left=25.63 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 304/3393 [01:56<25:23,  2.03batch/s, Batch Loss=0.1742, Avg Loss=0.0783, Time Left=25.62 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 305/3393 [01:56<25:16,  2.04batch/s, Batch Loss=0.1742, Avg Loss=0.0783, Time Left=25.62 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 305/3393 [01:56<25:16,  2.04batch/s, Batch Loss=0.0944, Avg Loss=0.0784, Time Left=25.61 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 306/3393 [01:56<25:09,  2.05batch/s, Batch Loss=0.0944, Avg Loss=0.0784, Time Left=25.61 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 306/3393 [01:57<25:09,  2.05batch/s, Batch Loss=0.0073, Avg Loss=0.0781, Time Left=25.60 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 307/3393 [01:57<25:03,  2.05batch/s, Batch Loss=0.0073, Avg Loss=0.0781, Time Left=25.60 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 307/3393 [01:57<25:03,  2.05batch/s, Batch Loss=0.0428, Avg Loss=0.0780, Time Left=25.59 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 308/3393 [01:57<24:59,  2.06batch/s, Batch Loss=0.0428, Avg Loss=0.0780, Time Left=25.59 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 308/3393 [01:58<24:59,  2.06batch/s, Batch Loss=0.0087, Avg Loss=0.0777, Time Left=25.59 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 309/3393 [01:58<25:27,  2.02batch/s, Batch Loss=0.0087, Avg Loss=0.0777, Time Left=25.59 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 309/3393 [01:58<25:27,  2.02batch/s, Batch Loss=0.0825, Avg Loss=0.0777, Time Left=25.58 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 310/3393 [01:58<25:27,  2.02batch/s, Batch Loss=0.0825, Avg Loss=0.0777, Time Left=25.58 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 310/3393 [01:59<25:27,  2.02batch/s, Batch Loss=0.0098, Avg Loss=0.0774, Time Left=25.57 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 311/3393 [01:59<25:05,  2.05batch/s, Batch Loss=0.0098, Avg Loss=0.0774, Time Left=25.57 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 311/3393 [01:59<25:05,  2.05batch/s, Batch Loss=0.0997, Avg Loss=0.0775, Time Left=25.57 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 312/3393 [01:59<25:01,  2.05batch/s, Batch Loss=0.0997, Avg Loss=0.0775, Time Left=25.57 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 312/3393 [02:00<25:01,  2.05batch/s, Batch Loss=0.0269, Avg Loss=0.0773, Time Left=25.55 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 313/3393 [02:00<24:43,  2.08batch/s, Batch Loss=0.0269, Avg Loss=0.0773, Time Left=25.55 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 313/3393 [02:00<24:43,  2.08batch/s, Batch Loss=0.0248, Avg Loss=0.0771, Time Left=25.54 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 314/3393 [02:00<24:43,  2.07batch/s, Batch Loss=0.0248, Avg Loss=0.0771, Time Left=25.54 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 314/3393 [02:01<24:43,  2.07batch/s, Batch Loss=0.1742, Avg Loss=0.0775, Time Left=25.53 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 315/3393 [02:01<24:29,  2.09batch/s, Batch Loss=0.1742, Avg Loss=0.0775, Time Left=25.53 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 315/3393 [02:01<24:29,  2.09batch/s, Batch Loss=0.0243, Avg Loss=0.0773, Time Left=25.52 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 316/3393 [02:01<24:34,  2.09batch/s, Batch Loss=0.0243, Avg Loss=0.0773, Time Left=25.52 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 316/3393 [02:02<24:34,  2.09batch/s, Batch Loss=0.0101, Avg Loss=0.0770, Time Left=25.51 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 317/3393 [02:02<24:36,  2.08batch/s, Batch Loss=0.0101, Avg Loss=0.0770, Time Left=25.51 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 317/3393 [02:02<24:36,  2.08batch/s, Batch Loss=0.0170, Avg Loss=0.0768, Time Left=25.50 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 318/3393 [02:02<24:40,  2.08batch/s, Batch Loss=0.0170, Avg Loss=0.0768, Time Left=25.50 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 318/3393 [02:03<24:40,  2.08batch/s, Batch Loss=0.1860, Avg Loss=0.0772, Time Left=25.49 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 319/3393 [02:03<24:26,  2.10batch/s, Batch Loss=0.1860, Avg Loss=0.0772, Time Left=25.49 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 319/3393 [02:03<24:26,  2.10batch/s, Batch Loss=0.0143, Avg Loss=0.0769, Time Left=25.49 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 320/3393 [02:03<25:15,  2.03batch/s, Batch Loss=0.0143, Avg Loss=0.0769, Time Left=25.49 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 320/3393 [02:04<25:15,  2.03batch/s, Batch Loss=0.0428, Avg Loss=0.0768, Time Left=25.48 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 321/3393 [02:04<24:54,  2.05batch/s, Batch Loss=0.0428, Avg Loss=0.0768, Time Left=25.48 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 321/3393 [02:04<24:54,  2.05batch/s, Batch Loss=0.2831, Avg Loss=0.0776, Time Left=25.47 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 322/3393 [02:04<25:05,  2.04batch/s, Batch Loss=0.2831, Avg Loss=0.0776, Time Left=25.47 \u001b[A\n",
      "Epoch 3/3 - Training:   9%| | 322/3393 [02:05<25:05,  2.04batch/s, Batch Loss=0.5854, Avg Loss=0.0796, Time Left=25.47 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 323/3393 [02:05<25:27,  2.01batch/s, Batch Loss=0.5854, Avg Loss=0.0796, Time Left=25.47 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 323/3393 [02:05<25:27,  2.01batch/s, Batch Loss=0.0646, Avg Loss=0.0795, Time Left=25.47 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 324/3393 [02:05<25:40,  1.99batch/s, Batch Loss=0.0646, Avg Loss=0.0795, Time Left=25.47 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 324/3393 [02:06<25:40,  1.99batch/s, Batch Loss=0.0355, Avg Loss=0.0794, Time Left=25.46 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 325/3393 [02:06<25:30,  2.00batch/s, Batch Loss=0.0355, Avg Loss=0.0794, Time Left=25.46 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 325/3393 [02:06<25:30,  2.00batch/s, Batch Loss=0.0760, Avg Loss=0.0793, Time Left=25.46 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 326/3393 [02:06<25:16,  2.02batch/s, Batch Loss=0.0760, Avg Loss=0.0793, Time Left=25.46 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 326/3393 [02:07<25:16,  2.02batch/s, Batch Loss=0.0346, Avg Loss=0.0792, Time Left=25.45 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 327/3393 [02:07<25:35,  2.00batch/s, Batch Loss=0.0346, Avg Loss=0.0792, Time Left=25.45 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 327/3393 [02:07<25:35,  2.00batch/s, Batch Loss=0.0072, Avg Loss=0.0789, Time Left=25.44 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  10%| | 328/3393 [02:07<25:21,  2.01batch/s, Batch Loss=0.0072, Avg Loss=0.0789, Time Left=25.44 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 328/3393 [02:08<25:21,  2.01batch/s, Batch Loss=0.0466, Avg Loss=0.0788, Time Left=25.44 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 329/3393 [02:08<25:08,  2.03batch/s, Batch Loss=0.0466, Avg Loss=0.0788, Time Left=25.44 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 329/3393 [02:08<25:08,  2.03batch/s, Batch Loss=0.0042, Avg Loss=0.0785, Time Left=25.42 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 330/3393 [02:08<24:46,  2.06batch/s, Batch Loss=0.0042, Avg Loss=0.0785, Time Left=25.42 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 330/3393 [02:09<24:46,  2.06batch/s, Batch Loss=0.0354, Avg Loss=0.0783, Time Left=25.42 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 331/3393 [02:09<24:57,  2.05batch/s, Batch Loss=0.0354, Avg Loss=0.0783, Time Left=25.42 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 331/3393 [02:09<24:57,  2.05batch/s, Batch Loss=0.1151, Avg Loss=0.0785, Time Left=25.41 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 332/3393 [02:09<25:21,  2.01batch/s, Batch Loss=0.1151, Avg Loss=0.0785, Time Left=25.41 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 332/3393 [02:10<25:21,  2.01batch/s, Batch Loss=0.1810, Avg Loss=0.0789, Time Left=25.41 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 333/3393 [02:10<25:24,  2.01batch/s, Batch Loss=0.1810, Avg Loss=0.0789, Time Left=25.41 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 333/3393 [02:10<25:24,  2.01batch/s, Batch Loss=0.0179, Avg Loss=0.0786, Time Left=25.40 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 334/3393 [02:10<25:11,  2.02batch/s, Batch Loss=0.0179, Avg Loss=0.0786, Time Left=25.40 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 334/3393 [02:11<25:11,  2.02batch/s, Batch Loss=0.0618, Avg Loss=0.0786, Time Left=25.39 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 335/3393 [02:11<24:46,  2.06batch/s, Batch Loss=0.0618, Avg Loss=0.0786, Time Left=25.39 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 335/3393 [02:11<24:46,  2.06batch/s, Batch Loss=0.0128, Avg Loss=0.0783, Time Left=25.38 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 336/3393 [02:11<24:29,  2.08batch/s, Batch Loss=0.0128, Avg Loss=0.0783, Time Left=25.38 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 336/3393 [02:12<24:29,  2.08batch/s, Batch Loss=0.0157, Avg Loss=0.0781, Time Left=25.36 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 337/3393 [02:12<24:17,  2.10batch/s, Batch Loss=0.0157, Avg Loss=0.0781, Time Left=25.36 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 337/3393 [02:12<24:17,  2.10batch/s, Batch Loss=0.0057, Avg Loss=0.0778, Time Left=25.36 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 338/3393 [02:12<24:21,  2.09batch/s, Batch Loss=0.0057, Avg Loss=0.0778, Time Left=25.36 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 338/3393 [02:13<24:21,  2.09batch/s, Batch Loss=0.0186, Avg Loss=0.0776, Time Left=25.34 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 339/3393 [02:13<24:11,  2.10batch/s, Batch Loss=0.0186, Avg Loss=0.0776, Time Left=25.34 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 339/3393 [02:13<24:11,  2.10batch/s, Batch Loss=0.1059, Avg Loss=0.0777, Time Left=25.33 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 340/3393 [02:13<24:18,  2.09batch/s, Batch Loss=0.1059, Avg Loss=0.0777, Time Left=25.33 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 340/3393 [02:14<24:18,  2.09batch/s, Batch Loss=0.0108, Avg Loss=0.0775, Time Left=25.33 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 341/3393 [02:14<24:53,  2.04batch/s, Batch Loss=0.0108, Avg Loss=0.0775, Time Left=25.33 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 341/3393 [02:14<24:53,  2.04batch/s, Batch Loss=0.0079, Avg Loss=0.0772, Time Left=25.32 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 342/3393 [02:14<24:32,  2.07batch/s, Batch Loss=0.0079, Avg Loss=0.0772, Time Left=25.32 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 342/3393 [02:15<24:32,  2.07batch/s, Batch Loss=0.1499, Avg Loss=0.0775, Time Left=25.32 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 343/3393 [02:15<25:08,  2.02batch/s, Batch Loss=0.1499, Avg Loss=0.0775, Time Left=25.32 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 343/3393 [02:15<25:08,  2.02batch/s, Batch Loss=0.0217, Avg Loss=0.0773, Time Left=25.31 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 344/3393 [02:15<24:38,  2.06batch/s, Batch Loss=0.0217, Avg Loss=0.0773, Time Left=25.31 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 344/3393 [02:15<24:38,  2.06batch/s, Batch Loss=0.1115, Avg Loss=0.0774, Time Left=25.29 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 345/3393 [02:15<24:22,  2.08batch/s, Batch Loss=0.1115, Avg Loss=0.0774, Time Left=25.29 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 345/3393 [02:16<24:22,  2.08batch/s, Batch Loss=0.0428, Avg Loss=0.0773, Time Left=25.29 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 346/3393 [02:16<24:52,  2.04batch/s, Batch Loss=0.0428, Avg Loss=0.0773, Time Left=25.29 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 346/3393 [02:16<24:52,  2.04batch/s, Batch Loss=0.0305, Avg Loss=0.0771, Time Left=25.28 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 347/3393 [02:16<24:18,  2.09batch/s, Batch Loss=0.0305, Avg Loss=0.0771, Time Left=25.28 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 347/3393 [02:17<24:18,  2.09batch/s, Batch Loss=0.0267, Avg Loss=0.0769, Time Left=25.26 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 348/3393 [02:17<24:08,  2.10batch/s, Batch Loss=0.0267, Avg Loss=0.0769, Time Left=25.26 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 348/3393 [02:17<24:08,  2.10batch/s, Batch Loss=0.0177, Avg Loss=0.0767, Time Left=25.26 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 349/3393 [02:17<24:27,  2.07batch/s, Batch Loss=0.0177, Avg Loss=0.0767, Time Left=25.26 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 349/3393 [02:18<24:27,  2.07batch/s, Batch Loss=0.0544, Avg Loss=0.0766, Time Left=25.24 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 350/3393 [02:18<24:01,  2.11batch/s, Batch Loss=0.0544, Avg Loss=0.0766, Time Left=25.24 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 350/3393 [02:18<24:01,  2.11batch/s, Batch Loss=0.0342, Avg Loss=0.0765, Time Left=25.23 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 351/3393 [02:18<24:16,  2.09batch/s, Batch Loss=0.0342, Avg Loss=0.0765, Time Left=25.23 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 351/3393 [02:19<24:16,  2.09batch/s, Batch Loss=0.0675, Avg Loss=0.0765, Time Left=25.23 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 352/3393 [02:19<24:12,  2.09batch/s, Batch Loss=0.0675, Avg Loss=0.0765, Time Left=25.23 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 352/3393 [02:19<24:12,  2.09batch/s, Batch Loss=0.0121, Avg Loss=0.0762, Time Left=25.21 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 353/3393 [02:19<24:03,  2.11batch/s, Batch Loss=0.0121, Avg Loss=0.0762, Time Left=25.21 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 353/3393 [02:20<24:03,  2.11batch/s, Batch Loss=0.0817, Avg Loss=0.0763, Time Left=25.21 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 354/3393 [02:20<24:09,  2.10batch/s, Batch Loss=0.0817, Avg Loss=0.0763, Time Left=25.21 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 354/3393 [02:20<24:09,  2.10batch/s, Batch Loss=0.0204, Avg Loss=0.0761, Time Left=25.20 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 355/3393 [02:20<24:44,  2.05batch/s, Batch Loss=0.0204, Avg Loss=0.0761, Time Left=25.20 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 355/3393 [02:21<24:44,  2.05batch/s, Batch Loss=0.0192, Avg Loss=0.0759, Time Left=25.19 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 356/3393 [02:21<24:39,  2.05batch/s, Batch Loss=0.0192, Avg Loss=0.0759, Time Left=25.19 \u001b[A\n",
      "Epoch 3/3 - Training:  10%| | 356/3393 [02:21<24:39,  2.05batch/s, Batch Loss=0.0384, Avg Loss=0.0757, Time Left=25.18 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 357/3393 [02:21<24:23,  2.07batch/s, Batch Loss=0.0384, Avg Loss=0.0757, Time Left=25.18 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 357/3393 [02:22<24:23,  2.07batch/s, Batch Loss=0.0040, Avg Loss=0.0755, Time Left=25.17 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 358/3393 [02:22<24:22,  2.08batch/s, Batch Loss=0.0040, Avg Loss=0.0755, Time Left=25.17 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 358/3393 [02:22<24:22,  2.08batch/s, Batch Loss=0.1631, Avg Loss=0.0758, Time Left=25.16 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 359/3393 [02:22<24:09,  2.09batch/s, Batch Loss=0.1631, Avg Loss=0.0758, Time Left=25.16 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 359/3393 [02:23<24:09,  2.09batch/s, Batch Loss=0.0243, Avg Loss=0.0756, Time Left=25.15 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 360/3393 [02:23<24:19,  2.08batch/s, Batch Loss=0.0243, Avg Loss=0.0756, Time Left=25.15 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 360/3393 [02:23<24:19,  2.08batch/s, Batch Loss=0.2990, Avg Loss=0.0764, Time Left=25.14 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  11%| | 361/3393 [02:23<24:16,  2.08batch/s, Batch Loss=0.2990, Avg Loss=0.0764, Time Left=25.14 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 361/3393 [02:24<24:16,  2.08batch/s, Batch Loss=0.0032, Avg Loss=0.0761, Time Left=25.13 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 362/3393 [02:24<24:03,  2.10batch/s, Batch Loss=0.0032, Avg Loss=0.0761, Time Left=25.13 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 362/3393 [02:24<24:03,  2.10batch/s, Batch Loss=0.0043, Avg Loss=0.0759, Time Left=25.12 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 363/3393 [02:24<24:09,  2.09batch/s, Batch Loss=0.0043, Avg Loss=0.0759, Time Left=25.12 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 363/3393 [02:25<24:09,  2.09batch/s, Batch Loss=0.1034, Avg Loss=0.0760, Time Left=25.12 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 364/3393 [02:25<24:14,  2.08batch/s, Batch Loss=0.1034, Avg Loss=0.0760, Time Left=25.12 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 364/3393 [02:25<24:14,  2.08batch/s, Batch Loss=0.0520, Avg Loss=0.0759, Time Left=25.10 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 365/3393 [02:25<24:02,  2.10batch/s, Batch Loss=0.0520, Avg Loss=0.0759, Time Left=25.10 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 365/3393 [02:26<24:02,  2.10batch/s, Batch Loss=0.4737, Avg Loss=0.0772, Time Left=25.10 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 366/3393 [02:26<24:35,  2.05batch/s, Batch Loss=0.4737, Avg Loss=0.0772, Time Left=25.10 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 366/3393 [02:26<24:35,  2.05batch/s, Batch Loss=0.0525, Avg Loss=0.0771, Time Left=25.09 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 367/3393 [02:26<24:18,  2.07batch/s, Batch Loss=0.0525, Avg Loss=0.0771, Time Left=25.09 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 367/3393 [02:27<24:18,  2.07batch/s, Batch Loss=0.0642, Avg Loss=0.0771, Time Left=25.08 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 368/3393 [02:27<24:19,  2.07batch/s, Batch Loss=0.0642, Avg Loss=0.0771, Time Left=25.08 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 368/3393 [02:27<24:19,  2.07batch/s, Batch Loss=0.0339, Avg Loss=0.0770, Time Left=25.08 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 369/3393 [02:27<24:49,  2.03batch/s, Batch Loss=0.0339, Avg Loss=0.0770, Time Left=25.08 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 369/3393 [02:28<24:49,  2.03batch/s, Batch Loss=0.0015, Avg Loss=0.0767, Time Left=25.07 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 370/3393 [02:28<24:27,  2.06batch/s, Batch Loss=0.0015, Avg Loss=0.0767, Time Left=25.07 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 370/3393 [02:28<24:27,  2.06batch/s, Batch Loss=0.0307, Avg Loss=0.0766, Time Left=25.06 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 371/3393 [02:28<24:39,  2.04batch/s, Batch Loss=0.0307, Avg Loss=0.0766, Time Left=25.06 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 371/3393 [02:28<24:39,  2.04batch/s, Batch Loss=0.0179, Avg Loss=0.0764, Time Left=25.05 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 372/3393 [02:29<24:38,  2.04batch/s, Batch Loss=0.0179, Avg Loss=0.0764, Time Left=25.05 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 372/3393 [02:29<24:38,  2.04batch/s, Batch Loss=0.3683, Avg Loss=0.0773, Time Left=25.04 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 373/3393 [02:29<24:39,  2.04batch/s, Batch Loss=0.3683, Avg Loss=0.0773, Time Left=25.04 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 373/3393 [02:29<24:39,  2.04batch/s, Batch Loss=0.2219, Avg Loss=0.0778, Time Left=25.04 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 374/3393 [02:30<25:00,  2.01batch/s, Batch Loss=0.2219, Avg Loss=0.0778, Time Left=25.04 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 374/3393 [02:30<25:00,  2.01batch/s, Batch Loss=0.1137, Avg Loss=0.0779, Time Left=25.03 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 375/3393 [02:30<24:40,  2.04batch/s, Batch Loss=0.1137, Avg Loss=0.0779, Time Left=25.03 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 375/3393 [02:30<24:40,  2.04batch/s, Batch Loss=0.0175, Avg Loss=0.0777, Time Left=25.02 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 376/3393 [02:30<24:34,  2.05batch/s, Batch Loss=0.0175, Avg Loss=0.0777, Time Left=25.02 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 376/3393 [02:31<24:34,  2.05batch/s, Batch Loss=0.3907, Avg Loss=0.0787, Time Left=25.01 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 377/3393 [02:31<24:29,  2.05batch/s, Batch Loss=0.3907, Avg Loss=0.0787, Time Left=25.01 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 377/3393 [02:31<24:29,  2.05batch/s, Batch Loss=0.1229, Avg Loss=0.0789, Time Left=25.01 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 378/3393 [02:31<24:24,  2.06batch/s, Batch Loss=0.1229, Avg Loss=0.0789, Time Left=25.01 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 378/3393 [02:32<24:24,  2.06batch/s, Batch Loss=0.0310, Avg Loss=0.0787, Time Left=25.00 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 379/3393 [02:32<24:50,  2.02batch/s, Batch Loss=0.0310, Avg Loss=0.0787, Time Left=25.00 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 379/3393 [02:32<24:50,  2.02batch/s, Batch Loss=0.1110, Avg Loss=0.0788, Time Left=24.99 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 380/3393 [02:32<24:41,  2.03batch/s, Batch Loss=0.1110, Avg Loss=0.0788, Time Left=24.99 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 380/3393 [02:33<24:41,  2.03batch/s, Batch Loss=0.0314, Avg Loss=0.0787, Time Left=24.99 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 381/3393 [02:33<24:39,  2.04batch/s, Batch Loss=0.0314, Avg Loss=0.0787, Time Left=24.99 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 381/3393 [02:33<24:39,  2.04batch/s, Batch Loss=0.0170, Avg Loss=0.0785, Time Left=24.98 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 382/3393 [02:33<24:28,  2.05batch/s, Batch Loss=0.0170, Avg Loss=0.0785, Time Left=24.98 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 382/3393 [02:34<24:28,  2.05batch/s, Batch Loss=0.0253, Avg Loss=0.0783, Time Left=24.97 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 383/3393 [02:34<24:10,  2.08batch/s, Batch Loss=0.0253, Avg Loss=0.0783, Time Left=24.97 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 383/3393 [02:34<24:10,  2.08batch/s, Batch Loss=0.0236, Avg Loss=0.0781, Time Left=24.96 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 384/3393 [02:34<24:23,  2.06batch/s, Batch Loss=0.0236, Avg Loss=0.0781, Time Left=24.96 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 384/3393 [02:35<24:23,  2.06batch/s, Batch Loss=0.0449, Avg Loss=0.0780, Time Left=24.95 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 385/3393 [02:35<24:06,  2.08batch/s, Batch Loss=0.0449, Avg Loss=0.0780, Time Left=24.95 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 385/3393 [02:35<24:06,  2.08batch/s, Batch Loss=0.0128, Avg Loss=0.0778, Time Left=24.94 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 386/3393 [02:35<23:53,  2.10batch/s, Batch Loss=0.0128, Avg Loss=0.0778, Time Left=24.94 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 386/3393 [02:36<23:53,  2.10batch/s, Batch Loss=0.0310, Avg Loss=0.0777, Time Left=24.93 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 387/3393 [02:36<23:59,  2.09batch/s, Batch Loss=0.0310, Avg Loss=0.0777, Time Left=24.93 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 387/3393 [02:36<23:59,  2.09batch/s, Batch Loss=0.0233, Avg Loss=0.0775, Time Left=24.92 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 388/3393 [02:36<23:50,  2.10batch/s, Batch Loss=0.0233, Avg Loss=0.0775, Time Left=24.92 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 388/3393 [02:37<23:50,  2.10batch/s, Batch Loss=0.0160, Avg Loss=0.0773, Time Left=24.91 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 389/3393 [02:37<23:54,  2.09batch/s, Batch Loss=0.0160, Avg Loss=0.0773, Time Left=24.91 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 389/3393 [02:37<23:54,  2.09batch/s, Batch Loss=0.0193, Avg Loss=0.0771, Time Left=24.90 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 390/3393 [02:37<23:44,  2.11batch/s, Batch Loss=0.0193, Avg Loss=0.0771, Time Left=24.90 \u001b[A\n",
      "Epoch 3/3 - Training:  11%| | 390/3393 [02:38<23:44,  2.11batch/s, Batch Loss=0.0179, Avg Loss=0.0769, Time Left=24.89 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 391/3393 [02:38<24:21,  2.05batch/s, Batch Loss=0.0179, Avg Loss=0.0769, Time Left=24.89 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 391/3393 [02:38<24:21,  2.05batch/s, Batch Loss=0.0065, Avg Loss=0.0767, Time Left=24.88 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 392/3393 [02:38<24:18,  2.06batch/s, Batch Loss=0.0065, Avg Loss=0.0767, Time Left=24.88 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 392/3393 [02:39<24:18,  2.06batch/s, Batch Loss=0.1375, Avg Loss=0.0769, Time Left=24.88 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 393/3393 [02:39<24:48,  2.02batch/s, Batch Loss=0.1375, Avg Loss=0.0769, Time Left=24.88 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 393/3393 [02:39<24:48,  2.02batch/s, Batch Loss=0.0744, Avg Loss=0.0769, Time Left=24.87 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  12%| | 394/3393 [02:39<24:25,  2.05batch/s, Batch Loss=0.0744, Avg Loss=0.0769, Time Left=24.87 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 394/3393 [02:40<24:25,  2.05batch/s, Batch Loss=0.0968, Avg Loss=0.0770, Time Left=24.87 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 395/3393 [02:40<24:36,  2.03batch/s, Batch Loss=0.0968, Avg Loss=0.0770, Time Left=24.87 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 395/3393 [02:40<24:36,  2.03batch/s, Batch Loss=0.2776, Avg Loss=0.0776, Time Left=24.86 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 396/3393 [02:40<24:28,  2.04batch/s, Batch Loss=0.2776, Avg Loss=0.0776, Time Left=24.86 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 396/3393 [02:41<24:28,  2.04batch/s, Batch Loss=0.0246, Avg Loss=0.0774, Time Left=24.85 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 397/3393 [02:41<24:24,  2.05batch/s, Batch Loss=0.0246, Avg Loss=0.0774, Time Left=24.85 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 397/3393 [02:41<24:24,  2.05batch/s, Batch Loss=0.0727, Avg Loss=0.0774, Time Left=24.85 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 398/3393 [02:41<25:33,  1.95batch/s, Batch Loss=0.0727, Avg Loss=0.0774, Time Left=24.85 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 398/3393 [02:42<25:33,  1.95batch/s, Batch Loss=0.0234, Avg Loss=0.0772, Time Left=24.84 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 399/3393 [02:42<24:58,  2.00batch/s, Batch Loss=0.0234, Avg Loss=0.0772, Time Left=24.84 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 399/3393 [02:42<24:58,  2.00batch/s, Batch Loss=0.1202, Avg Loss=0.0774, Time Left=24.84 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 400/3393 [02:42<25:18,  1.97batch/s, Batch Loss=0.1202, Avg Loss=0.0774, Time Left=24.84 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 400/3393 [02:43<25:18,  1.97batch/s, Batch Loss=0.2340, Avg Loss=0.0778, Time Left=24.83 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 401/3393 [02:43<24:57,  2.00batch/s, Batch Loss=0.2340, Avg Loss=0.0778, Time Left=24.83 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 401/3393 [02:43<24:57,  2.00batch/s, Batch Loss=0.0136, Avg Loss=0.0776, Time Left=24.83 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 402/3393 [02:43<25:47,  1.93batch/s, Batch Loss=0.0136, Avg Loss=0.0776, Time Left=24.83 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 402/3393 [02:44<25:47,  1.93batch/s, Batch Loss=0.0120, Avg Loss=0.0775, Time Left=24.84 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 403/3393 [02:44<26:28,  1.88batch/s, Batch Loss=0.0120, Avg Loss=0.0775, Time Left=24.84 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 403/3393 [02:44<26:28,  1.88batch/s, Batch Loss=0.0035, Avg Loss=0.0772, Time Left=24.83 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 404/3393 [02:44<25:52,  1.93batch/s, Batch Loss=0.0035, Avg Loss=0.0772, Time Left=24.83 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 404/3393 [02:45<25:52,  1.93batch/s, Batch Loss=0.0062, Avg Loss=0.0770, Time Left=24.82 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 405/3393 [02:45<25:34,  1.95batch/s, Batch Loss=0.0062, Avg Loss=0.0770, Time Left=24.82 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 405/3393 [02:45<25:34,  1.95batch/s, Batch Loss=0.1888, Avg Loss=0.0774, Time Left=24.81 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 406/3393 [02:45<25:08,  1.98batch/s, Batch Loss=0.1888, Avg Loss=0.0774, Time Left=24.81 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 406/3393 [02:46<25:08,  1.98batch/s, Batch Loss=0.0102, Avg Loss=0.0772, Time Left=24.80 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 407/3393 [02:46<24:20,  2.04batch/s, Batch Loss=0.0102, Avg Loss=0.0772, Time Left=24.80 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 407/3393 [02:46<24:20,  2.04batch/s, Batch Loss=0.2033, Avg Loss=0.0775, Time Left=24.80 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 408/3393 [02:46<24:55,  2.00batch/s, Batch Loss=0.2033, Avg Loss=0.0775, Time Left=24.80 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 408/3393 [02:47<24:55,  2.00batch/s, Batch Loss=0.0919, Avg Loss=0.0776, Time Left=24.79 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 409/3393 [02:47<24:41,  2.01batch/s, Batch Loss=0.0919, Avg Loss=0.0776, Time Left=24.79 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 409/3393 [02:47<24:41,  2.01batch/s, Batch Loss=0.0714, Avg Loss=0.0775, Time Left=24.78 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 410/3393 [02:47<24:30,  2.03batch/s, Batch Loss=0.0714, Avg Loss=0.0775, Time Left=24.78 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 410/3393 [02:48<24:30,  2.03batch/s, Batch Loss=0.1594, Avg Loss=0.0778, Time Left=24.77 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 411/3393 [02:48<24:22,  2.04batch/s, Batch Loss=0.1594, Avg Loss=0.0778, Time Left=24.77 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 411/3393 [02:48<24:22,  2.04batch/s, Batch Loss=0.1310, Avg Loss=0.0779, Time Left=24.76 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 412/3393 [02:48<23:48,  2.09batch/s, Batch Loss=0.1310, Avg Loss=0.0779, Time Left=24.76 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 412/3393 [02:49<23:48,  2.09batch/s, Batch Loss=0.2257, Avg Loss=0.0784, Time Left=24.75 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 413/3393 [02:49<24:03,  2.06batch/s, Batch Loss=0.2257, Avg Loss=0.0784, Time Left=24.75 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 413/3393 [02:49<24:03,  2.06batch/s, Batch Loss=0.0978, Avg Loss=0.0784, Time Left=24.74 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 414/3393 [02:49<24:04,  2.06batch/s, Batch Loss=0.0978, Avg Loss=0.0784, Time Left=24.74 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 414/3393 [02:50<24:04,  2.06batch/s, Batch Loss=0.0290, Avg Loss=0.0783, Time Left=24.73 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 415/3393 [02:50<23:46,  2.09batch/s, Batch Loss=0.0290, Avg Loss=0.0783, Time Left=24.73 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 415/3393 [02:50<23:46,  2.09batch/s, Batch Loss=0.0038, Avg Loss=0.0781, Time Left=24.73 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 416/3393 [02:50<23:51,  2.08batch/s, Batch Loss=0.0038, Avg Loss=0.0781, Time Left=24.73 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 416/3393 [02:51<23:51,  2.08batch/s, Batch Loss=0.0382, Avg Loss=0.0780, Time Left=24.72 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 417/3393 [02:51<23:53,  2.08batch/s, Batch Loss=0.0382, Avg Loss=0.0780, Time Left=24.72 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 417/3393 [02:51<23:53,  2.08batch/s, Batch Loss=0.0436, Avg Loss=0.0779, Time Left=24.71 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 418/3393 [02:51<23:39,  2.10batch/s, Batch Loss=0.0436, Avg Loss=0.0779, Time Left=24.71 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 418/3393 [02:52<23:39,  2.10batch/s, Batch Loss=0.0910, Avg Loss=0.0779, Time Left=24.70 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 419/3393 [02:52<24:25,  2.03batch/s, Batch Loss=0.0910, Avg Loss=0.0779, Time Left=24.70 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 419/3393 [02:52<24:25,  2.03batch/s, Batch Loss=0.0345, Avg Loss=0.0778, Time Left=24.69 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 420/3393 [02:52<23:51,  2.08batch/s, Batch Loss=0.0345, Avg Loss=0.0778, Time Left=24.69 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 420/3393 [02:53<23:51,  2.08batch/s, Batch Loss=0.0182, Avg Loss=0.0776, Time Left=24.68 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 421/3393 [02:53<24:00,  2.06batch/s, Batch Loss=0.0182, Avg Loss=0.0776, Time Left=24.68 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 421/3393 [02:53<24:00,  2.06batch/s, Batch Loss=0.0212, Avg Loss=0.0774, Time Left=24.68 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 422/3393 [02:53<24:34,  2.02batch/s, Batch Loss=0.0212, Avg Loss=0.0774, Time Left=24.68 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 422/3393 [02:54<24:34,  2.02batch/s, Batch Loss=0.3330, Avg Loss=0.0782, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 423/3393 [02:54<24:17,  2.04batch/s, Batch Loss=0.3330, Avg Loss=0.0782, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 423/3393 [02:54<24:17,  2.04batch/s, Batch Loss=0.1911, Avg Loss=0.0785, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 424/3393 [02:54<24:40,  2.01batch/s, Batch Loss=0.1911, Avg Loss=0.0785, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  12%| | 424/3393 [02:55<24:40,  2.01batch/s, Batch Loss=0.0059, Avg Loss=0.0783, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 425/3393 [02:55<25:46,  1.92batch/s, Batch Loss=0.0059, Avg Loss=0.0783, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 425/3393 [02:55<25:46,  1.92batch/s, Batch Loss=0.0293, Avg Loss=0.0781, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 426/3393 [02:55<25:58,  1.90batch/s, Batch Loss=0.0293, Avg Loss=0.0781, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 426/3393 [02:56<25:58,  1.90batch/s, Batch Loss=0.0136, Avg Loss=0.0780, Time Left=24.66 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  13%|▏| 427/3393 [02:56<25:59,  1.90batch/s, Batch Loss=0.0136, Avg Loss=0.0780, Time Left=24.66 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 427/3393 [02:56<25:59,  1.90batch/s, Batch Loss=0.0912, Avg Loss=0.0780, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 428/3393 [02:56<26:31,  1.86batch/s, Batch Loss=0.0912, Avg Loss=0.0780, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 428/3393 [02:57<26:31,  1.86batch/s, Batch Loss=0.0178, Avg Loss=0.0778, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 429/3393 [02:57<26:32,  1.86batch/s, Batch Loss=0.0178, Avg Loss=0.0778, Time Left=24.67 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 429/3393 [02:57<26:32,  1.86batch/s, Batch Loss=0.0298, Avg Loss=0.0777, Time Left=24.66 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 430/3393 [02:57<25:30,  1.94batch/s, Batch Loss=0.0298, Avg Loss=0.0777, Time Left=24.66 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 430/3393 [02:58<25:30,  1.94batch/s, Batch Loss=0.0312, Avg Loss=0.0776, Time Left=24.66 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 431/3393 [02:58<25:47,  1.91batch/s, Batch Loss=0.0312, Avg Loss=0.0776, Time Left=24.66 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 431/3393 [02:58<25:47,  1.91batch/s, Batch Loss=0.4346, Avg Loss=0.0785, Time Left=24.64 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 432/3393 [02:58<24:53,  1.98batch/s, Batch Loss=0.4346, Avg Loss=0.0785, Time Left=24.64 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 432/3393 [02:59<24:53,  1.98batch/s, Batch Loss=0.0199, Avg Loss=0.0784, Time Left=24.64 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 433/3393 [02:59<24:56,  1.98batch/s, Batch Loss=0.0199, Avg Loss=0.0784, Time Left=24.64 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 433/3393 [02:59<24:56,  1.98batch/s, Batch Loss=0.0438, Avg Loss=0.0783, Time Left=24.63 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 434/3393 [02:59<24:22,  2.02batch/s, Batch Loss=0.0438, Avg Loss=0.0783, Time Left=24.63 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 434/3393 [03:00<24:22,  2.02batch/s, Batch Loss=0.0416, Avg Loss=0.0782, Time Left=24.62 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 435/3393 [03:00<24:53,  1.98batch/s, Batch Loss=0.0416, Avg Loss=0.0782, Time Left=24.62 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 435/3393 [03:00<24:53,  1.98batch/s, Batch Loss=0.1638, Avg Loss=0.0784, Time Left=24.62 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 436/3393 [03:00<24:57,  1.97batch/s, Batch Loss=0.1638, Avg Loss=0.0784, Time Left=24.62 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 436/3393 [03:01<24:57,  1.97batch/s, Batch Loss=0.0640, Avg Loss=0.0784, Time Left=24.62 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 437/3393 [03:01<25:35,  1.93batch/s, Batch Loss=0.0640, Avg Loss=0.0784, Time Left=24.62 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 437/3393 [03:01<25:35,  1.93batch/s, Batch Loss=0.1115, Avg Loss=0.0785, Time Left=24.62 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 438/3393 [03:01<25:52,  1.90batch/s, Batch Loss=0.1115, Avg Loss=0.0785, Time Left=24.62 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 438/3393 [03:02<25:52,  1.90batch/s, Batch Loss=0.0307, Avg Loss=0.0783, Time Left=24.61 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 439/3393 [03:02<25:40,  1.92batch/s, Batch Loss=0.0307, Avg Loss=0.0783, Time Left=24.61 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 439/3393 [03:02<25:40,  1.92batch/s, Batch Loss=0.0389, Avg Loss=0.0782, Time Left=24.61 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 440/3393 [03:02<25:17,  1.95batch/s, Batch Loss=0.0389, Avg Loss=0.0782, Time Left=24.61 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 440/3393 [03:03<25:17,  1.95batch/s, Batch Loss=0.0223, Avg Loss=0.0781, Time Left=24.60 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 441/3393 [03:03<25:18,  1.94batch/s, Batch Loss=0.0223, Avg Loss=0.0781, Time Left=24.60 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 441/3393 [03:03<25:18,  1.94batch/s, Batch Loss=0.0018, Avg Loss=0.0779, Time Left=24.60 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 442/3393 [03:03<25:21,  1.94batch/s, Batch Loss=0.0018, Avg Loss=0.0779, Time Left=24.60 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 442/3393 [03:04<25:21,  1.94batch/s, Batch Loss=0.0100, Avg Loss=0.0777, Time Left=24.59 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 443/3393 [03:04<24:48,  1.98batch/s, Batch Loss=0.0100, Avg Loss=0.0777, Time Left=24.59 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 443/3393 [03:04<24:48,  1.98batch/s, Batch Loss=0.0770, Avg Loss=0.0777, Time Left=24.58 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 444/3393 [03:04<24:43,  1.99batch/s, Batch Loss=0.0770, Avg Loss=0.0777, Time Left=24.58 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 444/3393 [03:05<24:43,  1.99batch/s, Batch Loss=0.2465, Avg Loss=0.0781, Time Left=24.58 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 445/3393 [03:05<25:33,  1.92batch/s, Batch Loss=0.2465, Avg Loss=0.0781, Time Left=24.58 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 445/3393 [03:05<25:33,  1.92batch/s, Batch Loss=0.0868, Avg Loss=0.0782, Time Left=24.57 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 446/3393 [03:05<24:54,  1.97batch/s, Batch Loss=0.0868, Avg Loss=0.0782, Time Left=24.57 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 446/3393 [03:06<24:54,  1.97batch/s, Batch Loss=0.0192, Avg Loss=0.0780, Time Left=24.56 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 447/3393 [03:06<24:34,  2.00batch/s, Batch Loss=0.0192, Avg Loss=0.0780, Time Left=24.56 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 447/3393 [03:06<24:34,  2.00batch/s, Batch Loss=0.1917, Avg Loss=0.0783, Time Left=24.55 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 448/3393 [03:06<24:18,  2.02batch/s, Batch Loss=0.1917, Avg Loss=0.0783, Time Left=24.55 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 448/3393 [03:07<24:18,  2.02batch/s, Batch Loss=0.0064, Avg Loss=0.0781, Time Left=24.55 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 449/3393 [03:07<24:21,  2.02batch/s, Batch Loss=0.0064, Avg Loss=0.0781, Time Left=24.55 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 449/3393 [03:07<24:21,  2.02batch/s, Batch Loss=0.2637, Avg Loss=0.0786, Time Left=24.54 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 450/3393 [03:07<24:10,  2.03batch/s, Batch Loss=0.2637, Avg Loss=0.0786, Time Left=24.54 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 450/3393 [03:08<24:10,  2.03batch/s, Batch Loss=0.1400, Avg Loss=0.0788, Time Left=24.53 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 451/3393 [03:08<23:55,  2.05batch/s, Batch Loss=0.1400, Avg Loss=0.0788, Time Left=24.53 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 451/3393 [03:08<23:55,  2.05batch/s, Batch Loss=0.0101, Avg Loss=0.0786, Time Left=24.52 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 452/3393 [03:08<23:55,  2.05batch/s, Batch Loss=0.0101, Avg Loss=0.0786, Time Left=24.52 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 452/3393 [03:09<23:55,  2.05batch/s, Batch Loss=0.0219, Avg Loss=0.0784, Time Left=24.52 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 453/3393 [03:09<24:16,  2.02batch/s, Batch Loss=0.0219, Avg Loss=0.0784, Time Left=24.52 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 453/3393 [03:10<24:16,  2.02batch/s, Batch Loss=0.0466, Avg Loss=0.0784, Time Left=24.53 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 454/3393 [03:10<26:30,  1.85batch/s, Batch Loss=0.0466, Avg Loss=0.0784, Time Left=24.53 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 454/3393 [03:10<26:30,  1.85batch/s, Batch Loss=0.0473, Avg Loss=0.0783, Time Left=24.52 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 455/3393 [03:10<25:19,  1.93batch/s, Batch Loss=0.0473, Avg Loss=0.0783, Time Left=24.52 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 455/3393 [03:10<25:19,  1.93batch/s, Batch Loss=0.0453, Avg Loss=0.0782, Time Left=24.51 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 456/3393 [03:10<24:48,  1.97batch/s, Batch Loss=0.0453, Avg Loss=0.0782, Time Left=24.51 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 456/3393 [03:11<24:48,  1.97batch/s, Batch Loss=0.0538, Avg Loss=0.0781, Time Left=24.50 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 457/3393 [03:11<24:55,  1.96batch/s, Batch Loss=0.0538, Avg Loss=0.0781, Time Left=24.50 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 457/3393 [03:11<24:55,  1.96batch/s, Batch Loss=0.0083, Avg Loss=0.0780, Time Left=24.49 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 458/3393 [03:11<24:42,  1.98batch/s, Batch Loss=0.0083, Avg Loss=0.0780, Time Left=24.49 \u001b[A\n",
      "Epoch 3/3 - Training:  13%|▏| 458/3393 [03:12<24:42,  1.98batch/s, Batch Loss=0.0203, Avg Loss=0.0778, Time Left=24.49 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 459/3393 [03:12<24:38,  1.98batch/s, Batch Loss=0.0203, Avg Loss=0.0778, Time Left=24.49 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 459/3393 [03:13<24:38,  1.98batch/s, Batch Loss=0.0786, Avg Loss=0.0778, Time Left=24.49 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  14%|▏| 460/3393 [03:13<25:33,  1.91batch/s, Batch Loss=0.0786, Avg Loss=0.0778, Time Left=24.49 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 460/3393 [03:13<25:33,  1.91batch/s, Batch Loss=0.0139, Avg Loss=0.0777, Time Left=24.49 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 461/3393 [03:13<26:11,  1.87batch/s, Batch Loss=0.0139, Avg Loss=0.0777, Time Left=24.49 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 461/3393 [03:14<26:11,  1.87batch/s, Batch Loss=0.2676, Avg Loss=0.0781, Time Left=24.48 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 462/3393 [03:14<25:07,  1.94batch/s, Batch Loss=0.2676, Avg Loss=0.0781, Time Left=24.48 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 462/3393 [03:14<25:07,  1.94batch/s, Batch Loss=0.0332, Avg Loss=0.0780, Time Left=24.48 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 463/3393 [03:14<25:28,  1.92batch/s, Batch Loss=0.0332, Avg Loss=0.0780, Time Left=24.48 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 463/3393 [03:15<25:28,  1.92batch/s, Batch Loss=0.2284, Avg Loss=0.0784, Time Left=24.47 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 464/3393 [03:15<25:10,  1.94batch/s, Batch Loss=0.2284, Avg Loss=0.0784, Time Left=24.47 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 464/3393 [03:15<25:10,  1.94batch/s, Batch Loss=0.0456, Avg Loss=0.0783, Time Left=24.46 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 465/3393 [03:15<24:22,  2.00batch/s, Batch Loss=0.0456, Avg Loss=0.0783, Time Left=24.46 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 465/3393 [03:16<24:22,  2.00batch/s, Batch Loss=0.0135, Avg Loss=0.0782, Time Left=24.46 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 466/3393 [03:16<25:38,  1.90batch/s, Batch Loss=0.0135, Avg Loss=0.0782, Time Left=24.46 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 466/3393 [03:16<25:38,  1.90batch/s, Batch Loss=0.0401, Avg Loss=0.0781, Time Left=24.46 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 467/3393 [03:16<26:15,  1.86batch/s, Batch Loss=0.0401, Avg Loss=0.0781, Time Left=24.46 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 467/3393 [03:17<26:15,  1.86batch/s, Batch Loss=0.0087, Avg Loss=0.0779, Time Left=24.47 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 468/3393 [03:17<26:42,  1.82batch/s, Batch Loss=0.0087, Avg Loss=0.0779, Time Left=24.47 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 468/3393 [03:17<26:42,  1.82batch/s, Batch Loss=0.0042, Avg Loss=0.0777, Time Left=24.46 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 469/3393 [03:17<25:48,  1.89batch/s, Batch Loss=0.0042, Avg Loss=0.0777, Time Left=24.46 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 469/3393 [03:18<25:48,  1.89batch/s, Batch Loss=0.0880, Avg Loss=0.0777, Time Left=24.45 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 470/3393 [03:18<25:00,  1.95batch/s, Batch Loss=0.0880, Avg Loss=0.0777, Time Left=24.45 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 470/3393 [03:18<25:00,  1.95batch/s, Batch Loss=0.0102, Avg Loss=0.0776, Time Left=24.44 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 471/3393 [03:18<25:20,  1.92batch/s, Batch Loss=0.0102, Avg Loss=0.0776, Time Left=24.44 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 471/3393 [03:19<25:20,  1.92batch/s, Batch Loss=0.0266, Avg Loss=0.0774, Time Left=24.43 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 472/3393 [03:19<24:35,  1.98batch/s, Batch Loss=0.0266, Avg Loss=0.0774, Time Left=24.43 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 472/3393 [03:19<24:35,  1.98batch/s, Batch Loss=0.0657, Avg Loss=0.0774, Time Left=24.43 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 473/3393 [03:19<24:21,  2.00batch/s, Batch Loss=0.0657, Avg Loss=0.0774, Time Left=24.43 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 473/3393 [03:20<24:21,  2.00batch/s, Batch Loss=0.0070, Avg Loss=0.0772, Time Left=24.41 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 474/3393 [03:20<23:53,  2.04batch/s, Batch Loss=0.0070, Avg Loss=0.0772, Time Left=24.41 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 474/3393 [03:20<23:53,  2.04batch/s, Batch Loss=0.0572, Avg Loss=0.0772, Time Left=24.41 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 475/3393 [03:20<24:06,  2.02batch/s, Batch Loss=0.0572, Avg Loss=0.0772, Time Left=24.41 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 475/3393 [03:21<24:06,  2.02batch/s, Batch Loss=0.0288, Avg Loss=0.0771, Time Left=24.40 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 476/3393 [03:21<23:44,  2.05batch/s, Batch Loss=0.0288, Avg Loss=0.0771, Time Left=24.40 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 476/3393 [03:21<23:44,  2.05batch/s, Batch Loss=0.0540, Avg Loss=0.0770, Time Left=24.39 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 477/3393 [03:21<23:36,  2.06batch/s, Batch Loss=0.0540, Avg Loss=0.0770, Time Left=24.39 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 477/3393 [03:22<23:36,  2.06batch/s, Batch Loss=0.0027, Avg Loss=0.0768, Time Left=24.38 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 478/3393 [03:22<23:55,  2.03batch/s, Batch Loss=0.0027, Avg Loss=0.0768, Time Left=24.38 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 478/3393 [03:22<23:55,  2.03batch/s, Batch Loss=0.0041, Avg Loss=0.0767, Time Left=24.37 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 479/3393 [03:22<23:56,  2.03batch/s, Batch Loss=0.0041, Avg Loss=0.0767, Time Left=24.37 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 479/3393 [03:23<23:56,  2.03batch/s, Batch Loss=0.0019, Avg Loss=0.0765, Time Left=24.37 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 480/3393 [03:23<23:42,  2.05batch/s, Batch Loss=0.0019, Avg Loss=0.0765, Time Left=24.37 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 480/3393 [03:23<23:42,  2.05batch/s, Batch Loss=0.0068, Avg Loss=0.0763, Time Left=24.36 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 481/3393 [03:23<24:13,  2.00batch/s, Batch Loss=0.0068, Avg Loss=0.0763, Time Left=24.36 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 481/3393 [03:24<24:13,  2.00batch/s, Batch Loss=0.0046, Avg Loss=0.0761, Time Left=24.35 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 482/3393 [03:24<23:50,  2.03batch/s, Batch Loss=0.0046, Avg Loss=0.0761, Time Left=24.35 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 482/3393 [03:24<23:50,  2.03batch/s, Batch Loss=0.0104, Avg Loss=0.0760, Time Left=24.35 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 483/3393 [03:24<24:10,  2.01batch/s, Batch Loss=0.0104, Avg Loss=0.0760, Time Left=24.35 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 483/3393 [03:25<24:10,  2.01batch/s, Batch Loss=0.0016, Avg Loss=0.0758, Time Left=24.34 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 484/3393 [03:25<24:09,  2.01batch/s, Batch Loss=0.0016, Avg Loss=0.0758, Time Left=24.34 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 484/3393 [03:25<24:09,  2.01batch/s, Batch Loss=0.0322, Avg Loss=0.0757, Time Left=24.33 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 485/3393 [03:25<24:24,  1.99batch/s, Batch Loss=0.0322, Avg Loss=0.0757, Time Left=24.33 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 485/3393 [03:26<24:24,  1.99batch/s, Batch Loss=0.0462, Avg Loss=0.0756, Time Left=24.33 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 486/3393 [03:26<24:20,  1.99batch/s, Batch Loss=0.0462, Avg Loss=0.0756, Time Left=24.33 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 486/3393 [03:26<24:20,  1.99batch/s, Batch Loss=0.0704, Avg Loss=0.0756, Time Left=24.31 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 487/3393 [03:26<23:47,  2.04batch/s, Batch Loss=0.0704, Avg Loss=0.0756, Time Left=24.31 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 487/3393 [03:27<23:47,  2.04batch/s, Batch Loss=0.0052, Avg Loss=0.0754, Time Left=24.31 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 488/3393 [03:27<24:11,  2.00batch/s, Batch Loss=0.0052, Avg Loss=0.0754, Time Left=24.31 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 488/3393 [03:27<24:11,  2.00batch/s, Batch Loss=0.1813, Avg Loss=0.0757, Time Left=24.30 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 489/3393 [03:27<23:47,  2.03batch/s, Batch Loss=0.1813, Avg Loss=0.0757, Time Left=24.30 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 489/3393 [03:28<23:47,  2.03batch/s, Batch Loss=0.1822, Avg Loss=0.0759, Time Left=24.29 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 490/3393 [03:28<23:52,  2.03batch/s, Batch Loss=0.1822, Avg Loss=0.0759, Time Left=24.29 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 490/3393 [03:28<23:52,  2.03batch/s, Batch Loss=0.3610, Avg Loss=0.0766, Time Left=24.28 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 491/3393 [03:28<23:36,  2.05batch/s, Batch Loss=0.3610, Avg Loss=0.0766, Time Left=24.28 \u001b[A\n",
      "Epoch 3/3 - Training:  14%|▏| 491/3393 [03:29<23:36,  2.05batch/s, Batch Loss=0.1673, Avg Loss=0.0768, Time Left=24.27 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 492/3393 [03:29<23:30,  2.06batch/s, Batch Loss=0.1673, Avg Loss=0.0768, Time Left=24.27 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 492/3393 [03:29<23:30,  2.06batch/s, Batch Loss=0.1130, Avg Loss=0.0769, Time Left=24.27 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  15%|▏| 493/3393 [03:29<24:00,  2.01batch/s, Batch Loss=0.1130, Avg Loss=0.0769, Time Left=24.27 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 493/3393 [03:30<24:00,  2.01batch/s, Batch Loss=0.0869, Avg Loss=0.0769, Time Left=24.26 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 494/3393 [03:30<23:54,  2.02batch/s, Batch Loss=0.0869, Avg Loss=0.0769, Time Left=24.26 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 494/3393 [03:30<23:54,  2.02batch/s, Batch Loss=0.1066, Avg Loss=0.0770, Time Left=24.26 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 495/3393 [03:30<24:43,  1.95batch/s, Batch Loss=0.1066, Avg Loss=0.0770, Time Left=24.26 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 495/3393 [03:31<24:43,  1.95batch/s, Batch Loss=0.0014, Avg Loss=0.0768, Time Left=24.27 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 496/3393 [03:31<27:05,  1.78batch/s, Batch Loss=0.0014, Avg Loss=0.0768, Time Left=24.27 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 496/3393 [03:31<27:05,  1.78batch/s, Batch Loss=0.0024, Avg Loss=0.0767, Time Left=24.28 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 497/3393 [03:31<28:20,  1.70batch/s, Batch Loss=0.0024, Avg Loss=0.0767, Time Left=24.28 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 497/3393 [03:32<28:20,  1.70batch/s, Batch Loss=0.2367, Avg Loss=0.0770, Time Left=24.29 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 498/3393 [03:32<29:19,  1.65batch/s, Batch Loss=0.2367, Avg Loss=0.0770, Time Left=24.29 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 498/3393 [03:33<29:19,  1.65batch/s, Batch Loss=0.0699, Avg Loss=0.0770, Time Left=24.30 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 499/3393 [03:33<29:53,  1.61batch/s, Batch Loss=0.0699, Avg Loss=0.0770, Time Left=24.30 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 499/3393 [03:33<29:53,  1.61batch/s, Batch Loss=0.0090, Avg Loss=0.0769, Time Left=24.31 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 500/3393 [03:33<29:43,  1.62batch/s, Batch Loss=0.0090, Avg Loss=0.0769, Time Left=24.31 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 500/3393 [03:34<29:43,  1.62batch/s, Batch Loss=0.0759, Avg Loss=0.0769, Time Left=24.30 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 501/3393 [03:34<27:45,  1.74batch/s, Batch Loss=0.0759, Avg Loss=0.0769, Time Left=24.30 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 501/3393 [03:34<27:45,  1.74batch/s, Batch Loss=0.1287, Avg Loss=0.0770, Time Left=24.29 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 502/3393 [03:34<26:11,  1.84batch/s, Batch Loss=0.1287, Avg Loss=0.0770, Time Left=24.29 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 502/3393 [03:35<26:11,  1.84batch/s, Batch Loss=0.0114, Avg Loss=0.0768, Time Left=24.28 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 503/3393 [03:35<25:32,  1.89batch/s, Batch Loss=0.0114, Avg Loss=0.0768, Time Left=24.28 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 503/3393 [03:35<25:32,  1.89batch/s, Batch Loss=0.1241, Avg Loss=0.0769, Time Left=24.27 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 504/3393 [03:35<24:45,  1.94batch/s, Batch Loss=0.1241, Avg Loss=0.0769, Time Left=24.27 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 504/3393 [03:36<24:45,  1.94batch/s, Batch Loss=0.1641, Avg Loss=0.0771, Time Left=24.26 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 505/3393 [03:36<24:21,  1.98batch/s, Batch Loss=0.1641, Avg Loss=0.0771, Time Left=24.26 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 505/3393 [03:36<24:21,  1.98batch/s, Batch Loss=0.3021, Avg Loss=0.0776, Time Left=24.25 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 506/3393 [03:36<24:04,  2.00batch/s, Batch Loss=0.3021, Avg Loss=0.0776, Time Left=24.25 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 506/3393 [03:37<24:04,  2.00batch/s, Batch Loss=0.1124, Avg Loss=0.0777, Time Left=24.24 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 507/3393 [03:37<23:42,  2.03batch/s, Batch Loss=0.1124, Avg Loss=0.0777, Time Left=24.24 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 507/3393 [03:37<23:42,  2.03batch/s, Batch Loss=0.0251, Avg Loss=0.0776, Time Left=24.23 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 508/3393 [03:37<23:59,  2.00batch/s, Batch Loss=0.0251, Avg Loss=0.0776, Time Left=24.23 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 508/3393 [03:38<23:59,  2.00batch/s, Batch Loss=0.2785, Avg Loss=0.0781, Time Left=24.22 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 509/3393 [03:38<23:35,  2.04batch/s, Batch Loss=0.2785, Avg Loss=0.0781, Time Left=24.22 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 509/3393 [03:38<23:35,  2.04batch/s, Batch Loss=0.0724, Avg Loss=0.0780, Time Left=24.21 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 510/3393 [03:38<23:02,  2.08batch/s, Batch Loss=0.0724, Avg Loss=0.0780, Time Left=24.21 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 510/3393 [03:39<23:02,  2.08batch/s, Batch Loss=0.0237, Avg Loss=0.0779, Time Left=24.20 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 511/3393 [03:39<23:07,  2.08batch/s, Batch Loss=0.0237, Avg Loss=0.0779, Time Left=24.20 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 511/3393 [03:39<23:07,  2.08batch/s, Batch Loss=0.1484, Avg Loss=0.0781, Time Left=24.19 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 512/3393 [03:39<22:43,  2.11batch/s, Batch Loss=0.1484, Avg Loss=0.0781, Time Left=24.19 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 512/3393 [03:40<22:43,  2.11batch/s, Batch Loss=0.4059, Avg Loss=0.0788, Time Left=24.18 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 513/3393 [03:40<22:28,  2.13batch/s, Batch Loss=0.4059, Avg Loss=0.0788, Time Left=24.18 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 513/3393 [03:40<22:28,  2.13batch/s, Batch Loss=0.0809, Avg Loss=0.0788, Time Left=24.17 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 514/3393 [03:40<22:30,  2.13batch/s, Batch Loss=0.0809, Avg Loss=0.0788, Time Left=24.17 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 514/3393 [03:41<22:30,  2.13batch/s, Batch Loss=0.0893, Avg Loss=0.0788, Time Left=24.16 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 515/3393 [03:41<23:11,  2.07batch/s, Batch Loss=0.0893, Avg Loss=0.0788, Time Left=24.16 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 515/3393 [03:41<23:11,  2.07batch/s, Batch Loss=0.0416, Avg Loss=0.0788, Time Left=24.15 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 516/3393 [03:41<22:59,  2.09batch/s, Batch Loss=0.0416, Avg Loss=0.0788, Time Left=24.15 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 516/3393 [03:42<22:59,  2.09batch/s, Batch Loss=0.1559, Avg Loss=0.0789, Time Left=24.14 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 517/3393 [03:42<22:52,  2.09batch/s, Batch Loss=0.1559, Avg Loss=0.0789, Time Left=24.14 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 517/3393 [03:42<22:52,  2.09batch/s, Batch Loss=0.1873, Avg Loss=0.0792, Time Left=24.13 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 518/3393 [03:42<22:33,  2.12batch/s, Batch Loss=0.1873, Avg Loss=0.0792, Time Left=24.13 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 518/3393 [03:42<22:33,  2.12batch/s, Batch Loss=0.0186, Avg Loss=0.0790, Time Left=24.11 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 519/3393 [03:42<22:17,  2.15batch/s, Batch Loss=0.0186, Avg Loss=0.0790, Time Left=24.11 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 519/3393 [03:43<22:17,  2.15batch/s, Batch Loss=0.0630, Avg Loss=0.0790, Time Left=24.11 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 520/3393 [03:43<22:20,  2.14batch/s, Batch Loss=0.0630, Avg Loss=0.0790, Time Left=24.11 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 520/3393 [03:43<22:20,  2.14batch/s, Batch Loss=0.0721, Avg Loss=0.0790, Time Left=24.10 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 521/3393 [03:43<22:51,  2.09batch/s, Batch Loss=0.0721, Avg Loss=0.0790, Time Left=24.10 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 521/3393 [03:44<22:51,  2.09batch/s, Batch Loss=0.1543, Avg Loss=0.0791, Time Left=24.09 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 522/3393 [03:44<22:58,  2.08batch/s, Batch Loss=0.1543, Avg Loss=0.0791, Time Left=24.09 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 522/3393 [03:44<22:58,  2.08batch/s, Batch Loss=0.1322, Avg Loss=0.0793, Time Left=24.08 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 523/3393 [03:44<22:51,  2.09batch/s, Batch Loss=0.1322, Avg Loss=0.0793, Time Left=24.08 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 523/3393 [03:45<22:51,  2.09batch/s, Batch Loss=0.0282, Avg Loss=0.0792, Time Left=24.07 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 524/3393 [03:45<22:57,  2.08batch/s, Batch Loss=0.0282, Avg Loss=0.0792, Time Left=24.07 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 524/3393 [03:45<22:57,  2.08batch/s, Batch Loss=0.1300, Avg Loss=0.0793, Time Left=24.06 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 525/3393 [03:45<22:35,  2.12batch/s, Batch Loss=0.1300, Avg Loss=0.0793, Time Left=24.06 \u001b[A\n",
      "Epoch 3/3 - Training:  15%|▏| 525/3393 [03:46<22:35,  2.12batch/s, Batch Loss=0.0643, Avg Loss=0.0792, Time Left=24.05 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  16%|▏| 526/3393 [03:46<22:33,  2.12batch/s, Batch Loss=0.0643, Avg Loss=0.0792, Time Left=24.05 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 526/3393 [03:46<22:33,  2.12batch/s, Batch Loss=0.0197, Avg Loss=0.0791, Time Left=24.04 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 527/3393 [03:46<23:00,  2.08batch/s, Batch Loss=0.0197, Avg Loss=0.0791, Time Left=24.04 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 527/3393 [03:47<23:00,  2.08batch/s, Batch Loss=0.0226, Avg Loss=0.0790, Time Left=24.03 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 528/3393 [03:47<22:50,  2.09batch/s, Batch Loss=0.0226, Avg Loss=0.0790, Time Left=24.03 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 528/3393 [03:47<22:50,  2.09batch/s, Batch Loss=0.0279, Avg Loss=0.0789, Time Left=24.02 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 529/3393 [03:47<22:42,  2.10batch/s, Batch Loss=0.0279, Avg Loss=0.0789, Time Left=24.02 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 529/3393 [03:48<22:42,  2.10batch/s, Batch Loss=0.0201, Avg Loss=0.0787, Time Left=24.01 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 530/3393 [03:48<22:49,  2.09batch/s, Batch Loss=0.0201, Avg Loss=0.0787, Time Left=24.01 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 530/3393 [03:48<22:49,  2.09batch/s, Batch Loss=0.0155, Avg Loss=0.0786, Time Left=24.00 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 531/3393 [03:48<22:30,  2.12batch/s, Batch Loss=0.0155, Avg Loss=0.0786, Time Left=24.00 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 531/3393 [03:49<22:30,  2.12batch/s, Batch Loss=0.0571, Avg Loss=0.0786, Time Left=23.99 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 532/3393 [03:49<22:41,  2.10batch/s, Batch Loss=0.0571, Avg Loss=0.0786, Time Left=23.99 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 532/3393 [03:49<22:41,  2.10batch/s, Batch Loss=0.0798, Avg Loss=0.0786, Time Left=23.98 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 533/3393 [03:49<23:04,  2.07batch/s, Batch Loss=0.0798, Avg Loss=0.0786, Time Left=23.98 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 533/3393 [03:50<23:04,  2.07batch/s, Batch Loss=0.0344, Avg Loss=0.0785, Time Left=23.97 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 534/3393 [03:50<22:52,  2.08batch/s, Batch Loss=0.0344, Avg Loss=0.0785, Time Left=23.97 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 534/3393 [03:50<22:52,  2.08batch/s, Batch Loss=0.2260, Avg Loss=0.0788, Time Left=23.96 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 535/3393 [03:50<22:44,  2.09batch/s, Batch Loss=0.2260, Avg Loss=0.0788, Time Left=23.96 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 535/3393 [03:51<22:44,  2.09batch/s, Batch Loss=0.0127, Avg Loss=0.0786, Time Left=23.96 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 536/3393 [03:51<23:05,  2.06batch/s, Batch Loss=0.0127, Avg Loss=0.0786, Time Left=23.96 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 536/3393 [03:51<23:05,  2.06batch/s, Batch Loss=0.0067, Avg Loss=0.0785, Time Left=23.95 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 537/3393 [03:51<23:08,  2.06batch/s, Batch Loss=0.0067, Avg Loss=0.0785, Time Left=23.95 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 537/3393 [03:52<23:08,  2.06batch/s, Batch Loss=0.1207, Avg Loss=0.0786, Time Left=23.94 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 538/3393 [03:52<23:28,  2.03batch/s, Batch Loss=0.1207, Avg Loss=0.0786, Time Left=23.94 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 538/3393 [03:52<23:28,  2.03batch/s, Batch Loss=0.1023, Avg Loss=0.0786, Time Left=23.93 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 539/3393 [03:52<22:53,  2.08batch/s, Batch Loss=0.1023, Avg Loss=0.0786, Time Left=23.93 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 539/3393 [03:53<22:53,  2.08batch/s, Batch Loss=0.0467, Avg Loss=0.0786, Time Left=23.92 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 540/3393 [03:53<23:32,  2.02batch/s, Batch Loss=0.0467, Avg Loss=0.0786, Time Left=23.92 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 540/3393 [03:53<23:32,  2.02batch/s, Batch Loss=0.0051, Avg Loss=0.0784, Time Left=23.92 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 541/3393 [03:53<23:59,  1.98batch/s, Batch Loss=0.0051, Avg Loss=0.0784, Time Left=23.92 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 541/3393 [03:54<23:59,  1.98batch/s, Batch Loss=0.0528, Avg Loss=0.0784, Time Left=23.91 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 542/3393 [03:54<23:44,  2.00batch/s, Batch Loss=0.0528, Avg Loss=0.0784, Time Left=23.91 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 542/3393 [03:54<23:44,  2.00batch/s, Batch Loss=0.2425, Avg Loss=0.0787, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 543/3393 [03:54<23:39,  2.01batch/s, Batch Loss=0.2425, Avg Loss=0.0787, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 543/3393 [03:55<23:39,  2.01batch/s, Batch Loss=0.1947, Avg Loss=0.0789, Time Left=23.91 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 544/3393 [03:55<25:07,  1.89batch/s, Batch Loss=0.1947, Avg Loss=0.0789, Time Left=23.91 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 544/3393 [03:55<25:07,  1.89batch/s, Batch Loss=0.0172, Avg Loss=0.0788, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 545/3393 [03:55<25:00,  1.90batch/s, Batch Loss=0.0172, Avg Loss=0.0788, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 545/3393 [03:56<25:00,  1.90batch/s, Batch Loss=0.0211, Avg Loss=0.0787, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 546/3393 [03:56<25:50,  1.84batch/s, Batch Loss=0.0211, Avg Loss=0.0787, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 546/3393 [03:56<25:50,  1.84batch/s, Batch Loss=0.1731, Avg Loss=0.0789, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 547/3393 [03:56<26:32,  1.79batch/s, Batch Loss=0.1731, Avg Loss=0.0789, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 547/3393 [03:57<26:32,  1.79batch/s, Batch Loss=0.3236, Avg Loss=0.0794, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 548/3393 [03:57<26:27,  1.79batch/s, Batch Loss=0.3236, Avg Loss=0.0794, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 548/3393 [03:57<26:27,  1.79batch/s, Batch Loss=0.0342, Avg Loss=0.0793, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 549/3393 [03:57<25:57,  1.83batch/s, Batch Loss=0.0342, Avg Loss=0.0793, Time Left=23.90 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 549/3393 [03:58<25:57,  1.83batch/s, Batch Loss=0.0401, Avg Loss=0.0792, Time Left=23.89 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 550/3393 [03:58<25:31,  1.86batch/s, Batch Loss=0.0401, Avg Loss=0.0792, Time Left=23.89 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 550/3393 [03:58<25:31,  1.86batch/s, Batch Loss=0.0668, Avg Loss=0.0792, Time Left=23.88 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 551/3393 [03:58<24:49,  1.91batch/s, Batch Loss=0.0668, Avg Loss=0.0792, Time Left=23.88 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 551/3393 [03:59<24:49,  1.91batch/s, Batch Loss=0.0690, Avg Loss=0.0792, Time Left=23.88 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 552/3393 [03:59<24:40,  1.92batch/s, Batch Loss=0.0690, Avg Loss=0.0792, Time Left=23.88 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 552/3393 [03:59<24:40,  1.92batch/s, Batch Loss=0.0144, Avg Loss=0.0790, Time Left=23.87 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 553/3393 [03:59<24:10,  1.96batch/s, Batch Loss=0.0144, Avg Loss=0.0790, Time Left=23.87 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 553/3393 [04:00<24:10,  1.96batch/s, Batch Loss=0.0457, Avg Loss=0.0790, Time Left=23.86 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 554/3393 [04:00<23:24,  2.02batch/s, Batch Loss=0.0457, Avg Loss=0.0790, Time Left=23.86 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 554/3393 [04:00<23:24,  2.02batch/s, Batch Loss=0.2904, Avg Loss=0.0794, Time Left=23.85 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 555/3393 [04:00<23:28,  2.02batch/s, Batch Loss=0.2904, Avg Loss=0.0794, Time Left=23.85 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 555/3393 [04:01<23:28,  2.02batch/s, Batch Loss=0.0092, Avg Loss=0.0793, Time Left=23.84 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 556/3393 [04:01<22:54,  2.06batch/s, Batch Loss=0.0092, Avg Loss=0.0793, Time Left=23.84 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 556/3393 [04:01<22:54,  2.06batch/s, Batch Loss=0.0928, Avg Loss=0.0793, Time Left=23.83 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 557/3393 [04:01<22:44,  2.08batch/s, Batch Loss=0.0928, Avg Loss=0.0793, Time Left=23.83 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 557/3393 [04:02<22:44,  2.08batch/s, Batch Loss=0.1157, Avg Loss=0.0794, Time Left=23.82 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 558/3393 [04:02<22:40,  2.08batch/s, Batch Loss=0.1157, Avg Loss=0.0794, Time Left=23.82 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 558/3393 [04:02<22:40,  2.08batch/s, Batch Loss=0.1980, Avg Loss=0.0796, Time Left=23.80 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  16%|▏| 559/3393 [04:02<22:03,  2.14batch/s, Batch Loss=0.1980, Avg Loss=0.0796, Time Left=23.80 \u001b[A\n",
      "Epoch 3/3 - Training:  16%|▏| 559/3393 [04:03<22:03,  2.14batch/s, Batch Loss=0.0356, Avg Loss=0.0795, Time Left=23.80 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 560/3393 [04:03<23:11,  2.04batch/s, Batch Loss=0.0356, Avg Loss=0.0795, Time Left=23.80 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 560/3393 [04:03<23:11,  2.04batch/s, Batch Loss=0.0663, Avg Loss=0.0795, Time Left=23.79 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 561/3393 [04:03<23:21,  2.02batch/s, Batch Loss=0.0663, Avg Loss=0.0795, Time Left=23.79 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 561/3393 [04:04<23:21,  2.02batch/s, Batch Loss=0.0748, Avg Loss=0.0795, Time Left=23.78 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 562/3393 [04:04<23:06,  2.04batch/s, Batch Loss=0.0748, Avg Loss=0.0795, Time Left=23.78 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 562/3393 [04:04<23:06,  2.04batch/s, Batch Loss=0.0816, Avg Loss=0.0795, Time Left=23.78 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 563/3393 [04:04<23:34,  2.00batch/s, Batch Loss=0.0816, Avg Loss=0.0795, Time Left=23.78 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 563/3393 [04:05<23:34,  2.00batch/s, Batch Loss=0.0979, Avg Loss=0.0795, Time Left=23.78 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 564/3393 [04:05<25:30,  1.85batch/s, Batch Loss=0.0979, Avg Loss=0.0795, Time Left=23.78 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 564/3393 [04:06<25:30,  1.85batch/s, Batch Loss=0.0557, Avg Loss=0.0795, Time Left=23.78 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 565/3393 [04:06<25:47,  1.83batch/s, Batch Loss=0.0557, Avg Loss=0.0795, Time Left=23.78 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 565/3393 [04:06<25:47,  1.83batch/s, Batch Loss=0.0783, Avg Loss=0.0795, Time Left=23.77 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 566/3393 [04:06<24:38,  1.91batch/s, Batch Loss=0.0783, Avg Loss=0.0795, Time Left=23.77 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 566/3393 [04:06<24:38,  1.91batch/s, Batch Loss=0.0585, Avg Loss=0.0794, Time Left=23.76 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 567/3393 [04:07<24:07,  1.95batch/s, Batch Loss=0.0585, Avg Loss=0.0794, Time Left=23.76 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 567/3393 [04:07<24:07,  1.95batch/s, Batch Loss=0.1041, Avg Loss=0.0795, Time Left=23.75 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 568/3393 [04:07<23:30,  2.00batch/s, Batch Loss=0.1041, Avg Loss=0.0795, Time Left=23.75 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 568/3393 [04:07<23:30,  2.00batch/s, Batch Loss=0.0613, Avg Loss=0.0794, Time Left=23.75 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 569/3393 [04:08<23:56,  1.97batch/s, Batch Loss=0.0613, Avg Loss=0.0794, Time Left=23.75 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 569/3393 [04:08<23:56,  1.97batch/s, Batch Loss=0.2457, Avg Loss=0.0798, Time Left=23.74 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 570/3393 [04:08<23:59,  1.96batch/s, Batch Loss=0.2457, Avg Loss=0.0798, Time Left=23.74 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 570/3393 [04:08<23:59,  1.96batch/s, Batch Loss=0.0433, Avg Loss=0.0797, Time Left=23.73 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 571/3393 [04:08<23:19,  2.02batch/s, Batch Loss=0.0433, Avg Loss=0.0797, Time Left=23.73 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 571/3393 [04:09<23:19,  2.02batch/s, Batch Loss=0.0720, Avg Loss=0.0797, Time Left=23.73 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 572/3393 [04:09<24:05,  1.95batch/s, Batch Loss=0.0720, Avg Loss=0.0797, Time Left=23.73 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 572/3393 [04:10<24:05,  1.95batch/s, Batch Loss=0.0229, Avg Loss=0.0796, Time Left=23.72 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 573/3393 [04:10<23:50,  1.97batch/s, Batch Loss=0.0229, Avg Loss=0.0796, Time Left=23.72 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 573/3393 [04:10<23:50,  1.97batch/s, Batch Loss=0.0559, Avg Loss=0.0795, Time Left=23.71 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 574/3393 [04:10<23:56,  1.96batch/s, Batch Loss=0.0559, Avg Loss=0.0795, Time Left=23.71 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 574/3393 [04:10<23:56,  1.96batch/s, Batch Loss=0.2830, Avg Loss=0.0799, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 575/3393 [04:10<23:15,  2.02batch/s, Batch Loss=0.2830, Avg Loss=0.0799, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 575/3393 [04:11<23:15,  2.02batch/s, Batch Loss=0.0451, Avg Loss=0.0799, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 576/3393 [04:11<23:09,  2.03batch/s, Batch Loss=0.0451, Avg Loss=0.0799, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 576/3393 [04:12<23:09,  2.03batch/s, Batch Loss=0.0800, Avg Loss=0.0799, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 577/3393 [04:12<24:13,  1.94batch/s, Batch Loss=0.0800, Avg Loss=0.0799, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 577/3393 [04:12<24:13,  1.94batch/s, Batch Loss=0.0210, Avg Loss=0.0797, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 578/3393 [04:12<24:03,  1.95batch/s, Batch Loss=0.0210, Avg Loss=0.0797, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 578/3393 [04:13<24:03,  1.95batch/s, Batch Loss=0.1193, Avg Loss=0.0798, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 579/3393 [04:13<23:48,  1.97batch/s, Batch Loss=0.1193, Avg Loss=0.0798, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 579/3393 [04:13<23:48,  1.97batch/s, Batch Loss=0.0039, Avg Loss=0.0797, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 580/3393 [04:13<24:00,  1.95batch/s, Batch Loss=0.0039, Avg Loss=0.0797, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 580/3393 [04:14<24:00,  1.95batch/s, Batch Loss=0.0586, Avg Loss=0.0796, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 581/3393 [04:14<24:10,  1.94batch/s, Batch Loss=0.0586, Avg Loss=0.0796, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 581/3393 [04:14<24:10,  1.94batch/s, Batch Loss=0.0995, Avg Loss=0.0797, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 582/3393 [04:14<25:06,  1.87batch/s, Batch Loss=0.0995, Avg Loss=0.0797, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 582/3393 [04:15<25:06,  1.87batch/s, Batch Loss=0.0438, Avg Loss=0.0796, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 583/3393 [04:15<25:19,  1.85batch/s, Batch Loss=0.0438, Avg Loss=0.0796, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 583/3393 [04:15<25:19,  1.85batch/s, Batch Loss=0.0012, Avg Loss=0.0794, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 584/3393 [04:15<25:54,  1.81batch/s, Batch Loss=0.0012, Avg Loss=0.0794, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 584/3393 [04:16<25:54,  1.81batch/s, Batch Loss=0.0058, Avg Loss=0.0793, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 585/3393 [04:16<25:43,  1.82batch/s, Batch Loss=0.0058, Avg Loss=0.0793, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 585/3393 [04:16<25:43,  1.82batch/s, Batch Loss=0.1822, Avg Loss=0.0795, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 586/3393 [04:16<25:42,  1.82batch/s, Batch Loss=0.1822, Avg Loss=0.0795, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 586/3393 [04:17<25:42,  1.82batch/s, Batch Loss=0.0229, Avg Loss=0.0794, Time Left=23.65 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 587/3393 [04:17<24:26,  1.91batch/s, Batch Loss=0.0229, Avg Loss=0.0794, Time Left=23.65 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 587/3393 [04:17<24:26,  1.91batch/s, Batch Loss=0.0379, Avg Loss=0.0793, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 588/3393 [04:17<23:44,  1.97batch/s, Batch Loss=0.0379, Avg Loss=0.0793, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 588/3393 [04:18<23:44,  1.97batch/s, Batch Loss=0.0072, Avg Loss=0.0792, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 589/3393 [04:18<23:29,  1.99batch/s, Batch Loss=0.0072, Avg Loss=0.0792, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 589/3393 [04:18<23:29,  1.99batch/s, Batch Loss=0.0251, Avg Loss=0.0791, Time Left=23.62 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 590/3393 [04:18<22:47,  2.05batch/s, Batch Loss=0.0251, Avg Loss=0.0791, Time Left=23.62 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 590/3393 [04:19<22:47,  2.05batch/s, Batch Loss=0.0593, Avg Loss=0.0790, Time Left=23.61 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 591/3393 [04:19<23:24,  1.99batch/s, Batch Loss=0.0593, Avg Loss=0.0790, Time Left=23.61 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 591/3393 [04:19<23:24,  1.99batch/s, Batch Loss=0.0063, Avg Loss=0.0789, Time Left=23.60 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  17%|▏| 592/3393 [04:19<23:08,  2.02batch/s, Batch Loss=0.0063, Avg Loss=0.0789, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 592/3393 [04:20<23:08,  2.02batch/s, Batch Loss=0.0288, Avg Loss=0.0788, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 593/3393 [04:20<23:44,  1.97batch/s, Batch Loss=0.0288, Avg Loss=0.0788, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  17%|▏| 593/3393 [04:20<23:44,  1.97batch/s, Batch Loss=0.0306, Avg Loss=0.0787, Time Left=23.59 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 594/3393 [04:20<24:13,  1.93batch/s, Batch Loss=0.0306, Avg Loss=0.0787, Time Left=23.59 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 594/3393 [04:21<24:13,  1.93batch/s, Batch Loss=0.2252, Avg Loss=0.0790, Time Left=23.59 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 595/3393 [04:21<25:04,  1.86batch/s, Batch Loss=0.2252, Avg Loss=0.0790, Time Left=23.59 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 595/3393 [04:21<25:04,  1.86batch/s, Batch Loss=0.0432, Avg Loss=0.0789, Time Left=23.58 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 596/3393 [04:21<24:21,  1.91batch/s, Batch Loss=0.0432, Avg Loss=0.0789, Time Left=23.58 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 596/3393 [04:22<24:21,  1.91batch/s, Batch Loss=0.0523, Avg Loss=0.0789, Time Left=23.58 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 597/3393 [04:22<24:09,  1.93batch/s, Batch Loss=0.0523, Avg Loss=0.0789, Time Left=23.58 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 597/3393 [04:22<24:09,  1.93batch/s, Batch Loss=0.0505, Avg Loss=0.0788, Time Left=23.57 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 598/3393 [04:22<23:37,  1.97batch/s, Batch Loss=0.0505, Avg Loss=0.0788, Time Left=23.57 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 598/3393 [04:23<23:37,  1.97batch/s, Batch Loss=0.0325, Avg Loss=0.0787, Time Left=23.56 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 599/3393 [04:23<23:04,  2.02batch/s, Batch Loss=0.0325, Avg Loss=0.0787, Time Left=23.56 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 599/3393 [04:23<23:04,  2.02batch/s, Batch Loss=0.1043, Avg Loss=0.0788, Time Left=23.55 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 600/3393 [04:23<22:40,  2.05batch/s, Batch Loss=0.1043, Avg Loss=0.0788, Time Left=23.55 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 600/3393 [04:24<22:40,  2.05batch/s, Batch Loss=0.0544, Avg Loss=0.0787, Time Left=23.54 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 601/3393 [04:24<22:25,  2.07batch/s, Batch Loss=0.0544, Avg Loss=0.0787, Time Left=23.54 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 601/3393 [04:24<22:25,  2.07batch/s, Batch Loss=0.2307, Avg Loss=0.0790, Time Left=23.53 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 602/3393 [04:24<22:09,  2.10batch/s, Batch Loss=0.2307, Avg Loss=0.0790, Time Left=23.53 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 602/3393 [04:25<22:09,  2.10batch/s, Batch Loss=0.0155, Avg Loss=0.0789, Time Left=23.52 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 603/3393 [04:25<23:13,  2.00batch/s, Batch Loss=0.0155, Avg Loss=0.0789, Time Left=23.52 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 603/3393 [04:25<23:13,  2.00batch/s, Batch Loss=0.3243, Avg Loss=0.0794, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 604/3393 [04:25<22:56,  2.03batch/s, Batch Loss=0.3243, Avg Loss=0.0794, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 604/3393 [04:26<22:56,  2.03batch/s, Batch Loss=0.0108, Avg Loss=0.0792, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 605/3393 [04:26<23:04,  2.01batch/s, Batch Loss=0.0108, Avg Loss=0.0792, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 605/3393 [04:26<23:04,  2.01batch/s, Batch Loss=0.0065, Avg Loss=0.0791, Time Left=23.50 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 606/3393 [04:26<23:03,  2.01batch/s, Batch Loss=0.0065, Avg Loss=0.0791, Time Left=23.50 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 606/3393 [04:27<23:03,  2.01batch/s, Batch Loss=0.0193, Avg Loss=0.0790, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 607/3393 [04:27<23:57,  1.94batch/s, Batch Loss=0.0193, Avg Loss=0.0790, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 607/3393 [04:28<23:57,  1.94batch/s, Batch Loss=0.2010, Avg Loss=0.0792, Time Left=23.50 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 608/3393 [04:28<25:25,  1.83batch/s, Batch Loss=0.2010, Avg Loss=0.0792, Time Left=23.50 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 608/3393 [04:28<25:25,  1.83batch/s, Batch Loss=0.0977, Avg Loss=0.0792, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 609/3393 [04:28<25:27,  1.82batch/s, Batch Loss=0.0977, Avg Loss=0.0792, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 609/3393 [04:29<25:27,  1.82batch/s, Batch Loss=0.0082, Avg Loss=0.0791, Time Left=23.48 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 610/3393 [04:29<24:20,  1.91batch/s, Batch Loss=0.0082, Avg Loss=0.0791, Time Left=23.48 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 610/3393 [04:29<24:20,  1.91batch/s, Batch Loss=0.1189, Avg Loss=0.0792, Time Left=23.47 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 611/3393 [04:29<23:43,  1.95batch/s, Batch Loss=0.1189, Avg Loss=0.0792, Time Left=23.47 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 611/3393 [04:30<23:43,  1.95batch/s, Batch Loss=0.0833, Avg Loss=0.0792, Time Left=23.47 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 612/3393 [04:30<23:41,  1.96batch/s, Batch Loss=0.0833, Avg Loss=0.0792, Time Left=23.47 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 612/3393 [04:30<23:41,  1.96batch/s, Batch Loss=0.2184, Avg Loss=0.0794, Time Left=23.46 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 613/3393 [04:30<23:05,  2.01batch/s, Batch Loss=0.2184, Avg Loss=0.0794, Time Left=23.46 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 613/3393 [04:30<23:05,  2.01batch/s, Batch Loss=0.0602, Avg Loss=0.0794, Time Left=23.45 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 614/3393 [04:30<22:50,  2.03batch/s, Batch Loss=0.0602, Avg Loss=0.0794, Time Left=23.45 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 614/3393 [04:31<22:50,  2.03batch/s, Batch Loss=0.0086, Avg Loss=0.0793, Time Left=23.44 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 615/3393 [04:31<22:29,  2.06batch/s, Batch Loss=0.0086, Avg Loss=0.0793, Time Left=23.44 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 615/3393 [04:31<22:29,  2.06batch/s, Batch Loss=0.0044, Avg Loss=0.0791, Time Left=23.43 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 616/3393 [04:31<22:16,  2.08batch/s, Batch Loss=0.0044, Avg Loss=0.0791, Time Left=23.43 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 616/3393 [04:32<22:16,  2.08batch/s, Batch Loss=0.0120, Avg Loss=0.0790, Time Left=23.42 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 617/3393 [04:32<21:52,  2.12batch/s, Batch Loss=0.0120, Avg Loss=0.0790, Time Left=23.42 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 617/3393 [04:32<21:52,  2.12batch/s, Batch Loss=0.0825, Avg Loss=0.0790, Time Left=23.41 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 618/3393 [04:32<22:40,  2.04batch/s, Batch Loss=0.0825, Avg Loss=0.0790, Time Left=23.41 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 618/3393 [04:33<22:40,  2.04batch/s, Batch Loss=0.0408, Avg Loss=0.0790, Time Left=23.40 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 619/3393 [04:33<22:22,  2.07batch/s, Batch Loss=0.0408, Avg Loss=0.0790, Time Left=23.40 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 619/3393 [04:33<22:22,  2.07batch/s, Batch Loss=0.0152, Avg Loss=0.0788, Time Left=23.39 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 620/3393 [04:33<22:09,  2.09batch/s, Batch Loss=0.0152, Avg Loss=0.0788, Time Left=23.39 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 620/3393 [04:34<22:09,  2.09batch/s, Batch Loss=0.0142, Avg Loss=0.0787, Time Left=23.38 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 621/3393 [04:34<22:12,  2.08batch/s, Batch Loss=0.0142, Avg Loss=0.0787, Time Left=23.38 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 621/3393 [04:34<22:12,  2.08batch/s, Batch Loss=0.0062, Avg Loss=0.0786, Time Left=23.37 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 622/3393 [04:34<22:02,  2.10batch/s, Batch Loss=0.0062, Avg Loss=0.0786, Time Left=23.37 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 622/3393 [04:35<22:02,  2.10batch/s, Batch Loss=0.5971, Avg Loss=0.0795, Time Left=23.36 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 623/3393 [04:35<22:07,  2.09batch/s, Batch Loss=0.5971, Avg Loss=0.0795, Time Left=23.36 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 623/3393 [04:35<22:07,  2.09batch/s, Batch Loss=0.0044, Avg Loss=0.0794, Time Left=23.35 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 624/3393 [04:35<22:05,  2.09batch/s, Batch Loss=0.0044, Avg Loss=0.0794, Time Left=23.35 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 624/3393 [04:36<22:05,  2.09batch/s, Batch Loss=0.0047, Avg Loss=0.0793, Time Left=23.34 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  18%|▏| 625/3393 [04:36<21:54,  2.11batch/s, Batch Loss=0.0047, Avg Loss=0.0793, Time Left=23.34 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 625/3393 [04:36<21:54,  2.11batch/s, Batch Loss=0.2143, Avg Loss=0.0795, Time Left=23.33 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 626/3393 [04:36<21:48,  2.11batch/s, Batch Loss=0.2143, Avg Loss=0.0795, Time Left=23.33 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 626/3393 [04:37<21:48,  2.11batch/s, Batch Loss=0.0014, Avg Loss=0.0794, Time Left=23.32 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 627/3393 [04:37<22:10,  2.08batch/s, Batch Loss=0.0014, Avg Loss=0.0794, Time Left=23.32 \u001b[A\n",
      "Epoch 3/3 - Training:  18%|▏| 627/3393 [04:37<22:10,  2.08batch/s, Batch Loss=0.0250, Avg Loss=0.0793, Time Left=23.31 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 628/3393 [04:37<21:59,  2.09batch/s, Batch Loss=0.0250, Avg Loss=0.0793, Time Left=23.31 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 628/3393 [04:38<21:59,  2.09batch/s, Batch Loss=0.0341, Avg Loss=0.0792, Time Left=23.30 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 629/3393 [04:38<22:05,  2.09batch/s, Batch Loss=0.0341, Avg Loss=0.0792, Time Left=23.30 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 629/3393 [04:38<22:05,  2.09batch/s, Batch Loss=0.0492, Avg Loss=0.0791, Time Left=23.29 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 630/3393 [04:38<21:55,  2.10batch/s, Batch Loss=0.0492, Avg Loss=0.0791, Time Left=23.29 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 630/3393 [04:39<21:55,  2.10batch/s, Batch Loss=0.0087, Avg Loss=0.0790, Time Left=23.28 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 631/3393 [04:39<21:49,  2.11batch/s, Batch Loss=0.0087, Avg Loss=0.0790, Time Left=23.28 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 631/3393 [04:39<21:49,  2.11batch/s, Batch Loss=0.0030, Avg Loss=0.0789, Time Left=23.27 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 632/3393 [04:39<21:44,  2.12batch/s, Batch Loss=0.0030, Avg Loss=0.0789, Time Left=23.27 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 632/3393 [04:40<21:44,  2.12batch/s, Batch Loss=0.0117, Avg Loss=0.0788, Time Left=23.27 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 633/3393 [04:40<22:32,  2.04batch/s, Batch Loss=0.0117, Avg Loss=0.0788, Time Left=23.27 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 633/3393 [04:40<22:32,  2.04batch/s, Batch Loss=0.0100, Avg Loss=0.0786, Time Left=23.26 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 634/3393 [04:40<22:14,  2.07batch/s, Batch Loss=0.0100, Avg Loss=0.0786, Time Left=23.26 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 634/3393 [04:41<22:14,  2.07batch/s, Batch Loss=0.0290, Avg Loss=0.0785, Time Left=23.25 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 635/3393 [04:41<22:40,  2.03batch/s, Batch Loss=0.0290, Avg Loss=0.0785, Time Left=23.25 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 635/3393 [04:41<22:40,  2.03batch/s, Batch Loss=0.1417, Avg Loss=0.0787, Time Left=23.24 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 636/3393 [04:41<21:53,  2.10batch/s, Batch Loss=0.1417, Avg Loss=0.0787, Time Left=23.24 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 636/3393 [04:41<21:53,  2.10batch/s, Batch Loss=0.0590, Avg Loss=0.0786, Time Left=23.23 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 637/3393 [04:41<21:59,  2.09batch/s, Batch Loss=0.0590, Avg Loss=0.0786, Time Left=23.23 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 637/3393 [04:42<21:59,  2.09batch/s, Batch Loss=0.0121, Avg Loss=0.0785, Time Left=23.23 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 638/3393 [04:42<23:11,  1.98batch/s, Batch Loss=0.0121, Avg Loss=0.0785, Time Left=23.23 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 638/3393 [04:43<23:11,  1.98batch/s, Batch Loss=0.0097, Avg Loss=0.0784, Time Left=23.22 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 639/3393 [04:43<22:53,  2.00batch/s, Batch Loss=0.0097, Avg Loss=0.0784, Time Left=23.22 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 639/3393 [04:43<22:53,  2.00batch/s, Batch Loss=0.1975, Avg Loss=0.0786, Time Left=23.21 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 640/3393 [04:43<23:07,  1.98batch/s, Batch Loss=0.1975, Avg Loss=0.0786, Time Left=23.21 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 640/3393 [04:44<23:07,  1.98batch/s, Batch Loss=0.4243, Avg Loss=0.0792, Time Left=23.20 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 641/3393 [04:44<23:16,  1.97batch/s, Batch Loss=0.4243, Avg Loss=0.0792, Time Left=23.20 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 641/3393 [04:44<23:16,  1.97batch/s, Batch Loss=0.0083, Avg Loss=0.0791, Time Left=23.22 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 642/3393 [04:44<26:47,  1.71batch/s, Batch Loss=0.0083, Avg Loss=0.0791, Time Left=23.22 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 642/3393 [04:45<26:47,  1.71batch/s, Batch Loss=0.0930, Avg Loss=0.0791, Time Left=23.22 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 643/3393 [04:45<26:44,  1.71batch/s, Batch Loss=0.0930, Avg Loss=0.0791, Time Left=23.22 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 643/3393 [04:45<26:44,  1.71batch/s, Batch Loss=0.1162, Avg Loss=0.0792, Time Left=23.21 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 644/3393 [04:45<25:24,  1.80batch/s, Batch Loss=0.1162, Avg Loss=0.0792, Time Left=23.21 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 644/3393 [04:46<25:24,  1.80batch/s, Batch Loss=0.0125, Avg Loss=0.0790, Time Left=23.20 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 645/3393 [04:46<24:35,  1.86batch/s, Batch Loss=0.0125, Avg Loss=0.0790, Time Left=23.20 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 645/3393 [04:46<24:35,  1.86batch/s, Batch Loss=0.1073, Avg Loss=0.0791, Time Left=23.19 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 646/3393 [04:46<23:13,  1.97batch/s, Batch Loss=0.1073, Avg Loss=0.0791, Time Left=23.19 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 646/3393 [04:47<23:13,  1.97batch/s, Batch Loss=0.0643, Avg Loss=0.0791, Time Left=23.18 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 647/3393 [04:47<22:45,  2.01batch/s, Batch Loss=0.0643, Avg Loss=0.0791, Time Left=23.18 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 647/3393 [04:47<22:45,  2.01batch/s, Batch Loss=0.0085, Avg Loss=0.0789, Time Left=23.17 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 648/3393 [04:47<23:40,  1.93batch/s, Batch Loss=0.0085, Avg Loss=0.0789, Time Left=23.17 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 648/3393 [04:48<23:40,  1.93batch/s, Batch Loss=0.1016, Avg Loss=0.0790, Time Left=23.16 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 649/3393 [04:48<23:07,  1.98batch/s, Batch Loss=0.1016, Avg Loss=0.0790, Time Left=23.16 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 649/3393 [04:48<23:07,  1.98batch/s, Batch Loss=0.0559, Avg Loss=0.0789, Time Left=23.16 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 650/3393 [04:48<23:35,  1.94batch/s, Batch Loss=0.0559, Avg Loss=0.0789, Time Left=23.16 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 650/3393 [04:49<23:35,  1.94batch/s, Batch Loss=0.1883, Avg Loss=0.0791, Time Left=23.15 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 651/3393 [04:49<23:08,  1.98batch/s, Batch Loss=0.1883, Avg Loss=0.0791, Time Left=23.15 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 651/3393 [04:49<23:08,  1.98batch/s, Batch Loss=0.0374, Avg Loss=0.0791, Time Left=23.15 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 652/3393 [04:49<23:46,  1.92batch/s, Batch Loss=0.0374, Avg Loss=0.0791, Time Left=23.15 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 652/3393 [04:50<23:46,  1.92batch/s, Batch Loss=0.0923, Avg Loss=0.0791, Time Left=23.14 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 653/3393 [04:50<23:37,  1.93batch/s, Batch Loss=0.0923, Avg Loss=0.0791, Time Left=23.14 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 653/3393 [04:50<23:37,  1.93batch/s, Batch Loss=0.1721, Avg Loss=0.0792, Time Left=23.13 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 654/3393 [04:50<23:23,  1.95batch/s, Batch Loss=0.1721, Avg Loss=0.0792, Time Left=23.13 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 654/3393 [04:51<23:23,  1.95batch/s, Batch Loss=0.0466, Avg Loss=0.0792, Time Left=23.12 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 655/3393 [04:51<22:42,  2.01batch/s, Batch Loss=0.0466, Avg Loss=0.0792, Time Left=23.12 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 655/3393 [04:51<22:42,  2.01batch/s, Batch Loss=0.0013, Avg Loss=0.0791, Time Left=23.11 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 656/3393 [04:51<22:35,  2.02batch/s, Batch Loss=0.0013, Avg Loss=0.0791, Time Left=23.11 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 656/3393 [04:52<22:35,  2.02batch/s, Batch Loss=0.0150, Avg Loss=0.0789, Time Left=23.10 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 657/3393 [04:52<22:20,  2.04batch/s, Batch Loss=0.0150, Avg Loss=0.0789, Time Left=23.10 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 657/3393 [04:52<22:20,  2.04batch/s, Batch Loss=0.0120, Avg Loss=0.0788, Time Left=23.10 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  19%|▏| 658/3393 [04:52<22:43,  2.01batch/s, Batch Loss=0.0120, Avg Loss=0.0788, Time Left=23.10 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 658/3393 [04:53<22:43,  2.01batch/s, Batch Loss=0.2100, Avg Loss=0.0791, Time Left=23.09 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 659/3393 [04:53<22:49,  2.00batch/s, Batch Loss=0.2100, Avg Loss=0.0791, Time Left=23.09 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 659/3393 [04:53<22:49,  2.00batch/s, Batch Loss=0.0061, Avg Loss=0.0789, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 660/3393 [04:53<23:13,  1.96batch/s, Batch Loss=0.0061, Avg Loss=0.0789, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 660/3393 [04:54<23:13,  1.96batch/s, Batch Loss=0.1134, Avg Loss=0.0790, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 661/3393 [04:54<23:03,  1.97batch/s, Batch Loss=0.1134, Avg Loss=0.0790, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  19%|▏| 661/3393 [04:55<23:03,  1.97batch/s, Batch Loss=0.1210, Avg Loss=0.0791, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 662/3393 [04:55<24:36,  1.85batch/s, Batch Loss=0.1210, Avg Loss=0.0791, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 662/3393 [04:55<24:36,  1.85batch/s, Batch Loss=0.0131, Avg Loss=0.0790, Time Left=23.07 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 663/3393 [04:55<24:19,  1.87batch/s, Batch Loss=0.0131, Avg Loss=0.0790, Time Left=23.07 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 663/3393 [04:56<24:19,  1.87batch/s, Batch Loss=0.0086, Avg Loss=0.0788, Time Left=23.07 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 664/3393 [04:56<24:13,  1.88batch/s, Batch Loss=0.0086, Avg Loss=0.0788, Time Left=23.07 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 664/3393 [04:56<24:13,  1.88batch/s, Batch Loss=0.0095, Avg Loss=0.0787, Time Left=23.06 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 665/3393 [04:56<24:46,  1.84batch/s, Batch Loss=0.0095, Avg Loss=0.0787, Time Left=23.06 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 665/3393 [04:57<24:46,  1.84batch/s, Batch Loss=0.1843, Avg Loss=0.0789, Time Left=23.06 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 666/3393 [04:57<24:05,  1.89batch/s, Batch Loss=0.1843, Avg Loss=0.0789, Time Left=23.06 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 666/3393 [04:57<24:05,  1.89batch/s, Batch Loss=0.0099, Avg Loss=0.0788, Time Left=23.05 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 667/3393 [04:57<24:34,  1.85batch/s, Batch Loss=0.0099, Avg Loss=0.0788, Time Left=23.05 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 667/3393 [04:58<24:34,  1.85batch/s, Batch Loss=0.0537, Avg Loss=0.0787, Time Left=23.05 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 668/3393 [04:58<25:24,  1.79batch/s, Batch Loss=0.0537, Avg Loss=0.0787, Time Left=23.05 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 668/3393 [04:58<25:24,  1.79batch/s, Batch Loss=0.0314, Avg Loss=0.0787, Time Left=23.05 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 669/3393 [04:58<24:47,  1.83batch/s, Batch Loss=0.0314, Avg Loss=0.0787, Time Left=23.05 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 669/3393 [04:59<24:47,  1.83batch/s, Batch Loss=0.1478, Avg Loss=0.0788, Time Left=23.04 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 670/3393 [04:59<24:21,  1.86batch/s, Batch Loss=0.1478, Avg Loss=0.0788, Time Left=23.04 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 670/3393 [04:59<24:21,  1.86batch/s, Batch Loss=0.0589, Avg Loss=0.0787, Time Left=23.04 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 671/3393 [04:59<24:51,  1.83batch/s, Batch Loss=0.0589, Avg Loss=0.0787, Time Left=23.04 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 671/3393 [05:00<24:51,  1.83batch/s, Batch Loss=0.0017, Avg Loss=0.0786, Time Left=23.03 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 672/3393 [05:00<24:21,  1.86batch/s, Batch Loss=0.0017, Avg Loss=0.0786, Time Left=23.03 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 672/3393 [05:00<24:21,  1.86batch/s, Batch Loss=0.0212, Avg Loss=0.0785, Time Left=23.02 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 673/3393 [05:00<24:21,  1.86batch/s, Batch Loss=0.0212, Avg Loss=0.0785, Time Left=23.02 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 673/3393 [05:01<24:21,  1.86batch/s, Batch Loss=0.0634, Avg Loss=0.0785, Time Left=23.02 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 674/3393 [05:01<23:41,  1.91batch/s, Batch Loss=0.0634, Avg Loss=0.0785, Time Left=23.02 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 674/3393 [05:02<23:41,  1.91batch/s, Batch Loss=0.0043, Avg Loss=0.0784, Time Left=23.01 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 675/3393 [05:02<23:47,  1.90batch/s, Batch Loss=0.0043, Avg Loss=0.0784, Time Left=23.01 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 675/3393 [05:02<23:47,  1.90batch/s, Batch Loss=0.0422, Avg Loss=0.0783, Time Left=23.01 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 676/3393 [05:02<24:18,  1.86batch/s, Batch Loss=0.0422, Avg Loss=0.0783, Time Left=23.01 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 676/3393 [05:03<24:18,  1.86batch/s, Batch Loss=0.0605, Avg Loss=0.0783, Time Left=23.01 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 677/3393 [05:03<25:19,  1.79batch/s, Batch Loss=0.0605, Avg Loss=0.0783, Time Left=23.01 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 677/3393 [05:03<25:19,  1.79batch/s, Batch Loss=0.0812, Avg Loss=0.0783, Time Left=23.00 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 678/3393 [05:03<24:21,  1.86batch/s, Batch Loss=0.0812, Avg Loss=0.0783, Time Left=23.00 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 678/3393 [05:04<24:21,  1.86batch/s, Batch Loss=0.4082, Avg Loss=0.0788, Time Left=23.00 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 679/3393 [05:04<25:17,  1.79batch/s, Batch Loss=0.4082, Avg Loss=0.0788, Time Left=23.00 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 679/3393 [05:04<25:17,  1.79batch/s, Batch Loss=0.1740, Avg Loss=0.0790, Time Left=23.00 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 680/3393 [05:04<25:55,  1.74batch/s, Batch Loss=0.1740, Avg Loss=0.0790, Time Left=23.00 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 680/3393 [05:05<25:55,  1.74batch/s, Batch Loss=0.0040, Avg Loss=0.0789, Time Left=22.99 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 681/3393 [05:05<24:42,  1.83batch/s, Batch Loss=0.0040, Avg Loss=0.0789, Time Left=22.99 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 681/3393 [05:05<24:42,  1.83batch/s, Batch Loss=0.0015, Avg Loss=0.0787, Time Left=22.98 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 682/3393 [05:05<24:00,  1.88batch/s, Batch Loss=0.0015, Avg Loss=0.0787, Time Left=22.98 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 682/3393 [05:06<24:00,  1.88batch/s, Batch Loss=0.0057, Avg Loss=0.0786, Time Left=22.98 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 683/3393 [05:06<24:41,  1.83batch/s, Batch Loss=0.0057, Avg Loss=0.0786, Time Left=22.98 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 683/3393 [05:07<24:41,  1.83batch/s, Batch Loss=0.0773, Avg Loss=0.0786, Time Left=22.98 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 684/3393 [05:07<24:48,  1.82batch/s, Batch Loss=0.0773, Avg Loss=0.0786, Time Left=22.98 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 684/3393 [05:07<24:48,  1.82batch/s, Batch Loss=0.0193, Avg Loss=0.0785, Time Left=22.99 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 685/3393 [05:07<27:52,  1.62batch/s, Batch Loss=0.0193, Avg Loss=0.0785, Time Left=22.99 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 685/3393 [05:08<27:52,  1.62batch/s, Batch Loss=0.0037, Avg Loss=0.0784, Time Left=23.02 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 686/3393 [05:08<32:37,  1.38batch/s, Batch Loss=0.0037, Avg Loss=0.0784, Time Left=23.02 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 686/3393 [05:09<32:37,  1.38batch/s, Batch Loss=0.0968, Avg Loss=0.0784, Time Left=23.03 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 687/3393 [05:09<34:05,  1.32batch/s, Batch Loss=0.0968, Avg Loss=0.0784, Time Left=23.03 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 687/3393 [05:11<34:05,  1.32batch/s, Batch Loss=0.0072, Avg Loss=0.0783, Time Left=23.13 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 688/3393 [05:11<49:09,  1.09s/batch, Batch Loss=0.0072, Avg Loss=0.0783, Time Left=23.13 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 688/3393 [05:13<49:09,  1.09s/batch, Batch Loss=0.0150, Avg Loss=0.0782, Time Left=23.21 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 689/3393 [05:13<58:50,  1.31s/batch, Batch Loss=0.0150, Avg Loss=0.0782, Time Left=23.21 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 689/3393 [05:14<58:50,  1.31s/batch, Batch Loss=0.1485, Avg Loss=0.0783, Time Left=23.30 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 690/3393 [05:15<1:04:31,  1.43s/batch, Batch Loss=0.1485, Avg Loss=0.0783, Time Left=23.3\u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 690/3393 [05:19<1:04:31,  1.43s/batch, Batch Loss=0.0078, Avg Loss=0.0782, Time Left=23.5\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  20%|▏| 691/3393 [05:19<1:40:24,  2.23s/batch, Batch Loss=0.0078, Avg Loss=0.0782, Time Left=23.5\u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 691/3393 [05:19<1:40:24,  2.23s/batch, Batch Loss=0.0123, Avg Loss=0.0781, Time Left=23.5\u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 692/3393 [05:19<1:20:26,  1.79s/batch, Batch Loss=0.0123, Avg Loss=0.0781, Time Left=23.5\u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 692/3393 [05:20<1:20:26,  1.79s/batch, Batch Loss=0.0634, Avg Loss=0.0781, Time Left=23.5\u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 693/3393 [05:20<1:02:32,  1.39s/batch, Batch Loss=0.0634, Avg Loss=0.0781, Time Left=23.5\u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 693/3393 [05:20<1:02:32,  1.39s/batch, Batch Loss=0.0137, Avg Loss=0.0780, Time Left=23.5\u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 694/3393 [05:20<50:20,  1.12s/batch, Batch Loss=0.0137, Avg Loss=0.0780, Time Left=23.54 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 694/3393 [05:21<50:20,  1.12s/batch, Batch Loss=0.1121, Avg Loss=0.0780, Time Left=23.53 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 695/3393 [05:21<41:45,  1.08batch/s, Batch Loss=0.1121, Avg Loss=0.0780, Time Left=23.53 \u001b[A\n",
      "Epoch 3/3 - Training:  20%|▏| 695/3393 [05:21<41:45,  1.08batch/s, Batch Loss=0.0280, Avg Loss=0.0780, Time Left=23.52 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 696/3393 [05:21<36:11,  1.24batch/s, Batch Loss=0.0280, Avg Loss=0.0780, Time Left=23.52 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 696/3393 [05:22<36:11,  1.24batch/s, Batch Loss=0.0075, Avg Loss=0.0778, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 697/3393 [05:22<32:20,  1.39batch/s, Batch Loss=0.0075, Avg Loss=0.0778, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 697/3393 [05:22<32:20,  1.39batch/s, Batch Loss=0.0034, Avg Loss=0.0777, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 698/3393 [05:22<30:33,  1.47batch/s, Batch Loss=0.0034, Avg Loss=0.0777, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 698/3393 [05:23<30:33,  1.47batch/s, Batch Loss=0.0314, Avg Loss=0.0777, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 699/3393 [05:23<28:51,  1.56batch/s, Batch Loss=0.0314, Avg Loss=0.0777, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 699/3393 [05:23<28:51,  1.56batch/s, Batch Loss=0.0166, Avg Loss=0.0776, Time Left=23.50 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 700/3393 [05:23<26:44,  1.68batch/s, Batch Loss=0.0166, Avg Loss=0.0776, Time Left=23.50 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 700/3393 [05:24<26:44,  1.68batch/s, Batch Loss=0.0438, Avg Loss=0.0775, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 701/3393 [05:24<26:19,  1.70batch/s, Batch Loss=0.0438, Avg Loss=0.0775, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 701/3393 [05:25<26:19,  1.70batch/s, Batch Loss=0.0106, Avg Loss=0.0774, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 702/3393 [05:25<26:41,  1.68batch/s, Batch Loss=0.0106, Avg Loss=0.0774, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 702/3393 [05:25<26:41,  1.68batch/s, Batch Loss=0.1118, Avg Loss=0.0774, Time Left=23.48 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 703/3393 [05:25<25:43,  1.74batch/s, Batch Loss=0.1118, Avg Loss=0.0774, Time Left=23.48 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 703/3393 [05:26<25:43,  1.74batch/s, Batch Loss=0.1941, Avg Loss=0.0776, Time Left=23.48 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 704/3393 [05:26<26:35,  1.68batch/s, Batch Loss=0.1941, Avg Loss=0.0776, Time Left=23.48 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 704/3393 [05:26<26:35,  1.68batch/s, Batch Loss=0.1528, Avg Loss=0.0778, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 705/3393 [05:26<27:48,  1.61batch/s, Batch Loss=0.1528, Avg Loss=0.0778, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 705/3393 [05:27<27:48,  1.61batch/s, Batch Loss=0.0305, Avg Loss=0.0777, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 706/3393 [05:27<29:20,  1.53batch/s, Batch Loss=0.0305, Avg Loss=0.0777, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 706/3393 [05:28<29:20,  1.53batch/s, Batch Loss=0.0087, Avg Loss=0.0776, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 707/3393 [05:28<28:15,  1.58batch/s, Batch Loss=0.0087, Avg Loss=0.0776, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 707/3393 [05:28<28:15,  1.58batch/s, Batch Loss=0.0059, Avg Loss=0.0775, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 708/3393 [05:28<27:59,  1.60batch/s, Batch Loss=0.0059, Avg Loss=0.0775, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 708/3393 [05:29<27:59,  1.60batch/s, Batch Loss=0.2539, Avg Loss=0.0777, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 709/3393 [05:29<31:13,  1.43batch/s, Batch Loss=0.2539, Avg Loss=0.0777, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 709/3393 [05:31<31:13,  1.43batch/s, Batch Loss=0.0133, Avg Loss=0.0776, Time Left=23.56 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 710/3393 [05:31<40:07,  1.11batch/s, Batch Loss=0.0133, Avg Loss=0.0776, Time Left=23.56 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 710/3393 [05:31<40:07,  1.11batch/s, Batch Loss=0.0220, Avg Loss=0.0775, Time Left=23.57 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 711/3393 [05:31<39:24,  1.13batch/s, Batch Loss=0.0220, Avg Loss=0.0775, Time Left=23.57 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 711/3393 [05:32<39:24,  1.13batch/s, Batch Loss=0.0715, Avg Loss=0.0775, Time Left=23.58 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 712/3393 [05:32<38:02,  1.17batch/s, Batch Loss=0.0715, Avg Loss=0.0775, Time Left=23.58 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 712/3393 [05:33<38:02,  1.17batch/s, Batch Loss=0.1043, Avg Loss=0.0776, Time Left=23.59 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 713/3393 [05:33<36:19,  1.23batch/s, Batch Loss=0.1043, Avg Loss=0.0776, Time Left=23.59 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 713/3393 [05:34<36:19,  1.23batch/s, Batch Loss=0.0850, Avg Loss=0.0776, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 714/3393 [05:34<36:09,  1.24batch/s, Batch Loss=0.0850, Avg Loss=0.0776, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 714/3393 [05:34<36:09,  1.24batch/s, Batch Loss=0.0249, Avg Loss=0.0775, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 715/3393 [05:34<34:07,  1.31batch/s, Batch Loss=0.0249, Avg Loss=0.0775, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 715/3393 [05:35<34:07,  1.31batch/s, Batch Loss=0.1247, Avg Loss=0.0776, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 716/3393 [05:35<32:52,  1.36batch/s, Batch Loss=0.1247, Avg Loss=0.0776, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 716/3393 [05:36<32:52,  1.36batch/s, Batch Loss=0.4513, Avg Loss=0.0782, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 717/3393 [05:36<37:06,  1.20batch/s, Batch Loss=0.4513, Avg Loss=0.0782, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 717/3393 [05:37<37:06,  1.20batch/s, Batch Loss=0.0173, Avg Loss=0.0781, Time Left=23.65 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 718/3393 [05:37<38:34,  1.16batch/s, Batch Loss=0.0173, Avg Loss=0.0781, Time Left=23.65 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 718/3393 [05:38<38:34,  1.16batch/s, Batch Loss=0.2115, Avg Loss=0.0783, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 719/3393 [05:38<36:42,  1.21batch/s, Batch Loss=0.2115, Avg Loss=0.0783, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 719/3393 [05:38<36:42,  1.21batch/s, Batch Loss=0.0105, Avg Loss=0.0782, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 720/3393 [05:38<33:35,  1.33batch/s, Batch Loss=0.0105, Avg Loss=0.0782, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 720/3393 [05:39<33:35,  1.33batch/s, Batch Loss=0.0399, Avg Loss=0.0781, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 721/3393 [05:39<34:44,  1.28batch/s, Batch Loss=0.0399, Avg Loss=0.0781, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 721/3393 [05:40<34:44,  1.28batch/s, Batch Loss=0.0952, Avg Loss=0.0781, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 722/3393 [05:40<34:43,  1.28batch/s, Batch Loss=0.0952, Avg Loss=0.0781, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 722/3393 [05:41<34:43,  1.28batch/s, Batch Loss=0.6643, Avg Loss=0.0790, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 723/3393 [05:41<33:39,  1.32batch/s, Batch Loss=0.6643, Avg Loss=0.0790, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 723/3393 [05:41<33:39,  1.32batch/s, Batch Loss=0.0351, Avg Loss=0.0790, Time Left=23.69 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  21%|▏| 724/3393 [05:41<33:33,  1.33batch/s, Batch Loss=0.0351, Avg Loss=0.0790, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 724/3393 [05:42<33:33,  1.33batch/s, Batch Loss=0.0091, Avg Loss=0.0788, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 725/3393 [05:42<34:15,  1.30batch/s, Batch Loss=0.0091, Avg Loss=0.0788, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 725/3393 [05:43<34:15,  1.30batch/s, Batch Loss=0.1950, Avg Loss=0.0790, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 726/3393 [05:43<32:24,  1.37batch/s, Batch Loss=0.1950, Avg Loss=0.0790, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 726/3393 [05:44<32:24,  1.37batch/s, Batch Loss=0.0380, Avg Loss=0.0790, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 727/3393 [05:44<31:12,  1.42batch/s, Batch Loss=0.0380, Avg Loss=0.0790, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 727/3393 [05:44<31:12,  1.42batch/s, Batch Loss=0.0316, Avg Loss=0.0789, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 728/3393 [05:44<29:26,  1.51batch/s, Batch Loss=0.0316, Avg Loss=0.0789, Time Left=23.70 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 728/3393 [05:45<29:26,  1.51batch/s, Batch Loss=0.0161, Avg Loss=0.0788, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 729/3393 [05:45<27:45,  1.60batch/s, Batch Loss=0.0161, Avg Loss=0.0788, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  21%|▏| 729/3393 [05:45<27:45,  1.60batch/s, Batch Loss=0.2198, Avg Loss=0.0790, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 730/3393 [05:45<26:55,  1.65batch/s, Batch Loss=0.2198, Avg Loss=0.0790, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 730/3393 [05:46<26:55,  1.65batch/s, Batch Loss=0.0202, Avg Loss=0.0789, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 731/3393 [05:46<28:24,  1.56batch/s, Batch Loss=0.0202, Avg Loss=0.0789, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 731/3393 [05:47<28:24,  1.56batch/s, Batch Loss=0.0841, Avg Loss=0.0789, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 732/3393 [05:47<28:26,  1.56batch/s, Batch Loss=0.0841, Avg Loss=0.0789, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 732/3393 [05:47<28:26,  1.56batch/s, Batch Loss=0.0503, Avg Loss=0.0789, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 733/3393 [05:47<27:57,  1.59batch/s, Batch Loss=0.0503, Avg Loss=0.0789, Time Left=23.69 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 733/3393 [05:48<27:57,  1.59batch/s, Batch Loss=0.0749, Avg Loss=0.0789, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 734/3393 [05:48<26:55,  1.65batch/s, Batch Loss=0.0749, Avg Loss=0.0789, Time Left=23.68 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 734/3393 [05:48<26:55,  1.65batch/s, Batch Loss=0.0393, Avg Loss=0.0788, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 735/3393 [05:48<25:54,  1.71batch/s, Batch Loss=0.0393, Avg Loss=0.0788, Time Left=23.67 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 735/3393 [05:49<25:54,  1.71batch/s, Batch Loss=0.1106, Avg Loss=0.0789, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 736/3393 [05:49<24:50,  1.78batch/s, Batch Loss=0.1106, Avg Loss=0.0789, Time Left=23.66 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 736/3393 [05:49<24:50,  1.78batch/s, Batch Loss=0.0526, Avg Loss=0.0788, Time Left=23.65 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 737/3393 [05:49<23:29,  1.88batch/s, Batch Loss=0.0526, Avg Loss=0.0788, Time Left=23.65 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 737/3393 [05:50<23:29,  1.88batch/s, Batch Loss=0.0129, Avg Loss=0.0787, Time Left=23.64 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 738/3393 [05:50<24:10,  1.83batch/s, Batch Loss=0.0129, Avg Loss=0.0787, Time Left=23.64 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 738/3393 [05:51<24:10,  1.83batch/s, Batch Loss=0.1570, Avg Loss=0.0788, Time Left=23.64 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 739/3393 [05:51<25:49,  1.71batch/s, Batch Loss=0.1570, Avg Loss=0.0788, Time Left=23.64 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 739/3393 [05:51<25:49,  1.71batch/s, Batch Loss=0.0344, Avg Loss=0.0788, Time Left=23.64 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 740/3393 [05:51<25:05,  1.76batch/s, Batch Loss=0.0344, Avg Loss=0.0788, Time Left=23.64 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 740/3393 [05:52<25:05,  1.76batch/s, Batch Loss=0.0658, Avg Loss=0.0788, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 741/3393 [05:52<25:11,  1.75batch/s, Batch Loss=0.0658, Avg Loss=0.0788, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 741/3393 [05:52<25:11,  1.75batch/s, Batch Loss=0.0924, Avg Loss=0.0788, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 742/3393 [05:52<25:53,  1.71batch/s, Batch Loss=0.0924, Avg Loss=0.0788, Time Left=23.63 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 742/3393 [05:53<25:53,  1.71batch/s, Batch Loss=0.1736, Avg Loss=0.0789, Time Left=23.62 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 743/3393 [05:53<24:54,  1.77batch/s, Batch Loss=0.1736, Avg Loss=0.0789, Time Left=23.62 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 743/3393 [05:53<24:54,  1.77batch/s, Batch Loss=0.0365, Avg Loss=0.0789, Time Left=23.61 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 744/3393 [05:53<24:41,  1.79batch/s, Batch Loss=0.0365, Avg Loss=0.0789, Time Left=23.61 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 744/3393 [05:54<24:41,  1.79batch/s, Batch Loss=0.0079, Avg Loss=0.0788, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 745/3393 [05:54<23:16,  1.90batch/s, Batch Loss=0.0079, Avg Loss=0.0788, Time Left=23.60 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 745/3393 [05:54<23:16,  1.90batch/s, Batch Loss=0.3497, Avg Loss=0.0792, Time Left=23.59 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 746/3393 [05:54<22:42,  1.94batch/s, Batch Loss=0.3497, Avg Loss=0.0792, Time Left=23.59 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 746/3393 [05:55<22:42,  1.94batch/s, Batch Loss=0.0051, Avg Loss=0.0790, Time Left=23.58 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 747/3393 [05:55<22:17,  1.98batch/s, Batch Loss=0.0051, Avg Loss=0.0790, Time Left=23.58 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 747/3393 [05:55<22:17,  1.98batch/s, Batch Loss=0.1093, Avg Loss=0.0791, Time Left=23.57 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 748/3393 [05:55<21:47,  2.02batch/s, Batch Loss=0.1093, Avg Loss=0.0791, Time Left=23.57 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 748/3393 [05:56<21:47,  2.02batch/s, Batch Loss=0.1000, Avg Loss=0.0791, Time Left=23.56 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 749/3393 [05:56<22:16,  1.98batch/s, Batch Loss=0.1000, Avg Loss=0.0791, Time Left=23.56 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 749/3393 [05:56<22:16,  1.98batch/s, Batch Loss=0.1672, Avg Loss=0.0792, Time Left=23.55 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 750/3393 [05:56<21:46,  2.02batch/s, Batch Loss=0.1672, Avg Loss=0.0792, Time Left=23.55 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 750/3393 [05:57<21:46,  2.02batch/s, Batch Loss=0.1704, Avg Loss=0.0794, Time Left=23.53 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 751/3393 [05:57<21:38,  2.03batch/s, Batch Loss=0.1704, Avg Loss=0.0794, Time Left=23.53 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 751/3393 [05:57<21:38,  2.03batch/s, Batch Loss=0.0398, Avg Loss=0.0793, Time Left=23.52 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 752/3393 [05:57<21:32,  2.04batch/s, Batch Loss=0.0398, Avg Loss=0.0793, Time Left=23.52 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 752/3393 [05:58<21:32,  2.04batch/s, Batch Loss=0.1823, Avg Loss=0.0795, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 753/3393 [05:58<21:15,  2.07batch/s, Batch Loss=0.1823, Avg Loss=0.0795, Time Left=23.51 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 753/3393 [05:58<21:15,  2.07batch/s, Batch Loss=0.0048, Avg Loss=0.0794, Time Left=23.50 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 754/3393 [05:58<21:16,  2.07batch/s, Batch Loss=0.0048, Avg Loss=0.0794, Time Left=23.50 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 754/3393 [05:59<21:16,  2.07batch/s, Batch Loss=0.0886, Avg Loss=0.0794, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 755/3393 [05:59<21:28,  2.05batch/s, Batch Loss=0.0886, Avg Loss=0.0794, Time Left=23.49 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 755/3393 [05:59<21:28,  2.05batch/s, Batch Loss=0.1428, Avg Loss=0.0795, Time Left=23.48 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 756/3393 [05:59<21:24,  2.05batch/s, Batch Loss=0.1428, Avg Loss=0.0795, Time Left=23.48 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 756/3393 [06:00<21:24,  2.05batch/s, Batch Loss=0.0197, Avg Loss=0.0794, Time Left=23.47 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  22%|▏| 757/3393 [06:00<21:34,  2.04batch/s, Batch Loss=0.0197, Avg Loss=0.0794, Time Left=23.47 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 757/3393 [06:00<21:34,  2.04batch/s, Batch Loss=0.0408, Avg Loss=0.0793, Time Left=23.46 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 758/3393 [06:00<21:28,  2.04batch/s, Batch Loss=0.0408, Avg Loss=0.0793, Time Left=23.46 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 758/3393 [06:01<21:28,  2.04batch/s, Batch Loss=0.0046, Avg Loss=0.0792, Time Left=23.45 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 759/3393 [06:01<21:36,  2.03batch/s, Batch Loss=0.0046, Avg Loss=0.0792, Time Left=23.45 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 759/3393 [06:01<21:36,  2.03batch/s, Batch Loss=0.0208, Avg Loss=0.0791, Time Left=23.44 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 760/3393 [06:01<21:30,  2.04batch/s, Batch Loss=0.0208, Avg Loss=0.0791, Time Left=23.44 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 760/3393 [06:02<21:30,  2.04batch/s, Batch Loss=0.0789, Avg Loss=0.0791, Time Left=23.43 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 761/3393 [06:02<21:25,  2.05batch/s, Batch Loss=0.0789, Avg Loss=0.0791, Time Left=23.43 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 761/3393 [06:02<21:25,  2.05batch/s, Batch Loss=0.0190, Avg Loss=0.0790, Time Left=23.42 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 762/3393 [06:02<21:46,  2.01batch/s, Batch Loss=0.0190, Avg Loss=0.0790, Time Left=23.42 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 762/3393 [06:03<21:46,  2.01batch/s, Batch Loss=0.0642, Avg Loss=0.0790, Time Left=23.40 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 763/3393 [06:03<21:23,  2.05batch/s, Batch Loss=0.0642, Avg Loss=0.0790, Time Left=23.40 \u001b[A\n",
      "Epoch 3/3 - Training:  22%|▏| 763/3393 [06:03<21:23,  2.05batch/s, Batch Loss=0.0225, Avg Loss=0.0789, Time Left=23.39 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 764/3393 [06:03<21:08,  2.07batch/s, Batch Loss=0.0225, Avg Loss=0.0789, Time Left=23.39 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 764/3393 [06:04<21:08,  2.07batch/s, Batch Loss=0.0653, Avg Loss=0.0789, Time Left=23.38 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 765/3393 [06:04<21:21,  2.05batch/s, Batch Loss=0.0653, Avg Loss=0.0789, Time Left=23.38 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 765/3393 [06:04<21:21,  2.05batch/s, Batch Loss=0.0340, Avg Loss=0.0789, Time Left=23.37 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 766/3393 [06:04<21:18,  2.05batch/s, Batch Loss=0.0340, Avg Loss=0.0789, Time Left=23.37 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 766/3393 [06:04<21:18,  2.05batch/s, Batch Loss=0.0540, Avg Loss=0.0788, Time Left=23.36 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 767/3393 [06:04<21:16,  2.06batch/s, Batch Loss=0.0540, Avg Loss=0.0788, Time Left=23.36 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 767/3393 [06:05<21:16,  2.06batch/s, Batch Loss=0.1383, Avg Loss=0.0789, Time Left=23.35 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 768/3393 [06:05<21:02,  2.08batch/s, Batch Loss=0.1383, Avg Loss=0.0789, Time Left=23.35 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 768/3393 [06:05<21:02,  2.08batch/s, Batch Loss=0.0559, Avg Loss=0.0789, Time Left=23.34 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 769/3393 [06:05<20:39,  2.12batch/s, Batch Loss=0.0559, Avg Loss=0.0789, Time Left=23.34 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 769/3393 [06:06<20:39,  2.12batch/s, Batch Loss=0.0069, Avg Loss=0.0788, Time Left=23.32 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 770/3393 [06:06<20:48,  2.10batch/s, Batch Loss=0.0069, Avg Loss=0.0788, Time Left=23.32 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 770/3393 [06:06<20:48,  2.10batch/s, Batch Loss=0.0146, Avg Loss=0.0787, Time Left=23.32 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 771/3393 [06:06<21:19,  2.05batch/s, Batch Loss=0.0146, Avg Loss=0.0787, Time Left=23.32 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 771/3393 [06:07<21:19,  2.05batch/s, Batch Loss=0.1458, Avg Loss=0.0788, Time Left=23.30 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 772/3393 [06:07<21:16,  2.05batch/s, Batch Loss=0.1458, Avg Loss=0.0788, Time Left=23.30 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 772/3393 [06:07<21:16,  2.05batch/s, Batch Loss=0.1678, Avg Loss=0.0789, Time Left=23.30 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 773/3393 [06:07<21:38,  2.02batch/s, Batch Loss=0.1678, Avg Loss=0.0789, Time Left=23.30 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 773/3393 [06:08<21:38,  2.02batch/s, Batch Loss=0.0474, Avg Loss=0.0789, Time Left=23.28 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 774/3393 [06:08<21:17,  2.05batch/s, Batch Loss=0.0474, Avg Loss=0.0789, Time Left=23.28 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 774/3393 [06:08<21:17,  2.05batch/s, Batch Loss=0.0013, Avg Loss=0.0788, Time Left=23.27 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 775/3393 [06:08<21:01,  2.07batch/s, Batch Loss=0.0013, Avg Loss=0.0788, Time Left=23.27 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 775/3393 [06:09<21:01,  2.07batch/s, Batch Loss=0.1597, Avg Loss=0.0789, Time Left=23.26 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 776/3393 [06:09<21:15,  2.05batch/s, Batch Loss=0.1597, Avg Loss=0.0789, Time Left=23.26 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 776/3393 [06:09<21:15,  2.05batch/s, Batch Loss=0.1517, Avg Loss=0.0790, Time Left=23.25 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 777/3393 [06:09<20:49,  2.09batch/s, Batch Loss=0.1517, Avg Loss=0.0790, Time Left=23.25 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 777/3393 [06:10<20:49,  2.09batch/s, Batch Loss=0.2786, Avg Loss=0.0792, Time Left=23.24 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 778/3393 [06:10<20:45,  2.10batch/s, Batch Loss=0.2786, Avg Loss=0.0792, Time Left=23.24 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 778/3393 [06:10<20:45,  2.10batch/s, Batch Loss=0.3163, Avg Loss=0.0796, Time Left=23.23 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 779/3393 [06:10<21:12,  2.05batch/s, Batch Loss=0.3163, Avg Loss=0.0796, Time Left=23.23 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 779/3393 [06:11<21:12,  2.05batch/s, Batch Loss=0.3757, Avg Loss=0.0800, Time Left=23.22 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 780/3393 [06:11<20:44,  2.10batch/s, Batch Loss=0.3757, Avg Loss=0.0800, Time Left=23.22 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 780/3393 [06:11<20:44,  2.10batch/s, Batch Loss=0.0843, Avg Loss=0.0800, Time Left=23.21 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 781/3393 [06:11<21:02,  2.07batch/s, Batch Loss=0.0843, Avg Loss=0.0800, Time Left=23.21 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 781/3393 [06:12<21:02,  2.07batch/s, Batch Loss=0.0021, Avg Loss=0.0799, Time Left=23.20 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 782/3393 [06:12<21:12,  2.05batch/s, Batch Loss=0.0021, Avg Loss=0.0799, Time Left=23.20 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 782/3393 [06:12<21:12,  2.05batch/s, Batch Loss=0.2434, Avg Loss=0.0801, Time Left=23.19 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 783/3393 [06:12<21:30,  2.02batch/s, Batch Loss=0.2434, Avg Loss=0.0801, Time Left=23.19 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 783/3393 [06:13<21:30,  2.02batch/s, Batch Loss=0.0025, Avg Loss=0.0800, Time Left=23.18 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 784/3393 [06:13<22:48,  1.91batch/s, Batch Loss=0.0025, Avg Loss=0.0800, Time Left=23.18 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 784/3393 [06:13<22:48,  1.91batch/s, Batch Loss=0.3640, Avg Loss=0.0804, Time Left=23.18 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 785/3393 [06:13<23:30,  1.85batch/s, Batch Loss=0.3640, Avg Loss=0.0804, Time Left=23.18 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 785/3393 [06:14<23:30,  1.85batch/s, Batch Loss=0.3623, Avg Loss=0.0808, Time Left=23.17 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 786/3393 [06:14<22:39,  1.92batch/s, Batch Loss=0.3623, Avg Loss=0.0808, Time Left=23.17 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 786/3393 [06:15<22:39,  1.92batch/s, Batch Loss=0.1903, Avg Loss=0.0810, Time Left=23.17 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 787/3393 [06:15<24:34,  1.77batch/s, Batch Loss=0.1903, Avg Loss=0.0810, Time Left=23.17 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 787/3393 [06:15<24:34,  1.77batch/s, Batch Loss=0.0220, Avg Loss=0.0809, Time Left=23.16 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 788/3393 [06:15<24:45,  1.75batch/s, Batch Loss=0.0220, Avg Loss=0.0809, Time Left=23.16 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 788/3393 [06:16<24:45,  1.75batch/s, Batch Loss=0.2163, Avg Loss=0.0811, Time Left=23.16 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 789/3393 [06:16<25:10,  1.72batch/s, Batch Loss=0.2163, Avg Loss=0.0811, Time Left=23.16 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 789/3393 [06:16<25:10,  1.72batch/s, Batch Loss=0.4394, Avg Loss=0.0816, Time Left=23.15 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  23%|▏| 790/3393 [06:16<23:41,  1.83batch/s, Batch Loss=0.4394, Avg Loss=0.0816, Time Left=23.15 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 790/3393 [06:17<23:41,  1.83batch/s, Batch Loss=0.0715, Avg Loss=0.0815, Time Left=23.14 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 791/3393 [06:17<24:38,  1.76batch/s, Batch Loss=0.0715, Avg Loss=0.0815, Time Left=23.14 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 791/3393 [06:17<24:38,  1.76batch/s, Batch Loss=0.1384, Avg Loss=0.0816, Time Left=23.13 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 792/3393 [06:17<23:01,  1.88batch/s, Batch Loss=0.1384, Avg Loss=0.0816, Time Left=23.13 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 792/3393 [06:18<23:01,  1.88batch/s, Batch Loss=0.2341, Avg Loss=0.0818, Time Left=23.12 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 793/3393 [06:18<22:34,  1.92batch/s, Batch Loss=0.2341, Avg Loss=0.0818, Time Left=23.12 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 793/3393 [06:18<22:34,  1.92batch/s, Batch Loss=0.1602, Avg Loss=0.0819, Time Left=23.11 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 794/3393 [06:18<23:20,  1.86batch/s, Batch Loss=0.1602, Avg Loss=0.0819, Time Left=23.11 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 794/3393 [06:19<23:20,  1.86batch/s, Batch Loss=0.1910, Avg Loss=0.0821, Time Left=23.11 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 795/3393 [06:19<22:56,  1.89batch/s, Batch Loss=0.1910, Avg Loss=0.0821, Time Left=23.11 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 795/3393 [06:19<22:56,  1.89batch/s, Batch Loss=0.0887, Avg Loss=0.0821, Time Left=23.10 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 796/3393 [06:19<23:36,  1.83batch/s, Batch Loss=0.0887, Avg Loss=0.0821, Time Left=23.10 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 796/3393 [06:20<23:36,  1.83batch/s, Batch Loss=0.0765, Avg Loss=0.0821, Time Left=23.09 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 797/3393 [06:20<22:48,  1.90batch/s, Batch Loss=0.0765, Avg Loss=0.0821, Time Left=23.09 \u001b[A\n",
      "Epoch 3/3 - Training:  23%|▏| 797/3393 [06:20<22:48,  1.90batch/s, Batch Loss=0.2613, Avg Loss=0.0823, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 798/3393 [06:21<23:13,  1.86batch/s, Batch Loss=0.2613, Avg Loss=0.0823, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 798/3393 [06:21<23:13,  1.86batch/s, Batch Loss=0.1657, Avg Loss=0.0824, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 799/3393 [06:21<24:42,  1.75batch/s, Batch Loss=0.1657, Avg Loss=0.0824, Time Left=23.08 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 799/3393 [06:22<24:42,  1.75batch/s, Batch Loss=0.0346, Avg Loss=0.0824, Time Left=23.07 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 800/3393 [06:22<23:34,  1.83batch/s, Batch Loss=0.0346, Avg Loss=0.0824, Time Left=23.07 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 800/3393 [06:22<23:34,  1.83batch/s, Batch Loss=0.2104, Avg Loss=0.0826, Time Left=23.06 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 801/3393 [06:22<22:48,  1.89batch/s, Batch Loss=0.2104, Avg Loss=0.0826, Time Left=23.06 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 801/3393 [06:23<22:48,  1.89batch/s, Batch Loss=0.1055, Avg Loss=0.0826, Time Left=23.05 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 802/3393 [06:23<22:16,  1.94batch/s, Batch Loss=0.1055, Avg Loss=0.0826, Time Left=23.05 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 802/3393 [06:23<22:16,  1.94batch/s, Batch Loss=0.0954, Avg Loss=0.0826, Time Left=23.04 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 803/3393 [06:23<21:41,  1.99batch/s, Batch Loss=0.0954, Avg Loss=0.0826, Time Left=23.04 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 803/3393 [06:24<21:41,  1.99batch/s, Batch Loss=0.0791, Avg Loss=0.0826, Time Left=23.03 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 804/3393 [06:24<22:05,  1.95batch/s, Batch Loss=0.0791, Avg Loss=0.0826, Time Left=23.03 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 804/3393 [06:24<22:05,  1.95batch/s, Batch Loss=0.0326, Avg Loss=0.0825, Time Left=23.02 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 805/3393 [06:24<21:44,  1.98batch/s, Batch Loss=0.0326, Avg Loss=0.0825, Time Left=23.02 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 805/3393 [06:25<21:44,  1.98batch/s, Batch Loss=0.1174, Avg Loss=0.0826, Time Left=23.01 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 806/3393 [06:25<22:08,  1.95batch/s, Batch Loss=0.1174, Avg Loss=0.0826, Time Left=23.01 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 806/3393 [06:25<22:08,  1.95batch/s, Batch Loss=0.0118, Avg Loss=0.0825, Time Left=23.00 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 807/3393 [06:25<21:46,  1.98batch/s, Batch Loss=0.0118, Avg Loss=0.0825, Time Left=23.00 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 807/3393 [06:26<21:46,  1.98batch/s, Batch Loss=0.2280, Avg Loss=0.0827, Time Left=22.99 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 808/3393 [06:26<21:56,  1.96batch/s, Batch Loss=0.2280, Avg Loss=0.0827, Time Left=22.99 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 808/3393 [06:26<21:56,  1.96batch/s, Batch Loss=0.3142, Avg Loss=0.0830, Time Left=22.98 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 809/3393 [06:26<21:13,  2.03batch/s, Batch Loss=0.3142, Avg Loss=0.0830, Time Left=22.98 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 809/3393 [06:27<21:13,  2.03batch/s, Batch Loss=0.0028, Avg Loss=0.0829, Time Left=22.97 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 810/3393 [06:27<21:09,  2.04batch/s, Batch Loss=0.0028, Avg Loss=0.0829, Time Left=22.97 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 810/3393 [06:27<21:09,  2.04batch/s, Batch Loss=0.1019, Avg Loss=0.0829, Time Left=22.96 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 811/3393 [06:27<21:05,  2.04batch/s, Batch Loss=0.1019, Avg Loss=0.0829, Time Left=22.96 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 811/3393 [06:28<21:05,  2.04batch/s, Batch Loss=0.0167, Avg Loss=0.0828, Time Left=22.95 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 812/3393 [06:28<20:56,  2.05batch/s, Batch Loss=0.0167, Avg Loss=0.0828, Time Left=22.95 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 812/3393 [06:28<20:56,  2.05batch/s, Batch Loss=0.1003, Avg Loss=0.0828, Time Left=22.94 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 813/3393 [06:28<20:35,  2.09batch/s, Batch Loss=0.1003, Avg Loss=0.0828, Time Left=22.94 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 813/3393 [06:29<20:35,  2.09batch/s, Batch Loss=0.1356, Avg Loss=0.0829, Time Left=22.93 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 814/3393 [06:29<21:29,  2.00batch/s, Batch Loss=0.1356, Avg Loss=0.0829, Time Left=22.93 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 814/3393 [06:29<21:29,  2.00batch/s, Batch Loss=0.0060, Avg Loss=0.0828, Time Left=22.92 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 815/3393 [06:29<21:06,  2.04batch/s, Batch Loss=0.0060, Avg Loss=0.0828, Time Left=22.92 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 815/3393 [06:30<21:06,  2.04batch/s, Batch Loss=0.0124, Avg Loss=0.0827, Time Left=22.91 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 816/3393 [06:30<21:28,  2.00batch/s, Batch Loss=0.0124, Avg Loss=0.0827, Time Left=22.91 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 816/3393 [06:30<21:28,  2.00batch/s, Batch Loss=0.0081, Avg Loss=0.0826, Time Left=22.90 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 817/3393 [06:30<21:16,  2.02batch/s, Batch Loss=0.0081, Avg Loss=0.0826, Time Left=22.90 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 817/3393 [06:31<21:16,  2.02batch/s, Batch Loss=0.0187, Avg Loss=0.0825, Time Left=22.89 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 818/3393 [06:31<21:45,  1.97batch/s, Batch Loss=0.0187, Avg Loss=0.0825, Time Left=22.89 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 818/3393 [06:31<21:45,  1.97batch/s, Batch Loss=0.0734, Avg Loss=0.0825, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 819/3393 [06:31<21:30,  2.00batch/s, Batch Loss=0.0734, Avg Loss=0.0825, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 819/3393 [06:32<21:30,  2.00batch/s, Batch Loss=0.0175, Avg Loss=0.0824, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 820/3393 [06:32<22:45,  1.88batch/s, Batch Loss=0.0175, Avg Loss=0.0824, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 820/3393 [06:32<22:45,  1.88batch/s, Batch Loss=0.0350, Avg Loss=0.0824, Time Left=22.87 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 821/3393 [06:32<24:08,  1.78batch/s, Batch Loss=0.0350, Avg Loss=0.0824, Time Left=22.87 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 821/3393 [06:33<24:08,  1.78batch/s, Batch Loss=0.0165, Avg Loss=0.0823, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 822/3393 [06:33<26:10,  1.64batch/s, Batch Loss=0.0165, Avg Loss=0.0823, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 822/3393 [06:34<26:10,  1.64batch/s, Batch Loss=0.0465, Avg Loss=0.0822, Time Left=22.88 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  24%|▏| 823/3393 [06:34<28:00,  1.53batch/s, Batch Loss=0.0465, Avg Loss=0.0822, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 823/3393 [06:34<28:00,  1.53batch/s, Batch Loss=0.1734, Avg Loss=0.0824, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 824/3393 [06:34<27:21,  1.56batch/s, Batch Loss=0.1734, Avg Loss=0.0824, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 824/3393 [06:35<27:21,  1.56batch/s, Batch Loss=0.1343, Avg Loss=0.0824, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 825/3393 [06:35<28:40,  1.49batch/s, Batch Loss=0.1343, Avg Loss=0.0824, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 825/3393 [06:36<28:40,  1.49batch/s, Batch Loss=0.0879, Avg Loss=0.0824, Time Left=22.89 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 826/3393 [06:36<30:51,  1.39batch/s, Batch Loss=0.0879, Avg Loss=0.0824, Time Left=22.89 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 826/3393 [06:36<30:51,  1.39batch/s, Batch Loss=0.0416, Avg Loss=0.0824, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 827/3393 [06:36<27:54,  1.53batch/s, Batch Loss=0.0416, Avg Loss=0.0824, Time Left=22.88 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 827/3393 [06:37<27:54,  1.53batch/s, Batch Loss=0.0082, Avg Loss=0.0823, Time Left=22.87 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 828/3393 [06:37<25:23,  1.68batch/s, Batch Loss=0.0082, Avg Loss=0.0823, Time Left=22.87 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 828/3393 [06:37<25:23,  1.68batch/s, Batch Loss=0.5235, Avg Loss=0.0829, Time Left=22.86 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 829/3393 [06:37<24:40,  1.73batch/s, Batch Loss=0.5235, Avg Loss=0.0829, Time Left=22.86 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 829/3393 [06:38<24:40,  1.73batch/s, Batch Loss=0.0049, Avg Loss=0.0828, Time Left=22.85 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 830/3393 [06:38<23:45,  1.80batch/s, Batch Loss=0.0049, Avg Loss=0.0828, Time Left=22.85 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 830/3393 [06:38<23:45,  1.80batch/s, Batch Loss=0.0089, Avg Loss=0.0827, Time Left=22.84 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 831/3393 [06:38<23:10,  1.84batch/s, Batch Loss=0.0089, Avg Loss=0.0827, Time Left=22.84 \u001b[A\n",
      "Epoch 3/3 - Training:  24%|▏| 831/3393 [06:39<23:10,  1.84batch/s, Batch Loss=0.0827, Avg Loss=0.0827, Time Left=22.84 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 832/3393 [06:39<24:18,  1.76batch/s, Batch Loss=0.0827, Avg Loss=0.0827, Time Left=22.84 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 832/3393 [06:40<24:18,  1.76batch/s, Batch Loss=0.0507, Avg Loss=0.0826, Time Left=22.84 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 833/3393 [06:40<24:36,  1.73batch/s, Batch Loss=0.0507, Avg Loss=0.0826, Time Left=22.84 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 833/3393 [06:40<24:36,  1.73batch/s, Batch Loss=0.0171, Avg Loss=0.0825, Time Left=22.83 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 834/3393 [06:40<24:41,  1.73batch/s, Batch Loss=0.0171, Avg Loss=0.0825, Time Left=22.83 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 834/3393 [06:41<24:41,  1.73batch/s, Batch Loss=0.0612, Avg Loss=0.0825, Time Left=22.83 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 835/3393 [06:41<25:00,  1.70batch/s, Batch Loss=0.0612, Avg Loss=0.0825, Time Left=22.83 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 835/3393 [06:41<25:00,  1.70batch/s, Batch Loss=0.1195, Avg Loss=0.0826, Time Left=22.82 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 836/3393 [06:41<24:15,  1.76batch/s, Batch Loss=0.1195, Avg Loss=0.0826, Time Left=22.82 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 836/3393 [06:42<24:15,  1.76batch/s, Batch Loss=0.0136, Avg Loss=0.0825, Time Left=22.82 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 837/3393 [06:42<25:10,  1.69batch/s, Batch Loss=0.0136, Avg Loss=0.0825, Time Left=22.82 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 837/3393 [06:43<25:10,  1.69batch/s, Batch Loss=0.0107, Avg Loss=0.0824, Time Left=22.81 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 838/3393 [06:43<25:16,  1.68batch/s, Batch Loss=0.0107, Avg Loss=0.0824, Time Left=22.81 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 838/3393 [06:43<25:16,  1.68batch/s, Batch Loss=0.1166, Avg Loss=0.0824, Time Left=22.80 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 839/3393 [06:43<23:26,  1.82batch/s, Batch Loss=0.1166, Avg Loss=0.0824, Time Left=22.80 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 839/3393 [06:44<23:26,  1.82batch/s, Batch Loss=0.0167, Avg Loss=0.0823, Time Left=22.79 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 840/3393 [06:44<23:32,  1.81batch/s, Batch Loss=0.0167, Avg Loss=0.0823, Time Left=22.79 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 840/3393 [06:44<23:32,  1.81batch/s, Batch Loss=0.0929, Avg Loss=0.0823, Time Left=22.79 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 841/3393 [06:44<24:03,  1.77batch/s, Batch Loss=0.0929, Avg Loss=0.0823, Time Left=22.79 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 841/3393 [06:45<24:03,  1.77batch/s, Batch Loss=0.0401, Avg Loss=0.0823, Time Left=22.78 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 842/3393 [06:45<23:30,  1.81batch/s, Batch Loss=0.0401, Avg Loss=0.0823, Time Left=22.78 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 842/3393 [06:45<23:30,  1.81batch/s, Batch Loss=0.0651, Avg Loss=0.0823, Time Left=22.78 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 843/3393 [06:45<24:18,  1.75batch/s, Batch Loss=0.0651, Avg Loss=0.0823, Time Left=22.78 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 843/3393 [06:46<24:18,  1.75batch/s, Batch Loss=0.1247, Avg Loss=0.0823, Time Left=22.78 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 844/3393 [06:46<25:54,  1.64batch/s, Batch Loss=0.1247, Avg Loss=0.0823, Time Left=22.78 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 844/3393 [06:47<25:54,  1.64batch/s, Batch Loss=0.0531, Avg Loss=0.0823, Time Left=22.77 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 845/3393 [06:47<24:24,  1.74batch/s, Batch Loss=0.0531, Avg Loss=0.0823, Time Left=22.77 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 845/3393 [06:47<24:24,  1.74batch/s, Batch Loss=0.0139, Avg Loss=0.0822, Time Left=22.76 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 846/3393 [06:47<23:41,  1.79batch/s, Batch Loss=0.0139, Avg Loss=0.0822, Time Left=22.76 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 846/3393 [06:48<23:41,  1.79batch/s, Batch Loss=0.0349, Avg Loss=0.0821, Time Left=22.75 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 847/3393 [06:48<22:46,  1.86batch/s, Batch Loss=0.0349, Avg Loss=0.0821, Time Left=22.75 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 847/3393 [06:48<22:46,  1.86batch/s, Batch Loss=0.0289, Avg Loss=0.0821, Time Left=22.74 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 848/3393 [06:48<22:20,  1.90batch/s, Batch Loss=0.0289, Avg Loss=0.0821, Time Left=22.74 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▏| 848/3393 [06:49<22:20,  1.90batch/s, Batch Loss=0.0224, Avg Loss=0.0820, Time Left=22.74 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 849/3393 [06:49<24:21,  1.74batch/s, Batch Loss=0.0224, Avg Loss=0.0820, Time Left=22.74 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 849/3393 [06:50<24:21,  1.74batch/s, Batch Loss=0.1081, Avg Loss=0.0820, Time Left=22.79 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 850/3393 [06:50<36:41,  1.16batch/s, Batch Loss=0.1081, Avg Loss=0.0820, Time Left=22.79 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 850/3393 [06:51<36:41,  1.16batch/s, Batch Loss=0.0795, Avg Loss=0.0820, Time Left=22.77 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 851/3393 [06:51<31:48,  1.33batch/s, Batch Loss=0.0795, Avg Loss=0.0820, Time Left=22.77 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 851/3393 [06:51<31:48,  1.33batch/s, Batch Loss=0.0187, Avg Loss=0.0819, Time Left=22.76 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 852/3393 [06:51<28:18,  1.50batch/s, Batch Loss=0.0187, Avg Loss=0.0819, Time Left=22.76 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 852/3393 [06:52<28:18,  1.50batch/s, Batch Loss=0.0997, Avg Loss=0.0820, Time Left=22.75 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 853/3393 [06:52<25:57,  1.63batch/s, Batch Loss=0.0997, Avg Loss=0.0820, Time Left=22.75 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 853/3393 [06:52<25:57,  1.63batch/s, Batch Loss=0.1265, Avg Loss=0.0820, Time Left=22.75 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 854/3393 [06:52<25:20,  1.67batch/s, Batch Loss=0.1265, Avg Loss=0.0820, Time Left=22.75 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 854/3393 [06:53<25:20,  1.67batch/s, Batch Loss=0.0315, Avg Loss=0.0820, Time Left=22.74 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 855/3393 [06:53<24:35,  1.72batch/s, Batch Loss=0.0315, Avg Loss=0.0820, Time Left=22.74 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 855/3393 [06:53<24:35,  1.72batch/s, Batch Loss=0.0663, Avg Loss=0.0819, Time Left=22.73 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  25%|▎| 856/3393 [06:53<23:58,  1.76batch/s, Batch Loss=0.0663, Avg Loss=0.0819, Time Left=22.73 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 856/3393 [06:54<23:58,  1.76batch/s, Batch Loss=0.2253, Avg Loss=0.0821, Time Left=22.72 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 857/3393 [06:54<23:06,  1.83batch/s, Batch Loss=0.2253, Avg Loss=0.0821, Time Left=22.72 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 857/3393 [06:54<23:06,  1.83batch/s, Batch Loss=0.0985, Avg Loss=0.0821, Time Left=22.71 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 858/3393 [06:54<22:29,  1.88batch/s, Batch Loss=0.0985, Avg Loss=0.0821, Time Left=22.71 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 858/3393 [06:55<22:29,  1.88batch/s, Batch Loss=0.0911, Avg Loss=0.0821, Time Left=22.71 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 859/3393 [06:55<23:34,  1.79batch/s, Batch Loss=0.0911, Avg Loss=0.0821, Time Left=22.71 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 859/3393 [06:56<23:34,  1.79batch/s, Batch Loss=0.0064, Avg Loss=0.0821, Time Left=22.70 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 860/3393 [06:56<23:00,  1.83batch/s, Batch Loss=0.0064, Avg Loss=0.0821, Time Left=22.70 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 860/3393 [06:56<23:00,  1.83batch/s, Batch Loss=0.1115, Avg Loss=0.0821, Time Left=22.70 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 861/3393 [06:56<24:24,  1.73batch/s, Batch Loss=0.1115, Avg Loss=0.0821, Time Left=22.70 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 861/3393 [06:57<24:24,  1.73batch/s, Batch Loss=0.1462, Avg Loss=0.0822, Time Left=22.69 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 862/3393 [06:57<25:02,  1.68batch/s, Batch Loss=0.1462, Avg Loss=0.0822, Time Left=22.69 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 862/3393 [06:57<25:02,  1.68batch/s, Batch Loss=0.0720, Avg Loss=0.0822, Time Left=22.69 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 863/3393 [06:57<25:20,  1.66batch/s, Batch Loss=0.0720, Avg Loss=0.0822, Time Left=22.69 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 863/3393 [06:58<25:20,  1.66batch/s, Batch Loss=0.0514, Avg Loss=0.0821, Time Left=22.68 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 864/3393 [06:58<25:23,  1.66batch/s, Batch Loss=0.0514, Avg Loss=0.0821, Time Left=22.68 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 864/3393 [06:59<25:23,  1.66batch/s, Batch Loss=0.0372, Avg Loss=0.0821, Time Left=22.68 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 865/3393 [06:59<25:59,  1.62batch/s, Batch Loss=0.0372, Avg Loss=0.0821, Time Left=22.68 \u001b[A\n",
      "Epoch 3/3 - Training:  25%|▎| 865/3393 [06:59<25:59,  1.62batch/s, Batch Loss=0.2582, Avg Loss=0.0823, Time Left=22.68 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 866/3393 [06:59<25:54,  1.63batch/s, Batch Loss=0.2582, Avg Loss=0.0823, Time Left=22.68 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 866/3393 [07:00<25:54,  1.63batch/s, Batch Loss=0.0524, Avg Loss=0.0822, Time Left=22.67 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 867/3393 [07:00<25:39,  1.64batch/s, Batch Loss=0.0524, Avg Loss=0.0822, Time Left=22.67 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 867/3393 [07:01<25:39,  1.64batch/s, Batch Loss=0.1167, Avg Loss=0.0823, Time Left=22.67 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 868/3393 [07:01<26:05,  1.61batch/s, Batch Loss=0.1167, Avg Loss=0.0823, Time Left=22.67 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 868/3393 [07:01<26:05,  1.61batch/s, Batch Loss=0.0926, Avg Loss=0.0823, Time Left=22.66 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 869/3393 [07:01<24:52,  1.69batch/s, Batch Loss=0.0926, Avg Loss=0.0823, Time Left=22.66 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 869/3393 [07:02<24:52,  1.69batch/s, Batch Loss=0.0137, Avg Loss=0.0822, Time Left=22.66 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 870/3393 [07:02<25:26,  1.65batch/s, Batch Loss=0.0137, Avg Loss=0.0822, Time Left=22.66 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 870/3393 [07:02<25:26,  1.65batch/s, Batch Loss=0.1119, Avg Loss=0.0823, Time Left=22.65 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 871/3393 [07:02<23:48,  1.77batch/s, Batch Loss=0.1119, Avg Loss=0.0823, Time Left=22.65 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 871/3393 [07:03<23:48,  1.77batch/s, Batch Loss=0.1070, Avg Loss=0.0823, Time Left=22.65 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 872/3393 [07:03<24:40,  1.70batch/s, Batch Loss=0.1070, Avg Loss=0.0823, Time Left=22.65 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 872/3393 [07:03<24:40,  1.70batch/s, Batch Loss=0.1080, Avg Loss=0.0823, Time Left=22.64 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 873/3393 [07:03<24:56,  1.68batch/s, Batch Loss=0.1080, Avg Loss=0.0823, Time Left=22.64 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 873/3393 [07:04<24:56,  1.68batch/s, Batch Loss=0.0629, Avg Loss=0.0823, Time Left=22.63 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 874/3393 [07:04<23:39,  1.77batch/s, Batch Loss=0.0629, Avg Loss=0.0823, Time Left=22.63 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 874/3393 [07:05<23:39,  1.77batch/s, Batch Loss=0.0064, Avg Loss=0.0822, Time Left=22.63 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 875/3393 [07:05<24:36,  1.70batch/s, Batch Loss=0.0064, Avg Loss=0.0822, Time Left=22.63 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 875/3393 [07:05<24:36,  1.70batch/s, Batch Loss=0.0094, Avg Loss=0.0821, Time Left=22.62 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 876/3393 [07:05<23:37,  1.78batch/s, Batch Loss=0.0094, Avg Loss=0.0821, Time Left=22.62 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 876/3393 [07:06<23:37,  1.78batch/s, Batch Loss=0.1437, Avg Loss=0.0822, Time Left=22.61 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 877/3393 [07:06<23:13,  1.81batch/s, Batch Loss=0.1437, Avg Loss=0.0822, Time Left=22.61 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 877/3393 [07:06<23:13,  1.81batch/s, Batch Loss=0.0168, Avg Loss=0.0821, Time Left=22.60 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 878/3393 [07:06<22:34,  1.86batch/s, Batch Loss=0.0168, Avg Loss=0.0821, Time Left=22.60 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 878/3393 [07:07<22:34,  1.86batch/s, Batch Loss=0.0047, Avg Loss=0.0820, Time Left=22.59 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 879/3393 [07:07<22:04,  1.90batch/s, Batch Loss=0.0047, Avg Loss=0.0820, Time Left=22.59 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 879/3393 [07:07<22:04,  1.90batch/s, Batch Loss=0.0471, Avg Loss=0.0820, Time Left=22.58 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 880/3393 [07:07<21:50,  1.92batch/s, Batch Loss=0.0471, Avg Loss=0.0820, Time Left=22.58 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 880/3393 [07:08<21:50,  1.92batch/s, Batch Loss=0.0212, Avg Loss=0.0819, Time Left=22.57 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 881/3393 [07:08<21:52,  1.91batch/s, Batch Loss=0.0212, Avg Loss=0.0819, Time Left=22.57 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 881/3393 [07:08<21:52,  1.91batch/s, Batch Loss=0.0601, Avg Loss=0.0819, Time Left=22.56 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 882/3393 [07:08<21:36,  1.94batch/s, Batch Loss=0.0601, Avg Loss=0.0819, Time Left=22.56 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 882/3393 [07:09<21:36,  1.94batch/s, Batch Loss=0.1135, Avg Loss=0.0819, Time Left=22.55 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 883/3393 [07:09<21:46,  1.92batch/s, Batch Loss=0.1135, Avg Loss=0.0819, Time Left=22.55 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 883/3393 [07:09<21:46,  1.92batch/s, Batch Loss=0.0602, Avg Loss=0.0819, Time Left=22.54 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 884/3393 [07:09<21:20,  1.96batch/s, Batch Loss=0.0602, Avg Loss=0.0819, Time Left=22.54 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 884/3393 [07:10<21:20,  1.96batch/s, Batch Loss=0.0108, Avg Loss=0.0818, Time Left=22.53 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 885/3393 [07:10<21:24,  1.95batch/s, Batch Loss=0.0108, Avg Loss=0.0818, Time Left=22.53 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 885/3393 [07:10<21:24,  1.95batch/s, Batch Loss=0.3951, Avg Loss=0.0822, Time Left=22.52 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 886/3393 [07:10<21:16,  1.96batch/s, Batch Loss=0.3951, Avg Loss=0.0822, Time Left=22.52 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 886/3393 [07:11<21:16,  1.96batch/s, Batch Loss=0.1361, Avg Loss=0.0822, Time Left=22.51 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 887/3393 [07:11<21:19,  1.96batch/s, Batch Loss=0.1361, Avg Loss=0.0822, Time Left=22.51 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 887/3393 [07:11<21:19,  1.96batch/s, Batch Loss=0.0631, Avg Loss=0.0822, Time Left=22.50 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 888/3393 [07:11<21:12,  1.97batch/s, Batch Loss=0.0631, Avg Loss=0.0822, Time Left=22.50 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 888/3393 [07:12<21:12,  1.97batch/s, Batch Loss=0.0308, Avg Loss=0.0822, Time Left=22.50 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  26%|▎| 889/3393 [07:12<21:41,  1.92batch/s, Batch Loss=0.0308, Avg Loss=0.0822, Time Left=22.50 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 889/3393 [07:12<21:41,  1.92batch/s, Batch Loss=0.0173, Avg Loss=0.0821, Time Left=22.49 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 890/3393 [07:12<21:16,  1.96batch/s, Batch Loss=0.0173, Avg Loss=0.0821, Time Left=22.49 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 890/3393 [07:13<21:16,  1.96batch/s, Batch Loss=0.0382, Avg Loss=0.0820, Time Left=22.48 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 891/3393 [07:13<21:32,  1.94batch/s, Batch Loss=0.0382, Avg Loss=0.0820, Time Left=22.48 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 891/3393 [07:13<21:32,  1.94batch/s, Batch Loss=0.0155, Avg Loss=0.0819, Time Left=22.47 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 892/3393 [07:13<21:09,  1.97batch/s, Batch Loss=0.0155, Avg Loss=0.0819, Time Left=22.47 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 892/3393 [07:14<21:09,  1.97batch/s, Batch Loss=0.0286, Avg Loss=0.0819, Time Left=22.46 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 893/3393 [07:14<21:14,  1.96batch/s, Batch Loss=0.0286, Avg Loss=0.0819, Time Left=22.46 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 893/3393 [07:14<21:14,  1.96batch/s, Batch Loss=0.0873, Avg Loss=0.0819, Time Left=22.45 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 894/3393 [07:14<20:55,  1.99batch/s, Batch Loss=0.0873, Avg Loss=0.0819, Time Left=22.45 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 894/3393 [07:15<20:55,  1.99batch/s, Batch Loss=0.1426, Avg Loss=0.0820, Time Left=22.44 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 895/3393 [07:15<21:16,  1.96batch/s, Batch Loss=0.1426, Avg Loss=0.0820, Time Left=22.44 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 895/3393 [07:15<21:16,  1.96batch/s, Batch Loss=0.0452, Avg Loss=0.0819, Time Left=22.43 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 896/3393 [07:15<20:33,  2.02batch/s, Batch Loss=0.0452, Avg Loss=0.0819, Time Left=22.43 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 896/3393 [07:16<20:33,  2.02batch/s, Batch Loss=0.0347, Avg Loss=0.0819, Time Left=22.42 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 897/3393 [07:16<20:48,  2.00batch/s, Batch Loss=0.0347, Avg Loss=0.0819, Time Left=22.42 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 897/3393 [07:16<20:48,  2.00batch/s, Batch Loss=0.0949, Avg Loss=0.0819, Time Left=22.40 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 898/3393 [07:16<20:36,  2.02batch/s, Batch Loss=0.0949, Avg Loss=0.0819, Time Left=22.40 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 898/3393 [07:17<20:36,  2.02batch/s, Batch Loss=0.0080, Avg Loss=0.0818, Time Left=22.40 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 899/3393 [07:17<20:40,  2.01batch/s, Batch Loss=0.0080, Avg Loss=0.0818, Time Left=22.40 \u001b[A\n",
      "Epoch 3/3 - Training:  26%|▎| 899/3393 [07:17<20:40,  2.01batch/s, Batch Loss=0.0042, Avg Loss=0.0817, Time Left=22.38 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 900/3393 [07:17<20:53,  1.99batch/s, Batch Loss=0.0042, Avg Loss=0.0817, Time Left=22.38 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 900/3393 [07:18<20:53,  1.99batch/s, Batch Loss=0.0368, Avg Loss=0.0816, Time Left=22.37 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 901/3393 [07:18<20:39,  2.01batch/s, Batch Loss=0.0368, Avg Loss=0.0816, Time Left=22.37 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 901/3393 [07:18<20:39,  2.01batch/s, Batch Loss=0.0443, Avg Loss=0.0816, Time Left=22.37 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 902/3393 [07:18<21:05,  1.97batch/s, Batch Loss=0.0443, Avg Loss=0.0816, Time Left=22.37 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 902/3393 [07:19<21:05,  1.97batch/s, Batch Loss=0.2064, Avg Loss=0.0817, Time Left=22.36 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 903/3393 [07:19<20:59,  1.98batch/s, Batch Loss=0.2064, Avg Loss=0.0817, Time Left=22.36 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 903/3393 [07:19<20:59,  1.98batch/s, Batch Loss=0.3198, Avg Loss=0.0820, Time Left=22.35 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 904/3393 [07:19<21:06,  1.96batch/s, Batch Loss=0.3198, Avg Loss=0.0820, Time Left=22.35 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 904/3393 [07:20<21:06,  1.96batch/s, Batch Loss=0.0182, Avg Loss=0.0819, Time Left=22.34 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 905/3393 [07:20<21:00,  1.97batch/s, Batch Loss=0.0182, Avg Loss=0.0819, Time Left=22.34 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 905/3393 [07:20<21:00,  1.97batch/s, Batch Loss=0.0031, Avg Loss=0.0819, Time Left=22.33 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 906/3393 [07:20<21:18,  1.94batch/s, Batch Loss=0.0031, Avg Loss=0.0819, Time Left=22.33 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 906/3393 [07:21<21:18,  1.94batch/s, Batch Loss=0.0185, Avg Loss=0.0818, Time Left=22.32 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 907/3393 [07:21<20:57,  1.98batch/s, Batch Loss=0.0185, Avg Loss=0.0818, Time Left=22.32 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 907/3393 [07:21<20:57,  1.98batch/s, Batch Loss=0.0028, Avg Loss=0.0817, Time Left=22.31 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 908/3393 [07:21<20:52,  1.98batch/s, Batch Loss=0.0028, Avg Loss=0.0817, Time Left=22.31 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 908/3393 [07:22<20:52,  1.98batch/s, Batch Loss=0.2787, Avg Loss=0.0819, Time Left=22.30 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 909/3393 [07:22<20:25,  2.03batch/s, Batch Loss=0.2787, Avg Loss=0.0819, Time Left=22.30 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 909/3393 [07:22<20:25,  2.03batch/s, Batch Loss=0.0422, Avg Loss=0.0819, Time Left=22.29 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 910/3393 [07:22<20:41,  2.00batch/s, Batch Loss=0.0422, Avg Loss=0.0819, Time Left=22.29 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 910/3393 [07:23<20:41,  2.00batch/s, Batch Loss=0.0107, Avg Loss=0.0818, Time Left=22.28 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 911/3393 [07:23<20:41,  2.00batch/s, Batch Loss=0.0107, Avg Loss=0.0818, Time Left=22.28 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 911/3393 [07:23<20:41,  2.00batch/s, Batch Loss=0.4262, Avg Loss=0.0822, Time Left=22.27 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 912/3393 [07:23<20:29,  2.02batch/s, Batch Loss=0.4262, Avg Loss=0.0822, Time Left=22.27 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 912/3393 [07:24<20:29,  2.02batch/s, Batch Loss=0.0733, Avg Loss=0.0822, Time Left=22.26 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 913/3393 [07:24<20:44,  1.99batch/s, Batch Loss=0.0733, Avg Loss=0.0822, Time Left=22.26 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 913/3393 [07:24<20:44,  1.99batch/s, Batch Loss=0.2492, Avg Loss=0.0824, Time Left=22.25 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 914/3393 [07:24<20:42,  1.99batch/s, Batch Loss=0.2492, Avg Loss=0.0824, Time Left=22.25 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 914/3393 [07:25<20:42,  1.99batch/s, Batch Loss=0.0025, Avg Loss=0.0823, Time Left=22.24 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 915/3393 [07:25<21:05,  1.96batch/s, Batch Loss=0.0025, Avg Loss=0.0823, Time Left=22.24 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 915/3393 [07:25<21:05,  1.96batch/s, Batch Loss=0.0921, Avg Loss=0.0823, Time Left=22.23 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 916/3393 [07:25<20:58,  1.97batch/s, Batch Loss=0.0921, Avg Loss=0.0823, Time Left=22.23 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 916/3393 [07:26<20:58,  1.97batch/s, Batch Loss=0.1055, Avg Loss=0.0823, Time Left=22.22 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 917/3393 [07:26<20:52,  1.98batch/s, Batch Loss=0.1055, Avg Loss=0.0823, Time Left=22.22 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 917/3393 [07:26<20:52,  1.98batch/s, Batch Loss=0.0783, Avg Loss=0.0823, Time Left=22.21 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 918/3393 [07:26<20:47,  1.98batch/s, Batch Loss=0.0783, Avg Loss=0.0823, Time Left=22.21 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 918/3393 [07:27<20:47,  1.98batch/s, Batch Loss=0.1256, Avg Loss=0.0824, Time Left=22.20 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 919/3393 [07:27<20:43,  1.99batch/s, Batch Loss=0.1256, Avg Loss=0.0824, Time Left=22.20 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 919/3393 [07:27<20:43,  1.99batch/s, Batch Loss=0.0165, Avg Loss=0.0823, Time Left=22.19 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 920/3393 [07:27<20:29,  2.01batch/s, Batch Loss=0.0165, Avg Loss=0.0823, Time Left=22.19 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 920/3393 [07:28<20:29,  2.01batch/s, Batch Loss=0.0627, Avg Loss=0.0823, Time Left=22.18 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 921/3393 [07:28<20:07,  2.05batch/s, Batch Loss=0.0627, Avg Loss=0.0823, Time Left=22.18 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 921/3393 [07:28<20:07,  2.05batch/s, Batch Loss=0.0388, Avg Loss=0.0822, Time Left=22.17 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  27%|▎| 922/3393 [07:28<20:49,  1.98batch/s, Batch Loss=0.0388, Avg Loss=0.0822, Time Left=22.17 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 922/3393 [07:29<20:49,  1.98batch/s, Batch Loss=0.0046, Avg Loss=0.0821, Time Left=22.16 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 923/3393 [07:29<20:35,  2.00batch/s, Batch Loss=0.0046, Avg Loss=0.0821, Time Left=22.16 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 923/3393 [07:29<20:35,  2.00batch/s, Batch Loss=0.0236, Avg Loss=0.0821, Time Left=22.15 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 924/3393 [07:29<20:56,  1.96batch/s, Batch Loss=0.0236, Avg Loss=0.0821, Time Left=22.15 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 924/3393 [07:30<20:56,  1.96batch/s, Batch Loss=0.0728, Avg Loss=0.0821, Time Left=22.14 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 925/3393 [07:30<20:40,  1.99batch/s, Batch Loss=0.0728, Avg Loss=0.0821, Time Left=22.14 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 925/3393 [07:30<20:40,  1.99batch/s, Batch Loss=0.3683, Avg Loss=0.0824, Time Left=22.13 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 926/3393 [07:30<21:00,  1.96batch/s, Batch Loss=0.3683, Avg Loss=0.0824, Time Left=22.13 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 926/3393 [07:31<21:00,  1.96batch/s, Batch Loss=0.0842, Avg Loss=0.0824, Time Left=22.12 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 927/3393 [07:31<20:42,  1.98batch/s, Batch Loss=0.0842, Avg Loss=0.0824, Time Left=22.12 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 927/3393 [07:31<20:42,  1.98batch/s, Batch Loss=0.1755, Avg Loss=0.0825, Time Left=22.11 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 928/3393 [07:31<21:03,  1.95batch/s, Batch Loss=0.1755, Avg Loss=0.0825, Time Left=22.11 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 928/3393 [07:32<21:03,  1.95batch/s, Batch Loss=0.0034, Avg Loss=0.0824, Time Left=22.10 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 929/3393 [07:32<20:42,  1.98batch/s, Batch Loss=0.0034, Avg Loss=0.0824, Time Left=22.10 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 929/3393 [07:32<20:42,  1.98batch/s, Batch Loss=0.0065, Avg Loss=0.0823, Time Left=22.09 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 930/3393 [07:32<21:14,  1.93batch/s, Batch Loss=0.0065, Avg Loss=0.0823, Time Left=22.09 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 930/3393 [07:33<21:14,  1.93batch/s, Batch Loss=0.0122, Avg Loss=0.0822, Time Left=22.08 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 931/3393 [07:33<20:50,  1.97batch/s, Batch Loss=0.0122, Avg Loss=0.0822, Time Left=22.08 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 931/3393 [07:33<20:50,  1.97batch/s, Batch Loss=0.0096, Avg Loss=0.0821, Time Left=22.07 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 932/3393 [07:33<21:06,  1.94batch/s, Batch Loss=0.0096, Avg Loss=0.0821, Time Left=22.07 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 932/3393 [07:34<21:06,  1.94batch/s, Batch Loss=0.0058, Avg Loss=0.0821, Time Left=22.07 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 933/3393 [07:34<20:50,  1.97batch/s, Batch Loss=0.0058, Avg Loss=0.0821, Time Left=22.07 \u001b[A\n",
      "Epoch 3/3 - Training:  27%|▎| 933/3393 [07:34<20:50,  1.97batch/s, Batch Loss=0.0515, Avg Loss=0.0820, Time Left=22.06 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 934/3393 [07:34<21:12,  1.93batch/s, Batch Loss=0.0515, Avg Loss=0.0820, Time Left=22.06 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 934/3393 [07:35<21:12,  1.93batch/s, Batch Loss=0.0021, Avg Loss=0.0819, Time Left=22.05 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 935/3393 [07:35<20:50,  1.97batch/s, Batch Loss=0.0021, Avg Loss=0.0819, Time Left=22.05 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 935/3393 [07:35<20:50,  1.97batch/s, Batch Loss=0.0062, Avg Loss=0.0818, Time Left=22.04 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 936/3393 [07:35<20:55,  1.96batch/s, Batch Loss=0.0062, Avg Loss=0.0818, Time Left=22.04 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 936/3393 [07:36<20:55,  1.96batch/s, Batch Loss=0.0360, Avg Loss=0.0818, Time Left=22.03 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 937/3393 [07:36<20:47,  1.97batch/s, Batch Loss=0.0360, Avg Loss=0.0818, Time Left=22.03 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 937/3393 [07:36<20:47,  1.97batch/s, Batch Loss=0.0432, Avg Loss=0.0817, Time Left=22.02 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 938/3393 [07:36<20:52,  1.96batch/s, Batch Loss=0.0432, Avg Loss=0.0817, Time Left=22.02 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 938/3393 [07:37<20:52,  1.96batch/s, Batch Loss=0.0088, Avg Loss=0.0817, Time Left=22.01 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 939/3393 [07:37<20:47,  1.97batch/s, Batch Loss=0.0088, Avg Loss=0.0817, Time Left=22.01 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 939/3393 [07:38<20:47,  1.97batch/s, Batch Loss=0.0393, Avg Loss=0.0816, Time Left=22.00 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 940/3393 [07:38<21:16,  1.92batch/s, Batch Loss=0.0393, Avg Loss=0.0816, Time Left=22.00 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 940/3393 [07:38<21:16,  1.92batch/s, Batch Loss=0.0413, Avg Loss=0.0816, Time Left=21.99 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 941/3393 [07:38<21:01,  1.94batch/s, Batch Loss=0.0413, Avg Loss=0.0816, Time Left=21.99 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 941/3393 [07:39<21:01,  1.94batch/s, Batch Loss=0.0402, Avg Loss=0.0815, Time Left=21.98 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 942/3393 [07:39<20:50,  1.96batch/s, Batch Loss=0.0402, Avg Loss=0.0815, Time Left=21.98 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 942/3393 [07:39<20:50,  1.96batch/s, Batch Loss=0.0978, Avg Loss=0.0815, Time Left=21.97 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 943/3393 [07:39<20:31,  1.99batch/s, Batch Loss=0.0978, Avg Loss=0.0815, Time Left=21.97 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 943/3393 [07:40<20:31,  1.99batch/s, Batch Loss=0.0032, Avg Loss=0.0815, Time Left=21.96 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 944/3393 [07:40<20:29,  1.99batch/s, Batch Loss=0.0032, Avg Loss=0.0815, Time Left=21.96 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 944/3393 [07:40<20:29,  1.99batch/s, Batch Loss=0.1276, Avg Loss=0.0815, Time Left=21.95 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 945/3393 [07:40<20:16,  2.01batch/s, Batch Loss=0.1276, Avg Loss=0.0815, Time Left=21.95 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 945/3393 [07:40<20:16,  2.01batch/s, Batch Loss=0.0124, Avg Loss=0.0814, Time Left=21.94 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 946/3393 [07:40<20:18,  2.01batch/s, Batch Loss=0.0124, Avg Loss=0.0814, Time Left=21.94 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 946/3393 [07:41<20:18,  2.01batch/s, Batch Loss=0.0153, Avg Loss=0.0814, Time Left=21.93 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 947/3393 [07:41<20:41,  1.97batch/s, Batch Loss=0.0153, Avg Loss=0.0814, Time Left=21.93 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 947/3393 [07:42<20:41,  1.97batch/s, Batch Loss=0.0447, Avg Loss=0.0813, Time Left=21.92 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 948/3393 [07:42<20:25,  1.99batch/s, Batch Loss=0.0447, Avg Loss=0.0813, Time Left=21.92 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 948/3393 [07:42<20:25,  1.99batch/s, Batch Loss=0.0527, Avg Loss=0.0813, Time Left=21.91 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 949/3393 [07:42<20:46,  1.96batch/s, Batch Loss=0.0527, Avg Loss=0.0813, Time Left=21.91 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 949/3393 [07:43<20:46,  1.96batch/s, Batch Loss=0.0535, Avg Loss=0.0812, Time Left=21.90 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 950/3393 [07:43<20:40,  1.97batch/s, Batch Loss=0.0535, Avg Loss=0.0812, Time Left=21.90 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 950/3393 [07:43<20:40,  1.97batch/s, Batch Loss=0.0105, Avg Loss=0.0812, Time Left=21.89 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 951/3393 [07:43<20:23,  2.00batch/s, Batch Loss=0.0105, Avg Loss=0.0812, Time Left=21.89 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 951/3393 [07:44<20:23,  2.00batch/s, Batch Loss=0.1822, Avg Loss=0.0813, Time Left=21.88 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 952/3393 [07:44<20:32,  1.98batch/s, Batch Loss=0.1822, Avg Loss=0.0813, Time Left=21.88 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 952/3393 [07:44<20:32,  1.98batch/s, Batch Loss=0.0763, Avg Loss=0.0813, Time Left=21.88 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 953/3393 [07:44<20:40,  1.97batch/s, Batch Loss=0.0763, Avg Loss=0.0813, Time Left=21.88 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 953/3393 [07:45<20:40,  1.97batch/s, Batch Loss=0.0048, Avg Loss=0.0812, Time Left=21.87 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 954/3393 [07:45<20:35,  1.97batch/s, Batch Loss=0.0048, Avg Loss=0.0812, Time Left=21.87 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 954/3393 [07:45<20:35,  1.97batch/s, Batch Loss=0.4619, Avg Loss=0.0816, Time Left=21.86 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  28%|▎| 955/3393 [07:45<20:42,  1.96batch/s, Batch Loss=0.4619, Avg Loss=0.0816, Time Left=21.86 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 955/3393 [07:46<20:42,  1.96batch/s, Batch Loss=0.0061, Avg Loss=0.0815, Time Left=21.85 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 956/3393 [07:46<20:46,  1.95batch/s, Batch Loss=0.0061, Avg Loss=0.0815, Time Left=21.85 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 956/3393 [07:46<20:46,  1.95batch/s, Batch Loss=0.0264, Avg Loss=0.0815, Time Left=21.84 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 957/3393 [07:46<20:38,  1.97batch/s, Batch Loss=0.0264, Avg Loss=0.0815, Time Left=21.84 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 957/3393 [07:47<20:38,  1.97batch/s, Batch Loss=0.0192, Avg Loss=0.0814, Time Left=21.83 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 958/3393 [07:47<20:32,  1.98batch/s, Batch Loss=0.0192, Avg Loss=0.0814, Time Left=21.83 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 958/3393 [07:47<20:32,  1.98batch/s, Batch Loss=0.0082, Avg Loss=0.0813, Time Left=21.82 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 959/3393 [07:47<21:02,  1.93batch/s, Batch Loss=0.0082, Avg Loss=0.0813, Time Left=21.82 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 959/3393 [07:48<21:02,  1.93batch/s, Batch Loss=0.0057, Avg Loss=0.0812, Time Left=21.81 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 960/3393 [07:48<20:38,  1.96batch/s, Batch Loss=0.0057, Avg Loss=0.0812, Time Left=21.81 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 960/3393 [07:48<20:38,  1.96batch/s, Batch Loss=0.2327, Avg Loss=0.0814, Time Left=21.80 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 961/3393 [07:48<20:43,  1.96batch/s, Batch Loss=0.2327, Avg Loss=0.0814, Time Left=21.80 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 961/3393 [07:49<20:43,  1.96batch/s, Batch Loss=0.0074, Avg Loss=0.0813, Time Left=21.79 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 962/3393 [07:49<20:46,  1.95batch/s, Batch Loss=0.0074, Avg Loss=0.0813, Time Left=21.79 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 962/3393 [07:49<20:46,  1.95batch/s, Batch Loss=0.0653, Avg Loss=0.0813, Time Left=21.78 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 963/3393 [07:49<20:42,  1.96batch/s, Batch Loss=0.0653, Avg Loss=0.0813, Time Left=21.78 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 963/3393 [07:50<20:42,  1.96batch/s, Batch Loss=0.0367, Avg Loss=0.0813, Time Left=21.77 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 964/3393 [07:50<20:17,  1.99batch/s, Batch Loss=0.0367, Avg Loss=0.0813, Time Left=21.77 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 964/3393 [07:50<20:17,  1.99batch/s, Batch Loss=0.0075, Avg Loss=0.0812, Time Left=21.76 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 965/3393 [07:50<20:15,  2.00batch/s, Batch Loss=0.0075, Avg Loss=0.0812, Time Left=21.76 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 965/3393 [07:51<20:15,  2.00batch/s, Batch Loss=0.1451, Avg Loss=0.0812, Time Left=21.75 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 966/3393 [07:51<20:16,  2.00batch/s, Batch Loss=0.1451, Avg Loss=0.0812, Time Left=21.75 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 966/3393 [07:51<20:16,  2.00batch/s, Batch Loss=0.0183, Avg Loss=0.0812, Time Left=21.74 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 967/3393 [07:51<20:13,  2.00batch/s, Batch Loss=0.0183, Avg Loss=0.0812, Time Left=21.74 \u001b[A\n",
      "Epoch 3/3 - Training:  28%|▎| 967/3393 [07:52<20:13,  2.00batch/s, Batch Loss=0.2032, Avg Loss=0.0813, Time Left=21.73 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 968/3393 [07:52<20:12,  2.00batch/s, Batch Loss=0.2032, Avg Loss=0.0813, Time Left=21.73 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 968/3393 [07:52<20:12,  2.00batch/s, Batch Loss=0.0047, Avg Loss=0.0812, Time Left=21.72 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 969/3393 [07:52<20:13,  2.00batch/s, Batch Loss=0.0047, Avg Loss=0.0812, Time Left=21.72 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 969/3393 [07:53<20:13,  2.00batch/s, Batch Loss=0.0055, Avg Loss=0.0811, Time Left=21.71 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 970/3393 [07:53<20:48,  1.94batch/s, Batch Loss=0.0055, Avg Loss=0.0811, Time Left=21.71 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 970/3393 [07:53<20:48,  1.94batch/s, Batch Loss=0.0137, Avg Loss=0.0811, Time Left=21.70 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 971/3393 [07:53<20:36,  1.96batch/s, Batch Loss=0.0137, Avg Loss=0.0811, Time Left=21.70 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 971/3393 [07:54<20:36,  1.96batch/s, Batch Loss=0.0197, Avg Loss=0.0810, Time Left=21.70 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 972/3393 [07:54<20:40,  1.95batch/s, Batch Loss=0.0197, Avg Loss=0.0810, Time Left=21.70 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 972/3393 [07:54<20:40,  1.95batch/s, Batch Loss=0.0031, Avg Loss=0.0809, Time Left=21.69 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 973/3393 [07:54<20:21,  1.98batch/s, Batch Loss=0.0031, Avg Loss=0.0809, Time Left=21.69 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 973/3393 [07:55<20:21,  1.98batch/s, Batch Loss=0.0234, Avg Loss=0.0808, Time Left=21.68 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 974/3393 [07:55<20:50,  1.93batch/s, Batch Loss=0.0234, Avg Loss=0.0808, Time Left=21.68 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 974/3393 [07:55<20:50,  1.93batch/s, Batch Loss=0.1568, Avg Loss=0.0809, Time Left=21.67 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 975/3393 [07:55<20:28,  1.97batch/s, Batch Loss=0.1568, Avg Loss=0.0809, Time Left=21.67 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 975/3393 [07:56<20:28,  1.97batch/s, Batch Loss=0.1293, Avg Loss=0.0810, Time Left=21.66 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 976/3393 [07:56<20:42,  1.94batch/s, Batch Loss=0.1293, Avg Loss=0.0810, Time Left=21.66 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 976/3393 [07:56<20:42,  1.94batch/s, Batch Loss=0.0057, Avg Loss=0.0809, Time Left=21.65 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 977/3393 [07:56<20:24,  1.97batch/s, Batch Loss=0.0057, Avg Loss=0.0809, Time Left=21.65 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 977/3393 [07:57<20:24,  1.97batch/s, Batch Loss=0.0574, Avg Loss=0.0809, Time Left=21.64 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 978/3393 [07:57<20:42,  1.94batch/s, Batch Loss=0.0574, Avg Loss=0.0809, Time Left=21.64 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 978/3393 [07:57<20:42,  1.94batch/s, Batch Loss=0.0122, Avg Loss=0.0808, Time Left=21.63 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 979/3393 [07:57<20:32,  1.96batch/s, Batch Loss=0.0122, Avg Loss=0.0808, Time Left=21.63 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 979/3393 [07:58<20:32,  1.96batch/s, Batch Loss=0.0035, Avg Loss=0.0807, Time Left=21.62 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 980/3393 [07:58<20:36,  1.95batch/s, Batch Loss=0.0035, Avg Loss=0.0807, Time Left=21.62 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 980/3393 [07:58<20:36,  1.95batch/s, Batch Loss=0.0616, Avg Loss=0.0807, Time Left=21.61 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 981/3393 [07:58<20:16,  1.98batch/s, Batch Loss=0.0616, Avg Loss=0.0807, Time Left=21.61 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 981/3393 [07:59<20:16,  1.98batch/s, Batch Loss=0.0022, Avg Loss=0.0806, Time Left=21.60 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 982/3393 [07:59<20:34,  1.95batch/s, Batch Loss=0.0022, Avg Loss=0.0806, Time Left=21.60 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 982/3393 [07:59<20:34,  1.95batch/s, Batch Loss=0.0100, Avg Loss=0.0805, Time Left=21.59 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 983/3393 [07:59<20:27,  1.96batch/s, Batch Loss=0.0100, Avg Loss=0.0805, Time Left=21.59 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 983/3393 [08:00<20:27,  1.96batch/s, Batch Loss=0.1175, Avg Loss=0.0806, Time Left=21.58 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 984/3393 [08:00<20:08,  1.99batch/s, Batch Loss=0.1175, Avg Loss=0.0806, Time Left=21.58 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 984/3393 [08:00<20:08,  1.99batch/s, Batch Loss=0.0063, Avg Loss=0.0805, Time Left=21.57 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 985/3393 [08:00<19:55,  2.01batch/s, Batch Loss=0.0063, Avg Loss=0.0805, Time Left=21.57 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 985/3393 [08:01<19:55,  2.01batch/s, Batch Loss=0.0015, Avg Loss=0.0804, Time Left=21.56 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 986/3393 [08:01<19:57,  2.01batch/s, Batch Loss=0.0015, Avg Loss=0.0804, Time Left=21.56 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 986/3393 [08:01<19:57,  2.01batch/s, Batch Loss=0.0605, Avg Loss=0.0804, Time Left=21.55 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 987/3393 [08:01<19:59,  2.01batch/s, Batch Loss=0.0605, Avg Loss=0.0804, Time Left=21.55 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 987/3393 [08:02<19:59,  2.01batch/s, Batch Loss=0.0585, Avg Loss=0.0804, Time Left=21.54 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  29%|▎| 988/3393 [08:02<19:59,  2.00batch/s, Batch Loss=0.0585, Avg Loss=0.0804, Time Left=21.54 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 988/3393 [08:02<19:59,  2.00batch/s, Batch Loss=0.0712, Avg Loss=0.0804, Time Left=21.53 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 989/3393 [08:02<20:34,  1.95batch/s, Batch Loss=0.0712, Avg Loss=0.0804, Time Left=21.53 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 989/3393 [08:03<20:34,  1.95batch/s, Batch Loss=0.0032, Avg Loss=0.0803, Time Left=21.52 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 990/3393 [08:03<20:14,  1.98batch/s, Batch Loss=0.0032, Avg Loss=0.0803, Time Left=21.52 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 990/3393 [08:03<20:14,  1.98batch/s, Batch Loss=0.3897, Avg Loss=0.0806, Time Left=21.52 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 991/3393 [08:03<21:00,  1.91batch/s, Batch Loss=0.3897, Avg Loss=0.0806, Time Left=21.52 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 991/3393 [08:04<21:00,  1.91batch/s, Batch Loss=0.0012, Avg Loss=0.0805, Time Left=21.51 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 992/3393 [08:04<20:51,  1.92batch/s, Batch Loss=0.0012, Avg Loss=0.0805, Time Left=21.51 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 992/3393 [08:04<20:51,  1.92batch/s, Batch Loss=0.0147, Avg Loss=0.0804, Time Left=21.50 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 993/3393 [08:04<20:25,  1.96batch/s, Batch Loss=0.0147, Avg Loss=0.0804, Time Left=21.50 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 993/3393 [08:05<20:25,  1.96batch/s, Batch Loss=0.0224, Avg Loss=0.0804, Time Left=21.49 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 994/3393 [08:05<20:06,  1.99batch/s, Batch Loss=0.0224, Avg Loss=0.0804, Time Left=21.49 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 994/3393 [08:05<20:06,  1.99batch/s, Batch Loss=0.0267, Avg Loss=0.0803, Time Left=21.48 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 995/3393 [08:05<21:01,  1.90batch/s, Batch Loss=0.0267, Avg Loss=0.0803, Time Left=21.48 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 995/3393 [08:06<21:01,  1.90batch/s, Batch Loss=0.1311, Avg Loss=0.0804, Time Left=21.47 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 996/3393 [08:06<20:42,  1.93batch/s, Batch Loss=0.1311, Avg Loss=0.0804, Time Left=21.47 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 996/3393 [08:06<20:42,  1.93batch/s, Batch Loss=0.0421, Avg Loss=0.0803, Time Left=21.46 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 997/3393 [08:06<20:18,  1.97batch/s, Batch Loss=0.0421, Avg Loss=0.0803, Time Left=21.46 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 997/3393 [08:07<20:18,  1.97batch/s, Batch Loss=0.1329, Avg Loss=0.0804, Time Left=21.45 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 998/3393 [08:07<19:48,  2.01batch/s, Batch Loss=0.1329, Avg Loss=0.0804, Time Left=21.45 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 998/3393 [08:07<19:48,  2.01batch/s, Batch Loss=0.1530, Avg Loss=0.0805, Time Left=21.44 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 999/3393 [08:07<20:01,  1.99batch/s, Batch Loss=0.1530, Avg Loss=0.0805, Time Left=21.44 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 999/3393 [08:08<20:01,  1.99batch/s, Batch Loss=0.3612, Avg Loss=0.0808, Time Left=21.43 \u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 1000/3393 [08:08<19:48,  2.01batch/s, Batch Loss=0.3612, Avg Loss=0.0808, Time Left=21.43\u001b[A\n",
      "Epoch 3/3 - Training:  29%|▎| 1000/3393 [08:08<19:48,  2.01batch/s, Batch Loss=0.0043, Avg Loss=0.0807, Time Left=21.42\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1001/3393 [08:08<19:51,  2.01batch/s, Batch Loss=0.0043, Avg Loss=0.0807, Time Left=21.42\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1001/3393 [08:09<19:51,  2.01batch/s, Batch Loss=0.0599, Avg Loss=0.0807, Time Left=21.41\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1002/3393 [08:09<19:42,  2.02batch/s, Batch Loss=0.0599, Avg Loss=0.0807, Time Left=21.41\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1002/3393 [08:09<19:42,  2.02batch/s, Batch Loss=0.0016, Avg Loss=0.0806, Time Left=21.40\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1003/3393 [08:09<19:43,  2.02batch/s, Batch Loss=0.0016, Avg Loss=0.0806, Time Left=21.40\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1003/3393 [08:10<19:43,  2.02batch/s, Batch Loss=0.0587, Avg Loss=0.0806, Time Left=21.39\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1004/3393 [08:10<19:35,  2.03batch/s, Batch Loss=0.0587, Avg Loss=0.0806, Time Left=21.39\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1004/3393 [08:10<19:35,  2.03batch/s, Batch Loss=0.0057, Avg Loss=0.0805, Time Left=21.38\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1005/3393 [08:10<19:40,  2.02batch/s, Batch Loss=0.0057, Avg Loss=0.0805, Time Left=21.38\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1005/3393 [08:11<19:40,  2.02batch/s, Batch Loss=0.1310, Avg Loss=0.0805, Time Left=21.37\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1006/3393 [08:11<20:05,  1.98batch/s, Batch Loss=0.1310, Avg Loss=0.0805, Time Left=21.37\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1006/3393 [08:11<20:05,  1.98batch/s, Batch Loss=0.0094, Avg Loss=0.0805, Time Left=21.36\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1007/3393 [08:11<19:52,  2.00batch/s, Batch Loss=0.0094, Avg Loss=0.0805, Time Left=21.36\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1007/3393 [08:12<19:52,  2.00batch/s, Batch Loss=0.0445, Avg Loss=0.0804, Time Left=21.35\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1008/3393 [08:12<19:52,  2.00batch/s, Batch Loss=0.0445, Avg Loss=0.0804, Time Left=21.35\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1008/3393 [08:12<19:52,  2.00batch/s, Batch Loss=0.0297, Avg Loss=0.0804, Time Left=21.34\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1009/3393 [08:12<19:52,  2.00batch/s, Batch Loss=0.0297, Avg Loss=0.0804, Time Left=21.34\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1009/3393 [08:13<19:52,  2.00batch/s, Batch Loss=0.0024, Avg Loss=0.0803, Time Left=21.33\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1010/3393 [08:13<19:28,  2.04batch/s, Batch Loss=0.0024, Avg Loss=0.0803, Time Left=21.33\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1010/3393 [08:13<19:28,  2.04batch/s, Batch Loss=0.0132, Avg Loss=0.0802, Time Left=21.32\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1011/3393 [08:13<19:34,  2.03batch/s, Batch Loss=0.0132, Avg Loss=0.0802, Time Left=21.32\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1011/3393 [08:14<19:34,  2.03batch/s, Batch Loss=0.0694, Avg Loss=0.0802, Time Left=21.31\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1012/3393 [08:14<19:28,  2.04batch/s, Batch Loss=0.0694, Avg Loss=0.0802, Time Left=21.31\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1012/3393 [08:14<19:28,  2.04batch/s, Batch Loss=0.0092, Avg Loss=0.0801, Time Left=21.30\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1013/3393 [08:14<20:07,  1.97batch/s, Batch Loss=0.0092, Avg Loss=0.0801, Time Left=21.30\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1013/3393 [08:15<20:07,  1.97batch/s, Batch Loss=0.0485, Avg Loss=0.0801, Time Left=21.29\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1014/3393 [08:15<19:53,  1.99batch/s, Batch Loss=0.0485, Avg Loss=0.0801, Time Left=21.29\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1014/3393 [08:15<19:53,  1.99batch/s, Batch Loss=0.0071, Avg Loss=0.0800, Time Left=21.29\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1015/3393 [08:15<20:13,  1.96batch/s, Batch Loss=0.0071, Avg Loss=0.0800, Time Left=21.29\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1015/3393 [08:16<20:13,  1.96batch/s, Batch Loss=0.0844, Avg Loss=0.0800, Time Left=21.28\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1016/3393 [08:16<20:18,  1.95batch/s, Batch Loss=0.0844, Avg Loss=0.0800, Time Left=21.28\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1016/3393 [08:16<20:18,  1.95batch/s, Batch Loss=0.0236, Avg Loss=0.0800, Time Left=21.27\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1017/3393 [08:16<20:09,  1.96batch/s, Batch Loss=0.0236, Avg Loss=0.0800, Time Left=21.27\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1017/3393 [08:17<20:09,  1.96batch/s, Batch Loss=0.0157, Avg Loss=0.0799, Time Left=21.26\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1018/3393 [08:17<19:52,  1.99batch/s, Batch Loss=0.0157, Avg Loss=0.0799, Time Left=21.26\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1018/3393 [08:17<19:52,  1.99batch/s, Batch Loss=0.1903, Avg Loss=0.0800, Time Left=21.25\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1019/3393 [08:17<20:00,  1.98batch/s, Batch Loss=0.1903, Avg Loss=0.0800, Time Left=21.25\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1019/3393 [08:18<20:00,  1.98batch/s, Batch Loss=0.1007, Avg Loss=0.0800, Time Left=21.24\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1020/3393 [08:18<19:57,  1.98batch/s, Batch Loss=0.1007, Avg Loss=0.0800, Time Left=21.24\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1020/3393 [08:18<19:57,  1.98batch/s, Batch Loss=0.0500, Avg Loss=0.0800, Time Left=21.23\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  30%|▎| 1021/3393 [08:18<20:15,  1.95batch/s, Batch Loss=0.0500, Avg Loss=0.0800, Time Left=21.23\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1021/3393 [08:19<20:15,  1.95batch/s, Batch Loss=0.0104, Avg Loss=0.0799, Time Left=21.22\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1022/3393 [08:19<20:08,  1.96batch/s, Batch Loss=0.0104, Avg Loss=0.0799, Time Left=21.22\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1022/3393 [08:19<20:08,  1.96batch/s, Batch Loss=0.0222, Avg Loss=0.0799, Time Left=21.21\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1023/3393 [08:19<20:11,  1.96batch/s, Batch Loss=0.0222, Avg Loss=0.0799, Time Left=21.21\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1023/3393 [08:20<20:11,  1.96batch/s, Batch Loss=0.0588, Avg Loss=0.0798, Time Left=21.20\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1024/3393 [08:20<19:30,  2.02batch/s, Batch Loss=0.0588, Avg Loss=0.0798, Time Left=21.20\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1024/3393 [08:20<19:30,  2.02batch/s, Batch Loss=0.0068, Avg Loss=0.0798, Time Left=21.19\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1025/3393 [08:20<19:43,  2.00batch/s, Batch Loss=0.0068, Avg Loss=0.0798, Time Left=21.19\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1025/3393 [08:21<19:43,  2.00batch/s, Batch Loss=0.0805, Avg Loss=0.0798, Time Left=21.18\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1026/3393 [08:21<19:34,  2.02batch/s, Batch Loss=0.0805, Avg Loss=0.0798, Time Left=21.18\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1026/3393 [08:21<19:34,  2.02batch/s, Batch Loss=0.0066, Avg Loss=0.0797, Time Left=21.17\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1027/3393 [08:21<19:36,  2.01batch/s, Batch Loss=0.0066, Avg Loss=0.0797, Time Left=21.17\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1027/3393 [08:22<19:36,  2.01batch/s, Batch Loss=0.0867, Avg Loss=0.0797, Time Left=21.16\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1028/3393 [08:22<19:47,  1.99batch/s, Batch Loss=0.0867, Avg Loss=0.0797, Time Left=21.16\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1028/3393 [08:22<19:47,  1.99batch/s, Batch Loss=0.0124, Avg Loss=0.0796, Time Left=21.15\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1029/3393 [08:22<19:47,  1.99batch/s, Batch Loss=0.0124, Avg Loss=0.0796, Time Left=21.15\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1029/3393 [08:23<19:47,  1.99batch/s, Batch Loss=0.0727, Avg Loss=0.0796, Time Left=21.14\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1030/3393 [08:23<19:55,  1.98batch/s, Batch Loss=0.0727, Avg Loss=0.0796, Time Left=21.14\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1030/3393 [08:23<19:55,  1.98batch/s, Batch Loss=0.0208, Avg Loss=0.0796, Time Left=21.13\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1031/3393 [08:23<19:51,  1.98batch/s, Batch Loss=0.0208, Avg Loss=0.0796, Time Left=21.13\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1031/3393 [08:24<19:51,  1.98batch/s, Batch Loss=0.0031, Avg Loss=0.0795, Time Left=21.12\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1032/3393 [08:24<20:00,  1.97batch/s, Batch Loss=0.0031, Avg Loss=0.0795, Time Left=21.12\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1032/3393 [08:24<20:00,  1.97batch/s, Batch Loss=0.0261, Avg Loss=0.0794, Time Left=21.11\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1033/3393 [08:24<19:31,  2.01batch/s, Batch Loss=0.0261, Avg Loss=0.0794, Time Left=21.11\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1033/3393 [08:25<19:31,  2.01batch/s, Batch Loss=0.0250, Avg Loss=0.0794, Time Left=21.11\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1034/3393 [08:25<19:44,  1.99batch/s, Batch Loss=0.0250, Avg Loss=0.0794, Time Left=21.11\u001b[A\n",
      "Epoch 3/3 - Training:  30%|▎| 1034/3393 [08:25<19:44,  1.99batch/s, Batch Loss=0.0072, Avg Loss=0.0793, Time Left=21.09\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1035/3393 [08:25<19:31,  2.01batch/s, Batch Loss=0.0072, Avg Loss=0.0793, Time Left=21.09\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1035/3393 [08:26<19:31,  2.01batch/s, Batch Loss=0.0370, Avg Loss=0.0793, Time Left=21.08\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1036/3393 [08:26<19:33,  2.01batch/s, Batch Loss=0.0370, Avg Loss=0.0793, Time Left=21.08\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1036/3393 [08:26<19:33,  2.01batch/s, Batch Loss=0.1497, Avg Loss=0.0793, Time Left=21.08\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1037/3393 [08:26<19:45,  1.99batch/s, Batch Loss=0.1497, Avg Loss=0.0793, Time Left=21.08\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1037/3393 [08:27<19:45,  1.99batch/s, Batch Loss=0.0274, Avg Loss=0.0793, Time Left=21.07\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1038/3393 [08:27<19:44,  1.99batch/s, Batch Loss=0.0274, Avg Loss=0.0793, Time Left=21.07\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1038/3393 [08:27<19:44,  1.99batch/s, Batch Loss=0.0253, Avg Loss=0.0792, Time Left=21.06\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1039/3393 [08:27<19:52,  1.97batch/s, Batch Loss=0.0253, Avg Loss=0.0792, Time Left=21.06\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1039/3393 [08:28<19:52,  1.97batch/s, Batch Loss=0.0200, Avg Loss=0.0792, Time Left=21.05\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1040/3393 [08:28<19:48,  1.98batch/s, Batch Loss=0.0200, Avg Loss=0.0792, Time Left=21.05\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1040/3393 [08:29<19:48,  1.98batch/s, Batch Loss=0.0082, Avg Loss=0.0791, Time Left=21.04\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1041/3393 [08:29<19:46,  1.98batch/s, Batch Loss=0.0082, Avg Loss=0.0791, Time Left=21.04\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1041/3393 [08:29<19:46,  1.98batch/s, Batch Loss=0.0299, Avg Loss=0.0790, Time Left=21.03\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1042/3393 [08:29<19:19,  2.03batch/s, Batch Loss=0.0299, Avg Loss=0.0790, Time Left=21.03\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1042/3393 [08:29<19:19,  2.03batch/s, Batch Loss=0.0099, Avg Loss=0.0790, Time Left=21.02\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1043/3393 [08:29<19:22,  2.02batch/s, Batch Loss=0.0099, Avg Loss=0.0790, Time Left=21.02\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1043/3393 [08:30<19:22,  2.02batch/s, Batch Loss=0.0188, Avg Loss=0.0789, Time Left=21.01\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1044/3393 [08:30<19:15,  2.03batch/s, Batch Loss=0.0188, Avg Loss=0.0789, Time Left=21.01\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1044/3393 [08:30<19:15,  2.03batch/s, Batch Loss=0.0008, Avg Loss=0.0788, Time Left=21.00\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1045/3393 [08:30<19:08,  2.04batch/s, Batch Loss=0.0008, Avg Loss=0.0788, Time Left=21.00\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1045/3393 [08:31<19:08,  2.04batch/s, Batch Loss=0.1974, Avg Loss=0.0789, Time Left=20.99\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1046/3393 [08:31<19:38,  1.99batch/s, Batch Loss=0.1974, Avg Loss=0.0789, Time Left=20.99\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1046/3393 [08:31<19:38,  1.99batch/s, Batch Loss=0.1895, Avg Loss=0.0791, Time Left=20.98\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1047/3393 [08:31<19:37,  1.99batch/s, Batch Loss=0.1895, Avg Loss=0.0791, Time Left=20.98\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1047/3393 [08:32<19:37,  1.99batch/s, Batch Loss=0.0015, Avg Loss=0.0790, Time Left=20.97\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1048/3393 [08:32<19:47,  1.97batch/s, Batch Loss=0.0015, Avg Loss=0.0790, Time Left=20.97\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1048/3393 [08:33<19:47,  1.97batch/s, Batch Loss=0.0071, Avg Loss=0.0789, Time Left=20.96\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1049/3393 [08:33<19:53,  1.96batch/s, Batch Loss=0.0071, Avg Loss=0.0789, Time Left=20.96\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1049/3393 [08:33<19:53,  1.96batch/s, Batch Loss=0.0028, Avg Loss=0.0788, Time Left=20.95\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1050/3393 [08:33<19:35,  1.99batch/s, Batch Loss=0.0028, Avg Loss=0.0788, Time Left=20.95\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1050/3393 [08:33<19:35,  1.99batch/s, Batch Loss=0.0270, Avg Loss=0.0788, Time Left=20.94\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1051/3393 [08:33<19:35,  1.99batch/s, Batch Loss=0.0270, Avg Loss=0.0788, Time Left=20.94\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1051/3393 [08:34<19:35,  1.99batch/s, Batch Loss=0.0031, Avg Loss=0.0787, Time Left=20.93\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1052/3393 [08:34<20:05,  1.94batch/s, Batch Loss=0.0031, Avg Loss=0.0787, Time Left=20.93\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1052/3393 [08:35<20:05,  1.94batch/s, Batch Loss=0.1783, Avg Loss=0.0788, Time Left=20.92\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1053/3393 [08:35<19:57,  1.95batch/s, Batch Loss=0.1783, Avg Loss=0.0788, Time Left=20.92\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1053/3393 [08:35<19:57,  1.95batch/s, Batch Loss=0.1695, Avg Loss=0.0789, Time Left=20.92\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  31%|▎| 1054/3393 [08:35<19:58,  1.95batch/s, Batch Loss=0.1695, Avg Loss=0.0789, Time Left=20.92\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1054/3393 [08:36<19:58,  1.95batch/s, Batch Loss=0.0248, Avg Loss=0.0788, Time Left=20.91\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1055/3393 [08:36<19:51,  1.96batch/s, Batch Loss=0.0248, Avg Loss=0.0788, Time Left=20.91\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1055/3393 [08:36<19:51,  1.96batch/s, Batch Loss=0.0335, Avg Loss=0.0788, Time Left=20.90\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1056/3393 [08:36<19:43,  1.97batch/s, Batch Loss=0.0335, Avg Loss=0.0788, Time Left=20.90\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1056/3393 [08:37<19:43,  1.97batch/s, Batch Loss=0.0044, Avg Loss=0.0787, Time Left=20.89\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1057/3393 [08:37<19:27,  2.00batch/s, Batch Loss=0.0044, Avg Loss=0.0787, Time Left=20.89\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1057/3393 [08:37<19:27,  2.00batch/s, Batch Loss=0.0211, Avg Loss=0.0787, Time Left=20.88\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1058/3393 [08:37<19:28,  2.00batch/s, Batch Loss=0.0211, Avg Loss=0.0787, Time Left=20.88\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1058/3393 [08:38<19:28,  2.00batch/s, Batch Loss=0.0120, Avg Loss=0.0786, Time Left=20.87\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1059/3393 [08:38<19:38,  1.98batch/s, Batch Loss=0.0120, Avg Loss=0.0786, Time Left=20.87\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1059/3393 [08:38<19:38,  1.98batch/s, Batch Loss=0.0788, Avg Loss=0.0786, Time Left=20.86\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1060/3393 [08:38<19:23,  2.01batch/s, Batch Loss=0.0788, Avg Loss=0.0786, Time Left=20.86\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1060/3393 [08:39<19:23,  2.01batch/s, Batch Loss=0.0050, Avg Loss=0.0785, Time Left=20.85\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1061/3393 [08:39<19:12,  2.02batch/s, Batch Loss=0.0050, Avg Loss=0.0785, Time Left=20.85\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1061/3393 [08:39<19:12,  2.02batch/s, Batch Loss=0.1233, Avg Loss=0.0786, Time Left=20.84\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1062/3393 [08:39<19:05,  2.04batch/s, Batch Loss=0.1233, Avg Loss=0.0786, Time Left=20.84\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1062/3393 [08:39<19:05,  2.04batch/s, Batch Loss=0.1723, Avg Loss=0.0787, Time Left=20.83\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1063/3393 [08:40<18:58,  2.05batch/s, Batch Loss=0.1723, Avg Loss=0.0787, Time Left=20.83\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1063/3393 [08:40<18:58,  2.05batch/s, Batch Loss=0.0092, Avg Loss=0.0786, Time Left=20.82\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1064/3393 [08:40<19:06,  2.03batch/s, Batch Loss=0.0092, Avg Loss=0.0786, Time Left=20.82\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1064/3393 [08:40<19:06,  2.03batch/s, Batch Loss=0.0247, Avg Loss=0.0785, Time Left=20.81\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1065/3393 [08:40<19:11,  2.02batch/s, Batch Loss=0.0247, Avg Loss=0.0785, Time Left=20.81\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1065/3393 [08:41<19:11,  2.02batch/s, Batch Loss=0.0702, Avg Loss=0.0785, Time Left=20.80\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1066/3393 [08:41<19:37,  1.98batch/s, Batch Loss=0.0702, Avg Loss=0.0785, Time Left=20.80\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1066/3393 [08:42<19:37,  1.98batch/s, Batch Loss=0.0243, Avg Loss=0.0785, Time Left=20.79\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1067/3393 [08:42<19:22,  2.00batch/s, Batch Loss=0.0243, Avg Loss=0.0785, Time Left=20.79\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1067/3393 [08:42<19:22,  2.00batch/s, Batch Loss=0.0743, Avg Loss=0.0785, Time Left=20.78\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1068/3393 [08:42<19:33,  1.98batch/s, Batch Loss=0.0743, Avg Loss=0.0785, Time Left=20.78\u001b[A\n",
      "Epoch 3/3 - Training:  31%|▎| 1068/3393 [08:43<19:33,  1.98batch/s, Batch Loss=0.1086, Avg Loss=0.0785, Time Left=20.77\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1069/3393 [08:43<19:19,  2.00batch/s, Batch Loss=0.1086, Avg Loss=0.0785, Time Left=20.77\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1069/3393 [08:43<19:19,  2.00batch/s, Batch Loss=0.0165, Avg Loss=0.0784, Time Left=20.76\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1070/3393 [08:43<19:40,  1.97batch/s, Batch Loss=0.0165, Avg Loss=0.0784, Time Left=20.76\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1070/3393 [08:44<19:40,  1.97batch/s, Batch Loss=0.0383, Avg Loss=0.0784, Time Left=20.75\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1071/3393 [08:44<19:36,  1.97batch/s, Batch Loss=0.0383, Avg Loss=0.0784, Time Left=20.75\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1071/3393 [08:44<19:36,  1.97batch/s, Batch Loss=0.1680, Avg Loss=0.0785, Time Left=20.74\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1072/3393 [08:44<19:32,  1.98batch/s, Batch Loss=0.1680, Avg Loss=0.0785, Time Left=20.74\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1072/3393 [08:45<19:32,  1.98batch/s, Batch Loss=0.1520, Avg Loss=0.0786, Time Left=20.73\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1073/3393 [08:45<19:27,  1.99batch/s, Batch Loss=0.1520, Avg Loss=0.0786, Time Left=20.73\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1073/3393 [08:45<19:27,  1.99batch/s, Batch Loss=0.0600, Avg Loss=0.0785, Time Left=20.72\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1074/3393 [08:45<19:25,  1.99batch/s, Batch Loss=0.0600, Avg Loss=0.0785, Time Left=20.72\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1074/3393 [08:46<19:25,  1.99batch/s, Batch Loss=0.1163, Avg Loss=0.0786, Time Left=20.72\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1075/3393 [08:46<19:23,  1.99batch/s, Batch Loss=0.1163, Avg Loss=0.0786, Time Left=20.72\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1075/3393 [08:46<19:23,  1.99batch/s, Batch Loss=0.1011, Avg Loss=0.0786, Time Left=20.71\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1076/3393 [08:46<19:33,  1.97batch/s, Batch Loss=0.1011, Avg Loss=0.0786, Time Left=20.71\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1076/3393 [08:47<19:33,  1.97batch/s, Batch Loss=0.0041, Avg Loss=0.0785, Time Left=20.70\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1077/3393 [08:47<19:16,  2.00batch/s, Batch Loss=0.0041, Avg Loss=0.0785, Time Left=20.70\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1077/3393 [08:47<19:16,  2.00batch/s, Batch Loss=0.0043, Avg Loss=0.0784, Time Left=20.69\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1078/3393 [08:47<19:06,  2.02batch/s, Batch Loss=0.0043, Avg Loss=0.0784, Time Left=20.69\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1078/3393 [08:48<19:06,  2.02batch/s, Batch Loss=0.0670, Avg Loss=0.0784, Time Left=20.68\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1079/3393 [08:48<19:41,  1.96batch/s, Batch Loss=0.0670, Avg Loss=0.0784, Time Left=20.68\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1079/3393 [08:48<19:41,  1.96batch/s, Batch Loss=0.0277, Avg Loss=0.0784, Time Left=20.67\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1080/3393 [08:48<19:25,  1.98batch/s, Batch Loss=0.0277, Avg Loss=0.0784, Time Left=20.67\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1080/3393 [08:49<19:25,  1.98batch/s, Batch Loss=0.0247, Avg Loss=0.0783, Time Left=20.66\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1081/3393 [08:49<19:53,  1.94batch/s, Batch Loss=0.0247, Avg Loss=0.0783, Time Left=20.66\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1081/3393 [08:49<19:53,  1.94batch/s, Batch Loss=0.0130, Avg Loss=0.0783, Time Left=20.65\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1082/3393 [08:49<19:33,  1.97batch/s, Batch Loss=0.0130, Avg Loss=0.0783, Time Left=20.65\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1082/3393 [08:50<19:33,  1.97batch/s, Batch Loss=0.0699, Avg Loss=0.0783, Time Left=20.64\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1083/3393 [08:50<19:27,  1.98batch/s, Batch Loss=0.0699, Avg Loss=0.0783, Time Left=20.64\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1083/3393 [08:50<19:27,  1.98batch/s, Batch Loss=0.4055, Avg Loss=0.0786, Time Left=20.63\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1084/3393 [08:50<19:12,  2.00batch/s, Batch Loss=0.4055, Avg Loss=0.0786, Time Left=20.63\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1084/3393 [08:51<19:12,  2.00batch/s, Batch Loss=0.0011, Avg Loss=0.0785, Time Left=20.62\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1085/3393 [08:51<19:34,  1.97batch/s, Batch Loss=0.0011, Avg Loss=0.0785, Time Left=20.62\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1085/3393 [08:51<19:34,  1.97batch/s, Batch Loss=0.0014, Avg Loss=0.0784, Time Left=20.61\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1086/3393 [08:51<19:28,  1.97batch/s, Batch Loss=0.0014, Avg Loss=0.0784, Time Left=20.61\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1086/3393 [08:52<19:28,  1.97batch/s, Batch Loss=0.1690, Avg Loss=0.0785, Time Left=20.60\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  32%|▎| 1087/3393 [08:52<19:35,  1.96batch/s, Batch Loss=0.1690, Avg Loss=0.0785, Time Left=20.60\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1087/3393 [08:52<19:35,  1.96batch/s, Batch Loss=0.1090, Avg Loss=0.0786, Time Left=20.59\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1088/3393 [08:52<19:28,  1.97batch/s, Batch Loss=0.1090, Avg Loss=0.0786, Time Left=20.59\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1088/3393 [08:53<19:28,  1.97batch/s, Batch Loss=0.0719, Avg Loss=0.0785, Time Left=20.59\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1089/3393 [08:53<19:34,  1.96batch/s, Batch Loss=0.0719, Avg Loss=0.0785, Time Left=20.59\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1089/3393 [08:53<19:34,  1.96batch/s, Batch Loss=0.0084, Avg Loss=0.0785, Time Left=20.58\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1090/3393 [08:53<19:06,  2.01batch/s, Batch Loss=0.0084, Avg Loss=0.0785, Time Left=20.58\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1090/3393 [08:54<19:06,  2.01batch/s, Batch Loss=0.0265, Avg Loss=0.0784, Time Left=20.57\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1091/3393 [08:54<19:06,  2.01batch/s, Batch Loss=0.0265, Avg Loss=0.0784, Time Left=20.57\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1091/3393 [08:54<19:06,  2.01batch/s, Batch Loss=0.0199, Avg Loss=0.0784, Time Left=20.56\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1092/3393 [08:54<18:56,  2.02batch/s, Batch Loss=0.0199, Avg Loss=0.0784, Time Left=20.56\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1092/3393 [08:55<18:56,  2.02batch/s, Batch Loss=0.1009, Avg Loss=0.0784, Time Left=20.55\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1093/3393 [08:55<18:59,  2.02batch/s, Batch Loss=0.1009, Avg Loss=0.0784, Time Left=20.55\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1093/3393 [08:55<18:59,  2.02batch/s, Batch Loss=0.0332, Avg Loss=0.0783, Time Left=20.54\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1094/3393 [08:55<19:24,  1.97batch/s, Batch Loss=0.0332, Avg Loss=0.0783, Time Left=20.54\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1094/3393 [08:56<19:24,  1.97batch/s, Batch Loss=0.0969, Avg Loss=0.0784, Time Left=20.53\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1095/3393 [08:56<19:09,  2.00batch/s, Batch Loss=0.0969, Avg Loss=0.0784, Time Left=20.53\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1095/3393 [08:56<19:09,  2.00batch/s, Batch Loss=0.0275, Avg Loss=0.0783, Time Left=20.52\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1096/3393 [08:56<19:31,  1.96batch/s, Batch Loss=0.0275, Avg Loss=0.0783, Time Left=20.52\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1096/3393 [08:57<19:31,  1.96batch/s, Batch Loss=0.0960, Avg Loss=0.0783, Time Left=20.51\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1097/3393 [08:57<19:24,  1.97batch/s, Batch Loss=0.0960, Avg Loss=0.0783, Time Left=20.51\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1097/3393 [08:57<19:24,  1.97batch/s, Batch Loss=0.2500, Avg Loss=0.0785, Time Left=20.50\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1098/3393 [08:57<19:31,  1.96batch/s, Batch Loss=0.2500, Avg Loss=0.0785, Time Left=20.50\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1098/3393 [08:58<19:31,  1.96batch/s, Batch Loss=0.0284, Avg Loss=0.0784, Time Left=20.49\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1099/3393 [08:58<19:23,  1.97batch/s, Batch Loss=0.0284, Avg Loss=0.0784, Time Left=20.49\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1099/3393 [08:58<19:23,  1.97batch/s, Batch Loss=0.0739, Avg Loss=0.0784, Time Left=20.48\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1100/3393 [08:58<19:40,  1.94batch/s, Batch Loss=0.0739, Avg Loss=0.0784, Time Left=20.48\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1100/3393 [08:59<19:40,  1.94batch/s, Batch Loss=0.0309, Avg Loss=0.0784, Time Left=20.47\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1101/3393 [08:59<19:30,  1.96batch/s, Batch Loss=0.0309, Avg Loss=0.0784, Time Left=20.47\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1101/3393 [08:59<19:30,  1.96batch/s, Batch Loss=0.3350, Avg Loss=0.0786, Time Left=20.47\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1102/3393 [08:59<19:33,  1.95batch/s, Batch Loss=0.3350, Avg Loss=0.0786, Time Left=20.47\u001b[A\n",
      "Epoch 3/3 - Training:  32%|▎| 1102/3393 [09:00<19:33,  1.95batch/s, Batch Loss=0.1102, Avg Loss=0.0787, Time Left=20.46\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1103/3393 [09:00<19:34,  1.95batch/s, Batch Loss=0.1102, Avg Loss=0.0787, Time Left=20.46\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1103/3393 [09:00<19:34,  1.95batch/s, Batch Loss=0.0315, Avg Loss=0.0786, Time Left=20.45\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1104/3393 [09:00<19:29,  1.96batch/s, Batch Loss=0.0315, Avg Loss=0.0786, Time Left=20.45\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1104/3393 [09:01<19:29,  1.96batch/s, Batch Loss=0.0223, Avg Loss=0.0786, Time Left=20.44\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1105/3393 [09:01<19:21,  1.97batch/s, Batch Loss=0.0223, Avg Loss=0.0786, Time Left=20.44\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1105/3393 [09:01<19:21,  1.97batch/s, Batch Loss=0.1979, Avg Loss=0.0787, Time Left=20.43\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1106/3393 [09:01<19:37,  1.94batch/s, Batch Loss=0.1979, Avg Loss=0.0787, Time Left=20.43\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1106/3393 [09:02<19:37,  1.94batch/s, Batch Loss=0.0189, Avg Loss=0.0786, Time Left=20.42\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1107/3393 [09:02<19:28,  1.96batch/s, Batch Loss=0.0189, Avg Loss=0.0786, Time Left=20.42\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1107/3393 [09:02<19:28,  1.96batch/s, Batch Loss=0.1366, Avg Loss=0.0787, Time Left=20.41\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1108/3393 [09:02<19:42,  1.93batch/s, Batch Loss=0.1366, Avg Loss=0.0787, Time Left=20.41\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1108/3393 [09:03<19:42,  1.93batch/s, Batch Loss=0.4431, Avg Loss=0.0790, Time Left=20.40\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1109/3393 [09:03<19:20,  1.97batch/s, Batch Loss=0.4431, Avg Loss=0.0790, Time Left=20.40\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1109/3393 [09:03<19:20,  1.97batch/s, Batch Loss=0.0371, Avg Loss=0.0790, Time Left=20.39\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1110/3393 [09:03<19:35,  1.94batch/s, Batch Loss=0.0371, Avg Loss=0.0790, Time Left=20.39\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1110/3393 [09:04<19:35,  1.94batch/s, Batch Loss=0.0299, Avg Loss=0.0790, Time Left=20.38\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1111/3393 [09:04<19:25,  1.96batch/s, Batch Loss=0.0299, Avg Loss=0.0790, Time Left=20.38\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1111/3393 [09:04<19:25,  1.96batch/s, Batch Loss=0.1409, Avg Loss=0.0790, Time Left=20.38\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1112/3393 [09:04<19:28,  1.95batch/s, Batch Loss=0.1409, Avg Loss=0.0790, Time Left=20.38\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1112/3393 [09:05<19:28,  1.95batch/s, Batch Loss=0.2044, Avg Loss=0.0791, Time Left=20.37\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1113/3393 [09:05<19:11,  1.98batch/s, Batch Loss=0.2044, Avg Loss=0.0791, Time Left=20.37\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1113/3393 [09:05<19:11,  1.98batch/s, Batch Loss=0.0899, Avg Loss=0.0791, Time Left=20.36\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1114/3393 [09:05<19:27,  1.95batch/s, Batch Loss=0.0899, Avg Loss=0.0791, Time Left=20.36\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1114/3393 [09:06<19:27,  1.95batch/s, Batch Loss=0.1492, Avg Loss=0.0792, Time Left=20.35\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1115/3393 [09:06<19:09,  1.98batch/s, Batch Loss=0.1492, Avg Loss=0.0792, Time Left=20.35\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1115/3393 [09:06<19:09,  1.98batch/s, Batch Loss=0.0840, Avg Loss=0.0792, Time Left=20.34\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1116/3393 [09:06<19:05,  1.99batch/s, Batch Loss=0.0840, Avg Loss=0.0792, Time Left=20.34\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1116/3393 [09:07<19:05,  1.99batch/s, Batch Loss=0.0302, Avg Loss=0.0792, Time Left=20.33\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1117/3393 [09:07<18:41,  2.03batch/s, Batch Loss=0.0302, Avg Loss=0.0792, Time Left=20.33\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1117/3393 [09:07<18:41,  2.03batch/s, Batch Loss=0.0983, Avg Loss=0.0792, Time Left=20.32\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1118/3393 [09:07<18:34,  2.04batch/s, Batch Loss=0.0983, Avg Loss=0.0792, Time Left=20.32\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1118/3393 [09:08<18:34,  2.04batch/s, Batch Loss=0.2004, Avg Loss=0.0793, Time Left=20.31\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1119/3393 [09:08<19:02,  1.99batch/s, Batch Loss=0.2004, Avg Loss=0.0793, Time Left=20.31\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1119/3393 [09:08<19:02,  1.99batch/s, Batch Loss=0.1845, Avg Loss=0.0794, Time Left=20.30\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  33%|▎| 1120/3393 [09:08<19:00,  1.99batch/s, Batch Loss=0.1845, Avg Loss=0.0794, Time Left=20.30\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1120/3393 [09:09<19:00,  1.99batch/s, Batch Loss=0.1097, Avg Loss=0.0794, Time Left=20.29\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1121/3393 [09:09<19:09,  1.98batch/s, Batch Loss=0.1097, Avg Loss=0.0794, Time Left=20.29\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1121/3393 [09:09<19:09,  1.98batch/s, Batch Loss=0.0500, Avg Loss=0.0794, Time Left=20.28\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1122/3393 [09:09<19:18,  1.96batch/s, Batch Loss=0.0500, Avg Loss=0.0794, Time Left=20.28\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1122/3393 [09:10<19:18,  1.96batch/s, Batch Loss=0.0838, Avg Loss=0.0794, Time Left=20.27\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1123/3393 [09:10<19:33,  1.93batch/s, Batch Loss=0.0838, Avg Loss=0.0794, Time Left=20.27\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1123/3393 [09:10<19:33,  1.93batch/s, Batch Loss=0.0357, Avg Loss=0.0794, Time Left=20.26\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1124/3393 [09:10<19:21,  1.95batch/s, Batch Loss=0.0357, Avg Loss=0.0794, Time Left=20.26\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1124/3393 [09:11<19:21,  1.95batch/s, Batch Loss=0.1695, Avg Loss=0.0794, Time Left=20.26\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1125/3393 [09:11<19:36,  1.93batch/s, Batch Loss=0.1695, Avg Loss=0.0794, Time Left=20.26\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1125/3393 [09:11<19:36,  1.93batch/s, Batch Loss=0.1124, Avg Loss=0.0795, Time Left=20.25\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1126/3393 [09:11<19:23,  1.95batch/s, Batch Loss=0.1124, Avg Loss=0.0795, Time Left=20.25\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1126/3393 [09:12<19:23,  1.95batch/s, Batch Loss=0.0311, Avg Loss=0.0794, Time Left=20.24\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1127/3393 [09:12<19:24,  1.95batch/s, Batch Loss=0.0311, Avg Loss=0.0794, Time Left=20.24\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1127/3393 [09:12<19:24,  1.95batch/s, Batch Loss=0.0116, Avg Loss=0.0794, Time Left=20.23\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1128/3393 [09:12<19:16,  1.96batch/s, Batch Loss=0.0116, Avg Loss=0.0794, Time Left=20.23\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1128/3393 [09:13<19:16,  1.96batch/s, Batch Loss=0.0247, Avg Loss=0.0793, Time Left=20.22\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1129/3393 [09:13<19:09,  1.97batch/s, Batch Loss=0.0247, Avg Loss=0.0793, Time Left=20.22\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1129/3393 [09:13<19:09,  1.97batch/s, Batch Loss=0.1800, Avg Loss=0.0794, Time Left=20.21\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1130/3393 [09:13<19:03,  1.98batch/s, Batch Loss=0.1800, Avg Loss=0.0794, Time Left=20.21\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1130/3393 [09:14<19:03,  1.98batch/s, Batch Loss=0.3055, Avg Loss=0.0796, Time Left=20.20\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1131/3393 [09:14<19:09,  1.97batch/s, Batch Loss=0.3055, Avg Loss=0.0796, Time Left=20.20\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1131/3393 [09:14<19:09,  1.97batch/s, Batch Loss=0.0186, Avg Loss=0.0796, Time Left=20.19\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1132/3393 [09:14<19:04,  1.98batch/s, Batch Loss=0.0186, Avg Loss=0.0796, Time Left=20.19\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1132/3393 [09:15<19:04,  1.98batch/s, Batch Loss=0.1589, Avg Loss=0.0796, Time Left=20.18\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1133/3393 [09:15<19:10,  1.96batch/s, Batch Loss=0.1589, Avg Loss=0.0796, Time Left=20.18\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1133/3393 [09:15<19:10,  1.96batch/s, Batch Loss=0.0333, Avg Loss=0.0796, Time Left=20.17\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1134/3393 [09:16<19:05,  1.97batch/s, Batch Loss=0.0333, Avg Loss=0.0796, Time Left=20.17\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1134/3393 [09:16<19:05,  1.97batch/s, Batch Loss=0.1189, Avg Loss=0.0796, Time Left=20.16\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1135/3393 [09:16<19:21,  1.94batch/s, Batch Loss=0.1189, Avg Loss=0.0796, Time Left=20.16\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1135/3393 [09:17<19:21,  1.94batch/s, Batch Loss=0.4091, Avg Loss=0.0799, Time Left=20.15\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1136/3393 [09:17<19:03,  1.97batch/s, Batch Loss=0.4091, Avg Loss=0.0799, Time Left=20.15\u001b[A\n",
      "Epoch 3/3 - Training:  33%|▎| 1136/3393 [09:17<19:03,  1.97batch/s, Batch Loss=0.0639, Avg Loss=0.0799, Time Left=20.15\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1137/3393 [09:17<19:29,  1.93batch/s, Batch Loss=0.0639, Avg Loss=0.0799, Time Left=20.15\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1137/3393 [09:18<19:29,  1.93batch/s, Batch Loss=0.0180, Avg Loss=0.0799, Time Left=20.14\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1138/3393 [09:18<19:17,  1.95batch/s, Batch Loss=0.0180, Avg Loss=0.0799, Time Left=20.14\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1138/3393 [09:18<19:17,  1.95batch/s, Batch Loss=0.0324, Avg Loss=0.0798, Time Left=20.13\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1139/3393 [09:18<19:18,  1.95batch/s, Batch Loss=0.0324, Avg Loss=0.0798, Time Left=20.13\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1139/3393 [09:19<19:18,  1.95batch/s, Batch Loss=0.0287, Avg Loss=0.0798, Time Left=20.12\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1140/3393 [09:19<19:14,  1.95batch/s, Batch Loss=0.0287, Avg Loss=0.0798, Time Left=20.12\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1140/3393 [09:19<19:14,  1.95batch/s, Batch Loss=0.0057, Avg Loss=0.0797, Time Left=20.11\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1141/3393 [09:19<19:02,  1.97batch/s, Batch Loss=0.0057, Avg Loss=0.0797, Time Left=20.11\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1141/3393 [09:20<19:02,  1.97batch/s, Batch Loss=0.0559, Avg Loss=0.0797, Time Left=20.10\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1142/3393 [09:20<18:56,  1.98batch/s, Batch Loss=0.0559, Avg Loss=0.0797, Time Left=20.10\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1142/3393 [09:20<18:56,  1.98batch/s, Batch Loss=0.0334, Avg Loss=0.0796, Time Left=20.09\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1143/3393 [09:20<18:53,  1.98batch/s, Batch Loss=0.0334, Avg Loss=0.0796, Time Left=20.09\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1143/3393 [09:21<18:53,  1.98batch/s, Batch Loss=0.0679, Avg Loss=0.0796, Time Left=20.08\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1144/3393 [09:21<18:50,  1.99batch/s, Batch Loss=0.0679, Avg Loss=0.0796, Time Left=20.08\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1144/3393 [09:21<18:50,  1.99batch/s, Batch Loss=0.0780, Avg Loss=0.0796, Time Left=20.07\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1145/3393 [09:21<18:44,  2.00batch/s, Batch Loss=0.0780, Avg Loss=0.0796, Time Left=20.07\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1145/3393 [09:22<18:44,  2.00batch/s, Batch Loss=0.0708, Avg Loss=0.0796, Time Left=20.06\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1146/3393 [09:22<18:47,  1.99batch/s, Batch Loss=0.0708, Avg Loss=0.0796, Time Left=20.06\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1146/3393 [09:22<18:47,  1.99batch/s, Batch Loss=0.0406, Avg Loss=0.0796, Time Left=20.05\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1147/3393 [09:22<18:46,  1.99batch/s, Batch Loss=0.0406, Avg Loss=0.0796, Time Left=20.05\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1147/3393 [09:23<18:46,  1.99batch/s, Batch Loss=0.0126, Avg Loss=0.0795, Time Left=20.05\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1148/3393 [09:23<19:05,  1.96batch/s, Batch Loss=0.0126, Avg Loss=0.0795, Time Left=20.05\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1148/3393 [09:23<19:05,  1.96batch/s, Batch Loss=0.2141, Avg Loss=0.0797, Time Left=20.04\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1149/3393 [09:23<18:49,  1.99batch/s, Batch Loss=0.2141, Avg Loss=0.0797, Time Left=20.04\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1149/3393 [09:24<18:49,  1.99batch/s, Batch Loss=0.0521, Avg Loss=0.0796, Time Left=20.03\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1150/3393 [09:24<19:07,  1.95batch/s, Batch Loss=0.0521, Avg Loss=0.0796, Time Left=20.03\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1150/3393 [09:24<19:07,  1.95batch/s, Batch Loss=0.0161, Avg Loss=0.0796, Time Left=20.02\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1151/3393 [09:24<18:51,  1.98batch/s, Batch Loss=0.0161, Avg Loss=0.0796, Time Left=20.02\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1151/3393 [09:25<18:51,  1.98batch/s, Batch Loss=0.1524, Avg Loss=0.0796, Time Left=20.01\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1152/3393 [09:25<19:07,  1.95batch/s, Batch Loss=0.1524, Avg Loss=0.0796, Time Left=20.01\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1152/3393 [09:25<19:07,  1.95batch/s, Batch Loss=0.2057, Avg Loss=0.0798, Time Left=20.00\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  34%|▎| 1153/3393 [09:25<19:01,  1.96batch/s, Batch Loss=0.2057, Avg Loss=0.0798, Time Left=20.00\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1153/3393 [09:26<19:01,  1.96batch/s, Batch Loss=0.0212, Avg Loss=0.0797, Time Left=19.99\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1154/3393 [09:26<19:07,  1.95batch/s, Batch Loss=0.0212, Avg Loss=0.0797, Time Left=19.99\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1154/3393 [09:26<19:07,  1.95batch/s, Batch Loss=0.4567, Avg Loss=0.0800, Time Left=19.98\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1155/3393 [09:26<18:56,  1.97batch/s, Batch Loss=0.4567, Avg Loss=0.0800, Time Left=19.98\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1155/3393 [09:27<18:56,  1.97batch/s, Batch Loss=0.0376, Avg Loss=0.0800, Time Left=19.97\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1156/3393 [09:27<18:39,  2.00batch/s, Batch Loss=0.0376, Avg Loss=0.0800, Time Left=19.97\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1156/3393 [09:27<18:39,  2.00batch/s, Batch Loss=0.0077, Avg Loss=0.0799, Time Left=19.96\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1157/3393 [09:27<18:29,  2.02batch/s, Batch Loss=0.0077, Avg Loss=0.0799, Time Left=19.96\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1157/3393 [09:28<18:29,  2.02batch/s, Batch Loss=0.1933, Avg Loss=0.0800, Time Left=19.95\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1158/3393 [09:28<18:30,  2.01batch/s, Batch Loss=0.1933, Avg Loss=0.0800, Time Left=19.95\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1158/3393 [09:28<18:30,  2.01batch/s, Batch Loss=0.0624, Avg Loss=0.0800, Time Left=19.94\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1159/3393 [09:28<18:40,  1.99batch/s, Batch Loss=0.0624, Avg Loss=0.0800, Time Left=19.94\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1159/3393 [09:29<18:40,  1.99batch/s, Batch Loss=0.0317, Avg Loss=0.0800, Time Left=19.93\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1160/3393 [09:29<18:30,  2.01batch/s, Batch Loss=0.0317, Avg Loss=0.0800, Time Left=19.93\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1160/3393 [09:29<18:30,  2.01batch/s, Batch Loss=0.0567, Avg Loss=0.0800, Time Left=19.92\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1161/3393 [09:29<18:43,  1.99batch/s, Batch Loss=0.0567, Avg Loss=0.0800, Time Left=19.92\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1161/3393 [09:30<18:43,  1.99batch/s, Batch Loss=0.0093, Avg Loss=0.0799, Time Left=19.92\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1162/3393 [09:30<18:40,  1.99batch/s, Batch Loss=0.0093, Avg Loss=0.0799, Time Left=19.92\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1162/3393 [09:30<18:40,  1.99batch/s, Batch Loss=0.0212, Avg Loss=0.0798, Time Left=19.91\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1163/3393 [09:30<18:38,  1.99batch/s, Batch Loss=0.0212, Avg Loss=0.0798, Time Left=19.91\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1163/3393 [09:31<18:38,  1.99batch/s, Batch Loss=0.1094, Avg Loss=0.0799, Time Left=19.90\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1164/3393 [09:31<18:48,  1.98batch/s, Batch Loss=0.1094, Avg Loss=0.0799, Time Left=19.90\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1164/3393 [09:31<18:48,  1.98batch/s, Batch Loss=0.0216, Avg Loss=0.0798, Time Left=19.89\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1165/3393 [09:31<19:06,  1.94batch/s, Batch Loss=0.0216, Avg Loss=0.0798, Time Left=19.89\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1165/3393 [09:32<19:06,  1.94batch/s, Batch Loss=0.1586, Avg Loss=0.0799, Time Left=19.88\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1166/3393 [09:32<18:56,  1.96batch/s, Batch Loss=0.1586, Avg Loss=0.0799, Time Left=19.88\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1166/3393 [09:32<18:56,  1.96batch/s, Batch Loss=0.0187, Avg Loss=0.0798, Time Left=19.87\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1167/3393 [09:32<18:50,  1.97batch/s, Batch Loss=0.0187, Avg Loss=0.0798, Time Left=19.87\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1167/3393 [09:33<18:50,  1.97batch/s, Batch Loss=0.1245, Avg Loss=0.0799, Time Left=19.86\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1168/3393 [09:33<18:55,  1.96batch/s, Batch Loss=0.1245, Avg Loss=0.0799, Time Left=19.86\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1168/3393 [09:33<18:55,  1.96batch/s, Batch Loss=0.0017, Avg Loss=0.0798, Time Left=19.85\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1169/3393 [09:33<18:45,  1.98batch/s, Batch Loss=0.0017, Avg Loss=0.0798, Time Left=19.85\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1169/3393 [09:34<18:45,  1.98batch/s, Batch Loss=0.0498, Avg Loss=0.0798, Time Left=19.84\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1170/3393 [09:34<18:33,  2.00batch/s, Batch Loss=0.0498, Avg Loss=0.0798, Time Left=19.84\u001b[A\n",
      "Epoch 3/3 - Training:  34%|▎| 1170/3393 [09:34<18:33,  2.00batch/s, Batch Loss=0.0114, Avg Loss=0.0797, Time Left=19.83\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1171/3393 [09:34<18:21,  2.02batch/s, Batch Loss=0.0114, Avg Loss=0.0797, Time Left=19.83\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1171/3393 [09:35<18:21,  2.02batch/s, Batch Loss=0.0493, Avg Loss=0.0797, Time Left=19.82\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1172/3393 [09:35<18:36,  1.99batch/s, Batch Loss=0.0493, Avg Loss=0.0797, Time Left=19.82\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1172/3393 [09:35<18:36,  1.99batch/s, Batch Loss=0.0278, Avg Loss=0.0796, Time Left=19.81\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1173/3393 [09:35<18:21,  2.01batch/s, Batch Loss=0.0278, Avg Loss=0.0796, Time Left=19.81\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1173/3393 [09:36<18:21,  2.01batch/s, Batch Loss=0.2247, Avg Loss=0.0798, Time Left=19.80\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1174/3393 [09:36<18:23,  2.01batch/s, Batch Loss=0.2247, Avg Loss=0.0798, Time Left=19.80\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1174/3393 [09:36<18:23,  2.01batch/s, Batch Loss=0.0107, Avg Loss=0.0797, Time Left=19.79\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1175/3393 [09:36<17:53,  2.07batch/s, Batch Loss=0.0107, Avg Loss=0.0797, Time Left=19.79\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1175/3393 [09:37<17:53,  2.07batch/s, Batch Loss=0.0149, Avg Loss=0.0796, Time Left=19.78\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1176/3393 [09:37<18:02,  2.05batch/s, Batch Loss=0.0149, Avg Loss=0.0796, Time Left=19.78\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1176/3393 [09:37<18:02,  2.05batch/s, Batch Loss=0.0030, Avg Loss=0.0796, Time Left=19.77\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1177/3393 [09:37<18:00,  2.05batch/s, Batch Loss=0.0030, Avg Loss=0.0796, Time Left=19.77\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1177/3393 [09:38<18:00,  2.05batch/s, Batch Loss=0.0035, Avg Loss=0.0795, Time Left=19.76\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1178/3393 [09:38<18:07,  2.04batch/s, Batch Loss=0.0035, Avg Loss=0.0795, Time Left=19.76\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1178/3393 [09:38<18:07,  2.04batch/s, Batch Loss=0.0846, Avg Loss=0.0795, Time Left=19.76\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1179/3393 [09:38<18:23,  2.01batch/s, Batch Loss=0.0846, Avg Loss=0.0795, Time Left=19.76\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1179/3393 [09:39<18:23,  2.01batch/s, Batch Loss=0.0176, Avg Loss=0.0795, Time Left=19.75\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1180/3393 [09:39<18:35,  1.98batch/s, Batch Loss=0.0176, Avg Loss=0.0795, Time Left=19.75\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1180/3393 [09:39<18:35,  1.98batch/s, Batch Loss=0.0937, Avg Loss=0.0795, Time Left=19.74\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1181/3393 [09:39<18:42,  1.97batch/s, Batch Loss=0.0937, Avg Loss=0.0795, Time Left=19.74\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1181/3393 [09:40<18:42,  1.97batch/s, Batch Loss=0.0194, Avg Loss=0.0794, Time Left=19.73\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1182/3393 [09:40<18:50,  1.96batch/s, Batch Loss=0.0194, Avg Loss=0.0794, Time Left=19.73\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1182/3393 [09:40<18:50,  1.96batch/s, Batch Loss=0.0495, Avg Loss=0.0794, Time Left=19.72\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1183/3393 [09:40<18:42,  1.97batch/s, Batch Loss=0.0495, Avg Loss=0.0794, Time Left=19.72\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1183/3393 [09:41<18:42,  1.97batch/s, Batch Loss=0.0525, Avg Loss=0.0794, Time Left=19.71\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1184/3393 [09:41<18:26,  2.00batch/s, Batch Loss=0.0525, Avg Loss=0.0794, Time Left=19.71\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1184/3393 [09:41<18:26,  2.00batch/s, Batch Loss=0.0496, Avg Loss=0.0793, Time Left=19.70\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1185/3393 [09:41<18:45,  1.96batch/s, Batch Loss=0.0496, Avg Loss=0.0793, Time Left=19.70\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1185/3393 [09:42<18:45,  1.96batch/s, Batch Loss=0.1266, Avg Loss=0.0794, Time Left=19.69\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  35%|▎| 1186/3393 [09:42<18:51,  1.95batch/s, Batch Loss=0.1266, Avg Loss=0.0794, Time Left=19.69\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1186/3393 [09:42<18:51,  1.95batch/s, Batch Loss=0.0251, Avg Loss=0.0793, Time Left=19.68\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1187/3393 [09:42<18:31,  1.98batch/s, Batch Loss=0.0251, Avg Loss=0.0793, Time Left=19.68\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1187/3393 [09:43<18:31,  1.98batch/s, Batch Loss=0.0055, Avg Loss=0.0793, Time Left=19.67\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1188/3393 [09:43<18:28,  1.99batch/s, Batch Loss=0.0055, Avg Loss=0.0793, Time Left=19.67\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1188/3393 [09:43<18:28,  1.99batch/s, Batch Loss=0.0275, Avg Loss=0.0792, Time Left=19.66\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1189/3393 [09:43<18:15,  2.01batch/s, Batch Loss=0.0275, Avg Loss=0.0792, Time Left=19.66\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1189/3393 [09:44<18:15,  2.01batch/s, Batch Loss=0.0462, Avg Loss=0.0792, Time Left=19.66\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1190/3393 [09:44<18:27,  1.99batch/s, Batch Loss=0.0462, Avg Loss=0.0792, Time Left=19.66\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1190/3393 [09:44<18:27,  1.99batch/s, Batch Loss=0.0418, Avg Loss=0.0792, Time Left=19.65\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1191/3393 [09:44<18:15,  2.01batch/s, Batch Loss=0.0418, Avg Loss=0.0792, Time Left=19.65\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1191/3393 [09:45<18:15,  2.01batch/s, Batch Loss=0.1905, Avg Loss=0.0793, Time Left=19.64\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1192/3393 [09:45<18:26,  1.99batch/s, Batch Loss=0.1905, Avg Loss=0.0793, Time Left=19.64\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1192/3393 [09:45<18:26,  1.99batch/s, Batch Loss=0.0291, Avg Loss=0.0792, Time Left=19.63\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1193/3393 [09:45<18:25,  1.99batch/s, Batch Loss=0.0291, Avg Loss=0.0792, Time Left=19.63\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1193/3393 [09:46<18:25,  1.99batch/s, Batch Loss=0.0009, Avg Loss=0.0791, Time Left=19.62\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1194/3393 [09:46<18:33,  1.97batch/s, Batch Loss=0.0009, Avg Loss=0.0791, Time Left=19.62\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1194/3393 [09:46<18:33,  1.97batch/s, Batch Loss=0.0249, Avg Loss=0.0791, Time Left=19.61\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1195/3393 [09:46<18:30,  1.98batch/s, Batch Loss=0.0249, Avg Loss=0.0791, Time Left=19.61\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1195/3393 [09:47<18:30,  1.98batch/s, Batch Loss=0.0329, Avg Loss=0.0791, Time Left=19.60\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1196/3393 [09:47<18:47,  1.95batch/s, Batch Loss=0.0329, Avg Loss=0.0791, Time Left=19.60\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1196/3393 [09:47<18:47,  1.95batch/s, Batch Loss=0.0093, Avg Loss=0.0790, Time Left=19.59\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1197/3393 [09:47<18:39,  1.96batch/s, Batch Loss=0.0093, Avg Loss=0.0790, Time Left=19.59\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1197/3393 [09:48<18:39,  1.96batch/s, Batch Loss=0.1076, Avg Loss=0.0790, Time Left=19.58\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1198/3393 [09:48<18:53,  1.94batch/s, Batch Loss=0.1076, Avg Loss=0.0790, Time Left=19.58\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1198/3393 [09:48<18:53,  1.94batch/s, Batch Loss=0.0632, Avg Loss=0.0790, Time Left=19.57\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1199/3393 [09:48<18:33,  1.97batch/s, Batch Loss=0.0632, Avg Loss=0.0790, Time Left=19.57\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1199/3393 [09:49<18:33,  1.97batch/s, Batch Loss=0.0009, Avg Loss=0.0789, Time Left=19.57\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1200/3393 [09:49<18:48,  1.94batch/s, Batch Loss=0.0009, Avg Loss=0.0789, Time Left=19.57\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1200/3393 [09:49<18:48,  1.94batch/s, Batch Loss=0.0221, Avg Loss=0.0789, Time Left=19.56\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1201/3393 [09:49<18:45,  1.95batch/s, Batch Loss=0.0221, Avg Loss=0.0789, Time Left=19.56\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1201/3393 [09:50<18:45,  1.95batch/s, Batch Loss=0.0060, Avg Loss=0.0788, Time Left=19.55\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1202/3393 [09:50<18:20,  1.99batch/s, Batch Loss=0.0060, Avg Loss=0.0788, Time Left=19.55\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1202/3393 [09:50<18:20,  1.99batch/s, Batch Loss=0.0011, Avg Loss=0.0788, Time Left=19.54\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1203/3393 [09:50<18:30,  1.97batch/s, Batch Loss=0.0011, Avg Loss=0.0788, Time Left=19.54\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1203/3393 [09:51<18:30,  1.97batch/s, Batch Loss=0.0407, Avg Loss=0.0787, Time Left=19.53\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1204/3393 [09:51<18:33,  1.97batch/s, Batch Loss=0.0407, Avg Loss=0.0787, Time Left=19.53\u001b[A\n",
      "Epoch 3/3 - Training:  35%|▎| 1204/3393 [09:51<18:33,  1.97batch/s, Batch Loss=0.0472, Avg Loss=0.0787, Time Left=19.52\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1205/3393 [09:51<18:18,  1.99batch/s, Batch Loss=0.0472, Avg Loss=0.0787, Time Left=19.52\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1205/3393 [09:52<18:18,  1.99batch/s, Batch Loss=0.1422, Avg Loss=0.0787, Time Left=19.51\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1206/3393 [09:52<18:47,  1.94batch/s, Batch Loss=0.1422, Avg Loss=0.0787, Time Left=19.51\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1206/3393 [09:52<18:47,  1.94batch/s, Batch Loss=0.2430, Avg Loss=0.0789, Time Left=19.50\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1207/3393 [09:52<18:38,  1.96batch/s, Batch Loss=0.2430, Avg Loss=0.0789, Time Left=19.50\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1207/3393 [09:53<18:38,  1.96batch/s, Batch Loss=0.0088, Avg Loss=0.0788, Time Left=19.49\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1208/3393 [09:53<18:31,  1.97batch/s, Batch Loss=0.0088, Avg Loss=0.0788, Time Left=19.49\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1208/3393 [09:53<18:31,  1.97batch/s, Batch Loss=0.0307, Avg Loss=0.0788, Time Left=19.48\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1209/3393 [09:53<18:14,  2.00batch/s, Batch Loss=0.0307, Avg Loss=0.0788, Time Left=19.48\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1209/3393 [09:54<18:14,  2.00batch/s, Batch Loss=0.0075, Avg Loss=0.0787, Time Left=19.47\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1210/3393 [09:54<18:03,  2.02batch/s, Batch Loss=0.0075, Avg Loss=0.0787, Time Left=19.47\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1210/3393 [09:54<18:03,  2.02batch/s, Batch Loss=0.0274, Avg Loss=0.0787, Time Left=19.46\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1211/3393 [09:54<17:54,  2.03batch/s, Batch Loss=0.0274, Avg Loss=0.0787, Time Left=19.46\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1211/3393 [09:55<17:54,  2.03batch/s, Batch Loss=0.0387, Avg Loss=0.0786, Time Left=19.45\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1212/3393 [09:55<17:48,  2.04batch/s, Batch Loss=0.0387, Avg Loss=0.0786, Time Left=19.45\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1212/3393 [09:55<17:48,  2.04batch/s, Batch Loss=0.0009, Avg Loss=0.0786, Time Left=19.45\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1213/3393 [09:55<18:14,  1.99batch/s, Batch Loss=0.0009, Avg Loss=0.0786, Time Left=19.45\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1213/3393 [09:56<18:14,  1.99batch/s, Batch Loss=0.0017, Avg Loss=0.0785, Time Left=19.44\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1214/3393 [09:56<18:14,  1.99batch/s, Batch Loss=0.0017, Avg Loss=0.0785, Time Left=19.44\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1214/3393 [09:56<18:14,  1.99batch/s, Batch Loss=0.0009, Avg Loss=0.0784, Time Left=19.43\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1215/3393 [09:56<18:22,  1.98batch/s, Batch Loss=0.0009, Avg Loss=0.0784, Time Left=19.43\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1215/3393 [09:57<18:22,  1.98batch/s, Batch Loss=0.0661, Avg Loss=0.0784, Time Left=19.42\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1216/3393 [09:57<18:18,  1.98batch/s, Batch Loss=0.0661, Avg Loss=0.0784, Time Left=19.42\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1216/3393 [09:57<18:18,  1.98batch/s, Batch Loss=0.0236, Avg Loss=0.0784, Time Left=19.41\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1217/3393 [09:57<18:46,  1.93batch/s, Batch Loss=0.0236, Avg Loss=0.0784, Time Left=19.41\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1217/3393 [09:58<18:46,  1.93batch/s, Batch Loss=0.1489, Avg Loss=0.0784, Time Left=19.40\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1218/3393 [09:58<18:15,  1.99batch/s, Batch Loss=0.1489, Avg Loss=0.0784, Time Left=19.40\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1218/3393 [09:58<18:15,  1.99batch/s, Batch Loss=0.0034, Avg Loss=0.0784, Time Left=19.39\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  36%|▎| 1219/3393 [09:58<18:42,  1.94batch/s, Batch Loss=0.0034, Avg Loss=0.0784, Time Left=19.39\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1219/3393 [09:59<18:42,  1.94batch/s, Batch Loss=0.1692, Avg Loss=0.0785, Time Left=19.38\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1220/3393 [09:59<18:22,  1.97batch/s, Batch Loss=0.1692, Avg Loss=0.0785, Time Left=19.38\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1220/3393 [09:59<18:22,  1.97batch/s, Batch Loss=0.0671, Avg Loss=0.0785, Time Left=19.37\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1221/3393 [09:59<18:38,  1.94batch/s, Batch Loss=0.0671, Avg Loss=0.0785, Time Left=19.37\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1221/3393 [10:00<18:38,  1.94batch/s, Batch Loss=0.1384, Avg Loss=0.0785, Time Left=19.36\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1222/3393 [10:00<18:18,  1.98batch/s, Batch Loss=0.1384, Avg Loss=0.0785, Time Left=19.36\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1222/3393 [10:00<18:18,  1.98batch/s, Batch Loss=0.0283, Avg Loss=0.0785, Time Left=19.36\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1223/3393 [10:00<18:24,  1.96batch/s, Batch Loss=0.0283, Avg Loss=0.0785, Time Left=19.36\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1223/3393 [10:01<18:24,  1.96batch/s, Batch Loss=0.0544, Avg Loss=0.0784, Time Left=19.35\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1224/3393 [10:01<18:08,  1.99batch/s, Batch Loss=0.0544, Avg Loss=0.0784, Time Left=19.35\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1224/3393 [10:01<18:08,  1.99batch/s, Batch Loss=0.0371, Avg Loss=0.0784, Time Left=19.34\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1225/3393 [10:01<17:55,  2.01batch/s, Batch Loss=0.0371, Avg Loss=0.0784, Time Left=19.34\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1225/3393 [10:02<17:55,  2.01batch/s, Batch Loss=0.0079, Avg Loss=0.0783, Time Left=19.33\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1226/3393 [10:02<17:58,  2.01batch/s, Batch Loss=0.0079, Avg Loss=0.0783, Time Left=19.33\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1226/3393 [10:02<17:58,  2.01batch/s, Batch Loss=0.0592, Avg Loss=0.0783, Time Left=19.32\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1227/3393 [10:02<17:48,  2.03batch/s, Batch Loss=0.0592, Avg Loss=0.0783, Time Left=19.32\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1227/3393 [10:03<17:48,  2.03batch/s, Batch Loss=0.0514, Avg Loss=0.0783, Time Left=19.31\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1228/3393 [10:03<18:03,  2.00batch/s, Batch Loss=0.0514, Avg Loss=0.0783, Time Left=19.31\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1228/3393 [10:04<18:03,  2.00batch/s, Batch Loss=0.0038, Avg Loss=0.0782, Time Left=19.30\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1229/3393 [10:04<19:33,  1.84batch/s, Batch Loss=0.0038, Avg Loss=0.0782, Time Left=19.30\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1229/3393 [10:04<19:33,  1.84batch/s, Batch Loss=0.2349, Avg Loss=0.0784, Time Left=19.29\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1230/3393 [10:04<18:41,  1.93batch/s, Batch Loss=0.2349, Avg Loss=0.0784, Time Left=19.29\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1230/3393 [10:05<18:41,  1.93batch/s, Batch Loss=0.0715, Avg Loss=0.0784, Time Left=19.29\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1231/3393 [10:05<18:58,  1.90batch/s, Batch Loss=0.0715, Avg Loss=0.0784, Time Left=19.29\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1231/3393 [10:05<18:58,  1.90batch/s, Batch Loss=0.1538, Avg Loss=0.0784, Time Left=19.28\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1232/3393 [10:05<18:32,  1.94batch/s, Batch Loss=0.1538, Avg Loss=0.0784, Time Left=19.28\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1232/3393 [10:06<18:32,  1.94batch/s, Batch Loss=0.1425, Avg Loss=0.0785, Time Left=19.27\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1233/3393 [10:06<19:28,  1.85batch/s, Batch Loss=0.1425, Avg Loss=0.0785, Time Left=19.27\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1233/3393 [10:06<19:28,  1.85batch/s, Batch Loss=0.0289, Avg Loss=0.0784, Time Left=19.26\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1234/3393 [10:06<18:50,  1.91batch/s, Batch Loss=0.0289, Avg Loss=0.0784, Time Left=19.26\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1234/3393 [10:07<18:50,  1.91batch/s, Batch Loss=0.0665, Avg Loss=0.0784, Time Left=19.25\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1235/3393 [10:07<18:24,  1.95batch/s, Batch Loss=0.0665, Avg Loss=0.0784, Time Left=19.25\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1235/3393 [10:07<18:24,  1.95batch/s, Batch Loss=0.0046, Avg Loss=0.0784, Time Left=19.24\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1236/3393 [10:07<18:05,  1.99batch/s, Batch Loss=0.0046, Avg Loss=0.0784, Time Left=19.24\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1236/3393 [10:08<18:05,  1.99batch/s, Batch Loss=0.0718, Avg Loss=0.0784, Time Left=19.23\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1237/3393 [10:08<18:12,  1.97batch/s, Batch Loss=0.0718, Avg Loss=0.0784, Time Left=19.23\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1237/3393 [10:08<18:12,  1.97batch/s, Batch Loss=0.0208, Avg Loss=0.0783, Time Left=19.22\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1238/3393 [10:08<18:09,  1.98batch/s, Batch Loss=0.0208, Avg Loss=0.0783, Time Left=19.22\u001b[A\n",
      "Epoch 3/3 - Training:  36%|▎| 1238/3393 [10:09<18:09,  1.98batch/s, Batch Loss=0.0163, Avg Loss=0.0783, Time Left=19.21\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1239/3393 [10:09<18:37,  1.93batch/s, Batch Loss=0.0163, Avg Loss=0.0783, Time Left=19.21\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1239/3393 [10:09<18:37,  1.93batch/s, Batch Loss=0.0795, Avg Loss=0.0783, Time Left=19.20\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1240/3393 [10:09<18:13,  1.97batch/s, Batch Loss=0.0795, Avg Loss=0.0783, Time Left=19.20\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1240/3393 [10:10<18:13,  1.97batch/s, Batch Loss=0.0517, Avg Loss=0.0782, Time Left=19.20\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1241/3393 [10:10<18:17,  1.96batch/s, Batch Loss=0.0517, Avg Loss=0.0782, Time Left=19.20\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1241/3393 [10:10<18:17,  1.96batch/s, Batch Loss=0.0245, Avg Loss=0.0782, Time Left=19.19\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1242/3393 [10:10<18:12,  1.97batch/s, Batch Loss=0.0245, Avg Loss=0.0782, Time Left=19.19\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1242/3393 [10:11<18:12,  1.97batch/s, Batch Loss=0.0126, Avg Loss=0.0781, Time Left=19.18\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1243/3393 [10:11<18:27,  1.94batch/s, Batch Loss=0.0126, Avg Loss=0.0781, Time Left=19.18\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1243/3393 [10:11<18:27,  1.94batch/s, Batch Loss=0.0685, Avg Loss=0.0781, Time Left=19.17\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1244/3393 [10:11<18:21,  1.95batch/s, Batch Loss=0.0685, Avg Loss=0.0781, Time Left=19.17\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1244/3393 [10:12<18:21,  1.95batch/s, Batch Loss=0.0161, Avg Loss=0.0781, Time Left=19.16\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1245/3393 [10:12<18:28,  1.94batch/s, Batch Loss=0.0161, Avg Loss=0.0781, Time Left=19.16\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1245/3393 [10:12<18:28,  1.94batch/s, Batch Loss=0.0159, Avg Loss=0.0780, Time Left=19.15\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1246/3393 [10:12<18:10,  1.97batch/s, Batch Loss=0.0159, Avg Loss=0.0780, Time Left=19.15\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1246/3393 [10:13<18:10,  1.97batch/s, Batch Loss=0.3199, Avg Loss=0.0782, Time Left=19.14\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1247/3393 [10:13<18:14,  1.96batch/s, Batch Loss=0.3199, Avg Loss=0.0782, Time Left=19.14\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1247/3393 [10:13<18:14,  1.96batch/s, Batch Loss=0.0050, Avg Loss=0.0782, Time Left=19.13\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1248/3393 [10:13<18:08,  1.97batch/s, Batch Loss=0.0050, Avg Loss=0.0782, Time Left=19.13\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1248/3393 [10:14<18:08,  1.97batch/s, Batch Loss=0.0443, Avg Loss=0.0781, Time Left=19.13\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1249/3393 [10:14<18:33,  1.92batch/s, Batch Loss=0.0443, Avg Loss=0.0781, Time Left=19.13\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1249/3393 [10:14<18:33,  1.92batch/s, Batch Loss=0.0025, Avg Loss=0.0781, Time Left=19.12\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1250/3393 [10:14<17:52,  2.00batch/s, Batch Loss=0.0025, Avg Loss=0.0781, Time Left=19.12\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1250/3393 [10:15<17:52,  2.00batch/s, Batch Loss=0.0009, Avg Loss=0.0780, Time Left=19.11\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1251/3393 [10:15<17:50,  2.00batch/s, Batch Loss=0.0009, Avg Loss=0.0780, Time Left=19.11\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1251/3393 [10:15<17:50,  2.00batch/s, Batch Loss=0.1020, Avg Loss=0.0780, Time Left=19.10\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  37%|▎| 1252/3393 [10:15<17:40,  2.02batch/s, Batch Loss=0.1020, Avg Loss=0.0780, Time Left=19.10\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1252/3393 [10:16<17:40,  2.02batch/s, Batch Loss=0.0555, Avg Loss=0.0780, Time Left=19.09\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1253/3393 [10:16<17:42,  2.01batch/s, Batch Loss=0.0555, Avg Loss=0.0780, Time Left=19.09\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1253/3393 [10:16<17:42,  2.01batch/s, Batch Loss=0.1169, Avg Loss=0.0780, Time Left=19.08\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1254/3393 [10:16<17:44,  2.01batch/s, Batch Loss=0.1169, Avg Loss=0.0780, Time Left=19.08\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1254/3393 [10:17<17:44,  2.01batch/s, Batch Loss=0.0091, Avg Loss=0.0780, Time Left=19.07\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1255/3393 [10:17<17:46,  2.00batch/s, Batch Loss=0.0091, Avg Loss=0.0780, Time Left=19.07\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1255/3393 [10:17<17:46,  2.00batch/s, Batch Loss=0.0198, Avg Loss=0.0779, Time Left=19.06\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1256/3393 [10:17<18:06,  1.97batch/s, Batch Loss=0.0198, Avg Loss=0.0779, Time Left=19.06\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1256/3393 [10:18<18:06,  1.97batch/s, Batch Loss=0.0007, Avg Loss=0.0779, Time Left=19.05\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1257/3393 [10:18<18:02,  1.97batch/s, Batch Loss=0.0007, Avg Loss=0.0779, Time Left=19.05\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1257/3393 [10:18<18:02,  1.97batch/s, Batch Loss=0.0033, Avg Loss=0.0778, Time Left=19.04\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1258/3393 [10:18<18:07,  1.96batch/s, Batch Loss=0.0033, Avg Loss=0.0778, Time Left=19.04\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1258/3393 [10:19<18:07,  1.96batch/s, Batch Loss=0.0584, Avg Loss=0.0778, Time Left=19.03\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1259/3393 [10:19<18:01,  1.97batch/s, Batch Loss=0.0584, Avg Loss=0.0778, Time Left=19.03\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1259/3393 [10:19<18:01,  1.97batch/s, Batch Loss=0.0223, Avg Loss=0.0777, Time Left=19.02\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1260/3393 [10:19<17:36,  2.02batch/s, Batch Loss=0.0223, Avg Loss=0.0777, Time Left=19.02\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1260/3393 [10:20<17:36,  2.02batch/s, Batch Loss=0.5192, Avg Loss=0.0781, Time Left=19.01\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1261/3393 [10:20<17:29,  2.03batch/s, Batch Loss=0.5192, Avg Loss=0.0781, Time Left=19.01\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1261/3393 [10:20<17:29,  2.03batch/s, Batch Loss=0.0663, Avg Loss=0.0781, Time Left=19.00\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1262/3393 [10:20<17:33,  2.02batch/s, Batch Loss=0.0663, Avg Loss=0.0781, Time Left=19.00\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1262/3393 [10:21<17:33,  2.02batch/s, Batch Loss=0.0008, Avg Loss=0.0780, Time Left=18.99\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1263/3393 [10:21<17:35,  2.02batch/s, Batch Loss=0.0008, Avg Loss=0.0780, Time Left=18.99\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1263/3393 [10:21<17:35,  2.02batch/s, Batch Loss=0.4235, Avg Loss=0.0783, Time Left=18.98\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1264/3393 [10:21<17:08,  2.07batch/s, Batch Loss=0.4235, Avg Loss=0.0783, Time Left=18.98\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1264/3393 [10:22<17:08,  2.07batch/s, Batch Loss=0.0283, Avg Loss=0.0783, Time Left=18.97\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1265/3393 [10:22<17:17,  2.05batch/s, Batch Loss=0.0283, Avg Loss=0.0783, Time Left=18.97\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1265/3393 [10:22<17:17,  2.05batch/s, Batch Loss=0.0022, Avg Loss=0.0782, Time Left=18.97\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1266/3393 [10:22<17:45,  2.00batch/s, Batch Loss=0.0022, Avg Loss=0.0782, Time Left=18.97\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1266/3393 [10:23<17:45,  2.00batch/s, Batch Loss=0.0024, Avg Loss=0.0782, Time Left=18.96\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1267/3393 [10:23<17:34,  2.02batch/s, Batch Loss=0.0024, Avg Loss=0.0782, Time Left=18.96\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1267/3393 [10:23<17:34,  2.02batch/s, Batch Loss=0.0119, Avg Loss=0.0781, Time Left=18.95\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1268/3393 [10:23<17:58,  1.97batch/s, Batch Loss=0.0119, Avg Loss=0.0781, Time Left=18.95\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1268/3393 [10:24<17:58,  1.97batch/s, Batch Loss=0.1231, Avg Loss=0.0781, Time Left=18.94\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1269/3393 [10:24<17:43,  2.00batch/s, Batch Loss=0.1231, Avg Loss=0.0781, Time Left=18.94\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1269/3393 [10:24<17:43,  2.00batch/s, Batch Loss=0.0316, Avg Loss=0.0781, Time Left=18.93\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1270/3393 [10:24<17:42,  2.00batch/s, Batch Loss=0.0316, Avg Loss=0.0781, Time Left=18.93\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1270/3393 [10:25<17:42,  2.00batch/s, Batch Loss=0.2154, Avg Loss=0.0782, Time Left=18.92\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1271/3393 [10:25<17:41,  2.00batch/s, Batch Loss=0.2154, Avg Loss=0.0782, Time Left=18.92\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1271/3393 [10:25<17:41,  2.00batch/s, Batch Loss=0.4112, Avg Loss=0.0785, Time Left=18.91\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1272/3393 [10:25<17:21,  2.04batch/s, Batch Loss=0.4112, Avg Loss=0.0785, Time Left=18.91\u001b[A\n",
      "Epoch 3/3 - Training:  37%|▎| 1272/3393 [10:26<17:21,  2.04batch/s, Batch Loss=0.0019, Avg Loss=0.0784, Time Left=18.90\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1273/3393 [10:26<17:16,  2.05batch/s, Batch Loss=0.0019, Avg Loss=0.0784, Time Left=18.90\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1273/3393 [10:26<17:16,  2.05batch/s, Batch Loss=0.0028, Avg Loss=0.0784, Time Left=18.89\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1274/3393 [10:26<17:12,  2.05batch/s, Batch Loss=0.0028, Avg Loss=0.0784, Time Left=18.89\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1274/3393 [10:27<17:12,  2.05batch/s, Batch Loss=0.0014, Avg Loss=0.0783, Time Left=18.88\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1275/3393 [10:27<17:29,  2.02batch/s, Batch Loss=0.0014, Avg Loss=0.0783, Time Left=18.88\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1275/3393 [10:27<17:29,  2.02batch/s, Batch Loss=0.2034, Avg Loss=0.0784, Time Left=18.87\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1276/3393 [10:27<17:22,  2.03batch/s, Batch Loss=0.2034, Avg Loss=0.0784, Time Left=18.87\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1276/3393 [10:28<17:22,  2.03batch/s, Batch Loss=0.2172, Avg Loss=0.0785, Time Left=18.86\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1277/3393 [10:28<18:02,  1.95batch/s, Batch Loss=0.2172, Avg Loss=0.0785, Time Left=18.86\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1277/3393 [10:28<18:02,  1.95batch/s, Batch Loss=0.2364, Avg Loss=0.0787, Time Left=18.86\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1278/3393 [10:28<17:50,  1.98batch/s, Batch Loss=0.2364, Avg Loss=0.0787, Time Left=18.86\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1278/3393 [10:29<17:50,  1.98batch/s, Batch Loss=0.2014, Avg Loss=0.0788, Time Left=18.85\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1279/3393 [10:29<17:56,  1.96batch/s, Batch Loss=0.2014, Avg Loss=0.0788, Time Left=18.85\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1279/3393 [10:29<17:56,  1.96batch/s, Batch Loss=0.2719, Avg Loss=0.0789, Time Left=18.84\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1280/3393 [10:29<17:41,  1.99batch/s, Batch Loss=0.2719, Avg Loss=0.0789, Time Left=18.84\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1280/3393 [10:30<17:41,  1.99batch/s, Batch Loss=0.1113, Avg Loss=0.0789, Time Left=18.83\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1281/3393 [10:30<17:50,  1.97batch/s, Batch Loss=0.1113, Avg Loss=0.0789, Time Left=18.83\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1281/3393 [10:30<17:50,  1.97batch/s, Batch Loss=0.0918, Avg Loss=0.0790, Time Left=18.82\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1282/3393 [10:30<17:34,  2.00batch/s, Batch Loss=0.0918, Avg Loss=0.0790, Time Left=18.82\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1282/3393 [10:31<17:34,  2.00batch/s, Batch Loss=0.0306, Avg Loss=0.0789, Time Left=18.81\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1283/3393 [10:31<17:33,  2.00batch/s, Batch Loss=0.0306, Avg Loss=0.0789, Time Left=18.81\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1283/3393 [10:31<17:33,  2.00batch/s, Batch Loss=0.1000, Avg Loss=0.0789, Time Left=18.80\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1284/3393 [10:31<17:33,  2.00batch/s, Batch Loss=0.1000, Avg Loss=0.0789, Time Left=18.80\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1284/3393 [10:32<17:33,  2.00batch/s, Batch Loss=0.0473, Avg Loss=0.0789, Time Left=18.79\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  38%|▍| 1285/3393 [10:32<17:37,  1.99batch/s, Batch Loss=0.0473, Avg Loss=0.0789, Time Left=18.79\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1285/3393 [10:32<17:37,  1.99batch/s, Batch Loss=0.0696, Avg Loss=0.0789, Time Left=18.78\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1286/3393 [10:32<17:43,  1.98batch/s, Batch Loss=0.0696, Avg Loss=0.0789, Time Left=18.78\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1286/3393 [10:33<17:43,  1.98batch/s, Batch Loss=0.1522, Avg Loss=0.0790, Time Left=18.77\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1287/3393 [10:33<17:40,  1.99batch/s, Batch Loss=0.1522, Avg Loss=0.0790, Time Left=18.77\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1287/3393 [10:33<17:40,  1.99batch/s, Batch Loss=0.2421, Avg Loss=0.0791, Time Left=18.76\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1288/3393 [10:33<17:37,  1.99batch/s, Batch Loss=0.2421, Avg Loss=0.0791, Time Left=18.76\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1288/3393 [10:34<17:37,  1.99batch/s, Batch Loss=0.1363, Avg Loss=0.0791, Time Left=18.76\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1289/3393 [10:34<17:57,  1.95batch/s, Batch Loss=0.1363, Avg Loss=0.0791, Time Left=18.76\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1289/3393 [10:34<17:57,  1.95batch/s, Batch Loss=0.0612, Avg Loss=0.0791, Time Left=18.75\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1290/3393 [10:34<17:58,  1.95batch/s, Batch Loss=0.0612, Avg Loss=0.0791, Time Left=18.75\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1290/3393 [10:35<17:58,  1.95batch/s, Batch Loss=0.0784, Avg Loss=0.0791, Time Left=18.74\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1291/3393 [10:35<17:41,  1.98batch/s, Batch Loss=0.0784, Avg Loss=0.0791, Time Left=18.74\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1291/3393 [10:35<17:41,  1.98batch/s, Batch Loss=0.0541, Avg Loss=0.0791, Time Left=18.73\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1292/3393 [10:35<17:57,  1.95batch/s, Batch Loss=0.0541, Avg Loss=0.0791, Time Left=18.73\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1292/3393 [10:36<17:57,  1.95batch/s, Batch Loss=0.1245, Avg Loss=0.0791, Time Left=18.72\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1293/3393 [10:36<17:39,  1.98batch/s, Batch Loss=0.1245, Avg Loss=0.0791, Time Left=18.72\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1293/3393 [10:36<17:39,  1.98batch/s, Batch Loss=0.0446, Avg Loss=0.0791, Time Left=18.71\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1294/3393 [10:36<17:56,  1.95batch/s, Batch Loss=0.0446, Avg Loss=0.0791, Time Left=18.71\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1294/3393 [10:37<17:56,  1.95batch/s, Batch Loss=0.0565, Avg Loss=0.0791, Time Left=18.70\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1295/3393 [10:37<17:49,  1.96batch/s, Batch Loss=0.0565, Avg Loss=0.0791, Time Left=18.70\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1295/3393 [10:37<17:49,  1.96batch/s, Batch Loss=0.0085, Avg Loss=0.0790, Time Left=18.69\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1296/3393 [10:37<18:02,  1.94batch/s, Batch Loss=0.0085, Avg Loss=0.0790, Time Left=18.69\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1296/3393 [10:38<18:02,  1.94batch/s, Batch Loss=0.0551, Avg Loss=0.0790, Time Left=18.69\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1297/3393 [10:38<17:53,  1.95batch/s, Batch Loss=0.0551, Avg Loss=0.0790, Time Left=18.69\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1297/3393 [10:38<17:53,  1.95batch/s, Batch Loss=0.0183, Avg Loss=0.0790, Time Left=18.68\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1298/3393 [10:38<18:04,  1.93batch/s, Batch Loss=0.0183, Avg Loss=0.0790, Time Left=18.68\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1298/3393 [10:39<18:04,  1.93batch/s, Batch Loss=0.0597, Avg Loss=0.0790, Time Left=18.67\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1299/3393 [10:39<17:45,  1.97batch/s, Batch Loss=0.0597, Avg Loss=0.0790, Time Left=18.67\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1299/3393 [10:39<17:45,  1.97batch/s, Batch Loss=0.0474, Avg Loss=0.0789, Time Left=18.66\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1300/3393 [10:39<17:58,  1.94batch/s, Batch Loss=0.0474, Avg Loss=0.0789, Time Left=18.66\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1300/3393 [10:40<17:58,  1.94batch/s, Batch Loss=0.0789, Avg Loss=0.0789, Time Left=18.65\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1301/3393 [10:40<17:39,  1.97batch/s, Batch Loss=0.0789, Avg Loss=0.0789, Time Left=18.65\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1301/3393 [10:40<17:39,  1.97batch/s, Batch Loss=0.0927, Avg Loss=0.0789, Time Left=18.64\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1302/3393 [10:40<18:05,  1.93batch/s, Batch Loss=0.0927, Avg Loss=0.0789, Time Left=18.64\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1302/3393 [10:41<18:05,  1.93batch/s, Batch Loss=0.0420, Avg Loss=0.0789, Time Left=18.63\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1303/3393 [10:41<17:54,  1.95batch/s, Batch Loss=0.0420, Avg Loss=0.0789, Time Left=18.63\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1303/3393 [10:41<17:54,  1.95batch/s, Batch Loss=0.0092, Avg Loss=0.0789, Time Left=18.62\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1304/3393 [10:41<17:55,  1.94batch/s, Batch Loss=0.0092, Avg Loss=0.0789, Time Left=18.62\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1304/3393 [10:42<17:55,  1.94batch/s, Batch Loss=0.0789, Avg Loss=0.0789, Time Left=18.61\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1305/3393 [10:42<17:36,  1.98batch/s, Batch Loss=0.0789, Avg Loss=0.0789, Time Left=18.61\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1305/3393 [10:42<17:36,  1.98batch/s, Batch Loss=0.0387, Avg Loss=0.0788, Time Left=18.61\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1306/3393 [10:42<17:41,  1.97batch/s, Batch Loss=0.0387, Avg Loss=0.0788, Time Left=18.61\u001b[A\n",
      "Epoch 3/3 - Training:  38%|▍| 1306/3393 [10:43<17:41,  1.97batch/s, Batch Loss=0.1077, Avg Loss=0.0788, Time Left=18.60\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1307/3393 [10:43<17:35,  1.98batch/s, Batch Loss=0.1077, Avg Loss=0.0788, Time Left=18.60\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1307/3393 [10:43<17:35,  1.98batch/s, Batch Loss=0.0316, Avg Loss=0.0788, Time Left=18.59\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1308/3393 [10:43<17:51,  1.95batch/s, Batch Loss=0.0316, Avg Loss=0.0788, Time Left=18.59\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1308/3393 [10:44<17:51,  1.95batch/s, Batch Loss=0.2578, Avg Loss=0.0789, Time Left=18.58\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1309/3393 [10:44<17:33,  1.98batch/s, Batch Loss=0.2578, Avg Loss=0.0789, Time Left=18.58\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1309/3393 [10:44<17:33,  1.98batch/s, Batch Loss=0.0467, Avg Loss=0.0789, Time Left=18.57\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1310/3393 [10:44<17:49,  1.95batch/s, Batch Loss=0.0467, Avg Loss=0.0789, Time Left=18.57\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1310/3393 [10:45<17:49,  1.95batch/s, Batch Loss=0.0017, Avg Loss=0.0789, Time Left=18.56\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1311/3393 [10:45<17:32,  1.98batch/s, Batch Loss=0.0017, Avg Loss=0.0789, Time Left=18.56\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1311/3393 [10:46<17:32,  1.98batch/s, Batch Loss=0.0014, Avg Loss=0.0788, Time Left=18.55\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1312/3393 [10:46<17:57,  1.93batch/s, Batch Loss=0.0014, Avg Loss=0.0788, Time Left=18.55\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1312/3393 [10:46<17:57,  1.93batch/s, Batch Loss=0.0142, Avg Loss=0.0787, Time Left=18.54\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1313/3393 [10:46<17:37,  1.97batch/s, Batch Loss=0.0142, Avg Loss=0.0787, Time Left=18.54\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1313/3393 [10:47<17:37,  1.97batch/s, Batch Loss=0.0069, Avg Loss=0.0787, Time Left=18.53\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1314/3393 [10:47<17:41,  1.96batch/s, Batch Loss=0.0069, Avg Loss=0.0787, Time Left=18.53\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1314/3393 [10:47<17:41,  1.96batch/s, Batch Loss=0.0013, Avg Loss=0.0786, Time Left=18.53\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1315/3393 [10:47<17:35,  1.97batch/s, Batch Loss=0.0013, Avg Loss=0.0786, Time Left=18.53\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1315/3393 [10:48<17:35,  1.97batch/s, Batch Loss=0.0162, Avg Loss=0.0786, Time Left=18.52\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1316/3393 [10:48<17:59,  1.92batch/s, Batch Loss=0.0162, Avg Loss=0.0786, Time Left=18.52\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1316/3393 [10:48<17:59,  1.92batch/s, Batch Loss=0.0571, Avg Loss=0.0786, Time Left=18.51\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1317/3393 [10:48<17:48,  1.94batch/s, Batch Loss=0.0571, Avg Loss=0.0786, Time Left=18.51\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1317/3393 [10:49<17:48,  1.94batch/s, Batch Loss=0.0028, Avg Loss=0.0785, Time Left=18.50\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  39%|▍| 1318/3393 [10:49<18:09,  1.90batch/s, Batch Loss=0.0028, Avg Loss=0.0785, Time Left=18.50\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1318/3393 [10:49<18:09,  1.90batch/s, Batch Loss=0.1937, Avg Loss=0.0786, Time Left=18.49\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1319/3393 [10:49<17:44,  1.95batch/s, Batch Loss=0.1937, Avg Loss=0.0786, Time Left=18.49\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1319/3393 [10:50<17:44,  1.95batch/s, Batch Loss=0.0132, Avg Loss=0.0785, Time Left=18.48\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1320/3393 [10:50<17:45,  1.95batch/s, Batch Loss=0.0132, Avg Loss=0.0785, Time Left=18.48\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1320/3393 [10:50<17:45,  1.95batch/s, Batch Loss=0.0035, Avg Loss=0.0785, Time Left=18.47\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1321/3393 [10:50<17:47,  1.94batch/s, Batch Loss=0.0035, Avg Loss=0.0785, Time Left=18.47\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1321/3393 [10:51<17:47,  1.94batch/s, Batch Loss=0.0183, Avg Loss=0.0784, Time Left=18.47\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1322/3393 [10:51<17:47,  1.94batch/s, Batch Loss=0.0183, Avg Loss=0.0784, Time Left=18.47\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1322/3393 [10:51<17:47,  1.94batch/s, Batch Loss=0.3047, Avg Loss=0.0786, Time Left=18.46\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1323/3393 [10:51<17:28,  1.97batch/s, Batch Loss=0.3047, Avg Loss=0.0786, Time Left=18.46\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1323/3393 [10:52<17:28,  1.97batch/s, Batch Loss=0.0191, Avg Loss=0.0786, Time Left=18.45\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1324/3393 [10:52<17:33,  1.96batch/s, Batch Loss=0.0191, Avg Loss=0.0786, Time Left=18.45\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1324/3393 [10:52<17:33,  1.96batch/s, Batch Loss=0.0171, Avg Loss=0.0785, Time Left=18.44\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1325/3393 [10:52<16:58,  2.03batch/s, Batch Loss=0.0171, Avg Loss=0.0785, Time Left=18.44\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1325/3393 [10:53<16:58,  2.03batch/s, Batch Loss=0.0408, Avg Loss=0.0785, Time Left=18.43\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1326/3393 [10:53<17:01,  2.02batch/s, Batch Loss=0.0408, Avg Loss=0.0785, Time Left=18.43\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1326/3393 [10:53<17:01,  2.02batch/s, Batch Loss=0.0228, Avg Loss=0.0784, Time Left=18.42\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1327/3393 [10:53<17:04,  2.02batch/s, Batch Loss=0.0228, Avg Loss=0.0784, Time Left=18.42\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1327/3393 [10:54<17:04,  2.02batch/s, Batch Loss=0.0128, Avg Loss=0.0784, Time Left=18.41\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1328/3393 [10:54<17:07,  2.01batch/s, Batch Loss=0.0128, Avg Loss=0.0784, Time Left=18.41\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1328/3393 [10:54<17:07,  2.01batch/s, Batch Loss=0.0070, Avg Loss=0.0783, Time Left=18.40\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1329/3393 [10:54<17:07,  2.01batch/s, Batch Loss=0.0070, Avg Loss=0.0783, Time Left=18.40\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1329/3393 [10:55<17:07,  2.01batch/s, Batch Loss=0.0563, Avg Loss=0.0783, Time Left=18.39\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1330/3393 [10:55<16:59,  2.02batch/s, Batch Loss=0.0563, Avg Loss=0.0783, Time Left=18.39\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1330/3393 [10:55<16:59,  2.02batch/s, Batch Loss=0.1019, Avg Loss=0.0783, Time Left=18.38\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1331/3393 [10:55<17:02,  2.02batch/s, Batch Loss=0.1019, Avg Loss=0.0783, Time Left=18.38\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1331/3393 [10:56<17:02,  2.02batch/s, Batch Loss=0.0052, Avg Loss=0.0783, Time Left=18.37\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1332/3393 [10:56<17:04,  2.01batch/s, Batch Loss=0.0052, Avg Loss=0.0783, Time Left=18.37\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1332/3393 [10:56<17:04,  2.01batch/s, Batch Loss=0.0079, Avg Loss=0.0782, Time Left=18.36\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1333/3393 [10:56<16:53,  2.03batch/s, Batch Loss=0.0079, Avg Loss=0.0782, Time Left=18.36\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1333/3393 [10:57<16:53,  2.03batch/s, Batch Loss=0.0014, Avg Loss=0.0782, Time Left=18.35\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1334/3393 [10:57<17:19,  1.98batch/s, Batch Loss=0.0014, Avg Loss=0.0782, Time Left=18.35\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1334/3393 [10:57<17:19,  1.98batch/s, Batch Loss=0.0677, Avg Loss=0.0782, Time Left=18.34\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1335/3393 [10:57<17:18,  1.98batch/s, Batch Loss=0.0677, Avg Loss=0.0782, Time Left=18.34\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1335/3393 [10:58<17:18,  1.98batch/s, Batch Loss=0.0046, Avg Loss=0.0781, Time Left=18.34\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1336/3393 [10:58<17:15,  1.99batch/s, Batch Loss=0.0046, Avg Loss=0.0781, Time Left=18.34\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1336/3393 [10:58<17:15,  1.99batch/s, Batch Loss=0.1365, Avg Loss=0.0781, Time Left=18.33\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1337/3393 [10:58<17:12,  1.99batch/s, Batch Loss=0.1365, Avg Loss=0.0781, Time Left=18.33\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1337/3393 [10:59<17:12,  1.99batch/s, Batch Loss=0.2033, Avg Loss=0.0782, Time Left=18.32\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1338/3393 [10:59<17:39,  1.94batch/s, Batch Loss=0.2033, Avg Loss=0.0782, Time Left=18.32\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1338/3393 [10:59<17:39,  1.94batch/s, Batch Loss=0.0085, Avg Loss=0.0782, Time Left=18.31\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1339/3393 [10:59<17:21,  1.97batch/s, Batch Loss=0.0085, Avg Loss=0.0782, Time Left=18.31\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1339/3393 [11:00<17:21,  1.97batch/s, Batch Loss=0.2310, Avg Loss=0.0783, Time Left=18.30\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1340/3393 [11:00<17:36,  1.94batch/s, Batch Loss=0.2310, Avg Loss=0.0783, Time Left=18.30\u001b[A\n",
      "Epoch 3/3 - Training:  39%|▍| 1340/3393 [11:00<17:36,  1.94batch/s, Batch Loss=0.0010, Avg Loss=0.0782, Time Left=18.29\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1341/3393 [11:00<17:38,  1.94batch/s, Batch Loss=0.0010, Avg Loss=0.0782, Time Left=18.29\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1341/3393 [11:01<17:38,  1.94batch/s, Batch Loss=0.0262, Avg Loss=0.0782, Time Left=18.28\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1342/3393 [11:01<17:28,  1.96batch/s, Batch Loss=0.0262, Avg Loss=0.0782, Time Left=18.28\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1342/3393 [11:01<17:28,  1.96batch/s, Batch Loss=0.1218, Avg Loss=0.0782, Time Left=18.27\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1343/3393 [11:01<17:29,  1.95batch/s, Batch Loss=0.1218, Avg Loss=0.0782, Time Left=18.27\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1343/3393 [11:02<17:29,  1.95batch/s, Batch Loss=0.0021, Avg Loss=0.0782, Time Left=18.27\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1344/3393 [11:02<17:13,  1.98batch/s, Batch Loss=0.0021, Avg Loss=0.0782, Time Left=18.27\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1344/3393 [11:02<17:13,  1.98batch/s, Batch Loss=0.0037, Avg Loss=0.0781, Time Left=18.26\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1345/3393 [11:02<17:00,  2.01batch/s, Batch Loss=0.0037, Avg Loss=0.0781, Time Left=18.26\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1345/3393 [11:03<17:00,  2.01batch/s, Batch Loss=0.0900, Avg Loss=0.0781, Time Left=18.25\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1346/3393 [11:03<17:23,  1.96batch/s, Batch Loss=0.0900, Avg Loss=0.0781, Time Left=18.25\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1346/3393 [11:03<17:23,  1.96batch/s, Batch Loss=0.0292, Avg Loss=0.0781, Time Left=18.24\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1347/3393 [11:03<17:15,  1.98batch/s, Batch Loss=0.0292, Avg Loss=0.0781, Time Left=18.24\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1347/3393 [11:04<17:15,  1.98batch/s, Batch Loss=0.0895, Avg Loss=0.0781, Time Left=18.23\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1348/3393 [11:04<17:20,  1.97batch/s, Batch Loss=0.0895, Avg Loss=0.0781, Time Left=18.23\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1348/3393 [11:04<17:20,  1.97batch/s, Batch Loss=0.0035, Avg Loss=0.0780, Time Left=18.22\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1349/3393 [11:04<17:20,  1.96batch/s, Batch Loss=0.0035, Avg Loss=0.0780, Time Left=18.22\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1349/3393 [11:05<17:20,  1.96batch/s, Batch Loss=0.0822, Avg Loss=0.0780, Time Left=18.21\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1350/3393 [11:05<17:20,  1.96batch/s, Batch Loss=0.0822, Avg Loss=0.0780, Time Left=18.21\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1350/3393 [11:05<17:20,  1.96batch/s, Batch Loss=0.0150, Avg Loss=0.0780, Time Left=18.20\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  40%|▍| 1351/3393 [11:05<16:58,  2.00batch/s, Batch Loss=0.0150, Avg Loss=0.0780, Time Left=18.20\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1351/3393 [11:06<16:58,  2.00batch/s, Batch Loss=0.0315, Avg Loss=0.0780, Time Left=18.19\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1352/3393 [11:06<16:55,  2.01batch/s, Batch Loss=0.0315, Avg Loss=0.0780, Time Left=18.19\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1352/3393 [11:06<16:55,  2.01batch/s, Batch Loss=0.0769, Avg Loss=0.0780, Time Left=18.18\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1353/3393 [11:06<16:46,  2.03batch/s, Batch Loss=0.0769, Avg Loss=0.0780, Time Left=18.18\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1353/3393 [11:07<16:46,  2.03batch/s, Batch Loss=0.0052, Avg Loss=0.0779, Time Left=18.17\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1354/3393 [11:07<16:39,  2.04batch/s, Batch Loss=0.0052, Avg Loss=0.0779, Time Left=18.17\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1354/3393 [11:07<16:39,  2.04batch/s, Batch Loss=0.0031, Avg Loss=0.0778, Time Left=18.17\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1355/3393 [11:07<17:03,  1.99batch/s, Batch Loss=0.0031, Avg Loss=0.0778, Time Left=18.17\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1355/3393 [11:08<17:03,  1.99batch/s, Batch Loss=0.1218, Avg Loss=0.0779, Time Left=18.16\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1356/3393 [11:08<17:02,  1.99batch/s, Batch Loss=0.1218, Avg Loss=0.0779, Time Left=18.16\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1356/3393 [11:08<17:02,  1.99batch/s, Batch Loss=0.0032, Avg Loss=0.0778, Time Left=18.15\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1357/3393 [11:08<17:10,  1.98batch/s, Batch Loss=0.0032, Avg Loss=0.0778, Time Left=18.15\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1357/3393 [11:09<17:10,  1.98batch/s, Batch Loss=0.0078, Avg Loss=0.0778, Time Left=18.14\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1358/3393 [11:09<16:58,  2.00batch/s, Batch Loss=0.0078, Avg Loss=0.0778, Time Left=18.14\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1358/3393 [11:09<16:58,  2.00batch/s, Batch Loss=0.2325, Avg Loss=0.0779, Time Left=18.13\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1359/3393 [11:09<17:06,  1.98batch/s, Batch Loss=0.2325, Avg Loss=0.0779, Time Left=18.13\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1359/3393 [11:10<17:06,  1.98batch/s, Batch Loss=0.0310, Avg Loss=0.0778, Time Left=18.12\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1360/3393 [11:10<16:54,  2.00batch/s, Batch Loss=0.0310, Avg Loss=0.0778, Time Left=18.12\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1360/3393 [11:10<16:54,  2.00batch/s, Batch Loss=0.0632, Avg Loss=0.0778, Time Left=18.11\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1361/3393 [11:10<17:13,  1.97batch/s, Batch Loss=0.0632, Avg Loss=0.0778, Time Left=18.11\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1361/3393 [11:11<17:13,  1.97batch/s, Batch Loss=0.0047, Avg Loss=0.0778, Time Left=18.10\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1362/3393 [11:11<16:54,  2.00batch/s, Batch Loss=0.0047, Avg Loss=0.0778, Time Left=18.10\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1362/3393 [11:11<16:54,  2.00batch/s, Batch Loss=0.0124, Avg Loss=0.0777, Time Left=18.09\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1363/3393 [11:11<16:49,  2.01batch/s, Batch Loss=0.0124, Avg Loss=0.0777, Time Left=18.09\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1363/3393 [11:12<16:49,  2.01batch/s, Batch Loss=0.0754, Avg Loss=0.0777, Time Left=18.08\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1364/3393 [11:12<16:50,  2.01batch/s, Batch Loss=0.0754, Avg Loss=0.0777, Time Left=18.08\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1364/3393 [11:12<16:50,  2.01batch/s, Batch Loss=0.0011, Avg Loss=0.0777, Time Left=18.07\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1365/3393 [11:12<16:51,  2.01batch/s, Batch Loss=0.0011, Avg Loss=0.0777, Time Left=18.07\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1365/3393 [11:13<16:51,  2.01batch/s, Batch Loss=0.0566, Avg Loss=0.0777, Time Left=18.07\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1366/3393 [11:13<17:01,  1.98batch/s, Batch Loss=0.0566, Avg Loss=0.0777, Time Left=18.07\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1366/3393 [11:13<17:01,  1.98batch/s, Batch Loss=0.0102, Avg Loss=0.0776, Time Left=18.06\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1367/3393 [11:13<16:30,  2.05batch/s, Batch Loss=0.0102, Avg Loss=0.0776, Time Left=18.06\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1367/3393 [11:14<16:30,  2.05batch/s, Batch Loss=0.0123, Avg Loss=0.0776, Time Left=18.05\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1368/3393 [11:14<16:35,  2.03batch/s, Batch Loss=0.0123, Avg Loss=0.0776, Time Left=18.05\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1368/3393 [11:14<16:35,  2.03batch/s, Batch Loss=0.1855, Avg Loss=0.0776, Time Left=18.04\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1369/3393 [11:14<16:49,  2.00batch/s, Batch Loss=0.1855, Avg Loss=0.0776, Time Left=18.04\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1369/3393 [11:15<16:49,  2.00batch/s, Batch Loss=0.0227, Avg Loss=0.0776, Time Left=18.03\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1370/3393 [11:15<16:40,  2.02batch/s, Batch Loss=0.0227, Avg Loss=0.0776, Time Left=18.03\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1370/3393 [11:15<16:40,  2.02batch/s, Batch Loss=0.2637, Avg Loss=0.0777, Time Left=18.02\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1371/3393 [11:15<16:43,  2.01batch/s, Batch Loss=0.2637, Avg Loss=0.0777, Time Left=18.02\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1371/3393 [11:16<16:43,  2.01batch/s, Batch Loss=0.0213, Avg Loss=0.0777, Time Left=18.01\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1372/3393 [11:16<16:46,  2.01batch/s, Batch Loss=0.0213, Avg Loss=0.0777, Time Left=18.01\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1372/3393 [11:16<16:46,  2.01batch/s, Batch Loss=0.4575, Avg Loss=0.0780, Time Left=18.00\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1373/3393 [11:16<17:06,  1.97batch/s, Batch Loss=0.4575, Avg Loss=0.0780, Time Left=18.00\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1373/3393 [11:17<17:06,  1.97batch/s, Batch Loss=0.0015, Avg Loss=0.0779, Time Left=17.99\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1374/3393 [11:17<17:01,  1.98batch/s, Batch Loss=0.0015, Avg Loss=0.0779, Time Left=17.99\u001b[A\n",
      "Epoch 3/3 - Training:  40%|▍| 1374/3393 [11:17<17:01,  1.98batch/s, Batch Loss=0.1214, Avg Loss=0.0780, Time Left=17.98\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1375/3393 [11:17<17:13,  1.95batch/s, Batch Loss=0.1214, Avg Loss=0.0780, Time Left=17.98\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1375/3393 [11:18<17:13,  1.95batch/s, Batch Loss=0.0875, Avg Loss=0.0780, Time Left=17.97\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1376/3393 [11:18<16:50,  2.00batch/s, Batch Loss=0.0875, Avg Loss=0.0780, Time Left=17.97\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1376/3393 [11:18<16:50,  2.00batch/s, Batch Loss=0.0028, Avg Loss=0.0779, Time Left=17.96\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1377/3393 [11:18<16:39,  2.02batch/s, Batch Loss=0.0028, Avg Loss=0.0779, Time Left=17.96\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1377/3393 [11:19<16:39,  2.02batch/s, Batch Loss=0.0359, Avg Loss=0.0779, Time Left=17.96\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1378/3393 [11:19<16:32,  2.03batch/s, Batch Loss=0.0359, Avg Loss=0.0779, Time Left=17.96\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1378/3393 [11:19<16:32,  2.03batch/s, Batch Loss=0.0273, Avg Loss=0.0778, Time Left=17.95\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1379/3393 [11:19<16:16,  2.06batch/s, Batch Loss=0.0273, Avg Loss=0.0778, Time Left=17.95\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1379/3393 [11:20<16:16,  2.06batch/s, Batch Loss=0.0012, Avg Loss=0.0778, Time Left=17.94\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1380/3393 [11:20<16:43,  2.01batch/s, Batch Loss=0.0012, Avg Loss=0.0778, Time Left=17.94\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1380/3393 [11:20<16:43,  2.01batch/s, Batch Loss=0.1356, Avg Loss=0.0778, Time Left=17.93\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1381/3393 [11:20<16:45,  2.00batch/s, Batch Loss=0.1356, Avg Loss=0.0778, Time Left=17.93\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1381/3393 [11:21<16:45,  2.00batch/s, Batch Loss=0.0982, Avg Loss=0.0778, Time Left=17.92\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1382/3393 [11:21<17:04,  1.96batch/s, Batch Loss=0.0982, Avg Loss=0.0778, Time Left=17.92\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1382/3393 [11:21<17:04,  1.96batch/s, Batch Loss=0.0978, Avg Loss=0.0779, Time Left=17.91\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1383/3393 [11:21<16:49,  1.99batch/s, Batch Loss=0.0978, Avg Loss=0.0779, Time Left=17.91\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1383/3393 [11:22<16:49,  1.99batch/s, Batch Loss=0.0059, Avg Loss=0.0778, Time Left=17.90\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  41%|▍| 1384/3393 [11:22<17:06,  1.96batch/s, Batch Loss=0.0059, Avg Loss=0.0778, Time Left=17.90\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1384/3393 [11:22<17:06,  1.96batch/s, Batch Loss=0.0903, Avg Loss=0.0778, Time Left=17.89\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1385/3393 [11:22<17:00,  1.97batch/s, Batch Loss=0.0903, Avg Loss=0.0778, Time Left=17.89\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1385/3393 [11:23<17:00,  1.97batch/s, Batch Loss=0.0214, Avg Loss=0.0778, Time Left=17.88\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1386/3393 [11:23<17:14,  1.94batch/s, Batch Loss=0.0214, Avg Loss=0.0778, Time Left=17.88\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1386/3393 [11:23<17:14,  1.94batch/s, Batch Loss=0.0585, Avg Loss=0.0777, Time Left=17.88\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1387/3393 [11:23<17:05,  1.96batch/s, Batch Loss=0.0585, Avg Loss=0.0777, Time Left=17.88\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1387/3393 [11:24<17:05,  1.96batch/s, Batch Loss=0.0242, Avg Loss=0.0777, Time Left=17.87\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1388/3393 [11:24<16:48,  1.99batch/s, Batch Loss=0.0242, Avg Loss=0.0777, Time Left=17.87\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1388/3393 [11:24<16:48,  1.99batch/s, Batch Loss=0.0015, Avg Loss=0.0777, Time Left=17.86\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1389/3393 [11:24<16:37,  2.01batch/s, Batch Loss=0.0015, Avg Loss=0.0777, Time Left=17.86\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1389/3393 [11:25<16:37,  2.01batch/s, Batch Loss=0.2794, Avg Loss=0.0778, Time Left=17.85\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1390/3393 [11:25<16:37,  2.01batch/s, Batch Loss=0.2794, Avg Loss=0.0778, Time Left=17.85\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1390/3393 [11:25<16:37,  2.01batch/s, Batch Loss=0.1442, Avg Loss=0.0779, Time Left=17.84\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1391/3393 [11:25<16:29,  2.02batch/s, Batch Loss=0.1442, Avg Loss=0.0779, Time Left=17.84\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1391/3393 [11:26<16:29,  2.02batch/s, Batch Loss=0.0120, Avg Loss=0.0778, Time Left=17.83\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1392/3393 [11:26<16:30,  2.02batch/s, Batch Loss=0.0120, Avg Loss=0.0778, Time Left=17.83\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1392/3393 [11:26<16:30,  2.02batch/s, Batch Loss=0.0017, Avg Loss=0.0777, Time Left=17.82\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1393/3393 [11:26<16:14,  2.05batch/s, Batch Loss=0.0017, Avg Loss=0.0777, Time Left=17.82\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1393/3393 [11:27<16:14,  2.05batch/s, Batch Loss=0.1225, Avg Loss=0.0778, Time Left=17.81\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1394/3393 [11:27<16:21,  2.04batch/s, Batch Loss=0.1225, Avg Loss=0.0778, Time Left=17.81\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1394/3393 [11:27<16:21,  2.04batch/s, Batch Loss=0.4953, Avg Loss=0.0781, Time Left=17.80\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1395/3393 [11:27<16:27,  2.02batch/s, Batch Loss=0.4953, Avg Loss=0.0781, Time Left=17.80\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1395/3393 [11:28<16:27,  2.02batch/s, Batch Loss=0.4921, Avg Loss=0.0784, Time Left=17.79\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1396/3393 [11:28<16:29,  2.02batch/s, Batch Loss=0.4921, Avg Loss=0.0784, Time Left=17.79\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1396/3393 [11:28<16:29,  2.02batch/s, Batch Loss=0.2816, Avg Loss=0.0786, Time Left=17.78\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1397/3393 [11:28<16:23,  2.03batch/s, Batch Loss=0.2816, Avg Loss=0.0786, Time Left=17.78\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1397/3393 [11:29<16:23,  2.03batch/s, Batch Loss=0.1916, Avg Loss=0.0786, Time Left=17.77\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1398/3393 [11:29<16:54,  1.97batch/s, Batch Loss=0.1916, Avg Loss=0.0786, Time Left=17.77\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1398/3393 [11:29<16:54,  1.97batch/s, Batch Loss=0.1389, Avg Loss=0.0787, Time Left=17.76\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1399/3393 [11:29<16:41,  1.99batch/s, Batch Loss=0.1389, Avg Loss=0.0787, Time Left=17.76\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1399/3393 [11:30<16:41,  1.99batch/s, Batch Loss=0.0147, Avg Loss=0.0786, Time Left=17.76\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1400/3393 [11:30<16:48,  1.98batch/s, Batch Loss=0.0147, Avg Loss=0.0786, Time Left=17.76\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1400/3393 [11:30<16:48,  1.98batch/s, Batch Loss=0.0019, Avg Loss=0.0786, Time Left=17.75\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1401/3393 [11:30<16:16,  2.04batch/s, Batch Loss=0.0019, Avg Loss=0.0786, Time Left=17.75\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1401/3393 [11:31<16:16,  2.04batch/s, Batch Loss=0.0335, Avg Loss=0.0785, Time Left=17.74\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1402/3393 [11:31<16:21,  2.03batch/s, Batch Loss=0.0335, Avg Loss=0.0785, Time Left=17.74\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1402/3393 [11:31<16:21,  2.03batch/s, Batch Loss=0.0487, Avg Loss=0.0785, Time Left=17.73\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1403/3393 [11:31<16:25,  2.02batch/s, Batch Loss=0.0487, Avg Loss=0.0785, Time Left=17.73\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1403/3393 [11:32<16:25,  2.02batch/s, Batch Loss=0.0382, Avg Loss=0.0785, Time Left=17.72\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1404/3393 [11:32<16:26,  2.02batch/s, Batch Loss=0.0382, Avg Loss=0.0785, Time Left=17.72\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1404/3393 [11:32<16:26,  2.02batch/s, Batch Loss=0.0788, Avg Loss=0.0785, Time Left=17.71\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1405/3393 [11:32<16:38,  1.99batch/s, Batch Loss=0.0788, Avg Loss=0.0785, Time Left=17.71\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1405/3393 [11:33<16:38,  1.99batch/s, Batch Loss=0.0748, Avg Loss=0.0785, Time Left=17.70\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1406/3393 [11:33<16:38,  1.99batch/s, Batch Loss=0.0748, Avg Loss=0.0785, Time Left=17.70\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1406/3393 [11:33<16:38,  1.99batch/s, Batch Loss=0.0746, Avg Loss=0.0785, Time Left=17.69\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1407/3393 [11:33<16:45,  1.97batch/s, Batch Loss=0.0746, Avg Loss=0.0785, Time Left=17.69\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1407/3393 [11:34<16:45,  1.97batch/s, Batch Loss=0.0206, Avg Loss=0.0784, Time Left=17.68\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1408/3393 [11:34<16:41,  1.98batch/s, Batch Loss=0.0206, Avg Loss=0.0784, Time Left=17.68\u001b[A\n",
      "Epoch 3/3 - Training:  41%|▍| 1408/3393 [11:34<16:41,  1.98batch/s, Batch Loss=0.0266, Avg Loss=0.0784, Time Left=17.67\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1409/3393 [11:34<16:39,  1.99batch/s, Batch Loss=0.0266, Avg Loss=0.0784, Time Left=17.67\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1409/3393 [11:35<16:39,  1.99batch/s, Batch Loss=0.1826, Avg Loss=0.0785, Time Left=17.66\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1410/3393 [11:35<16:33,  2.00batch/s, Batch Loss=0.1826, Avg Loss=0.0785, Time Left=17.66\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1410/3393 [11:35<16:33,  2.00batch/s, Batch Loss=0.0062, Avg Loss=0.0784, Time Left=17.66\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1411/3393 [11:35<16:17,  2.03batch/s, Batch Loss=0.0062, Avg Loss=0.0784, Time Left=17.66\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1411/3393 [11:36<16:17,  2.03batch/s, Batch Loss=0.0040, Avg Loss=0.0784, Time Left=17.65\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1412/3393 [11:36<16:20,  2.02batch/s, Batch Loss=0.0040, Avg Loss=0.0784, Time Left=17.65\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1412/3393 [11:36<16:20,  2.02batch/s, Batch Loss=0.0097, Avg Loss=0.0783, Time Left=17.64\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1413/3393 [11:36<16:13,  2.03batch/s, Batch Loss=0.0097, Avg Loss=0.0783, Time Left=17.64\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1413/3393 [11:37<16:13,  2.03batch/s, Batch Loss=0.0062, Avg Loss=0.0783, Time Left=17.63\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1414/3393 [11:37<16:35,  1.99batch/s, Batch Loss=0.0062, Avg Loss=0.0783, Time Left=17.63\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1414/3393 [11:37<16:35,  1.99batch/s, Batch Loss=0.1561, Avg Loss=0.0783, Time Left=17.62\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1415/3393 [11:37<16:35,  1.99batch/s, Batch Loss=0.1561, Avg Loss=0.0783, Time Left=17.62\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1415/3393 [11:38<16:35,  1.99batch/s, Batch Loss=0.1508, Avg Loss=0.0784, Time Left=17.61\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1416/3393 [11:38<16:41,  1.97batch/s, Batch Loss=0.1508, Avg Loss=0.0784, Time Left=17.61\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1416/3393 [11:38<16:41,  1.97batch/s, Batch Loss=0.0170, Avg Loss=0.0783, Time Left=17.60\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  42%|▍| 1417/3393 [11:38<16:29,  2.00batch/s, Batch Loss=0.0170, Avg Loss=0.0783, Time Left=17.60\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1417/3393 [11:39<16:29,  2.00batch/s, Batch Loss=0.0340, Avg Loss=0.0783, Time Left=17.59\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1418/3393 [11:39<16:37,  1.98batch/s, Batch Loss=0.0340, Avg Loss=0.0783, Time Left=17.59\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1418/3393 [11:39<16:37,  1.98batch/s, Batch Loss=0.1619, Avg Loss=0.0784, Time Left=17.58\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1419/3393 [11:39<16:25,  2.00batch/s, Batch Loss=0.1619, Avg Loss=0.0784, Time Left=17.58\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1419/3393 [11:40<16:25,  2.00batch/s, Batch Loss=0.1092, Avg Loss=0.0784, Time Left=17.57\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1420/3393 [11:40<16:25,  2.00batch/s, Batch Loss=0.1092, Avg Loss=0.0784, Time Left=17.57\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1420/3393 [11:40<16:25,  2.00batch/s, Batch Loss=0.1130, Avg Loss=0.0784, Time Left=17.57\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1421/3393 [11:40<16:25,  2.00batch/s, Batch Loss=0.1130, Avg Loss=0.0784, Time Left=17.57\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1421/3393 [11:41<16:25,  2.00batch/s, Batch Loss=0.1173, Avg Loss=0.0784, Time Left=17.56\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1422/3393 [11:41<16:24,  2.00batch/s, Batch Loss=0.1173, Avg Loss=0.0784, Time Left=17.56\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1422/3393 [11:41<16:24,  2.00batch/s, Batch Loss=0.0517, Avg Loss=0.0784, Time Left=17.55\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1423/3393 [11:41<16:50,  1.95batch/s, Batch Loss=0.0517, Avg Loss=0.0784, Time Left=17.55\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1423/3393 [11:42<16:50,  1.95batch/s, Batch Loss=0.0696, Avg Loss=0.0784, Time Left=17.54\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1424/3393 [11:42<16:36,  1.98batch/s, Batch Loss=0.0696, Avg Loss=0.0784, Time Left=17.54\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1424/3393 [11:42<16:36,  1.98batch/s, Batch Loss=0.1553, Avg Loss=0.0785, Time Left=17.53\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1425/3393 [11:42<16:33,  1.98batch/s, Batch Loss=0.1553, Avg Loss=0.0785, Time Left=17.53\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1425/3393 [11:43<16:33,  1.98batch/s, Batch Loss=0.1164, Avg Loss=0.0785, Time Left=17.52\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1426/3393 [11:43<16:30,  1.99batch/s, Batch Loss=0.1164, Avg Loss=0.0785, Time Left=17.52\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1426/3393 [11:43<16:30,  1.99batch/s, Batch Loss=0.0029, Avg Loss=0.0784, Time Left=17.51\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1427/3393 [11:43<16:32,  1.98batch/s, Batch Loss=0.0029, Avg Loss=0.0784, Time Left=17.51\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1427/3393 [11:44<16:32,  1.98batch/s, Batch Loss=0.2935, Avg Loss=0.0786, Time Left=17.50\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1428/3393 [11:44<16:25,  1.99batch/s, Batch Loss=0.2935, Avg Loss=0.0786, Time Left=17.50\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1428/3393 [11:44<16:25,  1.99batch/s, Batch Loss=0.0305, Avg Loss=0.0786, Time Left=17.49\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1429/3393 [11:44<16:41,  1.96batch/s, Batch Loss=0.0305, Avg Loss=0.0786, Time Left=17.49\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1429/3393 [11:45<16:41,  1.96batch/s, Batch Loss=0.0976, Avg Loss=0.0786, Time Left=17.49\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1430/3393 [11:45<16:36,  1.97batch/s, Batch Loss=0.0976, Avg Loss=0.0786, Time Left=17.49\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1430/3393 [11:45<16:36,  1.97batch/s, Batch Loss=0.1783, Avg Loss=0.0787, Time Left=17.48\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1431/3393 [11:45<16:31,  1.98batch/s, Batch Loss=0.1783, Avg Loss=0.0787, Time Left=17.48\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1431/3393 [11:46<16:31,  1.98batch/s, Batch Loss=0.0035, Avg Loss=0.0786, Time Left=17.47\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1432/3393 [11:46<16:18,  2.00batch/s, Batch Loss=0.0035, Avg Loss=0.0786, Time Left=17.47\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1432/3393 [11:46<16:18,  2.00batch/s, Batch Loss=0.0285, Avg Loss=0.0786, Time Left=17.46\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1433/3393 [11:46<15:59,  2.04batch/s, Batch Loss=0.0285, Avg Loss=0.0786, Time Left=17.46\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1433/3393 [11:47<15:59,  2.04batch/s, Batch Loss=0.0184, Avg Loss=0.0785, Time Left=17.45\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1434/3393 [11:47<16:24,  1.99batch/s, Batch Loss=0.0184, Avg Loss=0.0785, Time Left=17.45\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1434/3393 [11:47<16:24,  1.99batch/s, Batch Loss=0.0223, Avg Loss=0.0785, Time Left=17.44\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1435/3393 [11:47<16:14,  2.01batch/s, Batch Loss=0.0223, Avg Loss=0.0785, Time Left=17.44\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1435/3393 [11:48<16:14,  2.01batch/s, Batch Loss=0.0246, Avg Loss=0.0784, Time Left=17.43\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1436/3393 [11:48<16:14,  2.01batch/s, Batch Loss=0.0246, Avg Loss=0.0784, Time Left=17.43\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1436/3393 [11:48<16:14,  2.01batch/s, Batch Loss=0.2758, Avg Loss=0.0786, Time Left=17.42\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1437/3393 [11:48<15:56,  2.04batch/s, Batch Loss=0.2758, Avg Loss=0.0786, Time Left=17.42\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1437/3393 [11:49<15:56,  2.04batch/s, Batch Loss=0.0280, Avg Loss=0.0785, Time Left=17.41\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1438/3393 [11:49<16:01,  2.03batch/s, Batch Loss=0.0280, Avg Loss=0.0785, Time Left=17.41\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1438/3393 [11:49<16:01,  2.03batch/s, Batch Loss=0.0377, Avg Loss=0.0785, Time Left=17.40\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1439/3393 [11:49<16:02,  2.03batch/s, Batch Loss=0.0377, Avg Loss=0.0785, Time Left=17.40\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1439/3393 [11:50<16:02,  2.03batch/s, Batch Loss=0.0067, Avg Loss=0.0785, Time Left=17.39\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1440/3393 [11:50<16:00,  2.03batch/s, Batch Loss=0.0067, Avg Loss=0.0785, Time Left=17.39\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1440/3393 [11:50<16:00,  2.03batch/s, Batch Loss=0.0093, Avg Loss=0.0784, Time Left=17.38\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1441/3393 [11:50<16:14,  2.00batch/s, Batch Loss=0.0093, Avg Loss=0.0784, Time Left=17.38\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1441/3393 [11:51<16:14,  2.00batch/s, Batch Loss=0.0031, Avg Loss=0.0784, Time Left=17.38\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1442/3393 [11:51<16:14,  2.00batch/s, Batch Loss=0.0031, Avg Loss=0.0784, Time Left=17.38\u001b[A\n",
      "Epoch 3/3 - Training:  42%|▍| 1442/3393 [11:51<16:14,  2.00batch/s, Batch Loss=0.1410, Avg Loss=0.0784, Time Left=17.37\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1443/3393 [11:51<16:33,  1.96batch/s, Batch Loss=0.1410, Avg Loss=0.0784, Time Left=17.37\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1443/3393 [11:52<16:33,  1.96batch/s, Batch Loss=0.2524, Avg Loss=0.0785, Time Left=17.36\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1444/3393 [11:52<16:24,  1.98batch/s, Batch Loss=0.2524, Avg Loss=0.0785, Time Left=17.36\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1444/3393 [11:52<16:24,  1.98batch/s, Batch Loss=0.1270, Avg Loss=0.0786, Time Left=17.35\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1445/3393 [11:52<16:34,  1.96batch/s, Batch Loss=0.1270, Avg Loss=0.0786, Time Left=17.35\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1445/3393 [11:53<16:34,  1.96batch/s, Batch Loss=0.3109, Avg Loss=0.0787, Time Left=17.34\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1446/3393 [11:53<16:19,  1.99batch/s, Batch Loss=0.3109, Avg Loss=0.0787, Time Left=17.34\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1446/3393 [11:53<16:19,  1.99batch/s, Batch Loss=0.0435, Avg Loss=0.0787, Time Left=17.33\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1447/3393 [11:53<16:16,  1.99batch/s, Batch Loss=0.0435, Avg Loss=0.0787, Time Left=17.33\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1447/3393 [11:54<16:16,  1.99batch/s, Batch Loss=0.1521, Avg Loss=0.0788, Time Left=17.32\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1448/3393 [11:54<16:15,  1.99batch/s, Batch Loss=0.1521, Avg Loss=0.0788, Time Left=17.32\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1448/3393 [11:54<16:15,  1.99batch/s, Batch Loss=0.0019, Avg Loss=0.0787, Time Left=17.31\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1449/3393 [11:54<15:55,  2.03batch/s, Batch Loss=0.0019, Avg Loss=0.0787, Time Left=17.31\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1449/3393 [11:55<15:55,  2.03batch/s, Batch Loss=0.1040, Avg Loss=0.0787, Time Left=17.30\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  43%|▍| 1450/3393 [11:55<16:18,  1.99batch/s, Batch Loss=0.1040, Avg Loss=0.0787, Time Left=17.30\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1450/3393 [11:55<16:18,  1.99batch/s, Batch Loss=0.5378, Avg Loss=0.0791, Time Left=17.30\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1451/3393 [11:55<16:16,  1.99batch/s, Batch Loss=0.5378, Avg Loss=0.0791, Time Left=17.30\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1451/3393 [11:56<16:16,  1.99batch/s, Batch Loss=0.0789, Avg Loss=0.0791, Time Left=17.29\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1452/3393 [11:56<16:23,  1.97batch/s, Batch Loss=0.0789, Avg Loss=0.0791, Time Left=17.29\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1452/3393 [11:56<16:23,  1.97batch/s, Batch Loss=0.1335, Avg Loss=0.0791, Time Left=17.28\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1453/3393 [11:56<16:20,  1.98batch/s, Batch Loss=0.1335, Avg Loss=0.0791, Time Left=17.28\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1453/3393 [11:57<16:20,  1.98batch/s, Batch Loss=0.1487, Avg Loss=0.0791, Time Left=17.27\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1454/3393 [11:57<16:35,  1.95batch/s, Batch Loss=0.1487, Avg Loss=0.0791, Time Left=17.27\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1454/3393 [11:57<16:35,  1.95batch/s, Batch Loss=0.1108, Avg Loss=0.0792, Time Left=17.26\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1455/3393 [11:57<16:28,  1.96batch/s, Batch Loss=0.1108, Avg Loss=0.0792, Time Left=17.26\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1455/3393 [11:58<16:28,  1.96batch/s, Batch Loss=0.0285, Avg Loss=0.0791, Time Left=17.25\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1456/3393 [11:58<16:36,  1.94batch/s, Batch Loss=0.0285, Avg Loss=0.0791, Time Left=17.25\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1456/3393 [11:58<16:36,  1.94batch/s, Batch Loss=0.0969, Avg Loss=0.0791, Time Left=17.24\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1457/3393 [11:58<16:23,  1.97batch/s, Batch Loss=0.0969, Avg Loss=0.0791, Time Left=17.24\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1457/3393 [11:59<16:23,  1.97batch/s, Batch Loss=0.1249, Avg Loss=0.0792, Time Left=17.23\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1458/3393 [11:59<16:36,  1.94batch/s, Batch Loss=0.1249, Avg Loss=0.0792, Time Left=17.23\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1458/3393 [11:59<16:36,  1.94batch/s, Batch Loss=0.1548, Avg Loss=0.0792, Time Left=17.23\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1459/3393 [11:59<16:28,  1.96batch/s, Batch Loss=0.1548, Avg Loss=0.0792, Time Left=17.23\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1459/3393 [12:00<16:28,  1.96batch/s, Batch Loss=0.0057, Avg Loss=0.0792, Time Left=17.22\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1460/3393 [12:00<16:22,  1.97batch/s, Batch Loss=0.0057, Avg Loss=0.0792, Time Left=17.22\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1460/3393 [12:00<16:22,  1.97batch/s, Batch Loss=0.1192, Avg Loss=0.0792, Time Left=17.21\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1461/3393 [12:00<16:26,  1.96batch/s, Batch Loss=0.1192, Avg Loss=0.0792, Time Left=17.21\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1461/3393 [12:01<16:26,  1.96batch/s, Batch Loss=0.0162, Avg Loss=0.0792, Time Left=17.20\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1462/3393 [12:01<16:28,  1.95batch/s, Batch Loss=0.0162, Avg Loss=0.0792, Time Left=17.20\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1462/3393 [12:01<16:28,  1.95batch/s, Batch Loss=0.0021, Avg Loss=0.0791, Time Left=17.19\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1463/3393 [12:01<16:23,  1.96batch/s, Batch Loss=0.0021, Avg Loss=0.0791, Time Left=17.19\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1463/3393 [12:02<16:23,  1.96batch/s, Batch Loss=0.0404, Avg Loss=0.0791, Time Left=17.18\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1464/3393 [12:02<16:34,  1.94batch/s, Batch Loss=0.0404, Avg Loss=0.0791, Time Left=17.18\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1464/3393 [12:02<16:34,  1.94batch/s, Batch Loss=0.0069, Avg Loss=0.0790, Time Left=17.17\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1465/3393 [12:02<16:26,  1.95batch/s, Batch Loss=0.0069, Avg Loss=0.0790, Time Left=17.17\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1465/3393 [12:03<16:26,  1.95batch/s, Batch Loss=0.1005, Avg Loss=0.0790, Time Left=17.16\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1466/3393 [12:03<16:11,  1.98batch/s, Batch Loss=0.1005, Avg Loss=0.0790, Time Left=17.16\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1466/3393 [12:04<16:11,  1.98batch/s, Batch Loss=0.0821, Avg Loss=0.0790, Time Left=17.16\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1467/3393 [12:04<17:11,  1.87batch/s, Batch Loss=0.0821, Avg Loss=0.0790, Time Left=17.16\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1467/3393 [12:04<17:11,  1.87batch/s, Batch Loss=0.0156, Avg Loss=0.0790, Time Left=17.15\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1468/3393 [12:04<16:53,  1.90batch/s, Batch Loss=0.0156, Avg Loss=0.0790, Time Left=17.15\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1468/3393 [12:05<16:53,  1.90batch/s, Batch Loss=0.0289, Avg Loss=0.0790, Time Left=17.14\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1469/3393 [12:05<17:02,  1.88batch/s, Batch Loss=0.0289, Avg Loss=0.0790, Time Left=17.14\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1469/3393 [12:05<17:02,  1.88batch/s, Batch Loss=0.0021, Avg Loss=0.0789, Time Left=17.13\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1470/3393 [12:05<16:39,  1.92batch/s, Batch Loss=0.0021, Avg Loss=0.0789, Time Left=17.13\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1470/3393 [12:06<16:39,  1.92batch/s, Batch Loss=0.0130, Avg Loss=0.0789, Time Left=17.12\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1471/3393 [12:06<17:17,  1.85batch/s, Batch Loss=0.0130, Avg Loss=0.0789, Time Left=17.12\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1471/3393 [12:06<17:17,  1.85batch/s, Batch Loss=0.4127, Avg Loss=0.0791, Time Left=17.12\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1472/3393 [12:06<16:43,  1.92batch/s, Batch Loss=0.4127, Avg Loss=0.0791, Time Left=17.12\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1472/3393 [12:07<16:43,  1.92batch/s, Batch Loss=0.0854, Avg Loss=0.0791, Time Left=17.11\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1473/3393 [12:07<16:31,  1.94batch/s, Batch Loss=0.0854, Avg Loss=0.0791, Time Left=17.11\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1473/3393 [12:07<16:31,  1.94batch/s, Batch Loss=0.0185, Avg Loss=0.0791, Time Left=17.10\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1474/3393 [12:07<16:10,  1.98batch/s, Batch Loss=0.0185, Avg Loss=0.0791, Time Left=17.10\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1474/3393 [12:08<16:10,  1.98batch/s, Batch Loss=0.2610, Avg Loss=0.0792, Time Left=17.09\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1475/3393 [12:08<16:34,  1.93batch/s, Batch Loss=0.2610, Avg Loss=0.0792, Time Left=17.09\u001b[A\n",
      "Epoch 3/3 - Training:  43%|▍| 1475/3393 [12:08<16:34,  1.93batch/s, Batch Loss=0.3988, Avg Loss=0.0794, Time Left=17.08\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1476/3393 [12:08<16:15,  1.97batch/s, Batch Loss=0.3988, Avg Loss=0.0794, Time Left=17.08\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1476/3393 [12:09<16:15,  1.97batch/s, Batch Loss=0.0057, Avg Loss=0.0794, Time Left=17.07\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1477/3393 [12:09<16:18,  1.96batch/s, Batch Loss=0.0057, Avg Loss=0.0794, Time Left=17.07\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1477/3393 [12:09<16:18,  1.96batch/s, Batch Loss=0.4728, Avg Loss=0.0796, Time Left=17.06\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1478/3393 [12:09<15:54,  2.01batch/s, Batch Loss=0.4728, Avg Loss=0.0796, Time Left=17.06\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1478/3393 [12:10<15:54,  2.01batch/s, Batch Loss=0.4260, Avg Loss=0.0799, Time Left=17.05\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1479/3393 [12:10<16:05,  1.98batch/s, Batch Loss=0.4260, Avg Loss=0.0799, Time Left=17.05\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1479/3393 [12:10<16:05,  1.98batch/s, Batch Loss=0.0097, Avg Loss=0.0798, Time Left=17.04\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1480/3393 [12:10<15:52,  2.01batch/s, Batch Loss=0.0097, Avg Loss=0.0798, Time Left=17.04\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1480/3393 [12:11<15:52,  2.01batch/s, Batch Loss=0.1433, Avg Loss=0.0799, Time Left=17.03\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1481/3393 [12:11<16:01,  1.99batch/s, Batch Loss=0.1433, Avg Loss=0.0799, Time Left=17.03\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1481/3393 [12:11<16:01,  1.99batch/s, Batch Loss=0.2611, Avg Loss=0.0800, Time Left=17.02\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1482/3393 [12:11<15:41,  2.03batch/s, Batch Loss=0.2611, Avg Loss=0.0800, Time Left=17.02\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1482/3393 [12:12<15:41,  2.03batch/s, Batch Loss=0.4071, Avg Loss=0.0802, Time Left=17.02\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  44%|▍| 1483/3393 [12:12<15:45,  2.02batch/s, Batch Loss=0.4071, Avg Loss=0.0802, Time Left=17.02\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1483/3393 [12:12<15:45,  2.02batch/s, Batch Loss=0.1103, Avg Loss=0.0803, Time Left=17.01\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1484/3393 [12:12<15:57,  1.99batch/s, Batch Loss=0.1103, Avg Loss=0.0803, Time Left=17.01\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1484/3393 [12:13<15:57,  1.99batch/s, Batch Loss=0.0548, Avg Loss=0.0802, Time Left=17.00\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1485/3393 [12:13<15:56,  2.00batch/s, Batch Loss=0.0548, Avg Loss=0.0802, Time Left=17.00\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1485/3393 [12:13<15:56,  2.00batch/s, Batch Loss=0.0658, Avg Loss=0.0802, Time Left=16.99\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1486/3393 [12:13<16:04,  1.98batch/s, Batch Loss=0.0658, Avg Loss=0.0802, Time Left=16.99\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1486/3393 [12:14<16:04,  1.98batch/s, Batch Loss=0.0158, Avg Loss=0.0802, Time Left=16.98\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1487/3393 [12:14<16:10,  1.96batch/s, Batch Loss=0.0158, Avg Loss=0.0802, Time Left=16.98\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1487/3393 [12:14<16:10,  1.96batch/s, Batch Loss=0.0284, Avg Loss=0.0802, Time Left=16.97\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1488/3393 [12:14<16:13,  1.96batch/s, Batch Loss=0.0284, Avg Loss=0.0802, Time Left=16.97\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1488/3393 [12:15<16:13,  1.96batch/s, Batch Loss=0.0834, Avg Loss=0.0802, Time Left=16.96\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1489/3393 [12:15<15:59,  1.98batch/s, Batch Loss=0.0834, Avg Loss=0.0802, Time Left=16.96\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1489/3393 [12:15<15:59,  1.98batch/s, Batch Loss=0.1367, Avg Loss=0.0802, Time Left=16.95\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1490/3393 [12:15<16:23,  1.94batch/s, Batch Loss=0.1367, Avg Loss=0.0802, Time Left=16.95\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1490/3393 [12:16<16:23,  1.94batch/s, Batch Loss=0.1225, Avg Loss=0.0802, Time Left=16.95\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1491/3393 [12:16<16:14,  1.95batch/s, Batch Loss=0.1225, Avg Loss=0.0802, Time Left=16.95\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1491/3393 [12:16<16:14,  1.95batch/s, Batch Loss=0.0688, Avg Loss=0.0802, Time Left=16.94\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1492/3393 [12:16<16:15,  1.95batch/s, Batch Loss=0.0688, Avg Loss=0.0802, Time Left=16.94\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1492/3393 [12:17<16:15,  1.95batch/s, Batch Loss=0.0399, Avg Loss=0.0802, Time Left=16.93\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1493/3393 [12:17<16:08,  1.96batch/s, Batch Loss=0.0399, Avg Loss=0.0802, Time Left=16.93\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1493/3393 [12:17<16:08,  1.96batch/s, Batch Loss=0.0518, Avg Loss=0.0802, Time Left=16.92\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1494/3393 [12:17<16:11,  1.95batch/s, Batch Loss=0.0518, Avg Loss=0.0802, Time Left=16.92\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1494/3393 [12:18<16:11,  1.95batch/s, Batch Loss=0.0506, Avg Loss=0.0802, Time Left=16.91\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1495/3393 [12:18<15:58,  1.98batch/s, Batch Loss=0.0506, Avg Loss=0.0802, Time Left=16.91\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1495/3393 [12:18<15:58,  1.98batch/s, Batch Loss=0.1800, Avg Loss=0.0802, Time Left=16.90\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1496/3393 [12:18<15:43,  2.01batch/s, Batch Loss=0.1800, Avg Loss=0.0802, Time Left=16.90\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1496/3393 [12:19<15:43,  2.01batch/s, Batch Loss=0.1087, Avg Loss=0.0802, Time Left=16.89\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1497/3393 [12:19<15:35,  2.03batch/s, Batch Loss=0.1087, Avg Loss=0.0802, Time Left=16.89\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1497/3393 [12:19<15:35,  2.03batch/s, Batch Loss=0.1333, Avg Loss=0.0803, Time Left=16.88\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1498/3393 [12:19<15:20,  2.06batch/s, Batch Loss=0.1333, Avg Loss=0.0803, Time Left=16.88\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1498/3393 [12:20<15:20,  2.06batch/s, Batch Loss=0.1190, Avg Loss=0.0803, Time Left=16.87\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1499/3393 [12:20<15:45,  2.00batch/s, Batch Loss=0.1190, Avg Loss=0.0803, Time Left=16.87\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1499/3393 [12:20<15:45,  2.00batch/s, Batch Loss=0.0443, Avg Loss=0.0803, Time Left=16.87\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1500/3393 [12:20<15:46,  2.00batch/s, Batch Loss=0.0443, Avg Loss=0.0803, Time Left=16.87\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1500/3393 [12:21<15:46,  2.00batch/s, Batch Loss=0.0286, Avg Loss=0.0802, Time Left=16.86\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1501/3393 [12:21<15:55,  1.98batch/s, Batch Loss=0.0286, Avg Loss=0.0802, Time Left=16.86\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1501/3393 [12:21<15:55,  1.98batch/s, Batch Loss=0.0228, Avg Loss=0.0802, Time Left=16.85\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1502/3393 [12:21<15:48,  1.99batch/s, Batch Loss=0.0228, Avg Loss=0.0802, Time Left=16.85\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1502/3393 [12:22<15:48,  1.99batch/s, Batch Loss=0.0292, Avg Loss=0.0802, Time Left=16.84\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1503/3393 [12:22<16:09,  1.95batch/s, Batch Loss=0.0292, Avg Loss=0.0802, Time Left=16.84\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1503/3393 [12:22<16:09,  1.95batch/s, Batch Loss=0.0125, Avg Loss=0.0801, Time Left=16.83\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1504/3393 [12:22<16:03,  1.96batch/s, Batch Loss=0.0125, Avg Loss=0.0801, Time Left=16.83\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1504/3393 [12:23<16:03,  1.96batch/s, Batch Loss=0.1784, Avg Loss=0.0802, Time Left=16.82\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1505/3393 [12:23<16:14,  1.94batch/s, Batch Loss=0.1784, Avg Loss=0.0802, Time Left=16.82\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1505/3393 [12:23<16:14,  1.94batch/s, Batch Loss=0.0410, Avg Loss=0.0802, Time Left=16.81\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1506/3393 [12:23<16:09,  1.95batch/s, Batch Loss=0.0410, Avg Loss=0.0802, Time Left=16.81\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1506/3393 [12:24<16:09,  1.95batch/s, Batch Loss=0.0091, Avg Loss=0.0801, Time Left=16.80\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1507/3393 [12:24<16:07,  1.95batch/s, Batch Loss=0.0091, Avg Loss=0.0801, Time Left=16.80\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1507/3393 [12:24<16:07,  1.95batch/s, Batch Loss=0.0388, Avg Loss=0.0801, Time Left=16.80\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1508/3393 [12:24<16:00,  1.96batch/s, Batch Loss=0.0388, Avg Loss=0.0801, Time Left=16.80\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1508/3393 [12:25<16:00,  1.96batch/s, Batch Loss=0.1134, Avg Loss=0.0801, Time Left=16.79\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1509/3393 [12:25<16:03,  1.96batch/s, Batch Loss=0.1134, Avg Loss=0.0801, Time Left=16.79\u001b[A\n",
      "Epoch 3/3 - Training:  44%|▍| 1509/3393 [12:25<16:03,  1.96batch/s, Batch Loss=0.0615, Avg Loss=0.0801, Time Left=16.78\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1510/3393 [12:25<16:02,  1.96batch/s, Batch Loss=0.0615, Avg Loss=0.0801, Time Left=16.78\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1510/3393 [12:26<16:02,  1.96batch/s, Batch Loss=0.0764, Avg Loss=0.0801, Time Left=16.77\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1511/3393 [12:26<16:08,  1.94batch/s, Batch Loss=0.0764, Avg Loss=0.0801, Time Left=16.77\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1511/3393 [12:26<16:08,  1.94batch/s, Batch Loss=0.3422, Avg Loss=0.0803, Time Left=16.76\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1512/3393 [12:26<16:00,  1.96batch/s, Batch Loss=0.3422, Avg Loss=0.0803, Time Left=16.76\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1512/3393 [12:27<16:00,  1.96batch/s, Batch Loss=0.0172, Avg Loss=0.0802, Time Left=16.75\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1513/3393 [12:27<16:12,  1.93batch/s, Batch Loss=0.0172, Avg Loss=0.0802, Time Left=16.75\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1513/3393 [12:27<16:12,  1.93batch/s, Batch Loss=0.1504, Avg Loss=0.0803, Time Left=16.74\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1514/3393 [12:27<16:02,  1.95batch/s, Batch Loss=0.1504, Avg Loss=0.0803, Time Left=16.74\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1514/3393 [12:28<16:02,  1.95batch/s, Batch Loss=0.0314, Avg Loss=0.0802, Time Left=16.73\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1515/3393 [12:28<16:04,  1.95batch/s, Batch Loss=0.0314, Avg Loss=0.0802, Time Left=16.73\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1515/3393 [12:28<16:04,  1.95batch/s, Batch Loss=0.0642, Avg Loss=0.0802, Time Left=16.73\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  45%|▍| 1516/3393 [12:28<15:48,  1.98batch/s, Batch Loss=0.0642, Avg Loss=0.0802, Time Left=16.73\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1516/3393 [12:29<15:48,  1.98batch/s, Batch Loss=0.1493, Avg Loss=0.0803, Time Left=16.72\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1517/3393 [12:29<16:02,  1.95batch/s, Batch Loss=0.1493, Avg Loss=0.0803, Time Left=16.72\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1517/3393 [12:29<16:02,  1.95batch/s, Batch Loss=0.0224, Avg Loss=0.0802, Time Left=16.71\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1518/3393 [12:29<15:47,  1.98batch/s, Batch Loss=0.0224, Avg Loss=0.0802, Time Left=16.71\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1518/3393 [12:30<15:47,  1.98batch/s, Batch Loss=0.1809, Avg Loss=0.0803, Time Left=16.70\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1519/3393 [12:30<16:01,  1.95batch/s, Batch Loss=0.1809, Avg Loss=0.0803, Time Left=16.70\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1519/3393 [12:30<16:01,  1.95batch/s, Batch Loss=0.0992, Avg Loss=0.0803, Time Left=16.69\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1520/3393 [12:30<15:54,  1.96batch/s, Batch Loss=0.0992, Avg Loss=0.0803, Time Left=16.69\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1520/3393 [12:31<15:54,  1.96batch/s, Batch Loss=0.0159, Avg Loss=0.0803, Time Left=16.68\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1521/3393 [12:31<16:05,  1.94batch/s, Batch Loss=0.0159, Avg Loss=0.0803, Time Left=16.68\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1521/3393 [12:31<16:05,  1.94batch/s, Batch Loss=0.0090, Avg Loss=0.0802, Time Left=16.67\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1522/3393 [12:32<15:58,  1.95batch/s, Batch Loss=0.0090, Avg Loss=0.0802, Time Left=16.67\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1522/3393 [12:32<15:58,  1.95batch/s, Batch Loss=0.0169, Avg Loss=0.0802, Time Left=16.66\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1523/3393 [12:32<15:43,  1.98batch/s, Batch Loss=0.0169, Avg Loss=0.0802, Time Left=16.66\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1523/3393 [12:32<15:43,  1.98batch/s, Batch Loss=0.0144, Avg Loss=0.0801, Time Left=16.66\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1524/3393 [12:32<15:30,  2.01batch/s, Batch Loss=0.0144, Avg Loss=0.0801, Time Left=16.66\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1524/3393 [12:33<15:30,  2.01batch/s, Batch Loss=0.0196, Avg Loss=0.0801, Time Left=16.65\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1525/3393 [12:33<15:30,  2.01batch/s, Batch Loss=0.0196, Avg Loss=0.0801, Time Left=16.65\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1525/3393 [12:33<15:30,  2.01batch/s, Batch Loss=0.0427, Avg Loss=0.0801, Time Left=16.64\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1526/3393 [12:33<15:40,  1.98batch/s, Batch Loss=0.0427, Avg Loss=0.0801, Time Left=16.64\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1526/3393 [12:34<15:40,  1.98batch/s, Batch Loss=0.1413, Avg Loss=0.0801, Time Left=16.63\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1527/3393 [12:34<15:28,  2.01batch/s, Batch Loss=0.1413, Avg Loss=0.0801, Time Left=16.63\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1527/3393 [12:34<15:28,  2.01batch/s, Batch Loss=0.0280, Avg Loss=0.0801, Time Left=16.62\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1528/3393 [12:35<15:54,  1.95batch/s, Batch Loss=0.0280, Avg Loss=0.0801, Time Left=16.62\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1528/3393 [12:35<15:54,  1.95batch/s, Batch Loss=0.0322, Avg Loss=0.0800, Time Left=16.61\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1529/3393 [12:35<15:42,  1.98batch/s, Batch Loss=0.0322, Avg Loss=0.0800, Time Left=16.61\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1529/3393 [12:36<15:42,  1.98batch/s, Batch Loss=0.0761, Avg Loss=0.0800, Time Left=16.60\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1530/3393 [12:36<15:46,  1.97batch/s, Batch Loss=0.0761, Avg Loss=0.0800, Time Left=16.60\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1530/3393 [12:36<15:46,  1.97batch/s, Batch Loss=0.0049, Avg Loss=0.0800, Time Left=16.59\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1531/3393 [12:36<15:34,  1.99batch/s, Batch Loss=0.0049, Avg Loss=0.0800, Time Left=16.59\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1531/3393 [12:37<15:34,  1.99batch/s, Batch Loss=0.0068, Avg Loss=0.0799, Time Left=16.58\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1532/3393 [12:37<15:41,  1.98batch/s, Batch Loss=0.0068, Avg Loss=0.0799, Time Left=16.58\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1532/3393 [12:37<15:41,  1.98batch/s, Batch Loss=0.0045, Avg Loss=0.0799, Time Left=16.58\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1533/3393 [12:37<15:37,  1.98batch/s, Batch Loss=0.0045, Avg Loss=0.0799, Time Left=16.58\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1533/3393 [12:38<15:37,  1.98batch/s, Batch Loss=0.0092, Avg Loss=0.0798, Time Left=16.57\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1534/3393 [12:38<15:47,  1.96batch/s, Batch Loss=0.0092, Avg Loss=0.0798, Time Left=16.57\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1534/3393 [12:38<15:47,  1.96batch/s, Batch Loss=0.0952, Avg Loss=0.0799, Time Left=16.56\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1535/3393 [12:38<15:39,  1.98batch/s, Batch Loss=0.0952, Avg Loss=0.0799, Time Left=16.56\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1535/3393 [12:39<15:39,  1.98batch/s, Batch Loss=0.0051, Avg Loss=0.0798, Time Left=16.55\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1536/3393 [12:39<15:53,  1.95batch/s, Batch Loss=0.0051, Avg Loss=0.0798, Time Left=16.55\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1536/3393 [12:39<15:53,  1.95batch/s, Batch Loss=0.1471, Avg Loss=0.0799, Time Left=16.54\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1537/3393 [12:39<15:47,  1.96batch/s, Batch Loss=0.1471, Avg Loss=0.0799, Time Left=16.54\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1537/3393 [12:40<15:47,  1.96batch/s, Batch Loss=0.0057, Avg Loss=0.0798, Time Left=16.53\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1538/3393 [12:40<15:49,  1.95batch/s, Batch Loss=0.0057, Avg Loss=0.0798, Time Left=16.53\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1538/3393 [12:40<15:49,  1.95batch/s, Batch Loss=0.0401, Avg Loss=0.0798, Time Left=16.52\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1539/3393 [12:40<15:43,  1.96batch/s, Batch Loss=0.0401, Avg Loss=0.0798, Time Left=16.52\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1539/3393 [12:41<15:43,  1.96batch/s, Batch Loss=0.0045, Avg Loss=0.0797, Time Left=16.52\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1540/3393 [12:41<16:11,  1.91batch/s, Batch Loss=0.0045, Avg Loss=0.0797, Time Left=16.52\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1540/3393 [12:41<16:11,  1.91batch/s, Batch Loss=0.1134, Avg Loss=0.0797, Time Left=16.51\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1541/3393 [12:41<15:52,  1.94batch/s, Batch Loss=0.1134, Avg Loss=0.0797, Time Left=16.51\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1541/3393 [12:42<15:52,  1.94batch/s, Batch Loss=0.0206, Avg Loss=0.0797, Time Left=16.50\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1542/3393 [12:42<15:51,  1.95batch/s, Batch Loss=0.0206, Avg Loss=0.0797, Time Left=16.50\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1542/3393 [12:42<15:51,  1.95batch/s, Batch Loss=0.3286, Avg Loss=0.0799, Time Left=16.49\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1543/3393 [12:42<15:44,  1.96batch/s, Batch Loss=0.3286, Avg Loss=0.0799, Time Left=16.49\u001b[A\n",
      "Epoch 3/3 - Training:  45%|▍| 1543/3393 [12:43<15:44,  1.96batch/s, Batch Loss=0.0072, Avg Loss=0.0798, Time Left=16.48\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1544/3393 [12:43<15:56,  1.93batch/s, Batch Loss=0.0072, Avg Loss=0.0798, Time Left=16.48\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1544/3393 [12:43<15:56,  1.93batch/s, Batch Loss=0.0686, Avg Loss=0.0798, Time Left=16.47\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1545/3393 [12:43<15:39,  1.97batch/s, Batch Loss=0.0686, Avg Loss=0.0798, Time Left=16.47\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1545/3393 [12:44<15:39,  1.97batch/s, Batch Loss=0.0037, Avg Loss=0.0798, Time Left=16.46\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1546/3393 [12:44<15:50,  1.94batch/s, Batch Loss=0.0037, Avg Loss=0.0798, Time Left=16.46\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1546/3393 [12:44<15:50,  1.94batch/s, Batch Loss=0.3899, Avg Loss=0.0800, Time Left=16.45\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1547/3393 [12:44<15:43,  1.96batch/s, Batch Loss=0.3899, Avg Loss=0.0800, Time Left=16.45\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1547/3393 [12:45<15:43,  1.96batch/s, Batch Loss=0.0063, Avg Loss=0.0799, Time Left=16.45\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1548/3393 [12:45<15:54,  1.93batch/s, Batch Loss=0.0063, Avg Loss=0.0799, Time Left=16.45\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1548/3393 [12:45<15:54,  1.93batch/s, Batch Loss=0.0304, Avg Loss=0.0799, Time Left=16.44\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  46%|▍| 1549/3393 [12:45<15:36,  1.97batch/s, Batch Loss=0.0304, Avg Loss=0.0799, Time Left=16.44\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1549/3393 [12:46<15:36,  1.97batch/s, Batch Loss=0.0260, Avg Loss=0.0799, Time Left=16.43\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1550/3393 [12:46<15:57,  1.92batch/s, Batch Loss=0.0260, Avg Loss=0.0799, Time Left=16.43\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1550/3393 [12:46<15:57,  1.92batch/s, Batch Loss=0.0871, Avg Loss=0.0799, Time Left=16.42\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1551/3393 [12:46<15:30,  1.98batch/s, Batch Loss=0.0871, Avg Loss=0.0799, Time Left=16.42\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1551/3393 [12:47<15:30,  1.98batch/s, Batch Loss=0.3873, Avg Loss=0.0801, Time Left=16.41\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1552/3393 [12:47<15:26,  1.99batch/s, Batch Loss=0.3873, Avg Loss=0.0801, Time Left=16.41\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1552/3393 [12:47<15:26,  1.99batch/s, Batch Loss=0.0037, Avg Loss=0.0800, Time Left=16.40\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1553/3393 [12:47<15:16,  2.01batch/s, Batch Loss=0.0037, Avg Loss=0.0800, Time Left=16.40\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1553/3393 [12:48<15:16,  2.01batch/s, Batch Loss=0.0238, Avg Loss=0.0800, Time Left=16.39\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1554/3393 [12:48<15:24,  1.99batch/s, Batch Loss=0.0238, Avg Loss=0.0800, Time Left=16.39\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1554/3393 [12:48<15:24,  1.99batch/s, Batch Loss=0.1059, Avg Loss=0.0800, Time Left=16.38\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1555/3393 [12:48<15:14,  2.01batch/s, Batch Loss=0.1059, Avg Loss=0.0800, Time Left=16.38\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1555/3393 [12:49<15:14,  2.01batch/s, Batch Loss=0.0632, Avg Loss=0.0800, Time Left=16.37\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1556/3393 [12:49<15:15,  2.01batch/s, Batch Loss=0.0632, Avg Loss=0.0800, Time Left=16.37\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1556/3393 [12:49<15:15,  2.01batch/s, Batch Loss=0.0034, Avg Loss=0.0799, Time Left=16.37\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1557/3393 [12:49<15:16,  2.00batch/s, Batch Loss=0.0034, Avg Loss=0.0799, Time Left=16.37\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1557/3393 [12:50<15:16,  2.00batch/s, Batch Loss=0.1764, Avg Loss=0.0800, Time Left=16.36\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1558/3393 [12:50<15:16,  2.00batch/s, Batch Loss=0.1764, Avg Loss=0.0800, Time Left=16.36\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1558/3393 [12:50<15:16,  2.00batch/s, Batch Loss=0.0582, Avg Loss=0.0800, Time Left=16.35\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1559/3393 [12:50<15:24,  1.98batch/s, Batch Loss=0.0582, Avg Loss=0.0800, Time Left=16.35\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1559/3393 [12:51<15:24,  1.98batch/s, Batch Loss=0.0289, Avg Loss=0.0799, Time Left=16.34\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1560/3393 [12:51<15:14,  2.01batch/s, Batch Loss=0.0289, Avg Loss=0.0799, Time Left=16.34\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1560/3393 [12:51<15:14,  2.01batch/s, Batch Loss=0.1073, Avg Loss=0.0800, Time Left=16.33\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1561/3393 [12:51<15:31,  1.97batch/s, Batch Loss=0.1073, Avg Loss=0.0800, Time Left=16.33\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1561/3393 [12:52<15:31,  1.97batch/s, Batch Loss=0.3107, Avg Loss=0.0801, Time Left=16.32\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1562/3393 [12:52<15:27,  1.97batch/s, Batch Loss=0.3107, Avg Loss=0.0801, Time Left=16.32\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1562/3393 [12:52<15:27,  1.97batch/s, Batch Loss=0.0138, Avg Loss=0.0801, Time Left=16.31\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1563/3393 [12:52<15:40,  1.95batch/s, Batch Loss=0.0138, Avg Loss=0.0801, Time Left=16.31\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1563/3393 [12:53<15:40,  1.95batch/s, Batch Loss=0.0065, Avg Loss=0.0800, Time Left=16.30\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1564/3393 [12:53<15:46,  1.93batch/s, Batch Loss=0.0065, Avg Loss=0.0800, Time Left=16.30\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1564/3393 [12:53<15:46,  1.93batch/s, Batch Loss=0.0285, Avg Loss=0.0800, Time Left=16.30\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1565/3393 [12:53<15:58,  1.91batch/s, Batch Loss=0.0285, Avg Loss=0.0800, Time Left=16.30\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1565/3393 [12:54<15:58,  1.91batch/s, Batch Loss=0.0869, Avg Loss=0.0800, Time Left=16.29\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1566/3393 [12:54<15:38,  1.95batch/s, Batch Loss=0.0869, Avg Loss=0.0800, Time Left=16.29\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1566/3393 [12:54<15:38,  1.95batch/s, Batch Loss=0.0524, Avg Loss=0.0800, Time Left=16.28\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1567/3393 [12:54<15:46,  1.93batch/s, Batch Loss=0.0524, Avg Loss=0.0800, Time Left=16.28\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1567/3393 [12:55<15:46,  1.93batch/s, Batch Loss=0.0239, Avg Loss=0.0799, Time Left=16.27\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1568/3393 [12:55<15:38,  1.95batch/s, Batch Loss=0.0239, Avg Loss=0.0799, Time Left=16.27\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1568/3393 [12:55<15:38,  1.95batch/s, Batch Loss=0.0243, Avg Loss=0.0799, Time Left=16.26\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1569/3393 [12:55<15:29,  1.96batch/s, Batch Loss=0.0243, Avg Loss=0.0799, Time Left=16.26\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1569/3393 [12:56<15:29,  1.96batch/s, Batch Loss=0.0366, Avg Loss=0.0799, Time Left=16.25\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1570/3393 [12:56<15:44,  1.93batch/s, Batch Loss=0.0366, Avg Loss=0.0799, Time Left=16.25\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1570/3393 [12:56<15:44,  1.93batch/s, Batch Loss=0.0177, Avg Loss=0.0798, Time Left=16.24\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1571/3393 [12:56<15:23,  1.97batch/s, Batch Loss=0.0177, Avg Loss=0.0798, Time Left=16.24\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1571/3393 [12:57<15:23,  1.97batch/s, Batch Loss=0.0234, Avg Loss=0.0798, Time Left=16.23\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1572/3393 [12:57<15:10,  2.00batch/s, Batch Loss=0.0234, Avg Loss=0.0798, Time Left=16.23\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1572/3393 [12:57<15:10,  2.00batch/s, Batch Loss=0.0489, Avg Loss=0.0798, Time Left=16.23\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1573/3393 [12:57<15:18,  1.98batch/s, Batch Loss=0.0489, Avg Loss=0.0798, Time Left=16.23\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1573/3393 [12:58<15:18,  1.98batch/s, Batch Loss=0.0146, Avg Loss=0.0797, Time Left=16.22\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1574/3393 [12:58<15:07,  2.00batch/s, Batch Loss=0.0146, Avg Loss=0.0797, Time Left=16.22\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1574/3393 [12:58<15:07,  2.00batch/s, Batch Loss=0.0145, Avg Loss=0.0797, Time Left=16.21\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1575/3393 [12:58<15:24,  1.97batch/s, Batch Loss=0.0145, Avg Loss=0.0797, Time Left=16.21\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1575/3393 [12:59<15:24,  1.97batch/s, Batch Loss=0.1152, Avg Loss=0.0797, Time Left=16.20\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1576/3393 [12:59<15:02,  2.01batch/s, Batch Loss=0.1152, Avg Loss=0.0797, Time Left=16.20\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1576/3393 [12:59<15:02,  2.01batch/s, Batch Loss=0.2264, Avg Loss=0.0798, Time Left=16.19\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1577/3393 [12:59<15:12,  1.99batch/s, Batch Loss=0.2264, Avg Loss=0.0798, Time Left=16.19\u001b[A\n",
      "Epoch 3/3 - Training:  46%|▍| 1577/3393 [13:00<15:12,  1.99batch/s, Batch Loss=0.2185, Avg Loss=0.0799, Time Left=16.18\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1578/3393 [13:00<14:53,  2.03batch/s, Batch Loss=0.2185, Avg Loss=0.0799, Time Left=16.18\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1578/3393 [13:00<14:53,  2.03batch/s, Batch Loss=0.1963, Avg Loss=0.0800, Time Left=16.17\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1579/3393 [13:00<14:48,  2.04batch/s, Batch Loss=0.1963, Avg Loss=0.0800, Time Left=16.17\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1579/3393 [13:01<14:48,  2.04batch/s, Batch Loss=0.0732, Avg Loss=0.0800, Time Left=16.16\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1580/3393 [13:01<15:10,  1.99batch/s, Batch Loss=0.0732, Avg Loss=0.0800, Time Left=16.16\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1580/3393 [13:01<15:10,  1.99batch/s, Batch Loss=0.0085, Avg Loss=0.0799, Time Left=16.15\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1581/3393 [13:01<15:09,  1.99batch/s, Batch Loss=0.0085, Avg Loss=0.0799, Time Left=16.15\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1581/3393 [13:02<15:09,  1.99batch/s, Batch Loss=0.0810, Avg Loss=0.0799, Time Left=16.15\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  47%|▍| 1582/3393 [13:02<15:25,  1.96batch/s, Batch Loss=0.0810, Avg Loss=0.0799, Time Left=16.15\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1582/3393 [13:02<15:25,  1.96batch/s, Batch Loss=0.0473, Avg Loss=0.0799, Time Left=16.14\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1583/3393 [13:02<15:19,  1.97batch/s, Batch Loss=0.0473, Avg Loss=0.0799, Time Left=16.14\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1583/3393 [13:03<15:19,  1.97batch/s, Batch Loss=0.0512, Avg Loss=0.0799, Time Left=16.13\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1584/3393 [13:03<15:15,  1.97batch/s, Batch Loss=0.0512, Avg Loss=0.0799, Time Left=16.13\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1584/3393 [13:03<15:15,  1.97batch/s, Batch Loss=0.1048, Avg Loss=0.0799, Time Left=16.12\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1585/3393 [13:03<15:03,  2.00batch/s, Batch Loss=0.1048, Avg Loss=0.0799, Time Left=16.12\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1585/3393 [13:04<15:03,  2.00batch/s, Batch Loss=0.0247, Avg Loss=0.0799, Time Left=16.11\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1586/3393 [13:04<14:50,  2.03batch/s, Batch Loss=0.0247, Avg Loss=0.0799, Time Left=16.11\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1586/3393 [13:04<14:50,  2.03batch/s, Batch Loss=0.0309, Avg Loss=0.0798, Time Left=16.10\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1587/3393 [13:04<15:06,  1.99batch/s, Batch Loss=0.0309, Avg Loss=0.0798, Time Left=16.10\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1587/3393 [13:05<15:06,  1.99batch/s, Batch Loss=0.1447, Avg Loss=0.0799, Time Left=16.09\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1588/3393 [13:05<15:05,  1.99batch/s, Batch Loss=0.1447, Avg Loss=0.0799, Time Left=16.09\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1588/3393 [13:05<15:05,  1.99batch/s, Batch Loss=0.0328, Avg Loss=0.0798, Time Left=16.08\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1589/3393 [13:05<15:12,  1.98batch/s, Batch Loss=0.0328, Avg Loss=0.0798, Time Left=16.08\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1589/3393 [13:06<15:12,  1.98batch/s, Batch Loss=0.0274, Avg Loss=0.0798, Time Left=16.07\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1590/3393 [13:06<15:00,  2.00batch/s, Batch Loss=0.0274, Avg Loss=0.0798, Time Left=16.07\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1590/3393 [13:06<15:00,  2.00batch/s, Batch Loss=0.0123, Avg Loss=0.0798, Time Left=16.06\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1591/3393 [13:06<15:01,  2.00batch/s, Batch Loss=0.0123, Avg Loss=0.0798, Time Left=16.06\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1591/3393 [13:07<15:01,  2.00batch/s, Batch Loss=0.0542, Avg Loss=0.0798, Time Left=16.06\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1592/3393 [13:07<15:01,  2.00batch/s, Batch Loss=0.0542, Avg Loss=0.0798, Time Left=16.06\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1592/3393 [13:07<15:01,  2.00batch/s, Batch Loss=0.0033, Avg Loss=0.0797, Time Left=16.05\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1593/3393 [13:07<15:16,  1.96batch/s, Batch Loss=0.0033, Avg Loss=0.0797, Time Left=16.05\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1593/3393 [13:08<15:16,  1.96batch/s, Batch Loss=0.0293, Avg Loss=0.0797, Time Left=16.04\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1594/3393 [13:08<15:12,  1.97batch/s, Batch Loss=0.0293, Avg Loss=0.0797, Time Left=16.04\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1594/3393 [13:08<15:12,  1.97batch/s, Batch Loss=0.1499, Avg Loss=0.0797, Time Left=16.03\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1595/3393 [13:08<15:25,  1.94batch/s, Batch Loss=0.1499, Avg Loss=0.0797, Time Left=16.03\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1595/3393 [13:09<15:25,  1.94batch/s, Batch Loss=0.0363, Avg Loss=0.0797, Time Left=16.02\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1596/3393 [13:09<15:17,  1.96batch/s, Batch Loss=0.0363, Avg Loss=0.0797, Time Left=16.02\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1596/3393 [13:09<15:17,  1.96batch/s, Batch Loss=0.0099, Avg Loss=0.0796, Time Left=16.01\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1597/3393 [13:10<15:28,  1.94batch/s, Batch Loss=0.0099, Avg Loss=0.0796, Time Left=16.01\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1597/3393 [13:10<15:28,  1.94batch/s, Batch Loss=0.0710, Avg Loss=0.0796, Time Left=16.00\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1598/3393 [13:10<15:11,  1.97batch/s, Batch Loss=0.0710, Avg Loss=0.0796, Time Left=16.00\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1598/3393 [13:10<15:11,  1.97batch/s, Batch Loss=0.1511, Avg Loss=0.0797, Time Left=16.00\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1599/3393 [13:10<15:07,  1.98batch/s, Batch Loss=0.1511, Avg Loss=0.0797, Time Left=16.00\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1599/3393 [13:11<15:07,  1.98batch/s, Batch Loss=0.0651, Avg Loss=0.0797, Time Left=15.99\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1600/3393 [13:11<14:55,  2.00batch/s, Batch Loss=0.0651, Avg Loss=0.0797, Time Left=15.99\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1600/3393 [13:11<14:55,  2.00batch/s, Batch Loss=0.0431, Avg Loss=0.0796, Time Left=15.98\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1601/3393 [13:11<14:55,  2.00batch/s, Batch Loss=0.0431, Avg Loss=0.0796, Time Left=15.98\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1601/3393 [13:12<14:55,  2.00batch/s, Batch Loss=0.0874, Avg Loss=0.0797, Time Left=15.97\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1602/3393 [13:12<14:38,  2.04batch/s, Batch Loss=0.0874, Avg Loss=0.0797, Time Left=15.97\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1602/3393 [13:12<14:38,  2.04batch/s, Batch Loss=0.0036, Avg Loss=0.0796, Time Left=15.96\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1603/3393 [13:12<14:49,  2.01batch/s, Batch Loss=0.0036, Avg Loss=0.0796, Time Left=15.96\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1603/3393 [13:13<14:49,  2.01batch/s, Batch Loss=0.0708, Avg Loss=0.0796, Time Left=15.95\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1604/3393 [13:13<15:07,  1.97batch/s, Batch Loss=0.0708, Avg Loss=0.0796, Time Left=15.95\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1604/3393 [13:13<15:07,  1.97batch/s, Batch Loss=0.1546, Avg Loss=0.0796, Time Left=15.94\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1605/3393 [13:13<15:05,  1.97batch/s, Batch Loss=0.1546, Avg Loss=0.0796, Time Left=15.94\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1605/3393 [13:14<15:05,  1.97batch/s, Batch Loss=0.0509, Avg Loss=0.0796, Time Left=15.93\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1606/3393 [13:14<15:19,  1.94batch/s, Batch Loss=0.0509, Avg Loss=0.0796, Time Left=15.93\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1606/3393 [13:15<15:19,  1.94batch/s, Batch Loss=0.0450, Avg Loss=0.0796, Time Left=15.92\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1607/3393 [13:15<15:14,  1.95batch/s, Batch Loss=0.0450, Avg Loss=0.0796, Time Left=15.92\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1607/3393 [13:15<15:14,  1.95batch/s, Batch Loss=0.1773, Avg Loss=0.0797, Time Left=15.92\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1608/3393 [13:15<15:07,  1.97batch/s, Batch Loss=0.1773, Avg Loss=0.0797, Time Left=15.92\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1608/3393 [13:16<15:07,  1.97batch/s, Batch Loss=0.0401, Avg Loss=0.0796, Time Left=15.91\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1609/3393 [13:16<14:51,  2.00batch/s, Batch Loss=0.0401, Avg Loss=0.0796, Time Left=15.91\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1609/3393 [13:16<14:51,  2.00batch/s, Batch Loss=0.0053, Avg Loss=0.0796, Time Left=15.90\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1610/3393 [13:16<14:51,  2.00batch/s, Batch Loss=0.0053, Avg Loss=0.0796, Time Left=15.90\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1610/3393 [13:16<14:51,  2.00batch/s, Batch Loss=0.0024, Avg Loss=0.0795, Time Left=15.89\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1611/3393 [13:17<14:51,  2.00batch/s, Batch Loss=0.0024, Avg Loss=0.0795, Time Left=15.89\u001b[A\n",
      "Epoch 3/3 - Training:  47%|▍| 1611/3393 [13:17<14:51,  2.00batch/s, Batch Loss=0.0482, Avg Loss=0.0795, Time Left=15.88\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1612/3393 [13:17<14:41,  2.02batch/s, Batch Loss=0.0482, Avg Loss=0.0795, Time Left=15.88\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1612/3393 [13:18<14:41,  2.02batch/s, Batch Loss=0.1862, Avg Loss=0.0796, Time Left=15.87\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1613/3393 [13:18<14:52,  1.99batch/s, Batch Loss=0.1862, Avg Loss=0.0796, Time Left=15.87\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1613/3393 [13:18<14:52,  1.99batch/s, Batch Loss=0.2499, Avg Loss=0.0797, Time Left=15.86\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1614/3393 [13:18<14:42,  2.02batch/s, Batch Loss=0.2499, Avg Loss=0.0797, Time Left=15.86\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1614/3393 [13:19<14:42,  2.02batch/s, Batch Loss=0.0103, Avg Loss=0.0797, Time Left=15.85\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  48%|▍| 1615/3393 [13:19<15:00,  1.97batch/s, Batch Loss=0.0103, Avg Loss=0.0797, Time Left=15.85\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1615/3393 [13:19<15:00,  1.97batch/s, Batch Loss=0.0406, Avg Loss=0.0796, Time Left=15.84\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1616/3393 [13:19<14:58,  1.98batch/s, Batch Loss=0.0406, Avg Loss=0.0796, Time Left=15.84\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1616/3393 [13:20<14:58,  1.98batch/s, Batch Loss=0.0835, Avg Loss=0.0796, Time Left=15.84\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1617/3393 [13:20<15:11,  1.95batch/s, Batch Loss=0.0835, Avg Loss=0.0796, Time Left=15.84\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1617/3393 [13:20<15:11,  1.95batch/s, Batch Loss=0.2015, Avg Loss=0.0797, Time Left=15.83\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1618/3393 [13:20<14:56,  1.98batch/s, Batch Loss=0.2015, Avg Loss=0.0797, Time Left=15.83\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1618/3393 [13:21<14:56,  1.98batch/s, Batch Loss=0.0038, Avg Loss=0.0797, Time Left=15.82\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1619/3393 [13:21<15:18,  1.93batch/s, Batch Loss=0.0038, Avg Loss=0.0797, Time Left=15.82\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1619/3393 [13:21<15:18,  1.93batch/s, Batch Loss=0.2309, Avg Loss=0.0798, Time Left=15.81\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1620/3393 [13:21<15:01,  1.97batch/s, Batch Loss=0.2309, Avg Loss=0.0798, Time Left=15.81\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1620/3393 [13:22<15:01,  1.97batch/s, Batch Loss=0.0580, Avg Loss=0.0798, Time Left=15.80\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1621/3393 [13:22<14:58,  1.97batch/s, Batch Loss=0.0580, Avg Loss=0.0798, Time Left=15.80\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1621/3393 [13:22<14:58,  1.97batch/s, Batch Loss=0.0520, Avg Loss=0.0797, Time Left=15.79\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1622/3393 [13:22<14:35,  2.02batch/s, Batch Loss=0.0520, Avg Loss=0.0797, Time Left=15.79\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1622/3393 [13:23<14:35,  2.02batch/s, Batch Loss=0.0749, Avg Loss=0.0797, Time Left=15.78\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1623/3393 [13:23<15:02,  1.96batch/s, Batch Loss=0.0749, Avg Loss=0.0797, Time Left=15.78\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1623/3393 [13:23<15:02,  1.96batch/s, Batch Loss=0.3812, Avg Loss=0.0799, Time Left=15.77\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1624/3393 [13:23<14:58,  1.97batch/s, Batch Loss=0.3812, Avg Loss=0.0799, Time Left=15.77\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1624/3393 [13:24<14:58,  1.97batch/s, Batch Loss=0.1369, Avg Loss=0.0800, Time Left=15.77\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1625/3393 [13:24<15:02,  1.96batch/s, Batch Loss=0.1369, Avg Loss=0.0800, Time Left=15.77\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1625/3393 [13:24<15:02,  1.96batch/s, Batch Loss=0.0829, Avg Loss=0.0800, Time Left=15.76\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1626/3393 [13:24<14:56,  1.97batch/s, Batch Loss=0.0829, Avg Loss=0.0800, Time Left=15.76\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1626/3393 [13:25<14:56,  1.97batch/s, Batch Loss=0.2472, Avg Loss=0.0801, Time Left=15.75\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1627/3393 [13:25<15:09,  1.94batch/s, Batch Loss=0.2472, Avg Loss=0.0801, Time Left=15.75\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1627/3393 [13:25<15:09,  1.94batch/s, Batch Loss=0.1944, Avg Loss=0.0801, Time Left=15.74\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1628/3393 [13:25<15:02,  1.96batch/s, Batch Loss=0.1944, Avg Loss=0.0801, Time Left=15.74\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1628/3393 [13:26<15:02,  1.96batch/s, Batch Loss=0.1434, Avg Loss=0.0802, Time Left=15.73\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1629/3393 [13:26<15:12,  1.93batch/s, Batch Loss=0.1434, Avg Loss=0.0802, Time Left=15.73\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1629/3393 [13:26<15:12,  1.93batch/s, Batch Loss=0.0486, Avg Loss=0.0802, Time Left=15.72\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1630/3393 [13:26<15:12,  1.93batch/s, Batch Loss=0.0486, Avg Loss=0.0802, Time Left=15.72\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1630/3393 [13:27<15:12,  1.93batch/s, Batch Loss=0.0725, Avg Loss=0.0802, Time Left=15.71\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1631/3393 [13:27<14:54,  1.97batch/s, Batch Loss=0.0725, Avg Loss=0.0802, Time Left=15.71\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1631/3393 [13:27<14:54,  1.97batch/s, Batch Loss=0.0458, Avg Loss=0.0801, Time Left=15.71\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1632/3393 [13:27<14:51,  1.98batch/s, Batch Loss=0.0458, Avg Loss=0.0801, Time Left=15.71\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1632/3393 [13:28<14:51,  1.98batch/s, Batch Loss=0.1869, Avg Loss=0.0802, Time Left=15.70\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1633/3393 [13:28<14:47,  1.98batch/s, Batch Loss=0.1869, Avg Loss=0.0802, Time Left=15.70\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1633/3393 [13:28<14:47,  1.98batch/s, Batch Loss=0.0285, Avg Loss=0.0802, Time Left=15.69\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1634/3393 [13:28<14:36,  2.01batch/s, Batch Loss=0.0285, Avg Loss=0.0802, Time Left=15.69\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1634/3393 [13:29<14:36,  2.01batch/s, Batch Loss=0.0317, Avg Loss=0.0801, Time Left=15.68\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1635/3393 [13:29<14:45,  1.99batch/s, Batch Loss=0.0317, Avg Loss=0.0801, Time Left=15.68\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1635/3393 [13:29<14:45,  1.99batch/s, Batch Loss=0.0467, Avg Loss=0.0801, Time Left=15.67\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1636/3393 [13:29<14:34,  2.01batch/s, Batch Loss=0.0467, Avg Loss=0.0801, Time Left=15.67\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1636/3393 [13:30<14:34,  2.01batch/s, Batch Loss=0.0693, Avg Loss=0.0801, Time Left=15.66\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1637/3393 [13:30<14:43,  1.99batch/s, Batch Loss=0.0693, Avg Loss=0.0801, Time Left=15.66\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1637/3393 [13:30<14:43,  1.99batch/s, Batch Loss=0.0632, Avg Loss=0.0801, Time Left=15.65\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1638/3393 [13:30<14:30,  2.02batch/s, Batch Loss=0.0632, Avg Loss=0.0801, Time Left=15.65\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1638/3393 [13:31<14:30,  2.02batch/s, Batch Loss=0.1904, Avg Loss=0.0802, Time Left=15.64\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1639/3393 [13:31<14:34,  2.00batch/s, Batch Loss=0.1904, Avg Loss=0.0802, Time Left=15.64\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1639/3393 [13:31<14:34,  2.00batch/s, Batch Loss=0.0169, Avg Loss=0.0801, Time Left=15.63\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1640/3393 [13:31<14:43,  1.98batch/s, Batch Loss=0.0169, Avg Loss=0.0801, Time Left=15.63\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1640/3393 [13:32<14:43,  1.98batch/s, Batch Loss=0.1671, Avg Loss=0.0802, Time Left=15.63\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1641/3393 [13:32<14:50,  1.97batch/s, Batch Loss=0.1671, Avg Loss=0.0802, Time Left=15.63\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1641/3393 [13:32<14:50,  1.97batch/s, Batch Loss=0.0675, Avg Loss=0.0802, Time Left=15.62\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1642/3393 [13:32<15:02,  1.94batch/s, Batch Loss=0.0675, Avg Loss=0.0802, Time Left=15.62\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1642/3393 [13:33<15:02,  1.94batch/s, Batch Loss=0.0628, Avg Loss=0.0802, Time Left=15.61\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1643/3393 [13:33<15:03,  1.94batch/s, Batch Loss=0.0628, Avg Loss=0.0802, Time Left=15.61\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1643/3393 [13:33<15:03,  1.94batch/s, Batch Loss=0.0700, Avg Loss=0.0802, Time Left=15.60\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1644/3393 [13:33<14:46,  1.97batch/s, Batch Loss=0.0700, Avg Loss=0.0802, Time Left=15.60\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1644/3393 [13:34<14:46,  1.97batch/s, Batch Loss=0.1809, Avg Loss=0.0802, Time Left=15.59\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1645/3393 [13:34<14:52,  1.96batch/s, Batch Loss=0.1809, Avg Loss=0.0802, Time Left=15.59\u001b[A\n",
      "Epoch 3/3 - Training:  48%|▍| 1645/3393 [13:34<14:52,  1.96batch/s, Batch Loss=0.0078, Avg Loss=0.0802, Time Left=15.58\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1646/3393 [13:34<15:01,  1.94batch/s, Batch Loss=0.0078, Avg Loss=0.0802, Time Left=15.58\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1646/3393 [13:35<15:01,  1.94batch/s, Batch Loss=0.0258, Avg Loss=0.0801, Time Left=15.57\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1647/3393 [13:35<14:54,  1.95batch/s, Batch Loss=0.0258, Avg Loss=0.0801, Time Left=15.57\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1647/3393 [13:35<14:54,  1.95batch/s, Batch Loss=0.0663, Avg Loss=0.0801, Time Left=15.57\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  49%|▍| 1648/3393 [13:35<15:03,  1.93batch/s, Batch Loss=0.0663, Avg Loss=0.0801, Time Left=15.57\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1648/3393 [13:36<15:03,  1.93batch/s, Batch Loss=0.0735, Avg Loss=0.0801, Time Left=15.56\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1649/3393 [13:36<14:38,  1.99batch/s, Batch Loss=0.0735, Avg Loss=0.0801, Time Left=15.56\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1649/3393 [13:36<14:38,  1.99batch/s, Batch Loss=0.0095, Avg Loss=0.0801, Time Left=15.55\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1650/3393 [13:36<14:43,  1.97batch/s, Batch Loss=0.0095, Avg Loss=0.0801, Time Left=15.55\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1650/3393 [13:37<14:43,  1.97batch/s, Batch Loss=0.1541, Avg Loss=0.0801, Time Left=15.54\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1651/3393 [13:37<14:40,  1.98batch/s, Batch Loss=0.1541, Avg Loss=0.0801, Time Left=15.54\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1651/3393 [13:37<14:40,  1.98batch/s, Batch Loss=0.0695, Avg Loss=0.0801, Time Left=15.53\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1652/3393 [13:37<14:53,  1.95batch/s, Batch Loss=0.0695, Avg Loss=0.0801, Time Left=15.53\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1652/3393 [13:38<14:53,  1.95batch/s, Batch Loss=0.0356, Avg Loss=0.0801, Time Left=15.52\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1653/3393 [13:38<14:38,  1.98batch/s, Batch Loss=0.0356, Avg Loss=0.0801, Time Left=15.52\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1653/3393 [13:38<14:38,  1.98batch/s, Batch Loss=0.0021, Avg Loss=0.0800, Time Left=15.51\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1654/3393 [13:38<15:00,  1.93batch/s, Batch Loss=0.0021, Avg Loss=0.0800, Time Left=15.51\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1654/3393 [13:39<15:00,  1.93batch/s, Batch Loss=0.2509, Avg Loss=0.0802, Time Left=15.50\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1655/3393 [13:39<14:27,  2.00batch/s, Batch Loss=0.2509, Avg Loss=0.0802, Time Left=15.50\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1655/3393 [13:39<14:27,  2.00batch/s, Batch Loss=0.1881, Avg Loss=0.0802, Time Left=15.49\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1656/3393 [13:39<14:34,  1.99batch/s, Batch Loss=0.1881, Avg Loss=0.0802, Time Left=15.49\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1656/3393 [13:40<14:34,  1.99batch/s, Batch Loss=0.1980, Avg Loss=0.0803, Time Left=15.49\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1657/3393 [13:40<14:24,  2.01batch/s, Batch Loss=0.1980, Avg Loss=0.0803, Time Left=15.49\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1657/3393 [13:40<14:24,  2.01batch/s, Batch Loss=0.0784, Avg Loss=0.0803, Time Left=15.48\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1658/3393 [13:40<14:16,  2.03batch/s, Batch Loss=0.0784, Avg Loss=0.0803, Time Left=15.48\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1658/3393 [13:41<14:16,  2.03batch/s, Batch Loss=0.3358, Avg Loss=0.0805, Time Left=15.47\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1659/3393 [13:41<14:35,  1.98batch/s, Batch Loss=0.3358, Avg Loss=0.0805, Time Left=15.47\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1659/3393 [13:41<14:35,  1.98batch/s, Batch Loss=0.3988, Avg Loss=0.0807, Time Left=15.46\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1660/3393 [13:41<14:42,  1.96batch/s, Batch Loss=0.3988, Avg Loss=0.0807, Time Left=15.46\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1660/3393 [13:42<14:42,  1.96batch/s, Batch Loss=0.1132, Avg Loss=0.0807, Time Left=15.45\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1661/3393 [13:42<14:29,  1.99batch/s, Batch Loss=0.1132, Avg Loss=0.0807, Time Left=15.45\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1661/3393 [13:42<14:29,  1.99batch/s, Batch Loss=0.1271, Avg Loss=0.0807, Time Left=15.44\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1662/3393 [13:42<14:27,  1.99batch/s, Batch Loss=0.1271, Avg Loss=0.0807, Time Left=15.44\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1662/3393 [13:43<14:27,  1.99batch/s, Batch Loss=0.0180, Avg Loss=0.0807, Time Left=15.43\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1663/3393 [13:43<14:37,  1.97batch/s, Batch Loss=0.0180, Avg Loss=0.0807, Time Left=15.43\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1663/3393 [13:43<14:37,  1.97batch/s, Batch Loss=0.1001, Avg Loss=0.0807, Time Left=15.42\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1664/3393 [13:43<14:30,  1.99batch/s, Batch Loss=0.1001, Avg Loss=0.0807, Time Left=15.42\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1664/3393 [13:44<14:30,  1.99batch/s, Batch Loss=0.0674, Avg Loss=0.0807, Time Left=15.41\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1665/3393 [13:44<14:20,  2.01batch/s, Batch Loss=0.0674, Avg Loss=0.0807, Time Left=15.41\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1665/3393 [13:44<14:20,  2.01batch/s, Batch Loss=0.0877, Avg Loss=0.0807, Time Left=15.41\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1666/3393 [13:44<14:13,  2.02batch/s, Batch Loss=0.0877, Avg Loss=0.0807, Time Left=15.41\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1666/3393 [13:45<14:13,  2.02batch/s, Batch Loss=0.1353, Avg Loss=0.0807, Time Left=15.40\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1667/3393 [13:45<14:15,  2.02batch/s, Batch Loss=0.1353, Avg Loss=0.0807, Time Left=15.40\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1667/3393 [13:45<14:15,  2.02batch/s, Batch Loss=0.1154, Avg Loss=0.0807, Time Left=15.39\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1668/3393 [13:45<14:37,  1.97batch/s, Batch Loss=0.1154, Avg Loss=0.0807, Time Left=15.39\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1668/3393 [13:46<14:37,  1.97batch/s, Batch Loss=0.0963, Avg Loss=0.0807, Time Left=15.38\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1669/3393 [13:46<14:30,  1.98batch/s, Batch Loss=0.0963, Avg Loss=0.0807, Time Left=15.38\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1669/3393 [13:46<14:30,  1.98batch/s, Batch Loss=0.0859, Avg Loss=0.0807, Time Left=15.37\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1670/3393 [13:46<14:27,  1.99batch/s, Batch Loss=0.0859, Avg Loss=0.0807, Time Left=15.37\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1670/3393 [13:47<14:27,  1.99batch/s, Batch Loss=0.0395, Avg Loss=0.0807, Time Left=15.36\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1671/3393 [13:47<14:08,  2.03batch/s, Batch Loss=0.0395, Avg Loss=0.0807, Time Left=15.36\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1671/3393 [13:47<14:08,  2.03batch/s, Batch Loss=0.0409, Avg Loss=0.0807, Time Left=15.35\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1672/3393 [13:47<14:35,  1.96batch/s, Batch Loss=0.0409, Avg Loss=0.0807, Time Left=15.35\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1672/3393 [13:48<14:35,  1.96batch/s, Batch Loss=0.0664, Avg Loss=0.0807, Time Left=15.34\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1673/3393 [13:48<14:24,  1.99batch/s, Batch Loss=0.0664, Avg Loss=0.0807, Time Left=15.34\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1673/3393 [13:48<14:24,  1.99batch/s, Batch Loss=0.0321, Avg Loss=0.0807, Time Left=15.34\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1674/3393 [13:48<14:38,  1.96batch/s, Batch Loss=0.0321, Avg Loss=0.0807, Time Left=15.34\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1674/3393 [13:49<14:38,  1.96batch/s, Batch Loss=0.1668, Avg Loss=0.0807, Time Left=15.33\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1675/3393 [13:49<14:17,  2.00batch/s, Batch Loss=0.1668, Avg Loss=0.0807, Time Left=15.33\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1675/3393 [13:49<14:17,  2.00batch/s, Batch Loss=0.2473, Avg Loss=0.0808, Time Left=15.32\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1676/3393 [13:49<14:16,  2.01batch/s, Batch Loss=0.2473, Avg Loss=0.0808, Time Left=15.32\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1676/3393 [13:50<14:16,  2.01batch/s, Batch Loss=0.0364, Avg Loss=0.0808, Time Left=15.31\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1677/3393 [13:50<14:08,  2.02batch/s, Batch Loss=0.0364, Avg Loss=0.0808, Time Left=15.31\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1677/3393 [13:50<14:08,  2.02batch/s, Batch Loss=0.0029, Avg Loss=0.0807, Time Left=15.30\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1678/3393 [13:50<13:54,  2.06batch/s, Batch Loss=0.0029, Avg Loss=0.0807, Time Left=15.30\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1678/3393 [13:51<13:54,  2.06batch/s, Batch Loss=0.0619, Avg Loss=0.0807, Time Left=15.29\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1679/3393 [13:51<14:16,  2.00batch/s, Batch Loss=0.0619, Avg Loss=0.0807, Time Left=15.29\u001b[A\n",
      "Epoch 3/3 - Training:  49%|▍| 1679/3393 [13:51<14:16,  2.00batch/s, Batch Loss=0.0521, Avg Loss=0.0807, Time Left=15.28\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1680/3393 [13:51<14:16,  2.00batch/s, Batch Loss=0.0521, Avg Loss=0.0807, Time Left=15.28\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1680/3393 [13:52<14:16,  2.00batch/s, Batch Loss=0.0167, Avg Loss=0.0807, Time Left=15.27\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  50%|▍| 1681/3393 [13:52<14:31,  1.96batch/s, Batch Loss=0.0167, Avg Loss=0.0807, Time Left=15.27\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1681/3393 [13:52<14:31,  1.96batch/s, Batch Loss=0.0702, Avg Loss=0.0807, Time Left=15.26\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1682/3393 [13:52<14:28,  1.97batch/s, Batch Loss=0.0702, Avg Loss=0.0807, Time Left=15.26\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1682/3393 [13:53<14:28,  1.97batch/s, Batch Loss=0.1273, Avg Loss=0.0807, Time Left=15.26\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1683/3393 [13:53<14:32,  1.96batch/s, Batch Loss=0.1273, Avg Loss=0.0807, Time Left=15.26\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1683/3393 [13:53<14:32,  1.96batch/s, Batch Loss=0.0336, Avg Loss=0.0807, Time Left=15.25\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1684/3393 [13:53<14:27,  1.97batch/s, Batch Loss=0.0336, Avg Loss=0.0807, Time Left=15.25\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1684/3393 [13:54<14:27,  1.97batch/s, Batch Loss=0.0925, Avg Loss=0.0807, Time Left=15.24\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1685/3393 [13:54<14:23,  1.98batch/s, Batch Loss=0.0925, Avg Loss=0.0807, Time Left=15.24\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1685/3393 [13:54<14:23,  1.98batch/s, Batch Loss=0.0099, Avg Loss=0.0806, Time Left=15.23\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1686/3393 [13:54<14:12,  2.00batch/s, Batch Loss=0.0099, Avg Loss=0.0806, Time Left=15.23\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1686/3393 [13:55<14:12,  2.00batch/s, Batch Loss=0.0146, Avg Loss=0.0806, Time Left=15.22\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1687/3393 [13:55<14:04,  2.02batch/s, Batch Loss=0.0146, Avg Loss=0.0806, Time Left=15.22\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1687/3393 [13:55<14:04,  2.02batch/s, Batch Loss=0.1211, Avg Loss=0.0806, Time Left=15.21\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1688/3393 [13:55<14:05,  2.02batch/s, Batch Loss=0.1211, Avg Loss=0.0806, Time Left=15.21\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1688/3393 [13:56<14:05,  2.02batch/s, Batch Loss=0.1617, Avg Loss=0.0807, Time Left=15.20\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1689/3393 [13:56<14:07,  2.01batch/s, Batch Loss=0.1617, Avg Loss=0.0807, Time Left=15.20\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1689/3393 [13:56<14:07,  2.01batch/s, Batch Loss=0.0398, Avg Loss=0.0806, Time Left=15.19\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1690/3393 [13:56<14:07,  2.01batch/s, Batch Loss=0.0398, Avg Loss=0.0806, Time Left=15.19\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1690/3393 [13:57<14:07,  2.01batch/s, Batch Loss=0.1292, Avg Loss=0.0807, Time Left=15.18\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1691/3393 [13:57<13:56,  2.03batch/s, Batch Loss=0.1292, Avg Loss=0.0807, Time Left=15.18\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1691/3393 [13:57<13:56,  2.03batch/s, Batch Loss=0.0028, Avg Loss=0.0806, Time Left=15.17\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1692/3393 [13:57<14:04,  2.01batch/s, Batch Loss=0.0028, Avg Loss=0.0806, Time Left=15.17\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1692/3393 [13:58<14:04,  2.01batch/s, Batch Loss=0.0062, Avg Loss=0.0806, Time Left=15.17\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1693/3393 [13:58<13:49,  2.05batch/s, Batch Loss=0.0062, Avg Loss=0.0806, Time Left=15.17\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1693/3393 [13:58<13:49,  2.05batch/s, Batch Loss=0.0068, Avg Loss=0.0805, Time Left=15.16\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1694/3393 [13:58<13:46,  2.05batch/s, Batch Loss=0.0068, Avg Loss=0.0805, Time Left=15.16\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1694/3393 [13:59<13:46,  2.05batch/s, Batch Loss=0.0184, Avg Loss=0.0805, Time Left=15.15\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1695/3393 [13:59<14:09,  2.00batch/s, Batch Loss=0.0184, Avg Loss=0.0805, Time Left=15.15\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1695/3393 [13:59<14:09,  2.00batch/s, Batch Loss=0.0838, Avg Loss=0.0805, Time Left=15.14\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1696/3393 [13:59<14:00,  2.02batch/s, Batch Loss=0.0838, Avg Loss=0.0805, Time Left=15.14\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▍| 1696/3393 [14:00<14:00,  2.02batch/s, Batch Loss=0.0043, Avg Loss=0.0804, Time Left=15.13\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1697/3393 [14:00<14:10,  1.99batch/s, Batch Loss=0.0043, Avg Loss=0.0804, Time Left=15.13\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1697/3393 [14:00<14:10,  1.99batch/s, Batch Loss=0.0028, Avg Loss=0.0804, Time Left=15.12\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1698/3393 [14:00<14:02,  2.01batch/s, Batch Loss=0.0028, Avg Loss=0.0804, Time Left=15.12\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1698/3393 [14:01<14:02,  2.01batch/s, Batch Loss=0.0888, Avg Loss=0.0804, Time Left=15.11\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1699/3393 [14:01<13:55,  2.03batch/s, Batch Loss=0.0888, Avg Loss=0.0804, Time Left=15.11\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1699/3393 [14:01<13:55,  2.03batch/s, Batch Loss=0.1484, Avg Loss=0.0804, Time Left=15.10\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1700/3393 [14:01<13:49,  2.04batch/s, Batch Loss=0.1484, Avg Loss=0.0804, Time Left=15.10\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1700/3393 [14:02<13:49,  2.04batch/s, Batch Loss=0.0068, Avg Loss=0.0804, Time Left=15.09\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1701/3393 [14:02<13:54,  2.03batch/s, Batch Loss=0.0068, Avg Loss=0.0804, Time Left=15.09\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1701/3393 [14:02<13:54,  2.03batch/s, Batch Loss=0.0148, Avg Loss=0.0804, Time Left=15.09\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1702/3393 [14:02<14:12,  1.98batch/s, Batch Loss=0.0148, Avg Loss=0.0804, Time Left=15.09\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1702/3393 [14:03<14:12,  1.98batch/s, Batch Loss=0.0237, Avg Loss=0.0803, Time Left=15.08\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1703/3393 [14:03<14:08,  1.99batch/s, Batch Loss=0.0237, Avg Loss=0.0803, Time Left=15.08\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1703/3393 [14:03<14:08,  1.99batch/s, Batch Loss=0.0048, Avg Loss=0.0803, Time Left=15.07\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1704/3393 [14:03<14:45,  1.91batch/s, Batch Loss=0.0048, Avg Loss=0.0803, Time Left=15.07\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1704/3393 [14:04<14:45,  1.91batch/s, Batch Loss=0.0215, Avg Loss=0.0802, Time Left=15.06\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1705/3393 [14:04<14:31,  1.94batch/s, Batch Loss=0.0215, Avg Loss=0.0802, Time Left=15.06\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1705/3393 [14:04<14:31,  1.94batch/s, Batch Loss=0.0037, Avg Loss=0.0802, Time Left=15.05\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1706/3393 [14:04<14:23,  1.95batch/s, Batch Loss=0.0037, Avg Loss=0.0802, Time Left=15.05\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1706/3393 [14:05<14:23,  1.95batch/s, Batch Loss=0.0072, Avg Loss=0.0801, Time Left=15.04\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1707/3393 [14:05<14:00,  2.01batch/s, Batch Loss=0.0072, Avg Loss=0.0801, Time Left=15.04\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1707/3393 [14:05<14:00,  2.01batch/s, Batch Loss=0.0023, Avg Loss=0.0801, Time Left=15.03\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1708/3393 [14:05<14:26,  1.94batch/s, Batch Loss=0.0023, Avg Loss=0.0801, Time Left=15.03\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1708/3393 [14:06<14:26,  1.94batch/s, Batch Loss=0.0087, Avg Loss=0.0801, Time Left=15.03\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1709/3393 [14:06<14:18,  1.96batch/s, Batch Loss=0.0087, Avg Loss=0.0801, Time Left=15.03\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1709/3393 [14:06<14:18,  1.96batch/s, Batch Loss=0.0371, Avg Loss=0.0800, Time Left=15.02\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1710/3393 [14:06<14:28,  1.94batch/s, Batch Loss=0.0371, Avg Loss=0.0800, Time Left=15.02\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1710/3393 [14:07<14:28,  1.94batch/s, Batch Loss=0.0334, Avg Loss=0.0800, Time Left=15.01\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1711/3393 [14:07<14:12,  1.97batch/s, Batch Loss=0.0334, Avg Loss=0.0800, Time Left=15.01\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1711/3393 [14:07<14:12,  1.97batch/s, Batch Loss=0.1877, Avg Loss=0.0801, Time Left=15.00\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1712/3393 [14:07<14:08,  1.98batch/s, Batch Loss=0.1877, Avg Loss=0.0801, Time Left=15.00\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1712/3393 [14:08<14:08,  1.98batch/s, Batch Loss=0.0474, Avg Loss=0.0800, Time Left=14.99\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1713/3393 [14:08<14:06,  1.98batch/s, Batch Loss=0.0474, Avg Loss=0.0800, Time Left=14.99\u001b[A\n",
      "Epoch 3/3 - Training:  50%|▌| 1713/3393 [14:08<14:06,  1.98batch/s, Batch Loss=0.4776, Avg Loss=0.0803, Time Left=14.98\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  51%|▌| 1714/3393 [14:08<14:19,  1.95batch/s, Batch Loss=0.4776, Avg Loss=0.0803, Time Left=14.98\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1714/3393 [14:09<14:19,  1.95batch/s, Batch Loss=0.0413, Avg Loss=0.0803, Time Left=14.97\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1715/3393 [14:09<14:14,  1.96batch/s, Batch Loss=0.0413, Avg Loss=0.0803, Time Left=14.97\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1715/3393 [14:10<14:14,  1.96batch/s, Batch Loss=0.0218, Avg Loss=0.0802, Time Left=14.97\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1716/3393 [14:10<14:24,  1.94batch/s, Batch Loss=0.0218, Avg Loss=0.0802, Time Left=14.97\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1716/3393 [14:10<14:24,  1.94batch/s, Batch Loss=0.0683, Avg Loss=0.0802, Time Left=14.96\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1717/3393 [14:10<14:09,  1.97batch/s, Batch Loss=0.0683, Avg Loss=0.0802, Time Left=14.96\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1717/3393 [14:11<14:09,  1.97batch/s, Batch Loss=0.0913, Avg Loss=0.0802, Time Left=14.95\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1718/3393 [14:11<14:21,  1.94batch/s, Batch Loss=0.0913, Avg Loss=0.0802, Time Left=14.95\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1718/3393 [14:11<14:21,  1.94batch/s, Batch Loss=0.0262, Avg Loss=0.0802, Time Left=14.94\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1719/3393 [14:11<13:58,  2.00batch/s, Batch Loss=0.0262, Avg Loss=0.0802, Time Left=14.94\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1719/3393 [14:11<13:58,  2.00batch/s, Batch Loss=0.3952, Avg Loss=0.0804, Time Left=14.93\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1720/3393 [14:11<13:57,  2.00batch/s, Batch Loss=0.3952, Avg Loss=0.0804, Time Left=14.93\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1720/3393 [14:12<13:57,  2.00batch/s, Batch Loss=0.4291, Avg Loss=0.0806, Time Left=14.92\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1721/3393 [14:12<13:40,  2.04batch/s, Batch Loss=0.4291, Avg Loss=0.0806, Time Left=14.92\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1721/3393 [14:12<13:40,  2.04batch/s, Batch Loss=0.2399, Avg Loss=0.0807, Time Left=14.91\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1722/3393 [14:12<13:44,  2.03batch/s, Batch Loss=0.2399, Avg Loss=0.0807, Time Left=14.91\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1722/3393 [14:13<13:44,  2.03batch/s, Batch Loss=0.0354, Avg Loss=0.0807, Time Left=14.90\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1723/3393 [14:13<13:55,  2.00batch/s, Batch Loss=0.0354, Avg Loss=0.0807, Time Left=14.90\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1723/3393 [14:13<13:55,  2.00batch/s, Batch Loss=0.0037, Avg Loss=0.0806, Time Left=14.89\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1724/3393 [14:13<13:47,  2.02batch/s, Batch Loss=0.0037, Avg Loss=0.0806, Time Left=14.89\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1724/3393 [14:14<13:47,  2.02batch/s, Batch Loss=0.0110, Avg Loss=0.0806, Time Left=14.88\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1725/3393 [14:14<13:40,  2.03batch/s, Batch Loss=0.0110, Avg Loss=0.0806, Time Left=14.88\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1725/3393 [14:14<13:40,  2.03batch/s, Batch Loss=0.0177, Avg Loss=0.0805, Time Left=14.88\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1726/3393 [14:14<13:35,  2.04batch/s, Batch Loss=0.0177, Avg Loss=0.0805, Time Left=14.88\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1726/3393 [14:15<13:35,  2.04batch/s, Batch Loss=0.0431, Avg Loss=0.0805, Time Left=14.87\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1727/3393 [14:15<13:32,  2.05batch/s, Batch Loss=0.0431, Avg Loss=0.0805, Time Left=14.87\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1727/3393 [14:15<13:32,  2.05batch/s, Batch Loss=0.0113, Avg Loss=0.0805, Time Left=14.86\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1728/3393 [14:15<13:46,  2.01batch/s, Batch Loss=0.0113, Avg Loss=0.0805, Time Left=14.86\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1728/3393 [14:16<13:46,  2.01batch/s, Batch Loss=0.0468, Avg Loss=0.0805, Time Left=14.85\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1729/3393 [14:16<13:48,  2.01batch/s, Batch Loss=0.0468, Avg Loss=0.0805, Time Left=14.85\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1729/3393 [14:16<13:48,  2.01batch/s, Batch Loss=0.0047, Avg Loss=0.0804, Time Left=14.84\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1730/3393 [14:16<14:12,  1.95batch/s, Batch Loss=0.0047, Avg Loss=0.0804, Time Left=14.84\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1730/3393 [14:17<14:12,  1.95batch/s, Batch Loss=0.1509, Avg Loss=0.0805, Time Left=14.83\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1731/3393 [14:17<13:59,  1.98batch/s, Batch Loss=0.1509, Avg Loss=0.0805, Time Left=14.83\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1731/3393 [14:17<13:59,  1.98batch/s, Batch Loss=0.0561, Avg Loss=0.0804, Time Left=14.82\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1732/3393 [14:17<14:04,  1.97batch/s, Batch Loss=0.0561, Avg Loss=0.0804, Time Left=14.82\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1732/3393 [14:18<14:04,  1.97batch/s, Batch Loss=0.3737, Avg Loss=0.0806, Time Left=14.81\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1733/3393 [14:18<13:43,  2.02batch/s, Batch Loss=0.3737, Avg Loss=0.0806, Time Left=14.81\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1733/3393 [14:18<13:43,  2.02batch/s, Batch Loss=0.0063, Avg Loss=0.0806, Time Left=14.80\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1734/3393 [14:18<13:37,  2.03batch/s, Batch Loss=0.0063, Avg Loss=0.0806, Time Left=14.80\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1734/3393 [14:19<13:37,  2.03batch/s, Batch Loss=0.0085, Avg Loss=0.0805, Time Left=14.80\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1735/3393 [14:19<13:40,  2.02batch/s, Batch Loss=0.0085, Avg Loss=0.0805, Time Left=14.80\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1735/3393 [14:19<13:40,  2.02batch/s, Batch Loss=0.5300, Avg Loss=0.0808, Time Left=14.79\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1736/3393 [14:19<13:34,  2.03batch/s, Batch Loss=0.5300, Avg Loss=0.0808, Time Left=14.79\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1736/3393 [14:20<13:34,  2.03batch/s, Batch Loss=0.0702, Avg Loss=0.0808, Time Left=14.78\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1737/3393 [14:20<14:01,  1.97batch/s, Batch Loss=0.0702, Avg Loss=0.0808, Time Left=14.78\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1737/3393 [14:20<14:01,  1.97batch/s, Batch Loss=0.0952, Avg Loss=0.0808, Time Left=14.77\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1738/3393 [14:20<13:58,  1.97batch/s, Batch Loss=0.0952, Avg Loss=0.0808, Time Left=14.77\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1738/3393 [14:21<13:58,  1.97batch/s, Batch Loss=0.0085, Avg Loss=0.0808, Time Left=14.76\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1739/3393 [14:21<13:55,  1.98batch/s, Batch Loss=0.0085, Avg Loss=0.0808, Time Left=14.76\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1739/3393 [14:21<13:55,  1.98batch/s, Batch Loss=0.0936, Avg Loss=0.0808, Time Left=14.75\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1740/3393 [14:21<13:47,  2.00batch/s, Batch Loss=0.0936, Avg Loss=0.0808, Time Left=14.75\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1740/3393 [14:22<13:47,  2.00batch/s, Batch Loss=0.0082, Avg Loss=0.0807, Time Left=14.74\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1741/3393 [14:22<13:59,  1.97batch/s, Batch Loss=0.0082, Avg Loss=0.0807, Time Left=14.74\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1741/3393 [14:22<13:59,  1.97batch/s, Batch Loss=0.1198, Avg Loss=0.0807, Time Left=14.73\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1742/3393 [14:22<13:56,  1.97batch/s, Batch Loss=0.1198, Avg Loss=0.0807, Time Left=14.73\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1742/3393 [14:23<13:56,  1.97batch/s, Batch Loss=0.2108, Avg Loss=0.0808, Time Left=14.73\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1743/3393 [14:23<14:00,  1.96batch/s, Batch Loss=0.2108, Avg Loss=0.0808, Time Left=14.73\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1743/3393 [14:23<14:00,  1.96batch/s, Batch Loss=0.0614, Avg Loss=0.0808, Time Left=14.72\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1744/3393 [14:23<13:39,  2.01batch/s, Batch Loss=0.0614, Avg Loss=0.0808, Time Left=14.72\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1744/3393 [14:24<13:39,  2.01batch/s, Batch Loss=0.0478, Avg Loss=0.0808, Time Left=14.71\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1745/3393 [14:24<13:40,  2.01batch/s, Batch Loss=0.0478, Avg Loss=0.0808, Time Left=14.71\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1745/3393 [14:24<13:40,  2.01batch/s, Batch Loss=0.0962, Avg Loss=0.0808, Time Left=14.70\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1746/3393 [14:24<13:32,  2.03batch/s, Batch Loss=0.0962, Avg Loss=0.0808, Time Left=14.70\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1746/3393 [14:25<13:32,  2.03batch/s, Batch Loss=0.2687, Avg Loss=0.0809, Time Left=14.69\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  51%|▌| 1747/3393 [14:25<13:28,  2.04batch/s, Batch Loss=0.2687, Avg Loss=0.0809, Time Left=14.69\u001b[A\n",
      "Epoch 3/3 - Training:  51%|▌| 1747/3393 [14:25<13:28,  2.04batch/s, Batch Loss=0.0420, Avg Loss=0.0809, Time Left=14.68\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1748/3393 [14:25<13:48,  1.99batch/s, Batch Loss=0.0420, Avg Loss=0.0809, Time Left=14.68\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1748/3393 [14:26<13:48,  1.99batch/s, Batch Loss=0.0200, Avg Loss=0.0809, Time Left=14.67\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1749/3393 [14:26<13:47,  1.99batch/s, Batch Loss=0.0200, Avg Loss=0.0809, Time Left=14.67\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1749/3393 [14:26<13:47,  1.99batch/s, Batch Loss=0.0274, Avg Loss=0.0808, Time Left=14.66\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1750/3393 [14:27<14:00,  1.95batch/s, Batch Loss=0.0274, Avg Loss=0.0808, Time Left=14.66\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1750/3393 [14:27<14:00,  1.95batch/s, Batch Loss=0.3211, Avg Loss=0.0810, Time Left=14.66\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1751/3393 [14:27<13:54,  1.97batch/s, Batch Loss=0.3211, Avg Loss=0.0810, Time Left=14.66\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1751/3393 [14:28<13:54,  1.97batch/s, Batch Loss=0.0708, Avg Loss=0.0810, Time Left=14.65\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1752/3393 [14:28<13:57,  1.96batch/s, Batch Loss=0.0708, Avg Loss=0.0810, Time Left=14.65\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1752/3393 [14:28<13:57,  1.96batch/s, Batch Loss=0.0098, Avg Loss=0.0809, Time Left=14.64\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1753/3393 [14:28<13:39,  2.00batch/s, Batch Loss=0.0098, Avg Loss=0.0809, Time Left=14.64\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1753/3393 [14:28<13:39,  2.00batch/s, Batch Loss=0.0689, Avg Loss=0.0809, Time Left=14.63\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1754/3393 [14:28<13:38,  2.00batch/s, Batch Loss=0.0689, Avg Loss=0.0809, Time Left=14.63\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1754/3393 [14:29<13:38,  2.00batch/s, Batch Loss=0.1276, Avg Loss=0.0809, Time Left=14.62\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1755/3393 [14:29<13:20,  2.05batch/s, Batch Loss=0.1276, Avg Loss=0.0809, Time Left=14.62\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1755/3393 [14:29<13:20,  2.05batch/s, Batch Loss=0.0672, Avg Loss=0.0809, Time Left=14.61\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1756/3393 [14:29<13:25,  2.03batch/s, Batch Loss=0.0672, Avg Loss=0.0809, Time Left=14.61\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1756/3393 [14:30<13:25,  2.03batch/s, Batch Loss=0.1535, Avg Loss=0.0810, Time Left=14.60\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1757/3393 [14:30<13:36,  2.00batch/s, Batch Loss=0.1535, Avg Loss=0.0810, Time Left=14.60\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1757/3393 [14:30<13:36,  2.00batch/s, Batch Loss=0.1679, Avg Loss=0.0810, Time Left=14.59\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1758/3393 [14:30<13:45,  1.98batch/s, Batch Loss=0.1679, Avg Loss=0.0810, Time Left=14.59\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1758/3393 [14:31<13:45,  1.98batch/s, Batch Loss=0.0815, Avg Loss=0.0810, Time Left=14.59\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1759/3393 [14:31<13:50,  1.97batch/s, Batch Loss=0.0815, Avg Loss=0.0810, Time Left=14.59\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1759/3393 [14:32<13:50,  1.97batch/s, Batch Loss=0.0252, Avg Loss=0.0810, Time Left=14.58\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1760/3393 [14:32<13:51,  1.96batch/s, Batch Loss=0.0252, Avg Loss=0.0810, Time Left=14.58\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1760/3393 [14:32<13:51,  1.96batch/s, Batch Loss=0.1027, Avg Loss=0.0810, Time Left=14.57\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1761/3393 [14:32<14:07,  1.92batch/s, Batch Loss=0.1027, Avg Loss=0.0810, Time Left=14.57\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1761/3393 [14:33<14:07,  1.92batch/s, Batch Loss=0.1299, Avg Loss=0.0810, Time Left=14.56\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1762/3393 [14:33<13:56,  1.95batch/s, Batch Loss=0.1299, Avg Loss=0.0810, Time Left=14.56\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1762/3393 [14:33<13:56,  1.95batch/s, Batch Loss=0.0916, Avg Loss=0.0810, Time Left=14.55\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1763/3393 [14:33<13:57,  1.95batch/s, Batch Loss=0.0916, Avg Loss=0.0810, Time Left=14.55\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1763/3393 [14:34<13:57,  1.95batch/s, Batch Loss=0.0209, Avg Loss=0.0810, Time Left=14.54\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1764/3393 [14:34<13:51,  1.96batch/s, Batch Loss=0.0209, Avg Loss=0.0810, Time Left=14.54\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1764/3393 [14:34<13:51,  1.96batch/s, Batch Loss=0.0891, Avg Loss=0.0810, Time Left=14.53\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1765/3393 [14:34<13:46,  1.97batch/s, Batch Loss=0.0891, Avg Loss=0.0810, Time Left=14.53\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1765/3393 [14:35<13:46,  1.97batch/s, Batch Loss=0.0200, Avg Loss=0.0810, Time Left=14.52\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1766/3393 [14:35<13:42,  1.98batch/s, Batch Loss=0.0200, Avg Loss=0.0810, Time Left=14.52\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1766/3393 [14:35<13:42,  1.98batch/s, Batch Loss=0.0522, Avg Loss=0.0810, Time Left=14.52\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1767/3393 [14:35<13:54,  1.95batch/s, Batch Loss=0.0522, Avg Loss=0.0810, Time Left=14.52\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1767/3393 [14:36<13:54,  1.95batch/s, Batch Loss=0.0460, Avg Loss=0.0809, Time Left=14.51\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1768/3393 [14:36<13:55,  1.95batch/s, Batch Loss=0.0460, Avg Loss=0.0809, Time Left=14.51\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1768/3393 [14:36<13:55,  1.95batch/s, Batch Loss=0.0114, Avg Loss=0.0809, Time Left=14.50\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1769/3393 [14:36<13:56,  1.94batch/s, Batch Loss=0.0114, Avg Loss=0.0809, Time Left=14.50\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1769/3393 [14:37<13:56,  1.94batch/s, Batch Loss=0.0069, Avg Loss=0.0808, Time Left=14.49\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1770/3393 [14:37<13:56,  1.94batch/s, Batch Loss=0.0069, Avg Loss=0.0808, Time Left=14.49\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1770/3393 [14:37<13:56,  1.94batch/s, Batch Loss=0.1584, Avg Loss=0.0809, Time Left=14.48\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1771/3393 [14:37<13:56,  1.94batch/s, Batch Loss=0.1584, Avg Loss=0.0809, Time Left=14.48\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1771/3393 [14:38<13:56,  1.94batch/s, Batch Loss=0.1632, Avg Loss=0.0809, Time Left=14.47\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1772/3393 [14:38<13:41,  1.97batch/s, Batch Loss=0.1632, Avg Loss=0.0809, Time Left=14.47\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1772/3393 [14:38<13:41,  1.97batch/s, Batch Loss=0.0605, Avg Loss=0.0809, Time Left=14.46\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1773/3393 [14:38<13:53,  1.94batch/s, Batch Loss=0.0605, Avg Loss=0.0809, Time Left=14.46\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1773/3393 [14:39<13:53,  1.94batch/s, Batch Loss=0.0205, Avg Loss=0.0809, Time Left=14.46\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1774/3393 [14:39<13:47,  1.96batch/s, Batch Loss=0.0205, Avg Loss=0.0809, Time Left=14.46\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1774/3393 [14:39<13:47,  1.96batch/s, Batch Loss=0.0043, Avg Loss=0.0808, Time Left=14.45\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1775/3393 [14:39<13:33,  1.99batch/s, Batch Loss=0.0043, Avg Loss=0.0808, Time Left=14.45\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1775/3393 [14:40<13:33,  1.99batch/s, Batch Loss=0.0046, Avg Loss=0.0808, Time Left=14.44\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1776/3393 [14:40<13:31,  1.99batch/s, Batch Loss=0.0046, Avg Loss=0.0808, Time Left=14.44\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1776/3393 [14:40<13:31,  1.99batch/s, Batch Loss=0.1792, Avg Loss=0.0809, Time Left=14.43\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1777/3393 [14:40<13:44,  1.96batch/s, Batch Loss=0.1792, Avg Loss=0.0809, Time Left=14.43\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1777/3393 [14:41<13:44,  1.96batch/s, Batch Loss=0.0080, Avg Loss=0.0808, Time Left=14.42\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1778/3393 [14:41<13:48,  1.95batch/s, Batch Loss=0.0080, Avg Loss=0.0808, Time Left=14.42\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1778/3393 [14:41<13:48,  1.95batch/s, Batch Loss=0.0749, Avg Loss=0.0808, Time Left=14.41\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1779/3393 [14:41<13:34,  1.98batch/s, Batch Loss=0.0749, Avg Loss=0.0808, Time Left=14.41\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1779/3393 [14:42<13:34,  1.98batch/s, Batch Loss=0.0389, Avg Loss=0.0808, Time Left=14.40\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  52%|▌| 1780/3393 [14:42<13:24,  2.01batch/s, Batch Loss=0.0389, Avg Loss=0.0808, Time Left=14.40\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1780/3393 [14:42<13:24,  2.01batch/s, Batch Loss=0.0468, Avg Loss=0.0808, Time Left=14.39\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1781/3393 [14:42<13:40,  1.96batch/s, Batch Loss=0.0468, Avg Loss=0.0808, Time Left=14.39\u001b[A\n",
      "Epoch 3/3 - Training:  52%|▌| 1781/3393 [14:43<13:40,  1.96batch/s, Batch Loss=0.2919, Avg Loss=0.0809, Time Left=14.39\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1782/3393 [14:43<13:25,  2.00batch/s, Batch Loss=0.2919, Avg Loss=0.0809, Time Left=14.39\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1782/3393 [14:43<13:25,  2.00batch/s, Batch Loss=0.0343, Avg Loss=0.0809, Time Left=14.38\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1783/3393 [14:43<13:19,  2.01batch/s, Batch Loss=0.0343, Avg Loss=0.0809, Time Left=14.38\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1783/3393 [14:44<13:19,  2.01batch/s, Batch Loss=0.0595, Avg Loss=0.0809, Time Left=14.37\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1784/3393 [14:44<13:35,  1.97batch/s, Batch Loss=0.0595, Avg Loss=0.0809, Time Left=14.37\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1784/3393 [14:44<13:35,  1.97batch/s, Batch Loss=0.0162, Avg Loss=0.0808, Time Left=14.36\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1785/3393 [14:44<13:25,  2.00batch/s, Batch Loss=0.0162, Avg Loss=0.0808, Time Left=14.36\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1785/3393 [14:45<13:25,  2.00batch/s, Batch Loss=0.0665, Avg Loss=0.0808, Time Left=14.35\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1786/3393 [14:45<13:24,  2.00batch/s, Batch Loss=0.0665, Avg Loss=0.0808, Time Left=14.35\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1786/3393 [14:45<13:24,  2.00batch/s, Batch Loss=0.0623, Avg Loss=0.0808, Time Left=14.34\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1787/3393 [14:45<13:24,  2.00batch/s, Batch Loss=0.0623, Avg Loss=0.0808, Time Left=14.34\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1787/3393 [14:46<13:24,  2.00batch/s, Batch Loss=0.0459, Avg Loss=0.0808, Time Left=14.33\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1788/3393 [14:46<13:38,  1.96batch/s, Batch Loss=0.0459, Avg Loss=0.0808, Time Left=14.33\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1788/3393 [14:46<13:38,  1.96batch/s, Batch Loss=0.1534, Avg Loss=0.0808, Time Left=14.32\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1789/3393 [14:46<13:34,  1.97batch/s, Batch Loss=0.1534, Avg Loss=0.0808, Time Left=14.32\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1789/3393 [14:47<13:34,  1.97batch/s, Batch Loss=0.0174, Avg Loss=0.0808, Time Left=14.32\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1790/3393 [14:47<13:30,  1.98batch/s, Batch Loss=0.0174, Avg Loss=0.0808, Time Left=14.32\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1790/3393 [14:47<13:30,  1.98batch/s, Batch Loss=0.0190, Avg Loss=0.0807, Time Left=14.31\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1791/3393 [14:47<13:11,  2.02batch/s, Batch Loss=0.0190, Avg Loss=0.0807, Time Left=14.31\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1791/3393 [14:48<13:11,  2.02batch/s, Batch Loss=0.0369, Avg Loss=0.0807, Time Left=14.30\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1792/3393 [14:48<13:20,  2.00batch/s, Batch Loss=0.0369, Avg Loss=0.0807, Time Left=14.30\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1792/3393 [14:48<13:20,  2.00batch/s, Batch Loss=0.0670, Avg Loss=0.0807, Time Left=14.29\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1793/3393 [14:48<13:05,  2.04batch/s, Batch Loss=0.0670, Avg Loss=0.0807, Time Left=14.29\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1793/3393 [14:49<13:05,  2.04batch/s, Batch Loss=0.0728, Avg Loss=0.0807, Time Left=14.28\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1794/3393 [14:49<13:08,  2.03batch/s, Batch Loss=0.0728, Avg Loss=0.0807, Time Left=14.28\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1794/3393 [14:49<13:08,  2.03batch/s, Batch Loss=0.0463, Avg Loss=0.0807, Time Left=14.27\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1795/3393 [14:49<13:34,  1.96batch/s, Batch Loss=0.0463, Avg Loss=0.0807, Time Left=14.27\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1795/3393 [14:50<13:34,  1.96batch/s, Batch Loss=0.0457, Avg Loss=0.0807, Time Left=14.26\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1796/3393 [14:50<13:23,  1.99batch/s, Batch Loss=0.0457, Avg Loss=0.0807, Time Left=14.26\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1796/3393 [14:50<13:23,  1.99batch/s, Batch Loss=0.0053, Avg Loss=0.0806, Time Left=14.25\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1797/3393 [14:50<13:47,  1.93batch/s, Batch Loss=0.0053, Avg Loss=0.0806, Time Left=14.25\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1797/3393 [14:51<13:47,  1.93batch/s, Batch Loss=0.0034, Avg Loss=0.0806, Time Left=14.25\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1798/3393 [14:51<13:29,  1.97batch/s, Batch Loss=0.0034, Avg Loss=0.0806, Time Left=14.25\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1798/3393 [14:51<13:29,  1.97batch/s, Batch Loss=0.0196, Avg Loss=0.0805, Time Left=14.24\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1799/3393 [14:51<13:31,  1.96batch/s, Batch Loss=0.0196, Avg Loss=0.0805, Time Left=14.24\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1799/3393 [14:52<13:31,  1.96batch/s, Batch Loss=0.0227, Avg Loss=0.0805, Time Left=14.23\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1800/3393 [14:52<13:26,  1.97batch/s, Batch Loss=0.0227, Avg Loss=0.0805, Time Left=14.23\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1800/3393 [14:52<13:26,  1.97batch/s, Batch Loss=0.0351, Avg Loss=0.0805, Time Left=14.22\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1801/3393 [14:52<13:31,  1.96batch/s, Batch Loss=0.0351, Avg Loss=0.0805, Time Left=14.22\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1801/3393 [14:53<13:31,  1.96batch/s, Batch Loss=0.1119, Avg Loss=0.0805, Time Left=14.21\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1802/3393 [14:53<13:21,  1.98batch/s, Batch Loss=0.1119, Avg Loss=0.0805, Time Left=14.21\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1802/3393 [14:53<13:21,  1.98batch/s, Batch Loss=0.1793, Avg Loss=0.0806, Time Left=14.20\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1803/3393 [14:53<13:32,  1.96batch/s, Batch Loss=0.1793, Avg Loss=0.0806, Time Left=14.20\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1803/3393 [14:54<13:32,  1.96batch/s, Batch Loss=0.2344, Avg Loss=0.0806, Time Left=14.19\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1804/3393 [14:54<13:29,  1.96batch/s, Batch Loss=0.2344, Avg Loss=0.0806, Time Left=14.19\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1804/3393 [14:54<13:29,  1.96batch/s, Batch Loss=0.1394, Avg Loss=0.0807, Time Left=14.18\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1805/3393 [14:54<13:29,  1.96batch/s, Batch Loss=0.1394, Avg Loss=0.0807, Time Left=14.18\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1805/3393 [14:55<13:29,  1.96batch/s, Batch Loss=0.0196, Avg Loss=0.0806, Time Left=14.18\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1806/3393 [14:55<13:25,  1.97batch/s, Batch Loss=0.0196, Avg Loss=0.0806, Time Left=14.18\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1806/3393 [14:55<13:25,  1.97batch/s, Batch Loss=0.0651, Avg Loss=0.0806, Time Left=14.17\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1807/3393 [14:55<13:37,  1.94batch/s, Batch Loss=0.0651, Avg Loss=0.0806, Time Left=14.17\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1807/3393 [14:56<13:37,  1.94batch/s, Batch Loss=0.0896, Avg Loss=0.0806, Time Left=14.16\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1808/3393 [14:56<13:23,  1.97batch/s, Batch Loss=0.0896, Avg Loss=0.0806, Time Left=14.16\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1808/3393 [14:56<13:23,  1.97batch/s, Batch Loss=0.0102, Avg Loss=0.0806, Time Left=14.15\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1809/3393 [14:56<13:33,  1.95batch/s, Batch Loss=0.0102, Avg Loss=0.0806, Time Left=14.15\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1809/3393 [14:57<13:33,  1.95batch/s, Batch Loss=0.0073, Avg Loss=0.0806, Time Left=14.14\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1810/3393 [14:57<13:12,  2.00batch/s, Batch Loss=0.0073, Avg Loss=0.0806, Time Left=14.14\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1810/3393 [14:57<13:12,  2.00batch/s, Batch Loss=0.1467, Avg Loss=0.0806, Time Left=14.13\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1811/3393 [14:57<13:40,  1.93batch/s, Batch Loss=0.1467, Avg Loss=0.0806, Time Left=14.13\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1811/3393 [14:58<13:40,  1.93batch/s, Batch Loss=0.0026, Avg Loss=0.0806, Time Left=14.12\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1812/3393 [14:58<13:25,  1.96batch/s, Batch Loss=0.0026, Avg Loss=0.0806, Time Left=14.12\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1812/3393 [14:58<13:25,  1.96batch/s, Batch Loss=0.0860, Avg Loss=0.0806, Time Left=14.12\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  53%|▌| 1813/3393 [14:58<13:35,  1.94batch/s, Batch Loss=0.0860, Avg Loss=0.0806, Time Left=14.12\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1813/3393 [14:59<13:35,  1.94batch/s, Batch Loss=0.1585, Avg Loss=0.0806, Time Left=14.11\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1814/3393 [14:59<13:28,  1.95batch/s, Batch Loss=0.1585, Avg Loss=0.0806, Time Left=14.11\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1814/3393 [14:59<13:28,  1.95batch/s, Batch Loss=0.0120, Avg Loss=0.0806, Time Left=14.10\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1815/3393 [14:59<13:29,  1.95batch/s, Batch Loss=0.0120, Avg Loss=0.0806, Time Left=14.10\u001b[A\n",
      "Epoch 3/3 - Training:  53%|▌| 1815/3393 [15:00<13:29,  1.95batch/s, Batch Loss=0.0149, Avg Loss=0.0805, Time Left=14.09\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1816/3393 [15:00<13:24,  1.96batch/s, Batch Loss=0.0149, Avg Loss=0.0805, Time Left=14.09\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1816/3393 [15:00<13:24,  1.96batch/s, Batch Loss=0.0721, Avg Loss=0.0805, Time Left=14.08\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1817/3393 [15:00<13:33,  1.94batch/s, Batch Loss=0.0721, Avg Loss=0.0805, Time Left=14.08\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1817/3393 [15:01<13:33,  1.94batch/s, Batch Loss=0.0368, Avg Loss=0.0805, Time Left=14.07\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1818/3393 [15:01<13:26,  1.95batch/s, Batch Loss=0.0368, Avg Loss=0.0805, Time Left=14.07\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1818/3393 [15:02<13:26,  1.95batch/s, Batch Loss=0.0024, Avg Loss=0.0805, Time Left=14.06\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1819/3393 [15:02<13:27,  1.95batch/s, Batch Loss=0.0024, Avg Loss=0.0805, Time Left=14.06\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1819/3393 [15:02<13:27,  1.95batch/s, Batch Loss=0.0281, Avg Loss=0.0804, Time Left=14.06\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1820/3393 [15:02<13:29,  1.94batch/s, Batch Loss=0.0281, Avg Loss=0.0804, Time Left=14.06\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1820/3393 [15:03<13:29,  1.94batch/s, Batch Loss=0.0334, Avg Loss=0.0804, Time Left=14.05\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1821/3393 [15:03<13:51,  1.89batch/s, Batch Loss=0.0334, Avg Loss=0.0804, Time Left=14.05\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1821/3393 [15:03<13:51,  1.89batch/s, Batch Loss=0.0130, Avg Loss=0.0804, Time Left=14.04\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1822/3393 [15:03<13:25,  1.95batch/s, Batch Loss=0.0130, Avg Loss=0.0804, Time Left=14.04\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1822/3393 [15:04<13:25,  1.95batch/s, Batch Loss=0.2057, Avg Loss=0.0804, Time Left=14.03\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1823/3393 [15:04<13:32,  1.93batch/s, Batch Loss=0.2057, Avg Loss=0.0804, Time Left=14.03\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1823/3393 [15:04<13:32,  1.93batch/s, Batch Loss=0.1308, Avg Loss=0.0805, Time Left=14.02\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1824/3393 [15:04<13:17,  1.97batch/s, Batch Loss=0.1308, Avg Loss=0.0805, Time Left=14.02\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1824/3393 [15:05<13:17,  1.97batch/s, Batch Loss=0.0310, Avg Loss=0.0804, Time Left=14.01\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1825/3393 [15:05<13:28,  1.94batch/s, Batch Loss=0.0310, Avg Loss=0.0804, Time Left=14.01\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1825/3393 [15:05<13:28,  1.94batch/s, Batch Loss=0.1131, Avg Loss=0.0804, Time Left=14.00\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1826/3393 [15:05<13:21,  1.96batch/s, Batch Loss=0.1131, Avg Loss=0.0804, Time Left=14.00\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1826/3393 [15:06<13:21,  1.96batch/s, Batch Loss=0.3299, Avg Loss=0.0806, Time Left=14.00\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1827/3393 [15:06<13:15,  1.97batch/s, Batch Loss=0.3299, Avg Loss=0.0806, Time Left=14.00\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1827/3393 [15:06<13:15,  1.97batch/s, Batch Loss=0.1502, Avg Loss=0.0806, Time Left=13.99\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1828/3393 [15:06<13:04,  2.00batch/s, Batch Loss=0.1502, Avg Loss=0.0806, Time Left=13.99\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1828/3393 [15:07<13:04,  2.00batch/s, Batch Loss=0.0973, Avg Loss=0.0806, Time Left=13.98\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1829/3393 [15:07<13:24,  1.94batch/s, Batch Loss=0.0973, Avg Loss=0.0806, Time Left=13.98\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1829/3393 [15:07<13:24,  1.94batch/s, Batch Loss=0.0022, Avg Loss=0.0806, Time Left=13.97\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1830/3393 [15:07<13:19,  1.96batch/s, Batch Loss=0.0022, Avg Loss=0.0806, Time Left=13.97\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1830/3393 [15:08<13:19,  1.96batch/s, Batch Loss=0.0206, Avg Loss=0.0806, Time Left=13.96\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1831/3393 [15:08<13:28,  1.93batch/s, Batch Loss=0.0206, Avg Loss=0.0806, Time Left=13.96\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1831/3393 [15:08<13:28,  1.93batch/s, Batch Loss=0.0517, Avg Loss=0.0805, Time Left=13.95\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1832/3393 [15:08<13:20,  1.95batch/s, Batch Loss=0.0517, Avg Loss=0.0805, Time Left=13.95\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1832/3393 [15:09<13:20,  1.95batch/s, Batch Loss=0.0023, Avg Loss=0.0805, Time Left=13.94\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1833/3393 [15:09<13:28,  1.93batch/s, Batch Loss=0.0023, Avg Loss=0.0805, Time Left=13.94\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1833/3393 [15:09<13:28,  1.93batch/s, Batch Loss=0.0025, Avg Loss=0.0805, Time Left=13.94\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1834/3393 [15:09<13:21,  1.94batch/s, Batch Loss=0.0025, Avg Loss=0.0805, Time Left=13.94\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1834/3393 [15:10<13:21,  1.94batch/s, Batch Loss=0.0168, Avg Loss=0.0804, Time Left=13.93\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1835/3393 [15:10<13:13,  1.96batch/s, Batch Loss=0.0168, Avg Loss=0.0804, Time Left=13.93\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1835/3393 [15:10<13:13,  1.96batch/s, Batch Loss=0.1106, Avg Loss=0.0804, Time Left=13.92\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1836/3393 [15:10<13:02,  1.99batch/s, Batch Loss=0.1106, Avg Loss=0.0804, Time Left=13.92\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1836/3393 [15:11<13:02,  1.99batch/s, Batch Loss=0.0778, Avg Loss=0.0804, Time Left=13.91\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1837/3393 [15:11<13:22,  1.94batch/s, Batch Loss=0.0778, Avg Loss=0.0804, Time Left=13.91\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1837/3393 [15:11<13:22,  1.94batch/s, Batch Loss=0.0262, Avg Loss=0.0804, Time Left=13.90\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1838/3393 [15:11<13:15,  1.95batch/s, Batch Loss=0.0262, Avg Loss=0.0804, Time Left=13.90\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1838/3393 [15:12<13:15,  1.95batch/s, Batch Loss=0.0186, Avg Loss=0.0804, Time Left=13.89\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1839/3393 [15:12<13:24,  1.93batch/s, Batch Loss=0.0186, Avg Loss=0.0804, Time Left=13.89\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1839/3393 [15:12<13:24,  1.93batch/s, Batch Loss=0.0735, Avg Loss=0.0804, Time Left=13.88\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1840/3393 [15:12<13:17,  1.95batch/s, Batch Loss=0.0735, Avg Loss=0.0804, Time Left=13.88\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1840/3393 [15:13<13:17,  1.95batch/s, Batch Loss=0.0430, Avg Loss=0.0803, Time Left=13.88\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1841/3393 [15:13<13:10,  1.96batch/s, Batch Loss=0.0430, Avg Loss=0.0803, Time Left=13.88\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1841/3393 [15:13<13:10,  1.96batch/s, Batch Loss=0.0131, Avg Loss=0.0803, Time Left=13.87\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1842/3393 [15:13<13:06,  1.97batch/s, Batch Loss=0.0131, Avg Loss=0.0803, Time Left=13.87\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1842/3393 [15:14<13:06,  1.97batch/s, Batch Loss=0.1403, Avg Loss=0.0803, Time Left=13.86\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1843/3393 [15:14<13:09,  1.96batch/s, Batch Loss=0.1403, Avg Loss=0.0803, Time Left=13.86\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1843/3393 [15:14<13:09,  1.96batch/s, Batch Loss=0.0245, Avg Loss=0.0803, Time Left=13.85\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1844/3393 [15:14<13:05,  1.97batch/s, Batch Loss=0.0245, Avg Loss=0.0803, Time Left=13.85\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1844/3393 [15:15<13:05,  1.97batch/s, Batch Loss=0.0021, Avg Loss=0.0803, Time Left=13.84\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1845/3393 [15:15<13:16,  1.94batch/s, Batch Loss=0.0021, Avg Loss=0.0803, Time Left=13.84\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1845/3393 [15:15<13:16,  1.94batch/s, Batch Loss=0.0486, Avg Loss=0.0802, Time Left=13.83\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  54%|▌| 1846/3393 [15:15<13:10,  1.96batch/s, Batch Loss=0.0486, Avg Loss=0.0802, Time Left=13.83\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1846/3393 [15:16<13:10,  1.96batch/s, Batch Loss=0.0375, Avg Loss=0.0802, Time Left=13.82\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1847/3393 [15:16<13:19,  1.93batch/s, Batch Loss=0.0375, Avg Loss=0.0802, Time Left=13.82\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1847/3393 [15:16<13:19,  1.93batch/s, Batch Loss=0.0016, Avg Loss=0.0802, Time Left=13.81\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1848/3393 [15:16<13:11,  1.95batch/s, Batch Loss=0.0016, Avg Loss=0.0802, Time Left=13.81\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1848/3393 [15:17<13:11,  1.95batch/s, Batch Loss=0.0052, Avg Loss=0.0801, Time Left=13.81\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1849/3393 [15:17<13:20,  1.93batch/s, Batch Loss=0.0052, Avg Loss=0.0801, Time Left=13.81\u001b[A\n",
      "Epoch 3/3 - Training:  54%|▌| 1849/3393 [15:17<13:20,  1.93batch/s, Batch Loss=0.0019, Avg Loss=0.0801, Time Left=13.80\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1850/3393 [15:17<13:04,  1.97batch/s, Batch Loss=0.0019, Avg Loss=0.0801, Time Left=13.80\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1850/3393 [15:18<13:04,  1.97batch/s, Batch Loss=0.0783, Avg Loss=0.0801, Time Left=13.79\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1851/3393 [15:18<13:07,  1.96batch/s, Batch Loss=0.0783, Avg Loss=0.0801, Time Left=13.79\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1851/3393 [15:18<13:07,  1.96batch/s, Batch Loss=0.0681, Avg Loss=0.0801, Time Left=13.78\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1852/3393 [15:18<13:02,  1.97batch/s, Batch Loss=0.0681, Avg Loss=0.0801, Time Left=13.78\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1852/3393 [15:19<13:02,  1.97batch/s, Batch Loss=0.0027, Avg Loss=0.0800, Time Left=13.77\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1853/3393 [15:19<13:13,  1.94batch/s, Batch Loss=0.0027, Avg Loss=0.0800, Time Left=13.77\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1853/3393 [15:19<13:13,  1.94batch/s, Batch Loss=0.0658, Avg Loss=0.0800, Time Left=13.76\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1854/3393 [15:19<12:59,  1.97batch/s, Batch Loss=0.0658, Avg Loss=0.0800, Time Left=13.76\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1854/3393 [15:20<12:59,  1.97batch/s, Batch Loss=0.0047, Avg Loss=0.0800, Time Left=13.75\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1855/3393 [15:20<13:10,  1.95batch/s, Batch Loss=0.0047, Avg Loss=0.0800, Time Left=13.75\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1855/3393 [15:20<13:10,  1.95batch/s, Batch Loss=0.3358, Avg Loss=0.0801, Time Left=13.75\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1856/3393 [15:20<13:04,  1.96batch/s, Batch Loss=0.3358, Avg Loss=0.0801, Time Left=13.75\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1856/3393 [15:21<13:04,  1.96batch/s, Batch Loss=0.1043, Avg Loss=0.0801, Time Left=13.74\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1857/3393 [15:21<13:13,  1.93batch/s, Batch Loss=0.1043, Avg Loss=0.0801, Time Left=13.74\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1857/3393 [15:21<13:13,  1.93batch/s, Batch Loss=0.0036, Avg Loss=0.0801, Time Left=13.73\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1858/3393 [15:21<13:06,  1.95batch/s, Batch Loss=0.0036, Avg Loss=0.0801, Time Left=13.73\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1858/3393 [15:22<13:06,  1.95batch/s, Batch Loss=0.0128, Avg Loss=0.0801, Time Left=13.72\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1859/3393 [15:22<13:07,  1.95batch/s, Batch Loss=0.0128, Avg Loss=0.0801, Time Left=13.72\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1859/3393 [15:23<13:07,  1.95batch/s, Batch Loss=0.0352, Avg Loss=0.0800, Time Left=13.71\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1860/3393 [15:23<13:01,  1.96batch/s, Batch Loss=0.0352, Avg Loss=0.0800, Time Left=13.71\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1860/3393 [15:23<13:01,  1.96batch/s, Batch Loss=0.0914, Avg Loss=0.0800, Time Left=13.70\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1861/3393 [15:23<13:11,  1.93batch/s, Batch Loss=0.0914, Avg Loss=0.0800, Time Left=13.70\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1861/3393 [15:24<13:11,  1.93batch/s, Batch Loss=0.0222, Avg Loss=0.0800, Time Left=13.69\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1862/3393 [15:24<13:04,  1.95batch/s, Batch Loss=0.0222, Avg Loss=0.0800, Time Left=13.69\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1862/3393 [15:24<13:04,  1.95batch/s, Batch Loss=0.0156, Avg Loss=0.0800, Time Left=13.69\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1863/3393 [15:24<12:51,  1.98batch/s, Batch Loss=0.0156, Avg Loss=0.0800, Time Left=13.69\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1863/3393 [15:25<12:51,  1.98batch/s, Batch Loss=0.2202, Avg Loss=0.0801, Time Left=13.68\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1864/3393 [15:25<12:55,  1.97batch/s, Batch Loss=0.2202, Avg Loss=0.0801, Time Left=13.68\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1864/3393 [15:25<12:55,  1.97batch/s, Batch Loss=0.0777, Avg Loss=0.0801, Time Left=13.67\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1865/3393 [15:25<13:14,  1.92batch/s, Batch Loss=0.0777, Avg Loss=0.0801, Time Left=13.67\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1865/3393 [15:26<13:14,  1.92batch/s, Batch Loss=0.0664, Avg Loss=0.0801, Time Left=13.66\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1866/3393 [15:26<13:05,  1.94batch/s, Batch Loss=0.0664, Avg Loss=0.0801, Time Left=13.66\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1866/3393 [15:26<13:05,  1.94batch/s, Batch Loss=0.0133, Avg Loss=0.0800, Time Left=13.65\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1867/3393 [15:26<12:59,  1.96batch/s, Batch Loss=0.0133, Avg Loss=0.0800, Time Left=13.65\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1867/3393 [15:27<12:59,  1.96batch/s, Batch Loss=0.0382, Avg Loss=0.0800, Time Left=13.64\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1868/3393 [15:27<12:53,  1.97batch/s, Batch Loss=0.0382, Avg Loss=0.0800, Time Left=13.64\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1868/3393 [15:27<12:53,  1.97batch/s, Batch Loss=0.1332, Avg Loss=0.0800, Time Left=13.63\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1869/3393 [15:27<13:04,  1.94batch/s, Batch Loss=0.1332, Avg Loss=0.0800, Time Left=13.63\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1869/3393 [15:28<13:04,  1.94batch/s, Batch Loss=0.2132, Avg Loss=0.0801, Time Left=13.63\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1870/3393 [15:28<13:01,  1.95batch/s, Batch Loss=0.2132, Avg Loss=0.0801, Time Left=13.63\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1870/3393 [15:28<13:01,  1.95batch/s, Batch Loss=0.0396, Avg Loss=0.0801, Time Left=13.62\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1871/3393 [15:28<13:06,  1.94batch/s, Batch Loss=0.0396, Avg Loss=0.0801, Time Left=13.62\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1871/3393 [15:29<13:06,  1.94batch/s, Batch Loss=0.1015, Avg Loss=0.0801, Time Left=13.61\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1872/3393 [15:29<12:58,  1.95batch/s, Batch Loss=0.1015, Avg Loss=0.0801, Time Left=13.61\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1872/3393 [15:29<12:58,  1.95batch/s, Batch Loss=0.0618, Avg Loss=0.0801, Time Left=13.60\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1873/3393 [15:29<13:09,  1.92batch/s, Batch Loss=0.0618, Avg Loss=0.0801, Time Left=13.60\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1873/3393 [15:30<13:09,  1.92batch/s, Batch Loss=0.0148, Avg Loss=0.0800, Time Left=13.59\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1874/3393 [15:30<12:58,  1.95batch/s, Batch Loss=0.0148, Avg Loss=0.0800, Time Left=13.59\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1874/3393 [15:30<12:58,  1.95batch/s, Batch Loss=0.0505, Avg Loss=0.0800, Time Left=13.58\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1875/3393 [15:30<12:53,  1.96batch/s, Batch Loss=0.0505, Avg Loss=0.0800, Time Left=13.58\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1875/3393 [15:31<12:53,  1.96batch/s, Batch Loss=0.0058, Avg Loss=0.0800, Time Left=13.57\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1876/3393 [15:31<12:52,  1.96batch/s, Batch Loss=0.0058, Avg Loss=0.0800, Time Left=13.57\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1876/3393 [15:31<12:52,  1.96batch/s, Batch Loss=0.3996, Avg Loss=0.0802, Time Left=13.57\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1877/3393 [15:31<12:58,  1.95batch/s, Batch Loss=0.3996, Avg Loss=0.0802, Time Left=13.57\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1877/3393 [15:32<12:58,  1.95batch/s, Batch Loss=0.5845, Avg Loss=0.0804, Time Left=13.56\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1878/3393 [15:32<12:53,  1.96batch/s, Batch Loss=0.5845, Avg Loss=0.0804, Time Left=13.56\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1878/3393 [15:32<12:53,  1.96batch/s, Batch Loss=0.0571, Avg Loss=0.0804, Time Left=13.55\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  55%|▌| 1879/3393 [15:32<12:48,  1.97batch/s, Batch Loss=0.0571, Avg Loss=0.0804, Time Left=13.55\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1879/3393 [15:33<12:48,  1.97batch/s, Batch Loss=0.0759, Avg Loss=0.0804, Time Left=13.54\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1880/3393 [15:33<12:37,  2.00batch/s, Batch Loss=0.0759, Avg Loss=0.0804, Time Left=13.54\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1880/3393 [15:33<12:37,  2.00batch/s, Batch Loss=0.0136, Avg Loss=0.0804, Time Left=13.53\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1881/3393 [15:33<12:49,  1.96batch/s, Batch Loss=0.0136, Avg Loss=0.0804, Time Left=13.53\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1881/3393 [15:34<12:49,  1.96batch/s, Batch Loss=0.0073, Avg Loss=0.0803, Time Left=13.52\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1882/3393 [15:34<12:39,  1.99batch/s, Batch Loss=0.0073, Avg Loss=0.0803, Time Left=13.52\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1882/3393 [15:34<12:39,  1.99batch/s, Batch Loss=0.0825, Avg Loss=0.0803, Time Left=13.51\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1883/3393 [15:34<12:51,  1.96batch/s, Batch Loss=0.0825, Avg Loss=0.0803, Time Left=13.51\u001b[A\n",
      "Epoch 3/3 - Training:  55%|▌| 1883/3393 [15:35<12:51,  1.96batch/s, Batch Loss=0.0607, Avg Loss=0.0803, Time Left=13.50\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1884/3393 [15:35<12:46,  1.97batch/s, Batch Loss=0.0607, Avg Loss=0.0803, Time Left=13.50\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1884/3393 [15:35<12:46,  1.97batch/s, Batch Loss=0.0206, Avg Loss=0.0803, Time Left=13.50\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1885/3393 [15:35<13:04,  1.92batch/s, Batch Loss=0.0206, Avg Loss=0.0803, Time Left=13.50\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1885/3393 [15:36<13:04,  1.92batch/s, Batch Loss=0.0346, Avg Loss=0.0803, Time Left=13.49\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1886/3393 [15:36<12:55,  1.94batch/s, Batch Loss=0.0346, Avg Loss=0.0803, Time Left=13.49\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1886/3393 [15:36<12:55,  1.94batch/s, Batch Loss=0.0336, Avg Loss=0.0802, Time Left=13.48\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1887/3393 [15:36<13:14,  1.90batch/s, Batch Loss=0.0336, Avg Loss=0.0802, Time Left=13.48\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1887/3393 [15:37<13:14,  1.90batch/s, Batch Loss=0.0119, Avg Loss=0.0802, Time Left=13.47\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1888/3393 [15:37<12:55,  1.94batch/s, Batch Loss=0.0119, Avg Loss=0.0802, Time Left=13.47\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1888/3393 [15:37<12:55,  1.94batch/s, Batch Loss=0.0324, Avg Loss=0.0802, Time Left=13.46\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1889/3393 [15:37<12:51,  1.95batch/s, Batch Loss=0.0324, Avg Loss=0.0802, Time Left=13.46\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1889/3393 [15:38<12:51,  1.95batch/s, Batch Loss=0.1536, Avg Loss=0.0802, Time Left=13.45\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1890/3393 [15:38<12:45,  1.96batch/s, Batch Loss=0.1536, Avg Loss=0.0802, Time Left=13.45\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1890/3393 [15:38<12:45,  1.96batch/s, Batch Loss=0.0162, Avg Loss=0.0802, Time Left=13.44\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1891/3393 [15:38<12:55,  1.94batch/s, Batch Loss=0.0162, Avg Loss=0.0802, Time Left=13.44\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1891/3393 [15:39<12:55,  1.94batch/s, Batch Loss=0.0278, Avg Loss=0.0802, Time Left=13.44\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1892/3393 [15:39<12:41,  1.97batch/s, Batch Loss=0.0278, Avg Loss=0.0802, Time Left=13.44\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1892/3393 [15:39<12:41,  1.97batch/s, Batch Loss=0.0455, Avg Loss=0.0801, Time Left=13.43\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1893/3393 [15:39<12:51,  1.94batch/s, Batch Loss=0.0455, Avg Loss=0.0801, Time Left=13.43\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1893/3393 [15:40<12:51,  1.94batch/s, Batch Loss=0.0153, Avg Loss=0.0801, Time Left=13.42\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1894/3393 [15:40<12:38,  1.98batch/s, Batch Loss=0.0153, Avg Loss=0.0801, Time Left=13.42\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1894/3393 [15:40<12:38,  1.98batch/s, Batch Loss=0.0110, Avg Loss=0.0801, Time Left=13.41\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1895/3393 [15:40<12:34,  1.98batch/s, Batch Loss=0.0110, Avg Loss=0.0801, Time Left=13.41\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1895/3393 [15:41<12:34,  1.98batch/s, Batch Loss=0.1249, Avg Loss=0.0801, Time Left=13.40\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1896/3393 [15:41<12:18,  2.03batch/s, Batch Loss=0.1249, Avg Loss=0.0801, Time Left=13.40\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1896/3393 [15:41<12:18,  2.03batch/s, Batch Loss=0.0156, Avg Loss=0.0801, Time Left=13.39\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1897/3393 [15:41<12:20,  2.02batch/s, Batch Loss=0.0156, Avg Loss=0.0801, Time Left=13.39\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1897/3393 [15:42<12:20,  2.02batch/s, Batch Loss=0.2129, Avg Loss=0.0801, Time Left=13.38\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1898/3393 [15:42<12:22,  2.01batch/s, Batch Loss=0.2129, Avg Loss=0.0801, Time Left=13.38\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1898/3393 [15:42<12:22,  2.01batch/s, Batch Loss=0.4574, Avg Loss=0.0803, Time Left=13.37\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1899/3393 [15:42<12:30,  1.99batch/s, Batch Loss=0.4574, Avg Loss=0.0803, Time Left=13.37\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1899/3393 [15:43<12:30,  1.99batch/s, Batch Loss=0.0220, Avg Loss=0.0803, Time Left=13.37\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1900/3393 [15:43<12:29,  1.99batch/s, Batch Loss=0.0220, Avg Loss=0.0803, Time Left=13.37\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1900/3393 [15:43<12:29,  1.99batch/s, Batch Loss=0.0335, Avg Loss=0.0803, Time Left=13.36\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1901/3393 [15:43<12:35,  1.98batch/s, Batch Loss=0.0335, Avg Loss=0.0803, Time Left=13.36\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1901/3393 [15:44<12:35,  1.98batch/s, Batch Loss=0.1232, Avg Loss=0.0803, Time Left=13.35\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1902/3393 [15:44<12:39,  1.96batch/s, Batch Loss=0.1232, Avg Loss=0.0803, Time Left=13.35\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1902/3393 [15:44<12:39,  1.96batch/s, Batch Loss=0.1219, Avg Loss=0.0803, Time Left=13.34\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1903/3393 [15:44<12:35,  1.97batch/s, Batch Loss=0.1219, Avg Loss=0.0803, Time Left=13.34\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1903/3393 [15:45<12:35,  1.97batch/s, Batch Loss=0.0469, Avg Loss=0.0803, Time Left=13.33\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1904/3393 [15:45<12:39,  1.96batch/s, Batch Loss=0.0469, Avg Loss=0.0803, Time Left=13.33\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1904/3393 [15:45<12:39,  1.96batch/s, Batch Loss=0.0601, Avg Loss=0.0803, Time Left=13.32\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1905/3393 [15:45<12:35,  1.97batch/s, Batch Loss=0.0601, Avg Loss=0.0803, Time Left=13.32\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1905/3393 [15:46<12:35,  1.97batch/s, Batch Loss=0.0370, Avg Loss=0.0803, Time Left=13.31\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1906/3393 [15:46<12:45,  1.94batch/s, Batch Loss=0.0370, Avg Loss=0.0803, Time Left=13.31\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1906/3393 [15:46<12:45,  1.94batch/s, Batch Loss=0.0250, Avg Loss=0.0802, Time Left=13.31\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1907/3393 [15:46<12:31,  1.98batch/s, Batch Loss=0.0250, Avg Loss=0.0802, Time Left=13.31\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1907/3393 [15:47<12:31,  1.98batch/s, Batch Loss=0.0027, Avg Loss=0.0802, Time Left=13.30\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1908/3393 [15:47<12:49,  1.93batch/s, Batch Loss=0.0027, Avg Loss=0.0802, Time Left=13.30\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1908/3393 [15:48<12:49,  1.93batch/s, Batch Loss=0.1310, Avg Loss=0.0802, Time Left=13.29\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1909/3393 [15:48<12:48,  1.93batch/s, Batch Loss=0.1310, Avg Loss=0.0802, Time Left=13.29\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1909/3393 [15:48<12:48,  1.93batch/s, Batch Loss=0.0082, Avg Loss=0.0802, Time Left=13.28\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1910/3393 [15:48<12:34,  1.97batch/s, Batch Loss=0.0082, Avg Loss=0.0802, Time Left=13.28\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1910/3393 [15:49<12:34,  1.97batch/s, Batch Loss=0.0139, Avg Loss=0.0802, Time Left=13.27\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1911/3393 [15:49<12:36,  1.96batch/s, Batch Loss=0.0139, Avg Loss=0.0802, Time Left=13.27\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1911/3393 [15:49<12:36,  1.96batch/s, Batch Loss=0.0989, Avg Loss=0.0802, Time Left=13.26\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  56%|▌| 1912/3393 [15:49<12:32,  1.97batch/s, Batch Loss=0.0989, Avg Loss=0.0802, Time Left=13.26\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1912/3393 [15:50<12:32,  1.97batch/s, Batch Loss=0.0091, Avg Loss=0.0801, Time Left=13.25\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1913/3393 [15:50<12:28,  1.98batch/s, Batch Loss=0.0091, Avg Loss=0.0801, Time Left=13.25\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1913/3393 [15:50<12:28,  1.98batch/s, Batch Loss=0.1191, Avg Loss=0.0801, Time Left=13.25\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1914/3393 [15:50<12:46,  1.93batch/s, Batch Loss=0.1191, Avg Loss=0.0801, Time Left=13.25\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1914/3393 [15:51<12:46,  1.93batch/s, Batch Loss=0.0129, Avg Loss=0.0801, Time Left=13.24\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1915/3393 [15:51<12:36,  1.95batch/s, Batch Loss=0.0129, Avg Loss=0.0801, Time Left=13.24\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1915/3393 [15:51<12:36,  1.95batch/s, Batch Loss=0.3447, Avg Loss=0.0803, Time Left=13.23\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1916/3393 [15:51<12:40,  1.94batch/s, Batch Loss=0.3447, Avg Loss=0.0803, Time Left=13.23\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1916/3393 [15:52<12:40,  1.94batch/s, Batch Loss=0.1753, Avg Loss=0.0803, Time Left=13.22\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1917/3393 [15:52<12:34,  1.96batch/s, Batch Loss=0.1753, Avg Loss=0.0803, Time Left=13.22\u001b[A\n",
      "Epoch 3/3 - Training:  56%|▌| 1917/3393 [15:52<12:34,  1.96batch/s, Batch Loss=0.0157, Avg Loss=0.0803, Time Left=13.21\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1918/3393 [15:52<12:35,  1.95batch/s, Batch Loss=0.0157, Avg Loss=0.0803, Time Left=13.21\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1918/3393 [15:53<12:35,  1.95batch/s, Batch Loss=0.2316, Avg Loss=0.0804, Time Left=13.20\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1919/3393 [15:53<12:29,  1.97batch/s, Batch Loss=0.2316, Avg Loss=0.0804, Time Left=13.20\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1919/3393 [15:53<12:29,  1.97batch/s, Batch Loss=0.0134, Avg Loss=0.0803, Time Left=13.19\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1920/3393 [15:53<12:32,  1.96batch/s, Batch Loss=0.0134, Avg Loss=0.0803, Time Left=13.19\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1920/3393 [15:54<12:32,  1.96batch/s, Batch Loss=0.1582, Avg Loss=0.0804, Time Left=13.18\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1921/3393 [15:54<12:28,  1.97batch/s, Batch Loss=0.1582, Avg Loss=0.0804, Time Left=13.18\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1921/3393 [15:54<12:28,  1.97batch/s, Batch Loss=0.0234, Avg Loss=0.0803, Time Left=13.18\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1922/3393 [15:54<12:38,  1.94batch/s, Batch Loss=0.0234, Avg Loss=0.0803, Time Left=13.18\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1922/3393 [15:55<12:38,  1.94batch/s, Batch Loss=0.0044, Avg Loss=0.0803, Time Left=13.17\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1923/3393 [15:55<12:32,  1.95batch/s, Batch Loss=0.0044, Avg Loss=0.0803, Time Left=13.17\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1923/3393 [15:55<12:32,  1.95batch/s, Batch Loss=0.0263, Avg Loss=0.0803, Time Left=13.16\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1924/3393 [15:55<12:33,  1.95batch/s, Batch Loss=0.0263, Avg Loss=0.0803, Time Left=13.16\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1924/3393 [15:56<12:33,  1.95batch/s, Batch Loss=0.0227, Avg Loss=0.0802, Time Left=13.15\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1925/3393 [15:56<12:28,  1.96batch/s, Batch Loss=0.0227, Avg Loss=0.0802, Time Left=13.15\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1925/3393 [15:56<12:28,  1.96batch/s, Batch Loss=0.0113, Avg Loss=0.0802, Time Left=13.14\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1926/3393 [15:56<12:30,  1.95batch/s, Batch Loss=0.0113, Avg Loss=0.0802, Time Left=13.14\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1926/3393 [15:57<12:30,  1.95batch/s, Batch Loss=0.4088, Avg Loss=0.0804, Time Left=13.13\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1927/3393 [15:57<12:31,  1.95batch/s, Batch Loss=0.4088, Avg Loss=0.0804, Time Left=13.13\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1927/3393 [15:57<12:31,  1.95batch/s, Batch Loss=0.0344, Avg Loss=0.0803, Time Left=13.12\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1928/3393 [15:57<12:33,  1.94batch/s, Batch Loss=0.0344, Avg Loss=0.0803, Time Left=13.12\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1928/3393 [15:58<12:33,  1.94batch/s, Batch Loss=0.0204, Avg Loss=0.0803, Time Left=13.12\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1929/3393 [15:58<12:27,  1.96batch/s, Batch Loss=0.0204, Avg Loss=0.0803, Time Left=13.12\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1929/3393 [15:58<12:27,  1.96batch/s, Batch Loss=0.0441, Avg Loss=0.0803, Time Left=13.11\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1930/3393 [15:58<12:35,  1.94batch/s, Batch Loss=0.0441, Avg Loss=0.0803, Time Left=13.11\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1930/3393 [15:59<12:35,  1.94batch/s, Batch Loss=0.1561, Avg Loss=0.0803, Time Left=13.10\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1931/3393 [15:59<12:29,  1.95batch/s, Batch Loss=0.1561, Avg Loss=0.0803, Time Left=13.10\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1931/3393 [15:59<12:29,  1.95batch/s, Batch Loss=0.0714, Avg Loss=0.0803, Time Left=13.09\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1932/3393 [15:59<12:23,  1.96batch/s, Batch Loss=0.0714, Avg Loss=0.0803, Time Left=13.09\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1932/3393 [16:00<12:23,  1.96batch/s, Batch Loss=0.0150, Avg Loss=0.0803, Time Left=13.08\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1933/3393 [16:00<12:12,  1.99batch/s, Batch Loss=0.0150, Avg Loss=0.0803, Time Left=13.08\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1933/3393 [16:00<12:12,  1.99batch/s, Batch Loss=0.0094, Avg Loss=0.0803, Time Left=13.07\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1934/3393 [16:00<12:24,  1.96batch/s, Batch Loss=0.0094, Avg Loss=0.0803, Time Left=13.07\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1934/3393 [16:01<12:24,  1.96batch/s, Batch Loss=0.0450, Avg Loss=0.0802, Time Left=13.06\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1935/3393 [16:01<12:20,  1.97batch/s, Batch Loss=0.0450, Avg Loss=0.0802, Time Left=13.06\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1935/3393 [16:01<12:20,  1.97batch/s, Batch Loss=0.0322, Avg Loss=0.0802, Time Left=13.06\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1936/3393 [16:01<12:22,  1.96batch/s, Batch Loss=0.0322, Avg Loss=0.0802, Time Left=13.06\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1936/3393 [16:02<12:22,  1.96batch/s, Batch Loss=0.0069, Avg Loss=0.0802, Time Left=13.05\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1937/3393 [16:02<12:05,  2.01batch/s, Batch Loss=0.0069, Avg Loss=0.0802, Time Left=13.05\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1937/3393 [16:02<12:05,  2.01batch/s, Batch Loss=0.0615, Avg Loss=0.0802, Time Left=13.04\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1938/3393 [16:02<12:25,  1.95batch/s, Batch Loss=0.0615, Avg Loss=0.0802, Time Left=13.04\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1938/3393 [16:03<12:25,  1.95batch/s, Batch Loss=0.0296, Avg Loss=0.0801, Time Left=13.03\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1939/3393 [16:03<12:21,  1.96batch/s, Batch Loss=0.0296, Avg Loss=0.0801, Time Left=13.03\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1939/3393 [16:03<12:21,  1.96batch/s, Batch Loss=0.0079, Avg Loss=0.0801, Time Left=13.02\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1940/3393 [16:03<12:46,  1.90batch/s, Batch Loss=0.0079, Avg Loss=0.0801, Time Left=13.02\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1940/3393 [16:04<12:46,  1.90batch/s, Batch Loss=0.0073, Avg Loss=0.0801, Time Left=13.01\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1941/3393 [16:04<12:56,  1.87batch/s, Batch Loss=0.0073, Avg Loss=0.0801, Time Left=13.01\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1941/3393 [16:04<12:56,  1.87batch/s, Batch Loss=0.0234, Avg Loss=0.0800, Time Left=13.00\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1942/3393 [16:04<12:40,  1.91batch/s, Batch Loss=0.0234, Avg Loss=0.0800, Time Left=13.00\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1942/3393 [16:05<12:40,  1.91batch/s, Batch Loss=0.0121, Avg Loss=0.0800, Time Left=13.00\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1943/3393 [16:05<12:55,  1.87batch/s, Batch Loss=0.0121, Avg Loss=0.0800, Time Left=13.00\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1943/3393 [16:06<12:55,  1.87batch/s, Batch Loss=0.0127, Avg Loss=0.0800, Time Left=12.99\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1944/3393 [16:06<13:14,  1.82batch/s, Batch Loss=0.0127, Avg Loss=0.0800, Time Left=12.99\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1944/3393 [16:06<13:14,  1.82batch/s, Batch Loss=0.0326, Avg Loss=0.0799, Time Left=12.98\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  57%|▌| 1945/3393 [16:06<12:36,  1.91batch/s, Batch Loss=0.0326, Avg Loss=0.0799, Time Left=12.98\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1945/3393 [16:07<12:36,  1.91batch/s, Batch Loss=0.2861, Avg Loss=0.0800, Time Left=12.97\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1946/3393 [16:07<12:32,  1.92batch/s, Batch Loss=0.2861, Avg Loss=0.0800, Time Left=12.97\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1946/3393 [16:07<12:32,  1.92batch/s, Batch Loss=0.0034, Avg Loss=0.0800, Time Left=12.96\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1947/3393 [16:07<12:23,  1.94batch/s, Batch Loss=0.0034, Avg Loss=0.0800, Time Left=12.96\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1947/3393 [16:08<12:23,  1.94batch/s, Batch Loss=0.1028, Avg Loss=0.0800, Time Left=12.95\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1948/3393 [16:08<12:17,  1.96batch/s, Batch Loss=0.1028, Avg Loss=0.0800, Time Left=12.95\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1948/3393 [16:08<12:17,  1.96batch/s, Batch Loss=0.0240, Avg Loss=0.0800, Time Left=12.95\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1949/3393 [16:08<12:30,  1.92batch/s, Batch Loss=0.0240, Avg Loss=0.0800, Time Left=12.95\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1949/3393 [16:09<12:30,  1.92batch/s, Batch Loss=0.0091, Avg Loss=0.0799, Time Left=12.94\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1950/3393 [16:09<12:17,  1.96batch/s, Batch Loss=0.0091, Avg Loss=0.0799, Time Left=12.94\u001b[A\n",
      "Epoch 3/3 - Training:  57%|▌| 1950/3393 [16:09<12:17,  1.96batch/s, Batch Loss=0.0219, Avg Loss=0.0799, Time Left=12.93\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1951/3393 [16:09<12:13,  1.97batch/s, Batch Loss=0.0219, Avg Loss=0.0799, Time Left=12.93\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1951/3393 [16:10<12:13,  1.97batch/s, Batch Loss=0.1890, Avg Loss=0.0800, Time Left=12.92\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1952/3393 [16:10<12:01,  2.00batch/s, Batch Loss=0.1890, Avg Loss=0.0800, Time Left=12.92\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1952/3393 [16:10<12:01,  2.00batch/s, Batch Loss=0.0041, Avg Loss=0.0799, Time Left=12.91\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1953/3393 [16:10<12:20,  1.94batch/s, Batch Loss=0.0041, Avg Loss=0.0799, Time Left=12.91\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1953/3393 [16:11<12:20,  1.94batch/s, Batch Loss=0.1531, Avg Loss=0.0800, Time Left=12.90\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1954/3393 [16:11<12:15,  1.96batch/s, Batch Loss=0.1531, Avg Loss=0.0800, Time Left=12.90\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1954/3393 [16:11<12:15,  1.96batch/s, Batch Loss=0.0651, Avg Loss=0.0800, Time Left=12.89\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1955/3393 [16:11<12:16,  1.95batch/s, Batch Loss=0.0651, Avg Loss=0.0800, Time Left=12.89\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1955/3393 [16:12<12:16,  1.95batch/s, Batch Loss=0.0024, Avg Loss=0.0799, Time Left=12.88\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1956/3393 [16:12<12:05,  1.98batch/s, Batch Loss=0.0024, Avg Loss=0.0799, Time Left=12.88\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1956/3393 [16:12<12:05,  1.98batch/s, Batch Loss=0.0034, Avg Loss=0.0799, Time Left=12.88\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1957/3393 [16:12<12:16,  1.95batch/s, Batch Loss=0.0034, Avg Loss=0.0799, Time Left=12.88\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1957/3393 [16:13<12:16,  1.95batch/s, Batch Loss=0.1564, Avg Loss=0.0799, Time Left=12.87\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1958/3393 [16:13<12:10,  1.96batch/s, Batch Loss=0.1564, Avg Loss=0.0799, Time Left=12.87\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1958/3393 [16:13<12:10,  1.96batch/s, Batch Loss=0.3143, Avg Loss=0.0800, Time Left=12.86\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1959/3393 [16:13<12:13,  1.95batch/s, Batch Loss=0.3143, Avg Loss=0.0800, Time Left=12.86\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1959/3393 [16:14<12:13,  1.95batch/s, Batch Loss=0.0305, Avg Loss=0.0800, Time Left=12.85\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1960/3393 [16:14<12:15,  1.95batch/s, Batch Loss=0.0305, Avg Loss=0.0800, Time Left=12.85\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1960/3393 [16:14<12:15,  1.95batch/s, Batch Loss=0.0548, Avg Loss=0.0800, Time Left=12.84\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1961/3393 [16:14<12:02,  1.98batch/s, Batch Loss=0.0548, Avg Loss=0.0800, Time Left=12.84\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1961/3393 [16:15<12:02,  1.98batch/s, Batch Loss=0.0113, Avg Loss=0.0800, Time Left=12.83\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1962/3393 [16:15<12:00,  1.99batch/s, Batch Loss=0.0113, Avg Loss=0.0800, Time Left=12.83\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1962/3393 [16:15<12:00,  1.99batch/s, Batch Loss=0.0048, Avg Loss=0.0799, Time Left=12.82\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1963/3393 [16:15<11:58,  1.99batch/s, Batch Loss=0.0048, Avg Loss=0.0799, Time Left=12.82\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1963/3393 [16:16<11:58,  1.99batch/s, Batch Loss=0.0194, Avg Loss=0.0799, Time Left=12.82\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1964/3393 [16:16<11:49,  2.01batch/s, Batch Loss=0.0194, Avg Loss=0.0799, Time Left=12.82\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1964/3393 [16:16<11:49,  2.01batch/s, Batch Loss=0.0339, Avg Loss=0.0799, Time Left=12.81\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1965/3393 [16:16<11:51,  2.01batch/s, Batch Loss=0.0339, Avg Loss=0.0799, Time Left=12.81\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1965/3393 [16:17<11:51,  2.01batch/s, Batch Loss=0.0760, Avg Loss=0.0799, Time Left=12.80\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1966/3393 [16:17<11:50,  2.01batch/s, Batch Loss=0.0760, Avg Loss=0.0799, Time Left=12.80\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1966/3393 [16:17<11:50,  2.01batch/s, Batch Loss=0.0068, Avg Loss=0.0798, Time Left=12.79\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1967/3393 [16:17<11:45,  2.02batch/s, Batch Loss=0.0068, Avg Loss=0.0798, Time Left=12.79\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1967/3393 [16:18<11:45,  2.02batch/s, Batch Loss=0.0167, Avg Loss=0.0798, Time Left=12.78\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1968/3393 [16:18<11:53,  2.00batch/s, Batch Loss=0.0167, Avg Loss=0.0798, Time Left=12.78\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1968/3393 [16:18<11:53,  2.00batch/s, Batch Loss=0.0490, Avg Loss=0.0798, Time Left=12.77\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1969/3393 [16:18<11:46,  2.02batch/s, Batch Loss=0.0490, Avg Loss=0.0798, Time Left=12.77\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1969/3393 [16:19<11:46,  2.02batch/s, Batch Loss=0.0102, Avg Loss=0.0797, Time Left=12.76\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1970/3393 [16:19<12:14,  1.94batch/s, Batch Loss=0.0102, Avg Loss=0.0797, Time Left=12.76\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1970/3393 [16:19<12:14,  1.94batch/s, Batch Loss=0.0490, Avg Loss=0.0797, Time Left=12.75\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1971/3393 [16:19<12:02,  1.97batch/s, Batch Loss=0.0490, Avg Loss=0.0797, Time Left=12.75\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1971/3393 [16:20<12:02,  1.97batch/s, Batch Loss=0.0091, Avg Loss=0.0797, Time Left=12.75\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1972/3393 [16:20<12:17,  1.93batch/s, Batch Loss=0.0091, Avg Loss=0.0797, Time Left=12.75\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1972/3393 [16:20<12:17,  1.93batch/s, Batch Loss=0.4492, Avg Loss=0.0799, Time Left=12.74\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1973/3393 [16:20<12:10,  1.94batch/s, Batch Loss=0.4492, Avg Loss=0.0799, Time Left=12.74\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1973/3393 [16:21<12:10,  1.94batch/s, Batch Loss=0.0050, Avg Loss=0.0798, Time Left=12.73\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1974/3393 [16:21<12:03,  1.96batch/s, Batch Loss=0.0050, Avg Loss=0.0798, Time Left=12.73\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1974/3393 [16:21<12:03,  1.96batch/s, Batch Loss=0.0168, Avg Loss=0.0798, Time Left=12.72\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1975/3393 [16:21<12:06,  1.95batch/s, Batch Loss=0.0168, Avg Loss=0.0798, Time Left=12.72\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1975/3393 [16:22<12:06,  1.95batch/s, Batch Loss=0.0169, Avg Loss=0.0798, Time Left=12.71\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1976/3393 [16:22<12:00,  1.97batch/s, Batch Loss=0.0169, Avg Loss=0.0798, Time Left=12.71\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1976/3393 [16:22<12:00,  1.97batch/s, Batch Loss=0.0545, Avg Loss=0.0798, Time Left=12.70\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1977/3393 [16:22<12:03,  1.96batch/s, Batch Loss=0.0545, Avg Loss=0.0798, Time Left=12.70\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1977/3393 [16:23<12:03,  1.96batch/s, Batch Loss=0.0102, Avg Loss=0.0797, Time Left=12.69\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  58%|▌| 1978/3393 [16:23<12:06,  1.95batch/s, Batch Loss=0.0102, Avg Loss=0.0797, Time Left=12.69\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1978/3393 [16:23<12:06,  1.95batch/s, Batch Loss=0.1740, Avg Loss=0.0798, Time Left=12.69\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1979/3393 [16:23<11:53,  1.98batch/s, Batch Loss=0.1740, Avg Loss=0.0798, Time Left=12.69\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1979/3393 [16:24<11:53,  1.98batch/s, Batch Loss=0.0119, Avg Loss=0.0797, Time Left=12.68\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1980/3393 [16:24<12:04,  1.95batch/s, Batch Loss=0.0119, Avg Loss=0.0797, Time Left=12.68\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1980/3393 [16:24<12:04,  1.95batch/s, Batch Loss=0.0113, Avg Loss=0.0797, Time Left=12.67\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1981/3393 [16:24<11:59,  1.96batch/s, Batch Loss=0.0113, Avg Loss=0.0797, Time Left=12.67\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1981/3393 [16:25<11:59,  1.96batch/s, Batch Loss=0.0777, Avg Loss=0.0797, Time Left=12.66\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1982/3393 [16:25<12:01,  1.96batch/s, Batch Loss=0.0777, Avg Loss=0.0797, Time Left=12.66\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1982/3393 [16:25<12:01,  1.96batch/s, Batch Loss=0.0091, Avg Loss=0.0797, Time Left=12.65\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1983/3393 [16:25<11:49,  1.99batch/s, Batch Loss=0.0091, Avg Loss=0.0797, Time Left=12.65\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1983/3393 [16:26<11:49,  1.99batch/s, Batch Loss=0.0156, Avg Loss=0.0796, Time Left=12.64\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1984/3393 [16:26<11:47,  1.99batch/s, Batch Loss=0.0156, Avg Loss=0.0796, Time Left=12.64\u001b[A\n",
      "Epoch 3/3 - Training:  58%|▌| 1984/3393 [16:26<11:47,  1.99batch/s, Batch Loss=0.1029, Avg Loss=0.0796, Time Left=12.63\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1985/3393 [16:26<11:32,  2.03batch/s, Batch Loss=0.1029, Avg Loss=0.0796, Time Left=12.63\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1985/3393 [16:27<11:32,  2.03batch/s, Batch Loss=0.0231, Avg Loss=0.0796, Time Left=12.62\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1986/3393 [16:27<11:22,  2.06batch/s, Batch Loss=0.0231, Avg Loss=0.0796, Time Left=12.62\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1986/3393 [16:27<11:22,  2.06batch/s, Batch Loss=0.0671, Avg Loss=0.0796, Time Left=12.62\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1987/3393 [16:27<11:40,  2.01batch/s, Batch Loss=0.0671, Avg Loss=0.0796, Time Left=12.62\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1987/3393 [16:28<11:40,  2.01batch/s, Batch Loss=0.0341, Avg Loss=0.0796, Time Left=12.61\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1988/3393 [16:28<11:21,  2.06batch/s, Batch Loss=0.0341, Avg Loss=0.0796, Time Left=12.61\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1988/3393 [16:28<11:21,  2.06batch/s, Batch Loss=0.0033, Avg Loss=0.0795, Time Left=12.60\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1989/3393 [16:28<11:26,  2.05batch/s, Batch Loss=0.0033, Avg Loss=0.0795, Time Left=12.60\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1989/3393 [16:29<11:26,  2.05batch/s, Batch Loss=0.0248, Avg Loss=0.0795, Time Left=12.59\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1990/3393 [16:29<11:38,  2.01batch/s, Batch Loss=0.0248, Avg Loss=0.0795, Time Left=12.59\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1990/3393 [16:29<11:38,  2.01batch/s, Batch Loss=0.0201, Avg Loss=0.0795, Time Left=12.58\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1991/3393 [16:29<11:24,  2.05batch/s, Batch Loss=0.0201, Avg Loss=0.0795, Time Left=12.58\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1991/3393 [16:30<11:24,  2.05batch/s, Batch Loss=0.1347, Avg Loss=0.0795, Time Left=12.57\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1992/3393 [16:30<11:49,  1.97batch/s, Batch Loss=0.1347, Avg Loss=0.0795, Time Left=12.57\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1992/3393 [16:30<11:49,  1.97batch/s, Batch Loss=0.0112, Avg Loss=0.0795, Time Left=12.56\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1993/3393 [16:30<11:39,  2.00batch/s, Batch Loss=0.0112, Avg Loss=0.0795, Time Left=12.56\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1993/3393 [16:31<11:39,  2.00batch/s, Batch Loss=0.0111, Avg Loss=0.0794, Time Left=12.55\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1994/3393 [16:31<11:46,  1.98batch/s, Batch Loss=0.0111, Avg Loss=0.0794, Time Left=12.55\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1994/3393 [16:31<11:46,  1.98batch/s, Batch Loss=0.0040, Avg Loss=0.0794, Time Left=12.55\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1995/3393 [16:31<11:44,  1.99batch/s, Batch Loss=0.0040, Avg Loss=0.0794, Time Left=12.55\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1995/3393 [16:32<11:44,  1.99batch/s, Batch Loss=0.0815, Avg Loss=0.0794, Time Left=12.54\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1996/3393 [16:32<11:55,  1.95batch/s, Batch Loss=0.0815, Avg Loss=0.0794, Time Left=12.54\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1996/3393 [16:32<11:55,  1.95batch/s, Batch Loss=0.0083, Avg Loss=0.0794, Time Left=12.53\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1997/3393 [16:32<11:50,  1.96batch/s, Batch Loss=0.0083, Avg Loss=0.0794, Time Left=12.53\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1997/3393 [16:33<11:50,  1.96batch/s, Batch Loss=0.0367, Avg Loss=0.0793, Time Left=12.52\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1998/3393 [16:33<11:53,  1.95batch/s, Batch Loss=0.0367, Avg Loss=0.0793, Time Left=12.52\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1998/3393 [16:33<11:53,  1.95batch/s, Batch Loss=0.0244, Avg Loss=0.0793, Time Left=12.51\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1999/3393 [16:33<11:55,  1.95batch/s, Batch Loss=0.0244, Avg Loss=0.0793, Time Left=12.51\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 1999/3393 [16:34<11:55,  1.95batch/s, Batch Loss=0.0355, Avg Loss=0.0793, Time Left=12.50\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2000/3393 [16:34<11:55,  1.95batch/s, Batch Loss=0.0355, Avg Loss=0.0793, Time Left=12.50\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2000/3393 [16:34<11:55,  1.95batch/s, Batch Loss=0.0054, Avg Loss=0.0793, Time Left=12.49\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2001/3393 [16:34<11:57,  1.94batch/s, Batch Loss=0.0054, Avg Loss=0.0793, Time Left=12.49\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2001/3393 [16:35<11:57,  1.94batch/s, Batch Loss=0.0066, Avg Loss=0.0792, Time Left=12.49\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2002/3393 [16:35<12:04,  1.92batch/s, Batch Loss=0.0066, Avg Loss=0.0792, Time Left=12.49\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2002/3393 [16:35<12:04,  1.92batch/s, Batch Loss=0.1832, Avg Loss=0.0793, Time Left=12.48\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2003/3393 [16:35<11:55,  1.94batch/s, Batch Loss=0.1832, Avg Loss=0.0793, Time Left=12.48\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2003/3393 [16:36<11:55,  1.94batch/s, Batch Loss=0.0166, Avg Loss=0.0792, Time Left=12.47\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2004/3393 [16:36<11:55,  1.94batch/s, Batch Loss=0.0166, Avg Loss=0.0792, Time Left=12.47\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2004/3393 [16:36<11:55,  1.94batch/s, Batch Loss=0.1274, Avg Loss=0.0793, Time Left=12.46\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2005/3393 [16:36<11:49,  1.96batch/s, Batch Loss=0.1274, Avg Loss=0.0793, Time Left=12.46\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2005/3393 [16:37<11:49,  1.96batch/s, Batch Loss=0.1883, Avg Loss=0.0793, Time Left=12.45\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2006/3393 [16:37<11:50,  1.95batch/s, Batch Loss=0.1883, Avg Loss=0.0793, Time Left=12.45\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2006/3393 [16:37<11:50,  1.95batch/s, Batch Loss=0.0403, Avg Loss=0.0793, Time Left=12.44\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2007/3393 [16:37<11:45,  1.96batch/s, Batch Loss=0.0403, Avg Loss=0.0793, Time Left=12.44\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2007/3393 [16:38<11:45,  1.96batch/s, Batch Loss=0.1696, Avg Loss=0.0794, Time Left=12.43\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2008/3393 [16:38<11:54,  1.94batch/s, Batch Loss=0.1696, Avg Loss=0.0794, Time Left=12.43\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2008/3393 [16:38<11:54,  1.94batch/s, Batch Loss=0.0073, Avg Loss=0.0793, Time Left=12.43\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2009/3393 [16:38<11:42,  1.97batch/s, Batch Loss=0.0073, Avg Loss=0.0793, Time Left=12.43\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2009/3393 [16:39<11:42,  1.97batch/s, Batch Loss=0.0379, Avg Loss=0.0793, Time Left=12.42\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2010/3393 [16:39<11:50,  1.95batch/s, Batch Loss=0.0379, Avg Loss=0.0793, Time Left=12.42\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2010/3393 [16:40<11:50,  1.95batch/s, Batch Loss=0.0033, Avg Loss=0.0793, Time Left=12.41\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  59%|▌| 2011/3393 [16:40<11:45,  1.96batch/s, Batch Loss=0.0033, Avg Loss=0.0793, Time Left=12.41\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2011/3393 [16:40<11:45,  1.96batch/s, Batch Loss=0.0038, Avg Loss=0.0792, Time Left=12.40\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2012/3393 [16:40<11:54,  1.93batch/s, Batch Loss=0.0038, Avg Loss=0.0792, Time Left=12.40\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2012/3393 [16:41<11:54,  1.93batch/s, Batch Loss=0.0079, Avg Loss=0.0792, Time Left=12.39\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2013/3393 [16:41<11:47,  1.95batch/s, Batch Loss=0.0079, Avg Loss=0.0792, Time Left=12.39\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2013/3393 [16:41<11:47,  1.95batch/s, Batch Loss=0.0067, Avg Loss=0.0791, Time Left=12.38\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2014/3393 [16:41<11:48,  1.95batch/s, Batch Loss=0.0067, Avg Loss=0.0791, Time Left=12.38\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2014/3393 [16:42<11:48,  1.95batch/s, Batch Loss=0.0054, Avg Loss=0.0791, Time Left=12.37\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2015/3393 [16:42<11:42,  1.96batch/s, Batch Loss=0.0054, Avg Loss=0.0791, Time Left=12.37\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2015/3393 [16:42<11:42,  1.96batch/s, Batch Loss=0.1014, Avg Loss=0.0791, Time Left=12.37\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2016/3393 [16:42<11:51,  1.94batch/s, Batch Loss=0.1014, Avg Loss=0.0791, Time Left=12.37\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2016/3393 [16:43<11:51,  1.94batch/s, Batch Loss=0.0078, Avg Loss=0.0791, Time Left=12.36\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2017/3393 [16:43<11:37,  1.97batch/s, Batch Loss=0.0078, Avg Loss=0.0791, Time Left=12.36\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2017/3393 [16:43<11:37,  1.97batch/s, Batch Loss=0.0026, Avg Loss=0.0790, Time Left=12.35\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2018/3393 [16:43<11:40,  1.96batch/s, Batch Loss=0.0026, Avg Loss=0.0790, Time Left=12.35\u001b[A\n",
      "Epoch 3/3 - Training:  59%|▌| 2018/3393 [16:44<11:40,  1.96batch/s, Batch Loss=0.0128, Avg Loss=0.0790, Time Left=12.34\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2019/3393 [16:44<11:43,  1.95batch/s, Batch Loss=0.0128, Avg Loss=0.0790, Time Left=12.34\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2019/3393 [16:44<11:43,  1.95batch/s, Batch Loss=0.1387, Avg Loss=0.0790, Time Left=12.33\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2020/3393 [16:44<11:39,  1.96batch/s, Batch Loss=0.1387, Avg Loss=0.0790, Time Left=12.33\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2020/3393 [16:45<11:39,  1.96batch/s, Batch Loss=0.0108, Avg Loss=0.0790, Time Left=12.32\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2021/3393 [16:45<11:27,  1.99batch/s, Batch Loss=0.0108, Avg Loss=0.0790, Time Left=12.32\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2021/3393 [16:45<11:27,  1.99batch/s, Batch Loss=0.0032, Avg Loss=0.0790, Time Left=12.31\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2022/3393 [16:45<11:39,  1.96batch/s, Batch Loss=0.0032, Avg Loss=0.0790, Time Left=12.31\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2022/3393 [16:46<11:39,  1.96batch/s, Batch Loss=0.0225, Avg Loss=0.0789, Time Left=12.30\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2023/3393 [16:46<11:36,  1.97batch/s, Batch Loss=0.0225, Avg Loss=0.0789, Time Left=12.30\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2023/3393 [16:46<11:36,  1.97batch/s, Batch Loss=0.2848, Avg Loss=0.0790, Time Left=12.30\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2024/3393 [16:46<11:44,  1.94batch/s, Batch Loss=0.2848, Avg Loss=0.0790, Time Left=12.30\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2024/3393 [16:47<11:44,  1.94batch/s, Batch Loss=0.0196, Avg Loss=0.0790, Time Left=12.29\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2025/3393 [16:47<11:38,  1.96batch/s, Batch Loss=0.0196, Avg Loss=0.0790, Time Left=12.29\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2025/3393 [16:47<11:38,  1.96batch/s, Batch Loss=0.1432, Avg Loss=0.0790, Time Left=12.28\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2026/3393 [16:47<11:40,  1.95batch/s, Batch Loss=0.1432, Avg Loss=0.0790, Time Left=12.28\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2026/3393 [16:48<11:40,  1.95batch/s, Batch Loss=0.0031, Avg Loss=0.0790, Time Left=12.27\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2027/3393 [16:48<11:40,  1.95batch/s, Batch Loss=0.0031, Avg Loss=0.0790, Time Left=12.27\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2027/3393 [16:48<11:40,  1.95batch/s, Batch Loss=0.0623, Avg Loss=0.0790, Time Left=12.26\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2028/3393 [16:48<11:37,  1.96batch/s, Batch Loss=0.0623, Avg Loss=0.0790, Time Left=12.26\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2028/3393 [16:49<11:37,  1.96batch/s, Batch Loss=0.1699, Avg Loss=0.0790, Time Left=12.25\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2029/3393 [16:49<11:32,  1.97batch/s, Batch Loss=0.1699, Avg Loss=0.0790, Time Left=12.25\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2029/3393 [16:49<11:32,  1.97batch/s, Batch Loss=0.2273, Avg Loss=0.0791, Time Left=12.24\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2030/3393 [16:49<11:41,  1.94batch/s, Batch Loss=0.2273, Avg Loss=0.0791, Time Left=12.24\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2030/3393 [16:50<11:41,  1.94batch/s, Batch Loss=0.0127, Avg Loss=0.0791, Time Left=12.24\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2031/3393 [16:50<11:36,  1.96batch/s, Batch Loss=0.0127, Avg Loss=0.0791, Time Left=12.24\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2031/3393 [16:50<11:36,  1.96batch/s, Batch Loss=0.0515, Avg Loss=0.0791, Time Left=12.23\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2032/3393 [16:50<11:44,  1.93batch/s, Batch Loss=0.0515, Avg Loss=0.0791, Time Left=12.23\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2032/3393 [16:51<11:44,  1.93batch/s, Batch Loss=0.2806, Avg Loss=0.0792, Time Left=12.22\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2033/3393 [16:51<11:37,  1.95batch/s, Batch Loss=0.2806, Avg Loss=0.0792, Time Left=12.22\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2033/3393 [16:51<11:37,  1.95batch/s, Batch Loss=0.0077, Avg Loss=0.0791, Time Left=12.21\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2034/3393 [16:51<11:37,  1.95batch/s, Batch Loss=0.0077, Avg Loss=0.0791, Time Left=12.21\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2034/3393 [16:52<11:37,  1.95batch/s, Batch Loss=0.1516, Avg Loss=0.0792, Time Left=12.20\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2035/3393 [16:52<11:32,  1.96batch/s, Batch Loss=0.1516, Avg Loss=0.0792, Time Left=12.20\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2035/3393 [16:52<11:32,  1.96batch/s, Batch Loss=0.0090, Avg Loss=0.0791, Time Left=12.19\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2036/3393 [16:52<11:34,  1.95batch/s, Batch Loss=0.0090, Avg Loss=0.0791, Time Left=12.19\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2036/3393 [16:53<11:34,  1.95batch/s, Batch Loss=0.0656, Avg Loss=0.0791, Time Left=12.18\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2037/3393 [16:53<11:30,  1.97batch/s, Batch Loss=0.0656, Avg Loss=0.0791, Time Left=12.18\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2037/3393 [16:53<11:30,  1.97batch/s, Batch Loss=0.0960, Avg Loss=0.0791, Time Left=12.18\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2038/3393 [16:53<11:38,  1.94batch/s, Batch Loss=0.0960, Avg Loss=0.0791, Time Left=12.18\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2038/3393 [16:54<11:38,  1.94batch/s, Batch Loss=0.0097, Avg Loss=0.0791, Time Left=12.17\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2039/3393 [16:54<11:33,  1.95batch/s, Batch Loss=0.0097, Avg Loss=0.0791, Time Left=12.17\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2039/3393 [16:54<11:33,  1.95batch/s, Batch Loss=0.0342, Avg Loss=0.0791, Time Left=12.16\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2040/3393 [16:54<11:47,  1.91batch/s, Batch Loss=0.0342, Avg Loss=0.0791, Time Left=12.16\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2040/3393 [16:55<11:47,  1.91batch/s, Batch Loss=0.1998, Avg Loss=0.0791, Time Left=12.15\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2041/3393 [16:55<11:31,  1.95batch/s, Batch Loss=0.1998, Avg Loss=0.0791, Time Left=12.15\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2041/3393 [16:55<11:31,  1.95batch/s, Batch Loss=0.0136, Avg Loss=0.0791, Time Left=12.14\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2042/3393 [16:55<11:45,  1.91batch/s, Batch Loss=0.0136, Avg Loss=0.0791, Time Left=12.14\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2042/3393 [16:56<11:45,  1.91batch/s, Batch Loss=0.1274, Avg Loss=0.0791, Time Left=12.13\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2043/3393 [16:56<11:30,  1.95batch/s, Batch Loss=0.1274, Avg Loss=0.0791, Time Left=12.13\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2043/3393 [16:56<11:30,  1.95batch/s, Batch Loss=0.5939, Avg Loss=0.0794, Time Left=12.12\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  60%|▌| 2044/3393 [16:56<11:32,  1.95batch/s, Batch Loss=0.5939, Avg Loss=0.0794, Time Left=12.12\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2044/3393 [16:57<11:32,  1.95batch/s, Batch Loss=0.0439, Avg Loss=0.0794, Time Left=12.12\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2045/3393 [16:57<11:25,  1.97batch/s, Batch Loss=0.0439, Avg Loss=0.0794, Time Left=12.12\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2045/3393 [16:57<11:25,  1.97batch/s, Batch Loss=0.1076, Avg Loss=0.0794, Time Left=12.11\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2046/3393 [16:57<11:22,  1.97batch/s, Batch Loss=0.1076, Avg Loss=0.0794, Time Left=12.11\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2046/3393 [16:58<11:22,  1.97batch/s, Batch Loss=0.0635, Avg Loss=0.0794, Time Left=12.10\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2047/3393 [16:58<11:13,  2.00batch/s, Batch Loss=0.0635, Avg Loss=0.0794, Time Left=12.10\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2047/3393 [16:58<11:13,  2.00batch/s, Batch Loss=0.1317, Avg Loss=0.0794, Time Left=12.09\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2048/3393 [16:58<11:06,  2.02batch/s, Batch Loss=0.1317, Avg Loss=0.0794, Time Left=12.09\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2048/3393 [16:59<11:06,  2.02batch/s, Batch Loss=0.0087, Avg Loss=0.0794, Time Left=12.08\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2049/3393 [16:59<11:01,  2.03batch/s, Batch Loss=0.0087, Avg Loss=0.0794, Time Left=12.08\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2049/3393 [16:59<11:01,  2.03batch/s, Batch Loss=0.2026, Avg Loss=0.0794, Time Left=12.07\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2050/3393 [16:59<10:57,  2.04batch/s, Batch Loss=0.2026, Avg Loss=0.0794, Time Left=12.07\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2050/3393 [17:00<10:57,  2.04batch/s, Batch Loss=0.1952, Avg Loss=0.0795, Time Left=12.06\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2051/3393 [17:00<11:01,  2.03batch/s, Batch Loss=0.1952, Avg Loss=0.0795, Time Left=12.06\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2051/3393 [17:00<11:01,  2.03batch/s, Batch Loss=0.0512, Avg Loss=0.0795, Time Left=12.05\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2052/3393 [17:00<10:58,  2.04batch/s, Batch Loss=0.0512, Avg Loss=0.0795, Time Left=12.05\u001b[A\n",
      "Epoch 3/3 - Training:  60%|▌| 2052/3393 [17:01<10:58,  2.04batch/s, Batch Loss=0.0784, Avg Loss=0.0795, Time Left=12.04\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2053/3393 [17:01<10:53,  2.05batch/s, Batch Loss=0.0784, Avg Loss=0.0795, Time Left=12.04\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2053/3393 [17:01<10:53,  2.05batch/s, Batch Loss=0.0286, Avg Loss=0.0794, Time Left=12.04\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2054/3393 [17:01<11:10,  2.00batch/s, Batch Loss=0.0286, Avg Loss=0.0794, Time Left=12.04\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2054/3393 [17:02<11:10,  2.00batch/s, Batch Loss=0.5450, Avg Loss=0.0797, Time Left=12.03\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2055/3393 [17:02<11:05,  2.01batch/s, Batch Loss=0.5450, Avg Loss=0.0797, Time Left=12.03\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2055/3393 [17:02<11:05,  2.01batch/s, Batch Loss=0.0717, Avg Loss=0.0797, Time Left=12.02\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2056/3393 [17:02<11:17,  1.97batch/s, Batch Loss=0.0717, Avg Loss=0.0797, Time Left=12.02\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2056/3393 [17:03<11:17,  1.97batch/s, Batch Loss=0.0084, Avg Loss=0.0796, Time Left=12.01\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2057/3393 [17:03<11:16,  1.98batch/s, Batch Loss=0.0084, Avg Loss=0.0796, Time Left=12.01\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2057/3393 [17:03<11:16,  1.98batch/s, Batch Loss=0.1528, Avg Loss=0.0797, Time Left=12.00\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2058/3393 [17:03<11:12,  1.98batch/s, Batch Loss=0.1528, Avg Loss=0.0797, Time Left=12.00\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2058/3393 [17:04<11:12,  1.98batch/s, Batch Loss=0.0413, Avg Loss=0.0797, Time Left=11.99\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2059/3393 [17:04<11:04,  2.01batch/s, Batch Loss=0.0413, Avg Loss=0.0797, Time Left=11.99\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2059/3393 [17:04<11:04,  2.01batch/s, Batch Loss=0.0231, Avg Loss=0.0796, Time Left=11.98\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2060/3393 [17:04<11:10,  1.99batch/s, Batch Loss=0.0231, Avg Loss=0.0796, Time Left=11.98\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2060/3393 [17:05<11:10,  1.99batch/s, Batch Loss=0.0559, Avg Loss=0.0796, Time Left=11.98\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2061/3393 [17:05<11:09,  1.99batch/s, Batch Loss=0.0559, Avg Loss=0.0796, Time Left=11.98\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2061/3393 [17:05<11:09,  1.99batch/s, Batch Loss=0.1061, Avg Loss=0.0796, Time Left=11.97\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2062/3393 [17:05<11:20,  1.95batch/s, Batch Loss=0.1061, Avg Loss=0.0796, Time Left=11.97\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2062/3393 [17:06<11:20,  1.95batch/s, Batch Loss=0.0275, Avg Loss=0.0796, Time Left=11.96\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2063/3393 [17:06<11:15,  1.97batch/s, Batch Loss=0.0275, Avg Loss=0.0796, Time Left=11.96\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2063/3393 [17:06<11:15,  1.97batch/s, Batch Loss=0.0138, Avg Loss=0.0796, Time Left=11.95\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2064/3393 [17:06<11:19,  1.96batch/s, Batch Loss=0.0138, Avg Loss=0.0796, Time Left=11.95\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2064/3393 [17:07<11:19,  1.96batch/s, Batch Loss=0.0458, Avg Loss=0.0796, Time Left=11.94\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2065/3393 [17:07<11:08,  1.99batch/s, Batch Loss=0.0458, Avg Loss=0.0796, Time Left=11.94\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2065/3393 [17:07<11:08,  1.99batch/s, Batch Loss=0.1493, Avg Loss=0.0796, Time Left=11.93\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2066/3393 [17:07<11:25,  1.94batch/s, Batch Loss=0.1493, Avg Loss=0.0796, Time Left=11.93\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2066/3393 [17:08<11:25,  1.94batch/s, Batch Loss=0.0626, Avg Loss=0.0796, Time Left=11.92\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2067/3393 [17:08<11:19,  1.95batch/s, Batch Loss=0.0626, Avg Loss=0.0796, Time Left=11.92\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2067/3393 [17:08<11:19,  1.95batch/s, Batch Loss=0.0327, Avg Loss=0.0796, Time Left=11.92\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2068/3393 [17:08<11:13,  1.97batch/s, Batch Loss=0.0327, Avg Loss=0.0796, Time Left=11.92\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2068/3393 [17:09<11:13,  1.97batch/s, Batch Loss=0.0101, Avg Loss=0.0795, Time Left=11.91\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2069/3393 [17:09<11:10,  1.98batch/s, Batch Loss=0.0101, Avg Loss=0.0795, Time Left=11.91\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2069/3393 [17:09<11:10,  1.98batch/s, Batch Loss=0.2128, Avg Loss=0.0796, Time Left=11.90\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2070/3393 [17:09<11:19,  1.95batch/s, Batch Loss=0.2128, Avg Loss=0.0796, Time Left=11.90\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2070/3393 [17:10<11:19,  1.95batch/s, Batch Loss=0.0253, Avg Loss=0.0796, Time Left=11.89\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2071/3393 [17:10<11:14,  1.96batch/s, Batch Loss=0.0253, Avg Loss=0.0796, Time Left=11.89\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2071/3393 [17:11<11:14,  1.96batch/s, Batch Loss=0.0158, Avg Loss=0.0795, Time Left=11.88\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2072/3393 [17:11<11:22,  1.94batch/s, Batch Loss=0.0158, Avg Loss=0.0795, Time Left=11.88\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2072/3393 [17:11<11:22,  1.94batch/s, Batch Loss=0.0559, Avg Loss=0.0795, Time Left=11.87\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2073/3393 [17:11<11:16,  1.95batch/s, Batch Loss=0.0559, Avg Loss=0.0795, Time Left=11.87\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2073/3393 [17:12<11:16,  1.95batch/s, Batch Loss=0.0116, Avg Loss=0.0795, Time Left=11.86\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2074/3393 [17:12<11:23,  1.93batch/s, Batch Loss=0.0116, Avg Loss=0.0795, Time Left=11.86\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2074/3393 [17:12<11:23,  1.93batch/s, Batch Loss=0.0235, Avg Loss=0.0795, Time Left=11.86\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2075/3393 [17:12<11:10,  1.97batch/s, Batch Loss=0.0235, Avg Loss=0.0795, Time Left=11.86\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2075/3393 [17:13<11:10,  1.97batch/s, Batch Loss=0.0905, Avg Loss=0.0795, Time Left=11.85\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2076/3393 [17:13<11:19,  1.94batch/s, Batch Loss=0.0905, Avg Loss=0.0795, Time Left=11.85\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2076/3393 [17:13<11:19,  1.94batch/s, Batch Loss=0.0944, Avg Loss=0.0795, Time Left=11.84\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  61%|▌| 2077/3393 [17:13<11:06,  1.97batch/s, Batch Loss=0.0944, Avg Loss=0.0795, Time Left=11.84\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2077/3393 [17:14<11:06,  1.97batch/s, Batch Loss=0.1941, Avg Loss=0.0795, Time Left=11.83\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2078/3393 [17:14<11:22,  1.93batch/s, Batch Loss=0.1941, Avg Loss=0.0795, Time Left=11.83\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2078/3393 [17:14<11:22,  1.93batch/s, Batch Loss=0.0861, Avg Loss=0.0795, Time Left=11.82\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2079/3393 [17:14<11:09,  1.96batch/s, Batch Loss=0.0861, Avg Loss=0.0795, Time Left=11.82\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2079/3393 [17:15<11:09,  1.96batch/s, Batch Loss=0.0307, Avg Loss=0.0795, Time Left=11.81\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2080/3393 [17:15<11:11,  1.96batch/s, Batch Loss=0.0307, Avg Loss=0.0795, Time Left=11.81\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2080/3393 [17:15<11:11,  1.96batch/s, Batch Loss=0.0081, Avg Loss=0.0795, Time Left=11.80\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2081/3393 [17:15<11:07,  1.97batch/s, Batch Loss=0.0081, Avg Loss=0.0795, Time Left=11.80\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2081/3393 [17:16<11:07,  1.97batch/s, Batch Loss=0.0779, Avg Loss=0.0795, Time Left=11.80\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2082/3393 [17:16<11:15,  1.94batch/s, Batch Loss=0.0779, Avg Loss=0.0795, Time Left=11.80\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2082/3393 [17:16<11:15,  1.94batch/s, Batch Loss=0.0952, Avg Loss=0.0795, Time Left=11.79\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2083/3393 [17:16<11:16,  1.94batch/s, Batch Loss=0.0952, Avg Loss=0.0795, Time Left=11.79\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2083/3393 [17:17<11:16,  1.94batch/s, Batch Loss=0.0512, Avg Loss=0.0795, Time Left=11.78\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2084/3393 [17:17<11:15,  1.94batch/s, Batch Loss=0.0512, Avg Loss=0.0795, Time Left=11.78\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2084/3393 [17:17<11:15,  1.94batch/s, Batch Loss=0.0059, Avg Loss=0.0794, Time Left=11.77\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2085/3393 [17:17<11:10,  1.95batch/s, Batch Loss=0.0059, Avg Loss=0.0794, Time Left=11.77\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2085/3393 [17:18<11:10,  1.95batch/s, Batch Loss=0.0197, Avg Loss=0.0794, Time Left=11.76\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2086/3393 [17:18<11:22,  1.92batch/s, Batch Loss=0.0197, Avg Loss=0.0794, Time Left=11.76\u001b[A\n",
      "Epoch 3/3 - Training:  61%|▌| 2086/3393 [17:18<11:22,  1.92batch/s, Batch Loss=0.0443, Avg Loss=0.0794, Time Left=11.75\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2087/3393 [17:18<11:02,  1.97batch/s, Batch Loss=0.0443, Avg Loss=0.0794, Time Left=11.75\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2087/3393 [17:19<11:02,  1.97batch/s, Batch Loss=0.2915, Avg Loss=0.0795, Time Left=11.74\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2088/3393 [17:19<11:00,  1.98batch/s, Batch Loss=0.2915, Avg Loss=0.0795, Time Left=11.74\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2088/3393 [17:19<11:00,  1.98batch/s, Batch Loss=0.0024, Avg Loss=0.0795, Time Left=11.73\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2089/3393 [17:19<10:43,  2.03batch/s, Batch Loss=0.0024, Avg Loss=0.0795, Time Left=11.73\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2089/3393 [17:20<10:43,  2.03batch/s, Batch Loss=0.0263, Avg Loss=0.0794, Time Left=11.73\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2090/3393 [17:20<10:45,  2.02batch/s, Batch Loss=0.0263, Avg Loss=0.0794, Time Left=11.73\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2090/3393 [17:20<10:45,  2.02batch/s, Batch Loss=0.1884, Avg Loss=0.0795, Time Left=11.72\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2091/3393 [17:20<10:46,  2.01batch/s, Batch Loss=0.1884, Avg Loss=0.0795, Time Left=11.72\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2091/3393 [17:21<10:46,  2.01batch/s, Batch Loss=0.2286, Avg Loss=0.0796, Time Left=11.71\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2092/3393 [17:21<10:41,  2.03batch/s, Batch Loss=0.2286, Avg Loss=0.0796, Time Left=11.71\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2092/3393 [17:21<10:41,  2.03batch/s, Batch Loss=0.2202, Avg Loss=0.0796, Time Left=11.70\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2093/3393 [17:21<10:55,  1.98batch/s, Batch Loss=0.2202, Avg Loss=0.0796, Time Left=11.70\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2093/3393 [17:22<10:55,  1.98batch/s, Batch Loss=0.0045, Avg Loss=0.0796, Time Left=11.69\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2094/3393 [17:22<10:48,  2.00batch/s, Batch Loss=0.0045, Avg Loss=0.0796, Time Left=11.69\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2094/3393 [17:22<10:48,  2.00batch/s, Batch Loss=0.0218, Avg Loss=0.0796, Time Left=11.68\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2095/3393 [17:22<11:00,  1.97batch/s, Batch Loss=0.0218, Avg Loss=0.0796, Time Left=11.68\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2095/3393 [17:23<11:00,  1.97batch/s, Batch Loss=0.2962, Avg Loss=0.0797, Time Left=11.67\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2096/3393 [17:23<10:51,  1.99batch/s, Batch Loss=0.2962, Avg Loss=0.0797, Time Left=11.67\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2096/3393 [17:23<10:51,  1.99batch/s, Batch Loss=0.0276, Avg Loss=0.0796, Time Left=11.67\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2097/3393 [17:23<11:07,  1.94batch/s, Batch Loss=0.0276, Avg Loss=0.0796, Time Left=11.67\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2097/3393 [17:24<11:07,  1.94batch/s, Batch Loss=0.2492, Avg Loss=0.0797, Time Left=11.66\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2098/3393 [17:24<11:02,  1.95batch/s, Batch Loss=0.2492, Avg Loss=0.0797, Time Left=11.66\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2098/3393 [17:24<11:02,  1.95batch/s, Batch Loss=0.0146, Avg Loss=0.0797, Time Left=11.65\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2099/3393 [17:24<11:04,  1.95batch/s, Batch Loss=0.0146, Avg Loss=0.0797, Time Left=11.65\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2099/3393 [17:25<11:04,  1.95batch/s, Batch Loss=0.1017, Avg Loss=0.0797, Time Left=11.64\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2100/3393 [17:25<10:59,  1.96batch/s, Batch Loss=0.1017, Avg Loss=0.0797, Time Left=11.64\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2100/3393 [17:25<10:59,  1.96batch/s, Batch Loss=0.0393, Avg Loss=0.0797, Time Left=11.63\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2101/3393 [17:25<10:55,  1.97batch/s, Batch Loss=0.0393, Avg Loss=0.0797, Time Left=11.63\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2101/3393 [17:26<10:55,  1.97batch/s, Batch Loss=0.1309, Avg Loss=0.0797, Time Left=11.62\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2102/3393 [17:26<10:45,  2.00batch/s, Batch Loss=0.1309, Avg Loss=0.0797, Time Left=11.62\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2102/3393 [17:26<10:45,  2.00batch/s, Batch Loss=0.1225, Avg Loss=0.0797, Time Left=11.61\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2103/3393 [17:26<10:38,  2.02batch/s, Batch Loss=0.1225, Avg Loss=0.0797, Time Left=11.61\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2103/3393 [17:27<10:38,  2.02batch/s, Batch Loss=0.0296, Avg Loss=0.0797, Time Left=11.60\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2104/3393 [17:27<10:34,  2.03batch/s, Batch Loss=0.0296, Avg Loss=0.0797, Time Left=11.60\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2104/3393 [17:27<10:34,  2.03batch/s, Batch Loss=0.0182, Avg Loss=0.0797, Time Left=11.60\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2105/3393 [17:27<10:36,  2.02batch/s, Batch Loss=0.0182, Avg Loss=0.0797, Time Left=11.60\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2105/3393 [17:28<10:36,  2.02batch/s, Batch Loss=0.0042, Avg Loss=0.0796, Time Left=11.59\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2106/3393 [17:28<10:43,  2.00batch/s, Batch Loss=0.0042, Avg Loss=0.0796, Time Left=11.59\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2106/3393 [17:28<10:43,  2.00batch/s, Batch Loss=0.0584, Avg Loss=0.0796, Time Left=11.58\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2107/3393 [17:28<10:42,  2.00batch/s, Batch Loss=0.0584, Avg Loss=0.0796, Time Left=11.58\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2107/3393 [17:29<10:42,  2.00batch/s, Batch Loss=0.0601, Avg Loss=0.0796, Time Left=11.57\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2108/3393 [17:29<10:49,  1.98batch/s, Batch Loss=0.0601, Avg Loss=0.0796, Time Left=11.57\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2108/3393 [17:29<10:49,  1.98batch/s, Batch Loss=0.0196, Avg Loss=0.0796, Time Left=11.56\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2109/3393 [17:29<10:42,  2.00batch/s, Batch Loss=0.0196, Avg Loss=0.0796, Time Left=11.56\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2109/3393 [17:30<10:42,  2.00batch/s, Batch Loss=0.0582, Avg Loss=0.0796, Time Left=11.55\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  62%|▌| 2110/3393 [17:30<10:35,  2.02batch/s, Batch Loss=0.0582, Avg Loss=0.0796, Time Left=11.55\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2110/3393 [17:30<10:35,  2.02batch/s, Batch Loss=0.0056, Avg Loss=0.0795, Time Left=11.54\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2111/3393 [17:30<10:24,  2.05batch/s, Batch Loss=0.0056, Avg Loss=0.0795, Time Left=11.54\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2111/3393 [17:31<10:24,  2.05batch/s, Batch Loss=0.1987, Avg Loss=0.0796, Time Left=11.53\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2112/3393 [17:31<10:27,  2.04batch/s, Batch Loss=0.1987, Avg Loss=0.0796, Time Left=11.53\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2112/3393 [17:31<10:27,  2.04batch/s, Batch Loss=0.1161, Avg Loss=0.0796, Time Left=11.53\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2113/3393 [17:31<10:43,  1.99batch/s, Batch Loss=0.1161, Avg Loss=0.0796, Time Left=11.53\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2113/3393 [17:32<10:43,  1.99batch/s, Batch Loss=0.0180, Avg Loss=0.0796, Time Left=11.52\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2114/3393 [17:32<10:48,  1.97batch/s, Batch Loss=0.0180, Avg Loss=0.0796, Time Left=11.52\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2114/3393 [17:32<10:48,  1.97batch/s, Batch Loss=0.0326, Avg Loss=0.0796, Time Left=11.51\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2115/3393 [17:32<10:45,  1.98batch/s, Batch Loss=0.0326, Avg Loss=0.0796, Time Left=11.51\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2115/3393 [17:33<10:45,  1.98batch/s, Batch Loss=0.0223, Avg Loss=0.0795, Time Left=11.50\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2116/3393 [17:33<10:44,  1.98batch/s, Batch Loss=0.0223, Avg Loss=0.0795, Time Left=11.50\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2116/3393 [17:33<10:44,  1.98batch/s, Batch Loss=0.0119, Avg Loss=0.0795, Time Left=11.49\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2117/3393 [17:33<10:53,  1.95batch/s, Batch Loss=0.0119, Avg Loss=0.0795, Time Left=11.49\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2117/3393 [17:34<10:53,  1.95batch/s, Batch Loss=0.0361, Avg Loss=0.0795, Time Left=11.48\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2118/3393 [17:34<10:43,  1.98batch/s, Batch Loss=0.0361, Avg Loss=0.0795, Time Left=11.48\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2118/3393 [17:34<10:43,  1.98batch/s, Batch Loss=0.0024, Avg Loss=0.0794, Time Left=11.47\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2119/3393 [17:34<10:46,  1.97batch/s, Batch Loss=0.0024, Avg Loss=0.0794, Time Left=11.47\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2119/3393 [17:35<10:46,  1.97batch/s, Batch Loss=0.0038, Avg Loss=0.0794, Time Left=11.46\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2120/3393 [17:35<10:31,  2.01batch/s, Batch Loss=0.0038, Avg Loss=0.0794, Time Left=11.46\u001b[A\n",
      "Epoch 3/3 - Training:  62%|▌| 2120/3393 [17:35<10:31,  2.01batch/s, Batch Loss=0.0023, Avg Loss=0.0794, Time Left=11.46\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2121/3393 [17:35<10:25,  2.03batch/s, Batch Loss=0.0023, Avg Loss=0.0794, Time Left=11.46\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2121/3393 [17:36<10:25,  2.03batch/s, Batch Loss=0.0230, Avg Loss=0.0793, Time Left=11.45\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2122/3393 [17:36<10:44,  1.97batch/s, Batch Loss=0.0230, Avg Loss=0.0793, Time Left=11.45\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2122/3393 [17:36<10:44,  1.97batch/s, Batch Loss=0.0028, Avg Loss=0.0793, Time Left=11.44\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2123/3393 [17:36<10:38,  1.99batch/s, Batch Loss=0.0028, Avg Loss=0.0793, Time Left=11.44\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2123/3393 [17:37<10:38,  1.99batch/s, Batch Loss=0.0319, Avg Loss=0.0793, Time Left=11.43\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2124/3393 [17:37<10:42,  1.97batch/s, Batch Loss=0.0319, Avg Loss=0.0793, Time Left=11.43\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2124/3393 [17:37<10:42,  1.97batch/s, Batch Loss=0.1802, Avg Loss=0.0793, Time Left=11.42\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2125/3393 [17:37<10:41,  1.98batch/s, Batch Loss=0.1802, Avg Loss=0.0793, Time Left=11.42\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2125/3393 [17:38<10:41,  1.98batch/s, Batch Loss=0.0014, Avg Loss=0.0793, Time Left=11.41\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2126/3393 [17:38<10:50,  1.95batch/s, Batch Loss=0.0014, Avg Loss=0.0793, Time Left=11.41\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2126/3393 [17:38<10:50,  1.95batch/s, Batch Loss=0.0615, Avg Loss=0.0793, Time Left=11.40\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2127/3393 [17:38<10:38,  1.98batch/s, Batch Loss=0.0615, Avg Loss=0.0793, Time Left=11.40\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2127/3393 [17:39<10:38,  1.98batch/s, Batch Loss=0.0089, Avg Loss=0.0792, Time Left=11.40\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2128/3393 [17:39<10:36,  1.99batch/s, Batch Loss=0.0089, Avg Loss=0.0792, Time Left=11.40\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2128/3393 [17:39<10:36,  1.99batch/s, Batch Loss=0.0161, Avg Loss=0.0792, Time Left=11.39\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2129/3393 [17:39<10:35,  1.99batch/s, Batch Loss=0.0161, Avg Loss=0.0792, Time Left=11.39\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2129/3393 [17:40<10:35,  1.99batch/s, Batch Loss=0.0185, Avg Loss=0.0792, Time Left=11.38\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2130/3393 [17:40<10:39,  1.97batch/s, Batch Loss=0.0185, Avg Loss=0.0792, Time Left=11.38\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2130/3393 [17:40<10:39,  1.97batch/s, Batch Loss=0.0679, Avg Loss=0.0792, Time Left=11.37\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2131/3393 [17:40<10:37,  1.98batch/s, Batch Loss=0.0679, Avg Loss=0.0792, Time Left=11.37\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2131/3393 [17:41<10:37,  1.98batch/s, Batch Loss=0.0092, Avg Loss=0.0791, Time Left=11.36\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2132/3393 [17:41<10:47,  1.95batch/s, Batch Loss=0.0092, Avg Loss=0.0791, Time Left=11.36\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2132/3393 [17:41<10:47,  1.95batch/s, Batch Loss=0.0263, Avg Loss=0.0791, Time Left=11.35\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2133/3393 [17:41<10:41,  1.96batch/s, Batch Loss=0.0263, Avg Loss=0.0791, Time Left=11.35\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2133/3393 [17:42<10:41,  1.96batch/s, Batch Loss=0.0035, Avg Loss=0.0791, Time Left=11.34\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2134/3393 [17:42<10:50,  1.94batch/s, Batch Loss=0.0035, Avg Loss=0.0791, Time Left=11.34\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2134/3393 [17:42<10:50,  1.94batch/s, Batch Loss=0.4367, Avg Loss=0.0793, Time Left=11.34\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2135/3393 [17:42<10:32,  1.99batch/s, Batch Loss=0.4367, Avg Loss=0.0793, Time Left=11.34\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2135/3393 [17:43<10:32,  1.99batch/s, Batch Loss=0.0273, Avg Loss=0.0792, Time Left=11.33\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2136/3393 [17:43<10:24,  2.01batch/s, Batch Loss=0.0273, Avg Loss=0.0792, Time Left=11.33\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2136/3393 [17:43<10:24,  2.01batch/s, Batch Loss=0.0765, Avg Loss=0.0792, Time Left=11.32\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2137/3393 [17:43<10:18,  2.03batch/s, Batch Loss=0.0765, Avg Loss=0.0792, Time Left=11.32\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2137/3393 [17:44<10:18,  2.03batch/s, Batch Loss=0.0012, Avg Loss=0.0792, Time Left=11.31\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2138/3393 [17:44<10:15,  2.04batch/s, Batch Loss=0.0012, Avg Loss=0.0792, Time Left=11.31\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2138/3393 [17:44<10:15,  2.04batch/s, Batch Loss=0.0016, Avg Loss=0.0792, Time Left=11.30\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2139/3393 [17:44<10:30,  1.99batch/s, Batch Loss=0.0016, Avg Loss=0.0792, Time Left=11.30\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2139/3393 [17:45<10:30,  1.99batch/s, Batch Loss=0.0031, Avg Loss=0.0791, Time Left=11.29\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2140/3393 [17:45<10:23,  2.01batch/s, Batch Loss=0.0031, Avg Loss=0.0791, Time Left=11.29\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2140/3393 [17:45<10:23,  2.01batch/s, Batch Loss=0.0419, Avg Loss=0.0791, Time Left=11.28\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2141/3393 [17:45<10:35,  1.97batch/s, Batch Loss=0.0419, Avg Loss=0.0791, Time Left=11.28\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2141/3393 [17:46<10:35,  1.97batch/s, Batch Loss=0.0036, Avg Loss=0.0791, Time Left=11.27\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2142/3393 [17:46<10:33,  1.97batch/s, Batch Loss=0.0036, Avg Loss=0.0791, Time Left=11.27\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2142/3393 [17:46<10:33,  1.97batch/s, Batch Loss=0.1722, Avg Loss=0.0791, Time Left=11.27\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  63%|▋| 2143/3393 [17:46<10:41,  1.95batch/s, Batch Loss=0.1722, Avg Loss=0.0791, Time Left=11.27\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2143/3393 [17:47<10:41,  1.95batch/s, Batch Loss=0.0019, Avg Loss=0.0791, Time Left=11.26\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2144/3393 [17:47<10:40,  1.95batch/s, Batch Loss=0.0019, Avg Loss=0.0791, Time Left=11.26\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2144/3393 [17:47<10:40,  1.95batch/s, Batch Loss=0.0321, Avg Loss=0.0791, Time Left=11.25\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2145/3393 [17:47<10:37,  1.96batch/s, Batch Loss=0.0321, Avg Loss=0.0791, Time Left=11.25\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2145/3393 [17:48<10:37,  1.96batch/s, Batch Loss=0.0020, Avg Loss=0.0790, Time Left=11.24\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2146/3393 [17:48<10:27,  1.99batch/s, Batch Loss=0.0020, Avg Loss=0.0790, Time Left=11.24\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2146/3393 [17:48<10:27,  1.99batch/s, Batch Loss=0.0918, Avg Loss=0.0790, Time Left=11.23\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2147/3393 [17:48<10:43,  1.94batch/s, Batch Loss=0.0918, Avg Loss=0.0790, Time Left=11.23\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2147/3393 [17:49<10:43,  1.94batch/s, Batch Loss=0.1014, Avg Loss=0.0790, Time Left=11.22\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2148/3393 [17:49<10:31,  1.97batch/s, Batch Loss=0.1014, Avg Loss=0.0790, Time Left=11.22\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2148/3393 [17:49<10:31,  1.97batch/s, Batch Loss=0.0106, Avg Loss=0.0790, Time Left=11.21\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2149/3393 [17:49<10:40,  1.94batch/s, Batch Loss=0.0106, Avg Loss=0.0790, Time Left=11.21\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2149/3393 [17:50<10:40,  1.94batch/s, Batch Loss=0.0450, Avg Loss=0.0790, Time Left=11.21\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2150/3393 [17:50<10:35,  1.96batch/s, Batch Loss=0.0450, Avg Loss=0.0790, Time Left=11.21\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2150/3393 [17:50<10:35,  1.96batch/s, Batch Loss=0.0131, Avg Loss=0.0790, Time Left=11.20\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2151/3393 [17:50<10:36,  1.95batch/s, Batch Loss=0.0131, Avg Loss=0.0790, Time Left=11.20\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2151/3393 [17:51<10:36,  1.95batch/s, Batch Loss=0.3247, Avg Loss=0.0791, Time Left=11.19\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2152/3393 [17:51<10:32,  1.96batch/s, Batch Loss=0.3247, Avg Loss=0.0791, Time Left=11.19\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2152/3393 [17:51<10:32,  1.96batch/s, Batch Loss=0.0414, Avg Loss=0.0791, Time Left=11.18\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2153/3393 [17:51<10:39,  1.94batch/s, Batch Loss=0.0414, Avg Loss=0.0791, Time Left=11.18\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2153/3393 [17:52<10:39,  1.94batch/s, Batch Loss=0.0128, Avg Loss=0.0790, Time Left=11.17\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2154/3393 [17:52<10:28,  1.97batch/s, Batch Loss=0.0128, Avg Loss=0.0790, Time Left=11.17\u001b[A\n",
      "Epoch 3/3 - Training:  63%|▋| 2154/3393 [17:53<10:28,  1.97batch/s, Batch Loss=0.0195, Avg Loss=0.0790, Time Left=11.16\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2155/3393 [17:53<10:37,  1.94batch/s, Batch Loss=0.0195, Avg Loss=0.0790, Time Left=11.16\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2155/3393 [17:53<10:37,  1.94batch/s, Batch Loss=0.2619, Avg Loss=0.0791, Time Left=11.15\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2156/3393 [17:53<10:38,  1.94batch/s, Batch Loss=0.2619, Avg Loss=0.0791, Time Left=11.15\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2156/3393 [17:54<10:38,  1.94batch/s, Batch Loss=0.0447, Avg Loss=0.0791, Time Left=11.15\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2157/3393 [17:54<10:48,  1.90batch/s, Batch Loss=0.0447, Avg Loss=0.0791, Time Left=11.15\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2157/3393 [17:54<10:48,  1.90batch/s, Batch Loss=0.0658, Avg Loss=0.0791, Time Left=11.14\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2158/3393 [17:54<10:35,  1.94batch/s, Batch Loss=0.0658, Avg Loss=0.0791, Time Left=11.14\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2158/3393 [17:55<10:35,  1.94batch/s, Batch Loss=0.0744, Avg Loss=0.0791, Time Left=11.13\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2159/3393 [17:55<10:46,  1.91batch/s, Batch Loss=0.0744, Avg Loss=0.0791, Time Left=11.13\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2159/3393 [17:55<10:46,  1.91batch/s, Batch Loss=0.0139, Avg Loss=0.0790, Time Left=11.12\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2160/3393 [17:55<10:31,  1.95batch/s, Batch Loss=0.0139, Avg Loss=0.0790, Time Left=11.12\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2160/3393 [17:56<10:31,  1.95batch/s, Batch Loss=0.1558, Avg Loss=0.0791, Time Left=11.11\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2161/3393 [17:56<10:27,  1.96batch/s, Batch Loss=0.1558, Avg Loss=0.0791, Time Left=11.11\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2161/3393 [17:56<10:27,  1.96batch/s, Batch Loss=0.0166, Avg Loss=0.0790, Time Left=11.10\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2162/3393 [17:56<10:25,  1.97batch/s, Batch Loss=0.0166, Avg Loss=0.0790, Time Left=11.10\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2162/3393 [17:57<10:25,  1.97batch/s, Batch Loss=0.2908, Avg Loss=0.0791, Time Left=11.10\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2163/3393 [17:57<10:37,  1.93batch/s, Batch Loss=0.2908, Avg Loss=0.0791, Time Left=11.10\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2163/3393 [17:57<10:37,  1.93batch/s, Batch Loss=0.0787, Avg Loss=0.0791, Time Left=11.09\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2164/3393 [17:57<10:25,  1.97batch/s, Batch Loss=0.0787, Avg Loss=0.0791, Time Left=11.09\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2164/3393 [17:58<10:25,  1.97batch/s, Batch Loss=0.0226, Avg Loss=0.0791, Time Left=11.08\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2165/3393 [17:58<10:26,  1.96batch/s, Batch Loss=0.0226, Avg Loss=0.0791, Time Left=11.08\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2165/3393 [17:58<10:26,  1.96batch/s, Batch Loss=0.2204, Avg Loss=0.0792, Time Left=11.07\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2166/3393 [17:58<10:17,  1.99batch/s, Batch Loss=0.2204, Avg Loss=0.0792, Time Left=11.07\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2166/3393 [17:59<10:17,  1.99batch/s, Batch Loss=0.0294, Avg Loss=0.0791, Time Left=11.06\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2167/3393 [17:59<10:21,  1.97batch/s, Batch Loss=0.0294, Avg Loss=0.0791, Time Left=11.06\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2167/3393 [17:59<10:21,  1.97batch/s, Batch Loss=0.0343, Avg Loss=0.0791, Time Left=11.05\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2168/3393 [17:59<10:01,  2.04batch/s, Batch Loss=0.0343, Avg Loss=0.0791, Time Left=11.05\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2168/3393 [18:00<10:01,  2.04batch/s, Batch Loss=0.2300, Avg Loss=0.0792, Time Left=11.04\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2169/3393 [18:00<10:09,  2.01batch/s, Batch Loss=0.2300, Avg Loss=0.0792, Time Left=11.04\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2169/3393 [18:00<10:09,  2.01batch/s, Batch Loss=0.0554, Avg Loss=0.0792, Time Left=11.03\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2170/3393 [18:00<09:58,  2.04batch/s, Batch Loss=0.0554, Avg Loss=0.0792, Time Left=11.03\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2170/3393 [18:01<09:58,  2.04batch/s, Batch Loss=0.0222, Avg Loss=0.0792, Time Left=11.02\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2171/3393 [18:01<10:01,  2.03batch/s, Batch Loss=0.0222, Avg Loss=0.0792, Time Left=11.02\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2171/3393 [18:01<10:01,  2.03batch/s, Batch Loss=0.0914, Avg Loss=0.0792, Time Left=11.02\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2172/3393 [18:01<10:09,  2.00batch/s, Batch Loss=0.0914, Avg Loss=0.0792, Time Left=11.02\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2172/3393 [18:02<10:09,  2.00batch/s, Batch Loss=0.0545, Avg Loss=0.0792, Time Left=11.01\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2173/3393 [18:02<10:10,  2.00batch/s, Batch Loss=0.0545, Avg Loss=0.0792, Time Left=11.01\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2173/3393 [18:02<10:10,  2.00batch/s, Batch Loss=0.1969, Avg Loss=0.0792, Time Left=11.00\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2174/3393 [18:02<10:21,  1.96batch/s, Batch Loss=0.1969, Avg Loss=0.0792, Time Left=11.00\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2174/3393 [18:03<10:21,  1.96batch/s, Batch Loss=0.1438, Avg Loss=0.0792, Time Left=10.99\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2175/3393 [18:03<10:17,  1.97batch/s, Batch Loss=0.1438, Avg Loss=0.0792, Time Left=10.99\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2175/3393 [18:03<10:17,  1.97batch/s, Batch Loss=0.1510, Avg Loss=0.0793, Time Left=10.98\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  64%|▋| 2176/3393 [18:03<10:25,  1.95batch/s, Batch Loss=0.1510, Avg Loss=0.0793, Time Left=10.98\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2176/3393 [18:04<10:25,  1.95batch/s, Batch Loss=0.0218, Avg Loss=0.0792, Time Left=10.97\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2177/3393 [18:04<10:27,  1.94batch/s, Batch Loss=0.0218, Avg Loss=0.0792, Time Left=10.97\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2177/3393 [18:04<10:27,  1.94batch/s, Batch Loss=0.0469, Avg Loss=0.0792, Time Left=10.97\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2178/3393 [18:04<10:33,  1.92batch/s, Batch Loss=0.0469, Avg Loss=0.0792, Time Left=10.97\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2178/3393 [18:05<10:33,  1.92batch/s, Batch Loss=0.2303, Avg Loss=0.0793, Time Left=10.96\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2179/3393 [18:05<10:25,  1.94batch/s, Batch Loss=0.2303, Avg Loss=0.0793, Time Left=10.96\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2179/3393 [18:05<10:25,  1.94batch/s, Batch Loss=0.0070, Avg Loss=0.0793, Time Left=10.95\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2180/3393 [18:05<10:41,  1.89batch/s, Batch Loss=0.0070, Avg Loss=0.0793, Time Left=10.95\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2180/3393 [18:06<10:41,  1.89batch/s, Batch Loss=0.0222, Avg Loss=0.0792, Time Left=10.94\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2181/3393 [18:06<10:25,  1.94batch/s, Batch Loss=0.0222, Avg Loss=0.0792, Time Left=10.94\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2181/3393 [18:06<10:25,  1.94batch/s, Batch Loss=0.4173, Avg Loss=0.0794, Time Left=10.93\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2182/3393 [18:06<10:14,  1.97batch/s, Batch Loss=0.4173, Avg Loss=0.0794, Time Left=10.93\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2182/3393 [18:07<10:14,  1.97batch/s, Batch Loss=0.0779, Avg Loss=0.0794, Time Left=10.92\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2183/3393 [18:07<10:05,  2.00batch/s, Batch Loss=0.0779, Avg Loss=0.0794, Time Left=10.92\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2183/3393 [18:07<10:05,  2.00batch/s, Batch Loss=0.0544, Avg Loss=0.0794, Time Left=10.91\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2184/3393 [18:07<10:10,  1.98batch/s, Batch Loss=0.0544, Avg Loss=0.0794, Time Left=10.91\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2184/3393 [18:08<10:10,  1.98batch/s, Batch Loss=0.0190, Avg Loss=0.0794, Time Left=10.90\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2185/3393 [18:08<09:57,  2.02batch/s, Batch Loss=0.0190, Avg Loss=0.0794, Time Left=10.90\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2185/3393 [18:08<09:57,  2.02batch/s, Batch Loss=0.0973, Avg Loss=0.0794, Time Left=10.90\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2186/3393 [18:08<09:53,  2.03batch/s, Batch Loss=0.0973, Avg Loss=0.0794, Time Left=10.90\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2186/3393 [18:09<09:53,  2.03batch/s, Batch Loss=0.0874, Avg Loss=0.0794, Time Left=10.89\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2187/3393 [18:09<09:49,  2.04batch/s, Batch Loss=0.0874, Avg Loss=0.0794, Time Left=10.89\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2187/3393 [18:09<09:49,  2.04batch/s, Batch Loss=0.2740, Avg Loss=0.0795, Time Left=10.88\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2188/3393 [18:09<09:52,  2.03batch/s, Batch Loss=0.2740, Avg Loss=0.0795, Time Left=10.88\u001b[A\n",
      "Epoch 3/3 - Training:  64%|▋| 2188/3393 [18:10<09:52,  2.03batch/s, Batch Loss=0.0070, Avg Loss=0.0794, Time Left=10.87\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2189/3393 [18:10<09:55,  2.02batch/s, Batch Loss=0.0070, Avg Loss=0.0794, Time Left=10.87\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2189/3393 [18:10<09:55,  2.02batch/s, Batch Loss=0.0623, Avg Loss=0.0794, Time Left=10.86\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2190/3393 [18:10<09:45,  2.05batch/s, Batch Loss=0.0623, Avg Loss=0.0794, Time Left=10.86\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2190/3393 [18:11<09:45,  2.05batch/s, Batch Loss=0.0316, Avg Loss=0.0794, Time Left=10.85\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2191/3393 [18:11<09:49,  2.04batch/s, Batch Loss=0.0316, Avg Loss=0.0794, Time Left=10.85\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2191/3393 [18:11<09:49,  2.04batch/s, Batch Loss=0.1105, Avg Loss=0.0794, Time Left=10.84\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2192/3393 [18:11<09:46,  2.05batch/s, Batch Loss=0.1105, Avg Loss=0.0794, Time Left=10.84\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2192/3393 [18:12<09:46,  2.05batch/s, Batch Loss=0.0291, Avg Loss=0.0794, Time Left=10.83\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2193/3393 [18:12<09:55,  2.01batch/s, Batch Loss=0.0291, Avg Loss=0.0794, Time Left=10.83\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2193/3393 [18:12<09:55,  2.01batch/s, Batch Loss=0.0104, Avg Loss=0.0794, Time Left=10.83\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2194/3393 [18:12<10:14,  1.95batch/s, Batch Loss=0.0104, Avg Loss=0.0794, Time Left=10.83\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2194/3393 [18:13<10:14,  1.95batch/s, Batch Loss=0.0076, Avg Loss=0.0793, Time Left=10.82\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2195/3393 [18:13<10:09,  1.96batch/s, Batch Loss=0.0076, Avg Loss=0.0793, Time Left=10.82\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2195/3393 [18:13<10:09,  1.96batch/s, Batch Loss=0.3284, Avg Loss=0.0794, Time Left=10.81\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2196/3393 [18:13<10:06,  1.97batch/s, Batch Loss=0.3284, Avg Loss=0.0794, Time Left=10.81\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2196/3393 [18:14<10:06,  1.97batch/s, Batch Loss=0.0761, Avg Loss=0.0794, Time Left=10.80\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2197/3393 [18:14<10:04,  1.98batch/s, Batch Loss=0.0761, Avg Loss=0.0794, Time Left=10.80\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2197/3393 [18:14<10:04,  1.98batch/s, Batch Loss=0.0102, Avg Loss=0.0794, Time Left=10.79\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2198/3393 [18:14<10:07,  1.97batch/s, Batch Loss=0.0102, Avg Loss=0.0794, Time Left=10.79\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2198/3393 [18:15<10:07,  1.97batch/s, Batch Loss=0.1985, Avg Loss=0.0795, Time Left=10.78\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2199/3393 [18:15<10:04,  1.98batch/s, Batch Loss=0.1985, Avg Loss=0.0795, Time Left=10.78\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2199/3393 [18:15<10:04,  1.98batch/s, Batch Loss=0.0131, Avg Loss=0.0794, Time Left=10.77\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2200/3393 [18:15<10:01,  1.98batch/s, Batch Loss=0.0131, Avg Loss=0.0794, Time Left=10.77\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2200/3393 [18:16<10:01,  1.98batch/s, Batch Loss=0.0481, Avg Loss=0.0794, Time Left=10.77\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2201/3393 [18:16<09:59,  1.99batch/s, Batch Loss=0.0481, Avg Loss=0.0794, Time Left=10.77\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2201/3393 [18:16<09:59,  1.99batch/s, Batch Loss=0.0975, Avg Loss=0.0794, Time Left=10.76\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2202/3393 [18:16<10:09,  1.95batch/s, Batch Loss=0.0975, Avg Loss=0.0794, Time Left=10.76\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2202/3393 [18:17<10:09,  1.95batch/s, Batch Loss=0.0282, Avg Loss=0.0794, Time Left=10.75\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2203/3393 [18:17<10:00,  1.98batch/s, Batch Loss=0.0282, Avg Loss=0.0794, Time Left=10.75\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2203/3393 [18:17<10:00,  1.98batch/s, Batch Loss=0.0309, Avg Loss=0.0794, Time Left=10.74\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2204/3393 [18:17<10:03,  1.97batch/s, Batch Loss=0.0309, Avg Loss=0.0794, Time Left=10.74\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2204/3393 [18:18<10:03,  1.97batch/s, Batch Loss=0.0263, Avg Loss=0.0794, Time Left=10.73\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2205/3393 [18:18<09:54,  2.00batch/s, Batch Loss=0.0263, Avg Loss=0.0794, Time Left=10.73\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2205/3393 [18:18<09:54,  2.00batch/s, Batch Loss=0.0419, Avg Loss=0.0793, Time Left=10.72\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2206/3393 [18:18<09:53,  2.00batch/s, Batch Loss=0.0419, Avg Loss=0.0793, Time Left=10.72\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2206/3393 [18:19<09:53,  2.00batch/s, Batch Loss=0.1803, Avg Loss=0.0794, Time Left=10.71\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2207/3393 [18:19<09:47,  2.02batch/s, Batch Loss=0.1803, Avg Loss=0.0794, Time Left=10.71\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2207/3393 [18:19<09:47,  2.02batch/s, Batch Loss=0.1395, Avg Loss=0.0794, Time Left=10.70\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2208/3393 [18:19<09:49,  2.01batch/s, Batch Loss=0.1395, Avg Loss=0.0794, Time Left=10.70\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2208/3393 [18:20<09:49,  2.01batch/s, Batch Loss=0.0440, Avg Loss=0.0794, Time Left=10.70\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  65%|▋| 2209/3393 [18:20<10:00,  1.97batch/s, Batch Loss=0.0440, Avg Loss=0.0794, Time Left=10.70\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2209/3393 [18:20<10:00,  1.97batch/s, Batch Loss=0.0410, Avg Loss=0.0794, Time Left=10.69\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2210/3393 [18:20<09:58,  1.98batch/s, Batch Loss=0.0410, Avg Loss=0.0794, Time Left=10.69\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2210/3393 [18:21<09:58,  1.98batch/s, Batch Loss=0.0920, Avg Loss=0.0794, Time Left=10.68\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2211/3393 [18:21<10:02,  1.96batch/s, Batch Loss=0.0920, Avg Loss=0.0794, Time Left=10.68\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2211/3393 [18:21<10:02,  1.96batch/s, Batch Loss=0.1182, Avg Loss=0.0794, Time Left=10.67\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2212/3393 [18:21<09:58,  1.97batch/s, Batch Loss=0.1182, Avg Loss=0.0794, Time Left=10.67\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2212/3393 [18:22<09:58,  1.97batch/s, Batch Loss=0.1831, Avg Loss=0.0794, Time Left=10.66\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2213/3393 [18:22<09:55,  1.98batch/s, Batch Loss=0.1831, Avg Loss=0.0794, Time Left=10.66\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2213/3393 [18:22<09:55,  1.98batch/s, Batch Loss=0.0618, Avg Loss=0.0794, Time Left=10.65\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2214/3393 [18:22<09:53,  1.99batch/s, Batch Loss=0.0618, Avg Loss=0.0794, Time Left=10.65\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2214/3393 [18:23<09:53,  1.99batch/s, Batch Loss=0.1009, Avg Loss=0.0795, Time Left=10.64\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2215/3393 [18:23<09:40,  2.03batch/s, Batch Loss=0.1009, Avg Loss=0.0795, Time Left=10.64\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2215/3393 [18:23<09:40,  2.03batch/s, Batch Loss=0.1228, Avg Loss=0.0795, Time Left=10.64\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2216/3393 [18:23<09:36,  2.04batch/s, Batch Loss=0.1228, Avg Loss=0.0795, Time Left=10.64\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2216/3393 [18:24<09:36,  2.04batch/s, Batch Loss=0.0372, Avg Loss=0.0795, Time Left=10.63\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2217/3393 [18:24<09:33,  2.05batch/s, Batch Loss=0.0372, Avg Loss=0.0795, Time Left=10.63\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2217/3393 [18:24<09:33,  2.05batch/s, Batch Loss=0.0746, Avg Loss=0.0794, Time Left=10.62\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2218/3393 [18:24<09:48,  2.00batch/s, Batch Loss=0.0746, Avg Loss=0.0794, Time Left=10.62\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2218/3393 [18:25<09:48,  2.00batch/s, Batch Loss=0.0016, Avg Loss=0.0794, Time Left=10.61\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2219/3393 [18:25<09:48,  1.99batch/s, Batch Loss=0.0016, Avg Loss=0.0794, Time Left=10.61\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2219/3393 [18:25<09:48,  1.99batch/s, Batch Loss=0.0073, Avg Loss=0.0794, Time Left=10.60\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2220/3393 [18:25<09:48,  1.99batch/s, Batch Loss=0.0073, Avg Loss=0.0794, Time Left=10.60\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2220/3393 [18:26<09:48,  1.99batch/s, Batch Loss=0.0032, Avg Loss=0.0793, Time Left=10.59\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2221/3393 [18:26<09:35,  2.04batch/s, Batch Loss=0.0032, Avg Loss=0.0793, Time Left=10.59\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2221/3393 [18:26<09:35,  2.04batch/s, Batch Loss=0.0603, Avg Loss=0.0793, Time Left=10.58\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2222/3393 [18:26<09:37,  2.03batch/s, Batch Loss=0.0603, Avg Loss=0.0793, Time Left=10.58\u001b[A\n",
      "Epoch 3/3 - Training:  65%|▋| 2222/3393 [18:27<09:37,  2.03batch/s, Batch Loss=0.3594, Avg Loss=0.0795, Time Left=10.57\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2223/3393 [18:27<09:34,  2.04batch/s, Batch Loss=0.3594, Avg Loss=0.0795, Time Left=10.57\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2223/3393 [18:27<09:34,  2.04batch/s, Batch Loss=0.0233, Avg Loss=0.0794, Time Left=10.57\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2224/3393 [18:27<09:25,  2.07batch/s, Batch Loss=0.0233, Avg Loss=0.0794, Time Left=10.57\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2224/3393 [18:28<09:25,  2.07batch/s, Batch Loss=0.0411, Avg Loss=0.0794, Time Left=10.56\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2225/3393 [18:28<09:35,  2.03batch/s, Batch Loss=0.0411, Avg Loss=0.0794, Time Left=10.56\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2225/3393 [18:28<09:35,  2.03batch/s, Batch Loss=0.0143, Avg Loss=0.0794, Time Left=10.55\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2226/3393 [18:28<09:26,  2.06batch/s, Batch Loss=0.0143, Avg Loss=0.0794, Time Left=10.55\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2226/3393 [18:29<09:26,  2.06batch/s, Batch Loss=0.0417, Avg Loss=0.0794, Time Left=10.54\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2227/3393 [18:29<09:31,  2.04batch/s, Batch Loss=0.0417, Avg Loss=0.0794, Time Left=10.54\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2227/3393 [18:29<09:31,  2.04batch/s, Batch Loss=0.0060, Avg Loss=0.0793, Time Left=10.53\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2228/3393 [18:29<09:45,  1.99batch/s, Batch Loss=0.0060, Avg Loss=0.0793, Time Left=10.53\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2228/3393 [18:30<09:45,  1.99batch/s, Batch Loss=0.0168, Avg Loss=0.0793, Time Left=10.52\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2229/3393 [18:30<09:45,  1.99batch/s, Batch Loss=0.0168, Avg Loss=0.0793, Time Left=10.52\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2229/3393 [18:30<09:45,  1.99batch/s, Batch Loss=0.0020, Avg Loss=0.0793, Time Left=10.51\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2230/3393 [18:30<09:49,  1.97batch/s, Batch Loss=0.0020, Avg Loss=0.0793, Time Left=10.51\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2230/3393 [18:31<09:49,  1.97batch/s, Batch Loss=0.0043, Avg Loss=0.0792, Time Left=10.50\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2231/3393 [18:31<09:47,  1.98batch/s, Batch Loss=0.0043, Avg Loss=0.0792, Time Left=10.50\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2231/3393 [18:31<09:47,  1.98batch/s, Batch Loss=0.0087, Avg Loss=0.0792, Time Left=10.50\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2232/3393 [18:31<09:55,  1.95batch/s, Batch Loss=0.0087, Avg Loss=0.0792, Time Left=10.50\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2232/3393 [18:32<09:55,  1.95batch/s, Batch Loss=0.0401, Avg Loss=0.0792, Time Left=10.49\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2233/3393 [18:32<09:51,  1.96batch/s, Batch Loss=0.0401, Avg Loss=0.0792, Time Left=10.49\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2233/3393 [18:32<09:51,  1.96batch/s, Batch Loss=0.0050, Avg Loss=0.0792, Time Left=10.48\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2234/3393 [18:32<09:47,  1.97batch/s, Batch Loss=0.0050, Avg Loss=0.0792, Time Left=10.48\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2234/3393 [18:33<09:47,  1.97batch/s, Batch Loss=0.0629, Avg Loss=0.0791, Time Left=10.47\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2235/3393 [18:33<09:39,  2.00batch/s, Batch Loss=0.0629, Avg Loss=0.0791, Time Left=10.47\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2235/3393 [18:33<09:39,  2.00batch/s, Batch Loss=0.0266, Avg Loss=0.0791, Time Left=10.46\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2236/3393 [18:33<09:50,  1.96batch/s, Batch Loss=0.0266, Avg Loss=0.0791, Time Left=10.46\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2236/3393 [18:34<09:50,  1.96batch/s, Batch Loss=0.0819, Avg Loss=0.0791, Time Left=10.45\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2237/3393 [18:34<09:52,  1.95batch/s, Batch Loss=0.0819, Avg Loss=0.0791, Time Left=10.45\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2237/3393 [18:34<09:52,  1.95batch/s, Batch Loss=0.0100, Avg Loss=0.0791, Time Left=10.45\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2238/3393 [18:34<10:04,  1.91batch/s, Batch Loss=0.0100, Avg Loss=0.0791, Time Left=10.45\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2238/3393 [18:35<10:04,  1.91batch/s, Batch Loss=0.0354, Avg Loss=0.0791, Time Left=10.44\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2239/3393 [18:35<09:56,  1.93batch/s, Batch Loss=0.0354, Avg Loss=0.0791, Time Left=10.44\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2239/3393 [18:35<09:56,  1.93batch/s, Batch Loss=0.0853, Avg Loss=0.0791, Time Left=10.43\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2240/3393 [18:35<09:51,  1.95batch/s, Batch Loss=0.0853, Avg Loss=0.0791, Time Left=10.43\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2240/3393 [18:36<09:51,  1.95batch/s, Batch Loss=0.0046, Avg Loss=0.0790, Time Left=10.42\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2241/3393 [18:36<09:45,  1.97batch/s, Batch Loss=0.0046, Avg Loss=0.0790, Time Left=10.42\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2241/3393 [18:36<09:45,  1.97batch/s, Batch Loss=0.4830, Avg Loss=0.0792, Time Left=10.41\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  66%|▋| 2242/3393 [18:36<09:36,  2.00batch/s, Batch Loss=0.4830, Avg Loss=0.0792, Time Left=10.41\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2242/3393 [18:37<09:36,  2.00batch/s, Batch Loss=0.0021, Avg Loss=0.0792, Time Left=10.40\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2243/3393 [18:37<09:30,  2.02batch/s, Batch Loss=0.0021, Avg Loss=0.0792, Time Left=10.40\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2243/3393 [18:37<09:30,  2.02batch/s, Batch Loss=0.1001, Avg Loss=0.0792, Time Left=10.39\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2244/3393 [18:37<09:31,  2.01batch/s, Batch Loss=0.1001, Avg Loss=0.0792, Time Left=10.39\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2244/3393 [18:38<09:31,  2.01batch/s, Batch Loss=0.0869, Avg Loss=0.0792, Time Left=10.38\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2245/3393 [18:38<09:26,  2.03batch/s, Batch Loss=0.0869, Avg Loss=0.0792, Time Left=10.38\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2245/3393 [18:38<09:26,  2.03batch/s, Batch Loss=0.0238, Avg Loss=0.0792, Time Left=10.38\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2246/3393 [18:38<09:27,  2.02batch/s, Batch Loss=0.0238, Avg Loss=0.0792, Time Left=10.38\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2246/3393 [18:39<09:27,  2.02batch/s, Batch Loss=0.0128, Avg Loss=0.0791, Time Left=10.37\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2247/3393 [18:39<09:24,  2.03batch/s, Batch Loss=0.0128, Avg Loss=0.0791, Time Left=10.37\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2247/3393 [18:39<09:24,  2.03batch/s, Batch Loss=0.0136, Avg Loss=0.0791, Time Left=10.36\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2248/3393 [18:39<09:20,  2.04batch/s, Batch Loss=0.0136, Avg Loss=0.0791, Time Left=10.36\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2248/3393 [18:40<09:20,  2.04batch/s, Batch Loss=0.0024, Avg Loss=0.0791, Time Left=10.35\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2249/3393 [18:40<09:23,  2.03batch/s, Batch Loss=0.0024, Avg Loss=0.0791, Time Left=10.35\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2249/3393 [18:40<09:23,  2.03batch/s, Batch Loss=0.0909, Avg Loss=0.0791, Time Left=10.34\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2250/3393 [18:40<09:20,  2.04batch/s, Batch Loss=0.0909, Avg Loss=0.0791, Time Left=10.34\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2250/3393 [18:41<09:20,  2.04batch/s, Batch Loss=0.0589, Avg Loss=0.0791, Time Left=10.33\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2251/3393 [18:41<09:17,  2.05batch/s, Batch Loss=0.0589, Avg Loss=0.0791, Time Left=10.33\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2251/3393 [18:41<09:17,  2.05batch/s, Batch Loss=0.0351, Avg Loss=0.0791, Time Left=10.32\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2252/3393 [18:41<09:33,  1.99batch/s, Batch Loss=0.0351, Avg Loss=0.0791, Time Left=10.32\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2252/3393 [18:42<09:33,  1.99batch/s, Batch Loss=0.0083, Avg Loss=0.0790, Time Left=10.31\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2253/3393 [18:42<09:25,  2.02batch/s, Batch Loss=0.0083, Avg Loss=0.0790, Time Left=10.31\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2253/3393 [18:42<09:25,  2.02batch/s, Batch Loss=0.1690, Avg Loss=0.0791, Time Left=10.31\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2254/3393 [18:42<09:31,  1.99batch/s, Batch Loss=0.1690, Avg Loss=0.0791, Time Left=10.31\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2254/3393 [18:43<09:31,  1.99batch/s, Batch Loss=0.0053, Avg Loss=0.0790, Time Left=10.30\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2255/3393 [18:43<09:25,  2.01batch/s, Batch Loss=0.0053, Avg Loss=0.0790, Time Left=10.30\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2255/3393 [18:43<09:25,  2.01batch/s, Batch Loss=0.0020, Avg Loss=0.0790, Time Left=10.29\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2256/3393 [18:43<09:20,  2.03batch/s, Batch Loss=0.0020, Avg Loss=0.0790, Time Left=10.29\u001b[A\n",
      "Epoch 3/3 - Training:  66%|▋| 2256/3393 [18:44<09:20,  2.03batch/s, Batch Loss=0.0105, Avg Loss=0.0790, Time Left=10.28\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2257/3393 [18:44<09:27,  2.00batch/s, Batch Loss=0.0105, Avg Loss=0.0790, Time Left=10.28\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2257/3393 [18:44<09:27,  2.00batch/s, Batch Loss=0.0056, Avg Loss=0.0789, Time Left=10.27\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2258/3393 [18:44<09:22,  2.02batch/s, Batch Loss=0.0056, Avg Loss=0.0789, Time Left=10.27\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2258/3393 [18:45<09:22,  2.02batch/s, Batch Loss=0.0041, Avg Loss=0.0789, Time Left=10.26\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2259/3393 [18:45<09:28,  1.99batch/s, Batch Loss=0.0041, Avg Loss=0.0789, Time Left=10.26\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2259/3393 [18:45<09:28,  1.99batch/s, Batch Loss=0.0110, Avg Loss=0.0789, Time Left=10.25\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2260/3393 [18:45<09:23,  2.01batch/s, Batch Loss=0.0110, Avg Loss=0.0789, Time Left=10.25\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2260/3393 [18:46<09:23,  2.01batch/s, Batch Loss=0.0362, Avg Loss=0.0789, Time Left=10.24\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2261/3393 [18:46<09:33,  1.97batch/s, Batch Loss=0.0362, Avg Loss=0.0789, Time Left=10.24\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2261/3393 [18:46<09:33,  1.97batch/s, Batch Loss=0.1552, Avg Loss=0.0789, Time Left=10.24\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2262/3393 [18:46<09:37,  1.96batch/s, Batch Loss=0.1552, Avg Loss=0.0789, Time Left=10.24\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2262/3393 [18:47<09:37,  1.96batch/s, Batch Loss=0.0017, Avg Loss=0.0788, Time Left=10.23\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2263/3393 [18:47<09:33,  1.97batch/s, Batch Loss=0.0017, Avg Loss=0.0788, Time Left=10.23\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2263/3393 [18:47<09:33,  1.97batch/s, Batch Loss=0.1357, Avg Loss=0.0789, Time Left=10.22\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2264/3393 [18:47<09:14,  2.04batch/s, Batch Loss=0.1357, Avg Loss=0.0789, Time Left=10.22\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2264/3393 [18:48<09:14,  2.04batch/s, Batch Loss=0.0098, Avg Loss=0.0788, Time Left=10.21\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2265/3393 [18:48<09:16,  2.03batch/s, Batch Loss=0.0098, Avg Loss=0.0788, Time Left=10.21\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2265/3393 [18:48<09:16,  2.03batch/s, Batch Loss=0.1479, Avg Loss=0.0789, Time Left=10.20\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2266/3393 [18:48<09:23,  2.00batch/s, Batch Loss=0.1479, Avg Loss=0.0789, Time Left=10.20\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2266/3393 [18:49<09:23,  2.00batch/s, Batch Loss=0.0037, Avg Loss=0.0788, Time Left=10.19\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2267/3393 [18:49<09:17,  2.02batch/s, Batch Loss=0.0037, Avg Loss=0.0788, Time Left=10.19\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2267/3393 [18:49<09:17,  2.02batch/s, Batch Loss=0.0434, Avg Loss=0.0788, Time Left=10.18\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2268/3393 [18:49<09:25,  1.99batch/s, Batch Loss=0.0434, Avg Loss=0.0788, Time Left=10.18\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2268/3393 [18:50<09:25,  1.99batch/s, Batch Loss=0.0965, Avg Loss=0.0788, Time Left=10.18\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2269/3393 [18:50<09:12,  2.04batch/s, Batch Loss=0.0965, Avg Loss=0.0788, Time Left=10.18\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2269/3393 [18:50<09:12,  2.04batch/s, Batch Loss=0.1062, Avg Loss=0.0788, Time Left=10.17\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2270/3393 [18:50<09:20,  2.00batch/s, Batch Loss=0.1062, Avg Loss=0.0788, Time Left=10.17\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2270/3393 [18:51<09:20,  2.00batch/s, Batch Loss=0.0029, Avg Loss=0.0788, Time Left=10.16\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2271/3393 [18:51<09:09,  2.04batch/s, Batch Loss=0.0029, Avg Loss=0.0788, Time Left=10.16\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2271/3393 [18:51<09:09,  2.04batch/s, Batch Loss=0.0697, Avg Loss=0.0788, Time Left=10.15\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2272/3393 [18:51<09:12,  2.03batch/s, Batch Loss=0.0697, Avg Loss=0.0788, Time Left=10.15\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2272/3393 [18:52<09:12,  2.03batch/s, Batch Loss=0.0013, Avg Loss=0.0788, Time Left=10.14\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2273/3393 [18:52<09:19,  2.00batch/s, Batch Loss=0.0013, Avg Loss=0.0788, Time Left=10.14\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2273/3393 [18:52<09:19,  2.00batch/s, Batch Loss=0.1765, Avg Loss=0.0788, Time Left=10.13\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2274/3393 [18:52<09:19,  2.00batch/s, Batch Loss=0.1765, Avg Loss=0.0788, Time Left=10.13\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2274/3393 [18:53<09:19,  2.00batch/s, Batch Loss=0.1000, Avg Loss=0.0788, Time Left=10.12\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  67%|▋| 2275/3393 [18:53<09:28,  1.97batch/s, Batch Loss=0.1000, Avg Loss=0.0788, Time Left=10.12\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2275/3393 [18:53<09:28,  1.97batch/s, Batch Loss=0.0013, Avg Loss=0.0788, Time Left=10.11\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2276/3393 [18:53<09:21,  1.99batch/s, Batch Loss=0.0013, Avg Loss=0.0788, Time Left=10.11\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2276/3393 [18:54<09:21,  1.99batch/s, Batch Loss=0.0093, Avg Loss=0.0788, Time Left=10.11\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2277/3393 [18:54<09:39,  1.93batch/s, Batch Loss=0.0093, Avg Loss=0.0788, Time Left=10.11\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2277/3393 [18:54<09:39,  1.93batch/s, Batch Loss=0.2767, Avg Loss=0.0788, Time Left=10.10\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2278/3393 [18:54<09:30,  1.96batch/s, Batch Loss=0.2767, Avg Loss=0.0788, Time Left=10.10\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2278/3393 [18:55<09:30,  1.96batch/s, Batch Loss=0.0036, Avg Loss=0.0788, Time Left=10.09\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2279/3393 [18:55<09:30,  1.95batch/s, Batch Loss=0.0036, Avg Loss=0.0788, Time Left=10.09\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2279/3393 [18:55<09:30,  1.95batch/s, Batch Loss=0.0013, Avg Loss=0.0788, Time Left=10.08\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2280/3393 [18:55<09:26,  1.96batch/s, Batch Loss=0.0013, Avg Loss=0.0788, Time Left=10.08\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2280/3393 [18:56<09:26,  1.96batch/s, Batch Loss=0.3051, Avg Loss=0.0789, Time Left=10.07\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2281/3393 [18:56<09:28,  1.96batch/s, Batch Loss=0.3051, Avg Loss=0.0789, Time Left=10.07\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2281/3393 [18:56<09:28,  1.96batch/s, Batch Loss=0.0577, Avg Loss=0.0789, Time Left=10.06\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2282/3393 [18:56<09:24,  1.97batch/s, Batch Loss=0.0577, Avg Loss=0.0789, Time Left=10.06\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2282/3393 [18:57<09:24,  1.97batch/s, Batch Loss=0.0489, Avg Loss=0.0789, Time Left=10.05\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2283/3393 [18:57<09:16,  1.99batch/s, Batch Loss=0.0489, Avg Loss=0.0789, Time Left=10.05\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2283/3393 [18:57<09:16,  1.99batch/s, Batch Loss=0.1490, Avg Loss=0.0789, Time Left=10.05\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2284/3393 [18:57<09:15,  2.00batch/s, Batch Loss=0.1490, Avg Loss=0.0789, Time Left=10.05\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2284/3393 [18:58<09:15,  2.00batch/s, Batch Loss=0.2071, Avg Loss=0.0789, Time Left=10.04\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2285/3393 [18:58<09:09,  2.02batch/s, Batch Loss=0.2071, Avg Loss=0.0789, Time Left=10.04\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2285/3393 [18:58<09:09,  2.02batch/s, Batch Loss=0.0918, Avg Loss=0.0790, Time Left=10.03\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2286/3393 [18:58<08:59,  2.05batch/s, Batch Loss=0.0918, Avg Loss=0.0790, Time Left=10.03\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2286/3393 [18:59<08:59,  2.05batch/s, Batch Loss=0.1054, Avg Loss=0.0790, Time Left=10.02\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2287/3393 [18:59<08:57,  2.06batch/s, Batch Loss=0.1054, Avg Loss=0.0790, Time Left=10.02\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2287/3393 [18:59<08:57,  2.06batch/s, Batch Loss=0.0686, Avg Loss=0.0790, Time Left=10.01\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2288/3393 [18:59<09:02,  2.04batch/s, Batch Loss=0.0686, Avg Loss=0.0790, Time Left=10.01\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2288/3393 [19:00<09:02,  2.04batch/s, Batch Loss=0.0869, Avg Loss=0.0790, Time Left=10.00\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2289/3393 [19:00<08:59,  2.05batch/s, Batch Loss=0.0869, Avg Loss=0.0790, Time Left=10.00\u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2289/3393 [19:00<08:59,  2.05batch/s, Batch Loss=0.0510, Avg Loss=0.0790, Time Left=9.99 \u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2290/3393 [19:00<08:52,  2.07batch/s, Batch Loss=0.0510, Avg Loss=0.0790, Time Left=9.99 \u001b[A\n",
      "Epoch 3/3 - Training:  67%|▋| 2290/3393 [19:01<08:52,  2.07batch/s, Batch Loss=0.0478, Avg Loss=0.0789, Time Left=9.98 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2291/3393 [19:01<09:07,  2.01batch/s, Batch Loss=0.0478, Avg Loss=0.0789, Time Left=9.98 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2291/3393 [19:01<09:07,  2.01batch/s, Batch Loss=0.0337, Avg Loss=0.0789, Time Left=9.98 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2292/3393 [19:01<09:08,  2.01batch/s, Batch Loss=0.0337, Avg Loss=0.0789, Time Left=9.98 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2292/3393 [19:02<09:08,  2.01batch/s, Batch Loss=0.0170, Avg Loss=0.0789, Time Left=9.97 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2293/3393 [19:02<09:22,  1.96batch/s, Batch Loss=0.0170, Avg Loss=0.0789, Time Left=9.97 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2293/3393 [19:02<09:22,  1.96batch/s, Batch Loss=0.0040, Avg Loss=0.0789, Time Left=9.96 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2294/3393 [19:02<09:15,  1.98batch/s, Batch Loss=0.0040, Avg Loss=0.0789, Time Left=9.96 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2294/3393 [19:03<09:15,  1.98batch/s, Batch Loss=0.0656, Avg Loss=0.0789, Time Left=9.95 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2295/3393 [19:03<09:26,  1.94batch/s, Batch Loss=0.0656, Avg Loss=0.0789, Time Left=9.95 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2295/3393 [19:03<09:26,  1.94batch/s, Batch Loss=0.0804, Avg Loss=0.0789, Time Left=9.94 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2296/3393 [19:03<09:19,  1.96batch/s, Batch Loss=0.0804, Avg Loss=0.0789, Time Left=9.94 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2296/3393 [19:04<09:19,  1.96batch/s, Batch Loss=0.0392, Avg Loss=0.0788, Time Left=9.93 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2297/3393 [19:04<09:20,  1.96batch/s, Batch Loss=0.0392, Avg Loss=0.0788, Time Left=9.93 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2297/3393 [19:04<09:20,  1.96batch/s, Batch Loss=0.1211, Avg Loss=0.0789, Time Left=9.92 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2298/3393 [19:04<09:16,  1.97batch/s, Batch Loss=0.1211, Avg Loss=0.0789, Time Left=9.92 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2298/3393 [19:05<09:16,  1.97batch/s, Batch Loss=0.0284, Avg Loss=0.0788, Time Left=9.92 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2299/3393 [19:05<09:08,  2.00batch/s, Batch Loss=0.0284, Avg Loss=0.0788, Time Left=9.92 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2299/3393 [19:05<09:08,  2.00batch/s, Batch Loss=0.0421, Avg Loss=0.0788, Time Left=9.91 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2300/3393 [19:05<08:57,  2.03batch/s, Batch Loss=0.0421, Avg Loss=0.0788, Time Left=9.91 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2300/3393 [19:06<08:57,  2.03batch/s, Batch Loss=0.0784, Avg Loss=0.0788, Time Left=9.90 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2301/3393 [19:06<08:58,  2.03batch/s, Batch Loss=0.0784, Avg Loss=0.0788, Time Left=9.90 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2301/3393 [19:06<08:58,  2.03batch/s, Batch Loss=0.0775, Avg Loss=0.0788, Time Left=9.89 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2302/3393 [19:06<09:01,  2.01batch/s, Batch Loss=0.0775, Avg Loss=0.0788, Time Left=9.89 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2302/3393 [19:07<09:01,  2.01batch/s, Batch Loss=0.2748, Avg Loss=0.0789, Time Left=9.88 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2303/3393 [19:07<08:55,  2.03batch/s, Batch Loss=0.2748, Avg Loss=0.0789, Time Left=9.88 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2303/3393 [19:07<08:55,  2.03batch/s, Batch Loss=0.0098, Avg Loss=0.0789, Time Left=9.87 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2304/3393 [19:07<09:02,  2.01batch/s, Batch Loss=0.0098, Avg Loss=0.0789, Time Left=9.87 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2304/3393 [19:08<09:02,  2.01batch/s, Batch Loss=0.0262, Avg Loss=0.0788, Time Left=9.86 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2305/3393 [19:08<09:03,  2.00batch/s, Batch Loss=0.0262, Avg Loss=0.0788, Time Left=9.86 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2305/3393 [19:08<09:03,  2.00batch/s, Batch Loss=0.3081, Avg Loss=0.0789, Time Left=9.86 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2306/3393 [19:08<09:13,  1.96batch/s, Batch Loss=0.3081, Avg Loss=0.0789, Time Left=9.86 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2306/3393 [19:09<09:13,  1.96batch/s, Batch Loss=0.0063, Avg Loss=0.0789, Time Left=9.85 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2307/3393 [19:09<09:05,  1.99batch/s, Batch Loss=0.0063, Avg Loss=0.0789, Time Left=9.85 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2307/3393 [19:09<09:05,  1.99batch/s, Batch Loss=0.1669, Avg Loss=0.0790, Time Left=9.84 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  68%|▋| 2308/3393 [19:09<09:14,  1.96batch/s, Batch Loss=0.1669, Avg Loss=0.0790, Time Left=9.84 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2308/3393 [19:10<09:14,  1.96batch/s, Batch Loss=0.0972, Avg Loss=0.0790, Time Left=9.83 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2309/3393 [19:10<09:11,  1.97batch/s, Batch Loss=0.0972, Avg Loss=0.0790, Time Left=9.83 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2309/3393 [19:10<09:11,  1.97batch/s, Batch Loss=0.0111, Avg Loss=0.0789, Time Left=9.82 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2310/3393 [19:10<09:13,  1.96batch/s, Batch Loss=0.0111, Avg Loss=0.0789, Time Left=9.82 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2310/3393 [19:11<09:13,  1.96batch/s, Batch Loss=0.0179, Avg Loss=0.0789, Time Left=9.81 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2311/3393 [19:11<09:09,  1.97batch/s, Batch Loss=0.0179, Avg Loss=0.0789, Time Left=9.81 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2311/3393 [19:11<09:09,  1.97batch/s, Batch Loss=0.0552, Avg Loss=0.0789, Time Left=9.80 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2312/3393 [19:11<09:16,  1.94batch/s, Batch Loss=0.0552, Avg Loss=0.0789, Time Left=9.80 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2312/3393 [19:12<09:16,  1.94batch/s, Batch Loss=0.0093, Avg Loss=0.0789, Time Left=9.80 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2313/3393 [19:12<09:14,  1.95batch/s, Batch Loss=0.0093, Avg Loss=0.0789, Time Left=9.80 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2313/3393 [19:12<09:14,  1.95batch/s, Batch Loss=0.1197, Avg Loss=0.0789, Time Left=9.79 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2314/3393 [19:12<09:17,  1.93batch/s, Batch Loss=0.1197, Avg Loss=0.0789, Time Left=9.79 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2314/3393 [19:13<09:17,  1.93batch/s, Batch Loss=0.0069, Avg Loss=0.0789, Time Left=9.78 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2315/3393 [19:13<09:01,  1.99batch/s, Batch Loss=0.0069, Avg Loss=0.0789, Time Left=9.78 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2315/3393 [19:13<09:01,  1.99batch/s, Batch Loss=0.0085, Avg Loss=0.0788, Time Left=9.77 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2316/3393 [19:13<09:15,  1.94batch/s, Batch Loss=0.0085, Avg Loss=0.0788, Time Left=9.77 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2316/3393 [19:14<09:15,  1.94batch/s, Batch Loss=0.2758, Avg Loss=0.0789, Time Left=9.76 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2317/3393 [19:14<09:00,  1.99batch/s, Batch Loss=0.2758, Avg Loss=0.0789, Time Left=9.76 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2317/3393 [19:14<09:00,  1.99batch/s, Batch Loss=0.0268, Avg Loss=0.0789, Time Left=9.75 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2318/3393 [19:14<09:09,  1.96batch/s, Batch Loss=0.0268, Avg Loss=0.0789, Time Left=9.75 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2318/3393 [19:15<09:09,  1.96batch/s, Batch Loss=0.0479, Avg Loss=0.0789, Time Left=9.74 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2319/3393 [19:15<09:05,  1.97batch/s, Batch Loss=0.0479, Avg Loss=0.0789, Time Left=9.74 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2319/3393 [19:15<09:05,  1.97batch/s, Batch Loss=0.0105, Avg Loss=0.0788, Time Left=9.74 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2320/3393 [19:15<09:12,  1.94batch/s, Batch Loss=0.0105, Avg Loss=0.0788, Time Left=9.74 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2320/3393 [19:16<09:12,  1.94batch/s, Batch Loss=0.1608, Avg Loss=0.0789, Time Left=9.73 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2321/3393 [19:16<09:03,  1.97batch/s, Batch Loss=0.1608, Avg Loss=0.0789, Time Left=9.73 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2321/3393 [19:16<09:03,  1.97batch/s, Batch Loss=0.0868, Avg Loss=0.0789, Time Left=9.72 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2322/3393 [19:16<09:10,  1.95batch/s, Batch Loss=0.0868, Avg Loss=0.0789, Time Left=9.72 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2322/3393 [19:17<09:10,  1.95batch/s, Batch Loss=0.0025, Avg Loss=0.0788, Time Left=9.71 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2323/3393 [19:17<08:56,  2.00batch/s, Batch Loss=0.0025, Avg Loss=0.0788, Time Left=9.71 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2323/3393 [19:17<08:56,  2.00batch/s, Batch Loss=0.0784, Avg Loss=0.0788, Time Left=9.70 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2324/3393 [19:17<09:00,  1.98batch/s, Batch Loss=0.0784, Avg Loss=0.0788, Time Left=9.70 \u001b[A\n",
      "Epoch 3/3 - Training:  68%|▋| 2324/3393 [19:18<09:00,  1.98batch/s, Batch Loss=0.0018, Avg Loss=0.0788, Time Left=9.69 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2325/3393 [19:18<08:47,  2.02batch/s, Batch Loss=0.0018, Avg Loss=0.0788, Time Left=9.69 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2325/3393 [19:18<08:47,  2.02batch/s, Batch Loss=0.0747, Avg Loss=0.0788, Time Left=9.68 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2326/3393 [19:18<08:49,  2.02batch/s, Batch Loss=0.0747, Avg Loss=0.0788, Time Left=9.68 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2326/3393 [19:19<08:49,  2.02batch/s, Batch Loss=0.0219, Avg Loss=0.0788, Time Left=9.67 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2327/3393 [19:19<09:00,  1.97batch/s, Batch Loss=0.0219, Avg Loss=0.0788, Time Left=9.67 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2327/3393 [19:19<09:00,  1.97batch/s, Batch Loss=0.0528, Avg Loss=0.0788, Time Left=9.67 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2328/3393 [19:19<08:57,  1.98batch/s, Batch Loss=0.0528, Avg Loss=0.0788, Time Left=9.67 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2328/3393 [19:20<08:57,  1.98batch/s, Batch Loss=0.1291, Avg Loss=0.0788, Time Left=9.66 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2329/3393 [19:20<09:06,  1.95batch/s, Batch Loss=0.1291, Avg Loss=0.0788, Time Left=9.66 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2329/3393 [19:20<09:06,  1.95batch/s, Batch Loss=0.0938, Avg Loss=0.0788, Time Left=9.65 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2330/3393 [19:20<09:02,  1.96batch/s, Batch Loss=0.0938, Avg Loss=0.0788, Time Left=9.65 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2330/3393 [19:21<09:02,  1.96batch/s, Batch Loss=0.1342, Avg Loss=0.0788, Time Left=9.64 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2331/3393 [19:21<09:07,  1.94batch/s, Batch Loss=0.1342, Avg Loss=0.0788, Time Left=9.64 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2331/3393 [19:22<09:07,  1.94batch/s, Batch Loss=0.0546, Avg Loss=0.0788, Time Left=9.63 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2332/3393 [19:22<09:00,  1.96batch/s, Batch Loss=0.0546, Avg Loss=0.0788, Time Left=9.63 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2332/3393 [19:22<09:00,  1.96batch/s, Batch Loss=0.0173, Avg Loss=0.0788, Time Left=9.62 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2333/3393 [19:22<09:04,  1.95batch/s, Batch Loss=0.0173, Avg Loss=0.0788, Time Left=9.62 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2333/3393 [19:23<09:04,  1.95batch/s, Batch Loss=0.0307, Avg Loss=0.0788, Time Left=9.62 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2334/3393 [19:23<09:05,  1.94batch/s, Batch Loss=0.0307, Avg Loss=0.0788, Time Left=9.62 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2334/3393 [19:23<09:05,  1.94batch/s, Batch Loss=0.1133, Avg Loss=0.0788, Time Left=9.61 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2335/3393 [19:23<08:55,  1.98batch/s, Batch Loss=0.1133, Avg Loss=0.0788, Time Left=9.61 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2335/3393 [19:24<08:55,  1.98batch/s, Batch Loss=0.0111, Avg Loss=0.0788, Time Left=9.60 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2336/3393 [19:24<08:53,  1.98batch/s, Batch Loss=0.0111, Avg Loss=0.0788, Time Left=9.60 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2336/3393 [19:24<08:53,  1.98batch/s, Batch Loss=0.0632, Avg Loss=0.0787, Time Left=9.59 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2337/3393 [19:24<08:51,  1.99batch/s, Batch Loss=0.0632, Avg Loss=0.0787, Time Left=9.59 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2337/3393 [19:25<08:51,  1.99batch/s, Batch Loss=0.0101, Avg Loss=0.0787, Time Left=9.58 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2338/3393 [19:25<08:50,  1.99batch/s, Batch Loss=0.0101, Avg Loss=0.0787, Time Left=9.58 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2338/3393 [19:25<08:50,  1.99batch/s, Batch Loss=0.0341, Avg Loss=0.0787, Time Left=9.57 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2339/3393 [19:25<08:58,  1.96batch/s, Batch Loss=0.0341, Avg Loss=0.0787, Time Left=9.57 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2339/3393 [19:26<08:58,  1.96batch/s, Batch Loss=0.1372, Avg Loss=0.0787, Time Left=9.56 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2340/3393 [19:26<08:55,  1.97batch/s, Batch Loss=0.1372, Avg Loss=0.0787, Time Left=9.56 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2340/3393 [19:26<08:55,  1.97batch/s, Batch Loss=0.0086, Avg Loss=0.0787, Time Left=9.55 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  69%|▋| 2341/3393 [19:26<08:57,  1.96batch/s, Batch Loss=0.0086, Avg Loss=0.0787, Time Left=9.55 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2341/3393 [19:27<08:57,  1.96batch/s, Batch Loss=0.0125, Avg Loss=0.0787, Time Left=9.55 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2342/3393 [19:27<08:53,  1.97batch/s, Batch Loss=0.0125, Avg Loss=0.0787, Time Left=9.55 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2342/3393 [19:27<08:53,  1.97batch/s, Batch Loss=0.0277, Avg Loss=0.0786, Time Left=9.54 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2343/3393 [19:27<09:06,  1.92batch/s, Batch Loss=0.0277, Avg Loss=0.0786, Time Left=9.54 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2343/3393 [19:28<09:06,  1.92batch/s, Batch Loss=0.0481, Avg Loss=0.0786, Time Left=9.53 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2344/3393 [19:28<08:54,  1.96batch/s, Batch Loss=0.0481, Avg Loss=0.0786, Time Left=9.53 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2344/3393 [19:28<08:54,  1.96batch/s, Batch Loss=0.0011, Avg Loss=0.0786, Time Left=9.52 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2345/3393 [19:28<08:56,  1.95batch/s, Batch Loss=0.0011, Avg Loss=0.0786, Time Left=9.52 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2345/3393 [19:29<08:56,  1.95batch/s, Batch Loss=0.1098, Avg Loss=0.0786, Time Left=9.51 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2346/3393 [19:29<08:56,  1.95batch/s, Batch Loss=0.1098, Avg Loss=0.0786, Time Left=9.51 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2346/3393 [19:29<08:56,  1.95batch/s, Batch Loss=0.0761, Avg Loss=0.0786, Time Left=9.50 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2347/3393 [19:29<08:58,  1.94batch/s, Batch Loss=0.0761, Avg Loss=0.0786, Time Left=9.50 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2347/3393 [19:30<08:58,  1.94batch/s, Batch Loss=0.0149, Avg Loss=0.0786, Time Left=9.50 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2348/3393 [19:30<08:57,  1.94batch/s, Batch Loss=0.0149, Avg Loss=0.0786, Time Left=9.50 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2348/3393 [19:30<08:57,  1.94batch/s, Batch Loss=0.4404, Avg Loss=0.0787, Time Left=9.49 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2349/3393 [19:30<08:58,  1.94batch/s, Batch Loss=0.4404, Avg Loss=0.0787, Time Left=9.49 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2349/3393 [19:31<08:58,  1.94batch/s, Batch Loss=0.0436, Avg Loss=0.0787, Time Left=9.48 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2350/3393 [19:31<08:53,  1.95batch/s, Batch Loss=0.0436, Avg Loss=0.0787, Time Left=9.48 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2350/3393 [19:31<08:53,  1.95batch/s, Batch Loss=0.0351, Avg Loss=0.0787, Time Left=9.47 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2351/3393 [19:31<08:57,  1.94batch/s, Batch Loss=0.0351, Avg Loss=0.0787, Time Left=9.47 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2351/3393 [19:32<08:57,  1.94batch/s, Batch Loss=0.0061, Avg Loss=0.0787, Time Left=9.46 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2352/3393 [19:32<08:54,  1.95batch/s, Batch Loss=0.0061, Avg Loss=0.0787, Time Left=9.46 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2352/3393 [19:32<08:54,  1.95batch/s, Batch Loss=0.0059, Avg Loss=0.0786, Time Left=9.45 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2353/3393 [19:32<08:45,  1.98batch/s, Batch Loss=0.0059, Avg Loss=0.0786, Time Left=9.45 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2353/3393 [19:33<08:45,  1.98batch/s, Batch Loss=0.4437, Avg Loss=0.0788, Time Left=9.44 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2354/3393 [19:33<08:38,  2.01batch/s, Batch Loss=0.4437, Avg Loss=0.0788, Time Left=9.44 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2354/3393 [19:33<08:38,  2.01batch/s, Batch Loss=0.1041, Avg Loss=0.0788, Time Left=9.43 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2355/3393 [19:33<08:33,  2.02batch/s, Batch Loss=0.1041, Avg Loss=0.0788, Time Left=9.43 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2355/3393 [19:34<08:33,  2.02batch/s, Batch Loss=0.0356, Avg Loss=0.0788, Time Left=9.43 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2356/3393 [19:34<08:29,  2.04batch/s, Batch Loss=0.0356, Avg Loss=0.0788, Time Left=9.43 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2356/3393 [19:34<08:29,  2.04batch/s, Batch Loss=0.0668, Avg Loss=0.0788, Time Left=9.42 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2357/3393 [19:34<08:29,  2.04batch/s, Batch Loss=0.0668, Avg Loss=0.0788, Time Left=9.42 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2357/3393 [19:35<08:29,  2.04batch/s, Batch Loss=0.0039, Avg Loss=0.0788, Time Left=9.41 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2358/3393 [19:35<08:38,  2.00batch/s, Batch Loss=0.0039, Avg Loss=0.0788, Time Left=9.41 \u001b[A\n",
      "Epoch 3/3 - Training:  69%|▋| 2358/3393 [19:35<08:38,  2.00batch/s, Batch Loss=0.0196, Avg Loss=0.0787, Time Left=9.40 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2359/3393 [19:35<08:38,  2.00batch/s, Batch Loss=0.0196, Avg Loss=0.0787, Time Left=9.40 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2359/3393 [19:36<08:38,  2.00batch/s, Batch Loss=0.0075, Avg Loss=0.0787, Time Left=9.39 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2360/3393 [19:36<08:41,  1.98batch/s, Batch Loss=0.0075, Avg Loss=0.0787, Time Left=9.39 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2360/3393 [19:36<08:41,  1.98batch/s, Batch Loss=0.0117, Avg Loss=0.0787, Time Left=9.38 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2361/3393 [19:36<08:40,  1.98batch/s, Batch Loss=0.0117, Avg Loss=0.0787, Time Left=9.38 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2361/3393 [19:37<08:40,  1.98batch/s, Batch Loss=0.1057, Avg Loss=0.0787, Time Left=9.37 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2362/3393 [19:37<08:53,  1.93batch/s, Batch Loss=0.1057, Avg Loss=0.0787, Time Left=9.37 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2362/3393 [19:37<08:53,  1.93batch/s, Batch Loss=0.0722, Avg Loss=0.0787, Time Left=9.37 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2363/3393 [19:37<08:38,  1.99batch/s, Batch Loss=0.0722, Avg Loss=0.0787, Time Left=9.37 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2363/3393 [19:38<08:38,  1.99batch/s, Batch Loss=0.0104, Avg Loss=0.0786, Time Left=9.36 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2364/3393 [19:38<08:46,  1.95batch/s, Batch Loss=0.0104, Avg Loss=0.0786, Time Left=9.36 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2364/3393 [19:38<08:46,  1.95batch/s, Batch Loss=0.1818, Avg Loss=0.0787, Time Left=9.35 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2365/3393 [19:38<08:43,  1.96batch/s, Batch Loss=0.1818, Avg Loss=0.0787, Time Left=9.35 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2365/3393 [19:39<08:43,  1.96batch/s, Batch Loss=0.1426, Avg Loss=0.0787, Time Left=9.34 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2366/3393 [19:39<08:49,  1.94batch/s, Batch Loss=0.1426, Avg Loss=0.0787, Time Left=9.34 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2366/3393 [19:39<08:49,  1.94batch/s, Batch Loss=0.0469, Avg Loss=0.0787, Time Left=9.33 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2367/3393 [19:39<08:39,  1.97batch/s, Batch Loss=0.0469, Avg Loss=0.0787, Time Left=9.33 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2367/3393 [19:40<08:39,  1.97batch/s, Batch Loss=0.0570, Avg Loss=0.0787, Time Left=9.32 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2368/3393 [19:40<08:42,  1.96batch/s, Batch Loss=0.0570, Avg Loss=0.0787, Time Left=9.32 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2368/3393 [19:40<08:42,  1.96batch/s, Batch Loss=0.0253, Avg Loss=0.0787, Time Left=9.31 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2369/3393 [19:40<08:34,  1.99batch/s, Batch Loss=0.0253, Avg Loss=0.0787, Time Left=9.31 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2369/3393 [19:41<08:34,  1.99batch/s, Batch Loss=0.0675, Avg Loss=0.0787, Time Left=9.31 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2370/3393 [19:41<08:34,  1.99batch/s, Batch Loss=0.0675, Avg Loss=0.0787, Time Left=9.31 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2370/3393 [19:41<08:34,  1.99batch/s, Batch Loss=0.0091, Avg Loss=0.0786, Time Left=9.30 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2371/3393 [19:41<08:27,  2.02batch/s, Batch Loss=0.0091, Avg Loss=0.0786, Time Left=9.30 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2371/3393 [19:42<08:27,  2.02batch/s, Batch Loss=0.0582, Avg Loss=0.0786, Time Left=9.29 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2372/3393 [19:42<08:23,  2.03batch/s, Batch Loss=0.0582, Avg Loss=0.0786, Time Left=9.29 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2372/3393 [19:42<08:23,  2.03batch/s, Batch Loss=0.0313, Avg Loss=0.0786, Time Left=9.28 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2373/3393 [19:42<08:35,  1.98batch/s, Batch Loss=0.0313, Avg Loss=0.0786, Time Left=9.28 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2373/3393 [19:43<08:35,  1.98batch/s, Batch Loss=0.0295, Avg Loss=0.0786, Time Left=9.27 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  70%|▋| 2374/3393 [19:43<08:32,  1.99batch/s, Batch Loss=0.0295, Avg Loss=0.0786, Time Left=9.27 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2374/3393 [19:43<08:32,  1.99batch/s, Batch Loss=0.0233, Avg Loss=0.0786, Time Left=9.26 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2375/3393 [19:43<08:31,  1.99batch/s, Batch Loss=0.0233, Avg Loss=0.0786, Time Left=9.26 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2375/3393 [19:44<08:31,  1.99batch/s, Batch Loss=0.1350, Avg Loss=0.0786, Time Left=9.25 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2376/3393 [19:44<08:25,  2.01batch/s, Batch Loss=0.1350, Avg Loss=0.0786, Time Left=9.25 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2376/3393 [19:44<08:25,  2.01batch/s, Batch Loss=0.0140, Avg Loss=0.0786, Time Left=9.24 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2377/3393 [19:44<08:25,  2.01batch/s, Batch Loss=0.0140, Avg Loss=0.0786, Time Left=9.24 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2377/3393 [19:45<08:25,  2.01batch/s, Batch Loss=0.0072, Avg Loss=0.0785, Time Left=9.24 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2378/3393 [19:45<08:15,  2.05batch/s, Batch Loss=0.0072, Avg Loss=0.0785, Time Left=9.24 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2378/3393 [19:45<08:15,  2.05batch/s, Batch Loss=0.1562, Avg Loss=0.0786, Time Left=9.23 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2379/3393 [19:45<08:09,  2.07batch/s, Batch Loss=0.1562, Avg Loss=0.0786, Time Left=9.23 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2379/3393 [19:46<08:09,  2.07batch/s, Batch Loss=0.0406, Avg Loss=0.0785, Time Left=9.22 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2380/3393 [19:46<08:22,  2.01batch/s, Batch Loss=0.0406, Avg Loss=0.0785, Time Left=9.22 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2380/3393 [19:46<08:22,  2.01batch/s, Batch Loss=0.0826, Avg Loss=0.0785, Time Left=9.21 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2381/3393 [19:46<08:14,  2.05batch/s, Batch Loss=0.0826, Avg Loss=0.0785, Time Left=9.21 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2381/3393 [19:47<08:14,  2.05batch/s, Batch Loss=0.1666, Avg Loss=0.0786, Time Left=9.20 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2382/3393 [19:47<08:12,  2.05batch/s, Batch Loss=0.1666, Avg Loss=0.0786, Time Left=9.20 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2382/3393 [19:47<08:12,  2.05batch/s, Batch Loss=0.0349, Avg Loss=0.0786, Time Left=9.19 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2383/3393 [19:47<08:15,  2.04batch/s, Batch Loss=0.0349, Avg Loss=0.0786, Time Left=9.19 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2383/3393 [19:48<08:15,  2.04batch/s, Batch Loss=0.0055, Avg Loss=0.0785, Time Left=9.18 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2384/3393 [19:48<08:18,  2.02batch/s, Batch Loss=0.0055, Avg Loss=0.0785, Time Left=9.18 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2384/3393 [19:48<08:18,  2.02batch/s, Batch Loss=0.0560, Avg Loss=0.0785, Time Left=9.18 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2385/3393 [19:48<08:24,  2.00batch/s, Batch Loss=0.0560, Avg Loss=0.0785, Time Left=9.18 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2385/3393 [19:49<08:24,  2.00batch/s, Batch Loss=0.0761, Avg Loss=0.0785, Time Left=9.17 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2386/3393 [19:49<08:24,  2.00batch/s, Batch Loss=0.0761, Avg Loss=0.0785, Time Left=9.17 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2386/3393 [19:49<08:24,  2.00batch/s, Batch Loss=0.0216, Avg Loss=0.0785, Time Left=9.16 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2387/3393 [19:49<08:32,  1.96batch/s, Batch Loss=0.0216, Avg Loss=0.0785, Time Left=9.16 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2387/3393 [19:50<08:32,  1.96batch/s, Batch Loss=0.0443, Avg Loss=0.0785, Time Left=9.15 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2388/3393 [19:50<08:25,  1.99batch/s, Batch Loss=0.0443, Avg Loss=0.0785, Time Left=9.15 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2388/3393 [19:50<08:25,  1.99batch/s, Batch Loss=0.2843, Avg Loss=0.0786, Time Left=9.14 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2389/3393 [19:50<08:28,  1.97batch/s, Batch Loss=0.2843, Avg Loss=0.0786, Time Left=9.14 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2389/3393 [19:51<08:28,  1.97batch/s, Batch Loss=0.0124, Avg Loss=0.0785, Time Left=9.13 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2390/3393 [19:51<08:21,  2.00batch/s, Batch Loss=0.0124, Avg Loss=0.0785, Time Left=9.13 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2390/3393 [19:51<08:21,  2.00batch/s, Batch Loss=0.0408, Avg Loss=0.0785, Time Left=9.12 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2391/3393 [19:51<08:26,  1.98batch/s, Batch Loss=0.0408, Avg Loss=0.0785, Time Left=9.12 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2391/3393 [19:52<08:26,  1.98batch/s, Batch Loss=0.0207, Avg Loss=0.0785, Time Left=9.11 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2392/3393 [19:52<08:19,  2.01batch/s, Batch Loss=0.0207, Avg Loss=0.0785, Time Left=9.11 \u001b[A\n",
      "Epoch 3/3 - Training:  70%|▋| 2392/3393 [19:52<08:19,  2.01batch/s, Batch Loss=0.0576, Avg Loss=0.0785, Time Left=9.11 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2393/3393 [19:52<08:09,  2.04batch/s, Batch Loss=0.0576, Avg Loss=0.0785, Time Left=9.11 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2393/3393 [19:53<08:09,  2.04batch/s, Batch Loss=0.0188, Avg Loss=0.0785, Time Left=9.10 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2394/3393 [19:53<08:21,  1.99batch/s, Batch Loss=0.0188, Avg Loss=0.0785, Time Left=9.10 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2394/3393 [19:53<08:21,  1.99batch/s, Batch Loss=0.0577, Avg Loss=0.0785, Time Left=9.09 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2395/3393 [19:53<08:15,  2.01batch/s, Batch Loss=0.0577, Avg Loss=0.0785, Time Left=9.09 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2395/3393 [19:54<08:15,  2.01batch/s, Batch Loss=0.0641, Avg Loss=0.0785, Time Left=9.08 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2396/3393 [19:54<08:31,  1.95batch/s, Batch Loss=0.0641, Avg Loss=0.0785, Time Left=9.08 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2396/3393 [19:54<08:31,  1.95batch/s, Batch Loss=0.0803, Avg Loss=0.0785, Time Left=9.07 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2397/3393 [19:54<08:17,  2.00batch/s, Batch Loss=0.0803, Avg Loss=0.0785, Time Left=9.07 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2397/3393 [19:55<08:17,  2.00batch/s, Batch Loss=0.0162, Avg Loss=0.0784, Time Left=9.06 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2398/3393 [19:55<08:30,  1.95batch/s, Batch Loss=0.0162, Avg Loss=0.0784, Time Left=9.06 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2398/3393 [19:55<08:30,  1.95batch/s, Batch Loss=0.0187, Avg Loss=0.0784, Time Left=9.05 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2399/3393 [19:55<08:22,  1.98batch/s, Batch Loss=0.0187, Avg Loss=0.0784, Time Left=9.05 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2399/3393 [19:56<08:22,  1.98batch/s, Batch Loss=0.1154, Avg Loss=0.0784, Time Left=9.05 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2400/3393 [19:56<08:29,  1.95batch/s, Batch Loss=0.1154, Avg Loss=0.0784, Time Left=9.05 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2400/3393 [19:56<08:29,  1.95batch/s, Batch Loss=0.0949, Avg Loss=0.0784, Time Left=9.04 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2401/3393 [19:56<08:21,  1.98batch/s, Batch Loss=0.0949, Avg Loss=0.0784, Time Left=9.04 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2401/3393 [19:57<08:21,  1.98batch/s, Batch Loss=0.0652, Avg Loss=0.0784, Time Left=9.03 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2402/3393 [19:57<08:18,  1.99batch/s, Batch Loss=0.0652, Avg Loss=0.0784, Time Left=9.03 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2402/3393 [19:57<08:18,  1.99batch/s, Batch Loss=0.0670, Avg Loss=0.0784, Time Left=9.02 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2403/3393 [19:57<08:12,  2.01batch/s, Batch Loss=0.0670, Avg Loss=0.0784, Time Left=9.02 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2403/3393 [19:58<08:12,  2.01batch/s, Batch Loss=0.0263, Avg Loss=0.0784, Time Left=9.01 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2404/3393 [19:58<08:12,  2.01batch/s, Batch Loss=0.0263, Avg Loss=0.0784, Time Left=9.01 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2404/3393 [19:58<08:12,  2.01batch/s, Batch Loss=0.0365, Avg Loss=0.0784, Time Left=9.00 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2405/3393 [19:58<08:17,  1.99batch/s, Batch Loss=0.0365, Avg Loss=0.0784, Time Left=9.00 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2405/3393 [19:59<08:17,  1.99batch/s, Batch Loss=0.0147, Avg Loss=0.0783, Time Left=8.99 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2406/3393 [19:59<08:21,  1.97batch/s, Batch Loss=0.0147, Avg Loss=0.0783, Time Left=8.99 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2406/3393 [19:59<08:21,  1.97batch/s, Batch Loss=0.0171, Avg Loss=0.0783, Time Left=8.99 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  71%|▋| 2407/3393 [19:59<08:14,  2.00batch/s, Batch Loss=0.0171, Avg Loss=0.0783, Time Left=8.99 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2407/3393 [20:00<08:14,  2.00batch/s, Batch Loss=0.1150, Avg Loss=0.0783, Time Left=8.98 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2408/3393 [20:00<08:04,  2.03batch/s, Batch Loss=0.1150, Avg Loss=0.0783, Time Left=8.98 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2408/3393 [20:00<08:04,  2.03batch/s, Batch Loss=0.1361, Avg Loss=0.0784, Time Left=8.97 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2409/3393 [20:00<08:10,  2.01batch/s, Batch Loss=0.1361, Avg Loss=0.0784, Time Left=8.97 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2409/3393 [20:01<08:10,  2.01batch/s, Batch Loss=0.0649, Avg Loss=0.0784, Time Left=8.96 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2410/3393 [20:01<08:05,  2.02batch/s, Batch Loss=0.0649, Avg Loss=0.0784, Time Left=8.96 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2410/3393 [20:01<08:05,  2.02batch/s, Batch Loss=0.0062, Avg Loss=0.0783, Time Left=8.95 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2411/3393 [20:01<08:06,  2.02batch/s, Batch Loss=0.0062, Avg Loss=0.0783, Time Left=8.95 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2411/3393 [20:02<08:06,  2.02batch/s, Batch Loss=0.0127, Avg Loss=0.0783, Time Left=8.94 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2412/3393 [20:02<08:07,  2.01batch/s, Batch Loss=0.0127, Avg Loss=0.0783, Time Left=8.94 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2412/3393 [20:02<08:07,  2.01batch/s, Batch Loss=0.0056, Avg Loss=0.0783, Time Left=8.93 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2413/3393 [20:02<08:13,  1.99batch/s, Batch Loss=0.0056, Avg Loss=0.0783, Time Left=8.93 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2413/3393 [20:03<08:13,  1.99batch/s, Batch Loss=0.0020, Avg Loss=0.0782, Time Left=8.93 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2414/3393 [20:03<08:20,  1.96batch/s, Batch Loss=0.0020, Avg Loss=0.0782, Time Left=8.93 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2414/3393 [20:03<08:20,  1.96batch/s, Batch Loss=0.0217, Avg Loss=0.0782, Time Left=8.92 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2415/3393 [20:03<08:41,  1.87batch/s, Batch Loss=0.0217, Avg Loss=0.0782, Time Left=8.92 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2415/3393 [20:04<08:41,  1.87batch/s, Batch Loss=0.0090, Avg Loss=0.0782, Time Left=8.91 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2416/3393 [20:04<08:27,  1.93batch/s, Batch Loss=0.0090, Avg Loss=0.0782, Time Left=8.91 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2416/3393 [20:04<08:27,  1.93batch/s, Batch Loss=0.0298, Avg Loss=0.0782, Time Left=8.90 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2417/3393 [20:04<08:31,  1.91batch/s, Batch Loss=0.0298, Avg Loss=0.0782, Time Left=8.90 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2417/3393 [20:05<08:31,  1.91batch/s, Batch Loss=0.0023, Avg Loss=0.0781, Time Left=8.89 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2418/3393 [20:05<08:23,  1.94batch/s, Batch Loss=0.0023, Avg Loss=0.0781, Time Left=8.89 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2418/3393 [20:05<08:23,  1.94batch/s, Batch Loss=0.0208, Avg Loss=0.0781, Time Left=8.88 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2419/3393 [20:05<08:37,  1.88batch/s, Batch Loss=0.0208, Avg Loss=0.0781, Time Left=8.88 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2419/3393 [20:06<08:37,  1.88batch/s, Batch Loss=0.4362, Avg Loss=0.0783, Time Left=8.87 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2420/3393 [20:06<08:23,  1.93batch/s, Batch Loss=0.4362, Avg Loss=0.0783, Time Left=8.87 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2420/3393 [20:06<08:23,  1.93batch/s, Batch Loss=0.0025, Avg Loss=0.0782, Time Left=8.87 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2421/3393 [20:06<08:22,  1.93batch/s, Batch Loss=0.0025, Avg Loss=0.0782, Time Left=8.87 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2421/3393 [20:07<08:22,  1.93batch/s, Batch Loss=0.0616, Avg Loss=0.0782, Time Left=8.86 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2422/3393 [20:07<08:08,  1.99batch/s, Batch Loss=0.0616, Avg Loss=0.0782, Time Left=8.86 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2422/3393 [20:07<08:08,  1.99batch/s, Batch Loss=0.0514, Avg Loss=0.0782, Time Left=8.85 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2423/3393 [20:07<08:15,  1.96batch/s, Batch Loss=0.0514, Avg Loss=0.0782, Time Left=8.85 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2423/3393 [20:08<08:15,  1.96batch/s, Batch Loss=0.0607, Avg Loss=0.0782, Time Left=8.84 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2424/3393 [20:08<08:12,  1.97batch/s, Batch Loss=0.0607, Avg Loss=0.0782, Time Left=8.84 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2424/3393 [20:08<08:12,  1.97batch/s, Batch Loss=0.0029, Avg Loss=0.0782, Time Left=8.83 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2425/3393 [20:09<08:18,  1.94batch/s, Batch Loss=0.0029, Avg Loss=0.0782, Time Left=8.83 \u001b[A\n",
      "Epoch 3/3 - Training:  71%|▋| 2425/3393 [20:09<08:18,  1.94batch/s, Batch Loss=0.0524, Avg Loss=0.0782, Time Left=8.82 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2426/3393 [20:09<08:12,  1.96batch/s, Batch Loss=0.0524, Avg Loss=0.0782, Time Left=8.82 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2426/3393 [20:10<08:12,  1.96batch/s, Batch Loss=0.0255, Avg Loss=0.0781, Time Left=8.81 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2427/3393 [20:10<08:11,  1.96batch/s, Batch Loss=0.0255, Avg Loss=0.0781, Time Left=8.81 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2427/3393 [20:10<08:11,  1.96batch/s, Batch Loss=0.0830, Avg Loss=0.0781, Time Left=8.81 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2428/3393 [20:10<07:59,  2.01batch/s, Batch Loss=0.0830, Avg Loss=0.0781, Time Left=8.81 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2428/3393 [20:10<07:59,  2.01batch/s, Batch Loss=0.0087, Avg Loss=0.0781, Time Left=8.80 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2429/3393 [20:10<08:03,  1.99batch/s, Batch Loss=0.0087, Avg Loss=0.0781, Time Left=8.80 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2429/3393 [20:11<08:03,  1.99batch/s, Batch Loss=0.0087, Avg Loss=0.0781, Time Left=8.79 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2430/3393 [20:11<07:58,  2.01batch/s, Batch Loss=0.0087, Avg Loss=0.0781, Time Left=8.79 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2430/3393 [20:11<07:58,  2.01batch/s, Batch Loss=0.0115, Avg Loss=0.0780, Time Left=8.78 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2431/3393 [20:11<07:59,  2.01batch/s, Batch Loss=0.0115, Avg Loss=0.0780, Time Left=8.78 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2431/3393 [20:12<07:59,  2.01batch/s, Batch Loss=0.0025, Avg Loss=0.0780, Time Left=8.77 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2432/3393 [20:12<08:07,  1.97batch/s, Batch Loss=0.0025, Avg Loss=0.0780, Time Left=8.77 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2432/3393 [20:13<08:07,  1.97batch/s, Batch Loss=0.1046, Avg Loss=0.0780, Time Left=8.76 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2433/3393 [20:13<08:06,  1.97batch/s, Batch Loss=0.1046, Avg Loss=0.0780, Time Left=8.76 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2433/3393 [20:13<08:06,  1.97batch/s, Batch Loss=0.1138, Avg Loss=0.0780, Time Left=8.75 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2434/3393 [20:13<08:04,  1.98batch/s, Batch Loss=0.1138, Avg Loss=0.0780, Time Left=8.75 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2434/3393 [20:13<08:04,  1.98batch/s, Batch Loss=0.0009, Avg Loss=0.0780, Time Left=8.75 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2435/3393 [20:13<07:57,  2.01batch/s, Batch Loss=0.0009, Avg Loss=0.0780, Time Left=8.75 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2435/3393 [20:14<07:57,  2.01batch/s, Batch Loss=0.1882, Avg Loss=0.0781, Time Left=8.74 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2436/3393 [20:14<08:05,  1.97batch/s, Batch Loss=0.1882, Avg Loss=0.0781, Time Left=8.74 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2436/3393 [20:15<08:05,  1.97batch/s, Batch Loss=0.0033, Avg Loss=0.0780, Time Left=8.73 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2437/3393 [20:15<08:04,  1.97batch/s, Batch Loss=0.0033, Avg Loss=0.0780, Time Left=8.73 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2437/3393 [20:15<08:04,  1.97batch/s, Batch Loss=0.0246, Avg Loss=0.0780, Time Left=8.72 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2438/3393 [20:15<08:11,  1.94batch/s, Batch Loss=0.0246, Avg Loss=0.0780, Time Left=8.72 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2438/3393 [20:16<08:11,  1.94batch/s, Batch Loss=0.0536, Avg Loss=0.0780, Time Left=8.71 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2439/3393 [20:16<08:06,  1.96batch/s, Batch Loss=0.0536, Avg Loss=0.0780, Time Left=8.71 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2439/3393 [20:16<08:06,  1.96batch/s, Batch Loss=0.1598, Avg Loss=0.0780, Time Left=8.70 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  72%|▋| 2440/3393 [20:16<07:58,  1.99batch/s, Batch Loss=0.1598, Avg Loss=0.0780, Time Left=8.70 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2440/3393 [20:17<07:58,  1.99batch/s, Batch Loss=0.0015, Avg Loss=0.0780, Time Left=8.69 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2441/3393 [20:17<08:02,  1.97batch/s, Batch Loss=0.0015, Avg Loss=0.0780, Time Left=8.69 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2441/3393 [20:17<08:02,  1.97batch/s, Batch Loss=0.0838, Avg Loss=0.0780, Time Left=8.69 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2442/3393 [20:17<08:08,  1.95batch/s, Batch Loss=0.0838, Avg Loss=0.0780, Time Left=8.69 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2442/3393 [20:18<08:08,  1.95batch/s, Batch Loss=0.0852, Avg Loss=0.0780, Time Left=8.68 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2443/3393 [20:18<08:05,  1.96batch/s, Batch Loss=0.0852, Avg Loss=0.0780, Time Left=8.68 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2443/3393 [20:18<08:05,  1.96batch/s, Batch Loss=0.0574, Avg Loss=0.0780, Time Left=8.67 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2444/3393 [20:18<08:06,  1.95batch/s, Batch Loss=0.0574, Avg Loss=0.0780, Time Left=8.67 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2444/3393 [20:19<08:06,  1.95batch/s, Batch Loss=0.0056, Avg Loss=0.0780, Time Left=8.66 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2445/3393 [20:19<08:07,  1.95batch/s, Batch Loss=0.0056, Avg Loss=0.0780, Time Left=8.66 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2445/3393 [20:19<08:07,  1.95batch/s, Batch Loss=0.3171, Avg Loss=0.0781, Time Left=8.65 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2446/3393 [20:19<08:07,  1.94batch/s, Batch Loss=0.3171, Avg Loss=0.0781, Time Left=8.65 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2446/3393 [20:20<08:07,  1.94batch/s, Batch Loss=0.0304, Avg Loss=0.0780, Time Left=8.64 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2447/3393 [20:20<07:58,  1.98batch/s, Batch Loss=0.0304, Avg Loss=0.0780, Time Left=8.64 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2447/3393 [20:20<07:58,  1.98batch/s, Batch Loss=0.0945, Avg Loss=0.0780, Time Left=8.63 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2448/3393 [20:20<08:05,  1.95batch/s, Batch Loss=0.0945, Avg Loss=0.0780, Time Left=8.63 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2448/3393 [20:21<08:05,  1.95batch/s, Batch Loss=0.0182, Avg Loss=0.0780, Time Left=8.63 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2449/3393 [20:21<08:01,  1.96batch/s, Batch Loss=0.0182, Avg Loss=0.0780, Time Left=8.63 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2449/3393 [20:21<08:01,  1.96batch/s, Batch Loss=0.1514, Avg Loss=0.0781, Time Left=8.62 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2450/3393 [20:21<08:02,  1.95batch/s, Batch Loss=0.1514, Avg Loss=0.0781, Time Left=8.62 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2450/3393 [20:22<08:02,  1.95batch/s, Batch Loss=0.0032, Avg Loss=0.0780, Time Left=8.61 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2451/3393 [20:22<07:59,  1.97batch/s, Batch Loss=0.0032, Avg Loss=0.0780, Time Left=8.61 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2451/3393 [20:22<07:59,  1.97batch/s, Batch Loss=0.0605, Avg Loss=0.0780, Time Left=8.60 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2452/3393 [20:22<08:05,  1.94batch/s, Batch Loss=0.0605, Avg Loss=0.0780, Time Left=8.60 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2452/3393 [20:23<08:05,  1.94batch/s, Batch Loss=0.0954, Avg Loss=0.0780, Time Left=8.59 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2453/3393 [20:23<07:56,  1.97batch/s, Batch Loss=0.0954, Avg Loss=0.0780, Time Left=8.59 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2453/3393 [20:23<07:56,  1.97batch/s, Batch Loss=0.2988, Avg Loss=0.0781, Time Left=8.58 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2454/3393 [20:23<08:03,  1.94batch/s, Batch Loss=0.2988, Avg Loss=0.0781, Time Left=8.58 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2454/3393 [20:24<08:03,  1.94batch/s, Batch Loss=0.0012, Avg Loss=0.0781, Time Left=8.57 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2455/3393 [20:24<07:58,  1.96batch/s, Batch Loss=0.0012, Avg Loss=0.0781, Time Left=8.57 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2455/3393 [20:24<07:58,  1.96batch/s, Batch Loss=0.0044, Avg Loss=0.0780, Time Left=8.57 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2456/3393 [20:24<08:04,  1.93batch/s, Batch Loss=0.0044, Avg Loss=0.0780, Time Left=8.57 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2456/3393 [20:25<08:04,  1.93batch/s, Batch Loss=0.0052, Avg Loss=0.0780, Time Left=8.56 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2457/3393 [20:25<07:59,  1.95batch/s, Batch Loss=0.0052, Avg Loss=0.0780, Time Left=8.56 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2457/3393 [20:25<07:59,  1.95batch/s, Batch Loss=0.0271, Avg Loss=0.0780, Time Left=8.55 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2458/3393 [20:25<07:59,  1.95batch/s, Batch Loss=0.0271, Avg Loss=0.0780, Time Left=8.55 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2458/3393 [20:26<07:59,  1.95batch/s, Batch Loss=0.0017, Avg Loss=0.0780, Time Left=8.54 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2459/3393 [20:26<07:52,  1.98batch/s, Batch Loss=0.0017, Avg Loss=0.0780, Time Left=8.54 \u001b[A\n",
      "Epoch 3/3 - Training:  72%|▋| 2459/3393 [20:26<07:52,  1.98batch/s, Batch Loss=0.2625, Avg Loss=0.0780, Time Left=8.53 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2460/3393 [20:26<07:54,  1.97batch/s, Batch Loss=0.2625, Avg Loss=0.0780, Time Left=8.53 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2460/3393 [20:27<07:54,  1.97batch/s, Batch Loss=0.0677, Avg Loss=0.0780, Time Left=8.52 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2461/3393 [20:27<07:42,  2.01batch/s, Batch Loss=0.0677, Avg Loss=0.0780, Time Left=8.52 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2461/3393 [20:27<07:42,  2.01batch/s, Batch Loss=0.1026, Avg Loss=0.0780, Time Left=8.51 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2462/3393 [20:27<07:41,  2.02batch/s, Batch Loss=0.1026, Avg Loss=0.0780, Time Left=8.51 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2462/3393 [20:28<07:41,  2.02batch/s, Batch Loss=0.1255, Avg Loss=0.0781, Time Left=8.51 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2463/3393 [20:28<07:52,  1.97batch/s, Batch Loss=0.1255, Avg Loss=0.0781, Time Left=8.51 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2463/3393 [20:28<07:52,  1.97batch/s, Batch Loss=0.0752, Avg Loss=0.0781, Time Left=8.50 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2464/3393 [20:28<07:45,  1.99batch/s, Batch Loss=0.0752, Avg Loss=0.0781, Time Left=8.50 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2464/3393 [20:29<07:45,  1.99batch/s, Batch Loss=0.0752, Avg Loss=0.0781, Time Left=8.49 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2465/3393 [20:29<07:49,  1.98batch/s, Batch Loss=0.0752, Avg Loss=0.0781, Time Left=8.49 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2465/3393 [20:29<07:49,  1.98batch/s, Batch Loss=0.0097, Avg Loss=0.0780, Time Left=8.48 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2466/3393 [20:29<07:48,  1.98batch/s, Batch Loss=0.0097, Avg Loss=0.0780, Time Left=8.48 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2466/3393 [20:30<07:48,  1.98batch/s, Batch Loss=0.0054, Avg Loss=0.0780, Time Left=8.47 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2467/3393 [20:30<07:50,  1.97batch/s, Batch Loss=0.0054, Avg Loss=0.0780, Time Left=8.47 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2467/3393 [20:30<07:50,  1.97batch/s, Batch Loss=0.0040, Avg Loss=0.0780, Time Left=8.46 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2468/3393 [20:30<07:43,  2.00batch/s, Batch Loss=0.0040, Avg Loss=0.0780, Time Left=8.46 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2468/3393 [20:31<07:43,  2.00batch/s, Batch Loss=0.0033, Avg Loss=0.0779, Time Left=8.45 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2469/3393 [20:31<07:47,  1.98batch/s, Batch Loss=0.0033, Avg Loss=0.0779, Time Left=8.45 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2469/3393 [20:31<07:47,  1.98batch/s, Batch Loss=0.0147, Avg Loss=0.0779, Time Left=8.45 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2470/3393 [20:31<07:41,  2.00batch/s, Batch Loss=0.0147, Avg Loss=0.0779, Time Left=8.45 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2470/3393 [20:32<07:41,  2.00batch/s, Batch Loss=0.0198, Avg Loss=0.0779, Time Left=8.44 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2471/3393 [20:32<07:54,  1.94batch/s, Batch Loss=0.0198, Avg Loss=0.0779, Time Left=8.44 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2471/3393 [20:32<07:54,  1.94batch/s, Batch Loss=0.0190, Avg Loss=0.0779, Time Left=8.43 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2472/3393 [20:32<07:45,  1.98batch/s, Batch Loss=0.0190, Avg Loss=0.0779, Time Left=8.43 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2472/3393 [20:33<07:45,  1.98batch/s, Batch Loss=0.0407, Avg Loss=0.0779, Time Left=8.42 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  73%|▋| 2473/3393 [20:33<07:47,  1.97batch/s, Batch Loss=0.0407, Avg Loss=0.0779, Time Left=8.42 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2473/3393 [20:33<07:47,  1.97batch/s, Batch Loss=0.0722, Avg Loss=0.0779, Time Left=8.41 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2474/3393 [20:33<07:36,  2.01batch/s, Batch Loss=0.0722, Avg Loss=0.0779, Time Left=8.41 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2474/3393 [20:34<07:36,  2.01batch/s, Batch Loss=0.0315, Avg Loss=0.0778, Time Left=8.40 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2475/3393 [20:34<07:36,  2.01batch/s, Batch Loss=0.0315, Avg Loss=0.0778, Time Left=8.40 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2475/3393 [20:34<07:36,  2.01batch/s, Batch Loss=0.1018, Avg Loss=0.0778, Time Left=8.39 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2476/3393 [20:34<07:41,  1.99batch/s, Batch Loss=0.1018, Avg Loss=0.0778, Time Left=8.39 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2476/3393 [20:35<07:41,  1.99batch/s, Batch Loss=0.0522, Avg Loss=0.0778, Time Left=8.39 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2477/3393 [20:35<07:40,  1.99batch/s, Batch Loss=0.0522, Avg Loss=0.0778, Time Left=8.39 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2477/3393 [20:35<07:40,  1.99batch/s, Batch Loss=0.1709, Avg Loss=0.0779, Time Left=8.38 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2478/3393 [20:35<07:51,  1.94batch/s, Batch Loss=0.1709, Avg Loss=0.0779, Time Left=8.38 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2478/3393 [20:36<07:51,  1.94batch/s, Batch Loss=0.0855, Avg Loss=0.0779, Time Left=8.37 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2479/3393 [20:36<07:47,  1.95batch/s, Batch Loss=0.0855, Avg Loss=0.0779, Time Left=8.37 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2479/3393 [20:36<07:47,  1.95batch/s, Batch Loss=0.0584, Avg Loss=0.0779, Time Left=8.36 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2480/3393 [20:36<07:49,  1.95batch/s, Batch Loss=0.0584, Avg Loss=0.0779, Time Left=8.36 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2480/3393 [20:37<07:49,  1.95batch/s, Batch Loss=0.0109, Avg Loss=0.0778, Time Left=8.35 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2481/3393 [20:37<07:45,  1.96batch/s, Batch Loss=0.0109, Avg Loss=0.0778, Time Left=8.35 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2481/3393 [20:37<07:45,  1.96batch/s, Batch Loss=0.0310, Avg Loss=0.0778, Time Left=8.34 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2482/3393 [20:37<07:41,  1.97batch/s, Batch Loss=0.0310, Avg Loss=0.0778, Time Left=8.34 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2482/3393 [20:38<07:41,  1.97batch/s, Batch Loss=0.0251, Avg Loss=0.0778, Time Left=8.33 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2483/3393 [20:38<07:35,  2.00batch/s, Batch Loss=0.0251, Avg Loss=0.0778, Time Left=8.33 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2483/3393 [20:38<07:35,  2.00batch/s, Batch Loss=0.5612, Avg Loss=0.0780, Time Left=8.33 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2484/3393 [20:38<07:38,  1.98batch/s, Batch Loss=0.5612, Avg Loss=0.0780, Time Left=8.33 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2484/3393 [20:39<07:38,  1.98batch/s, Batch Loss=0.2284, Avg Loss=0.0781, Time Left=8.32 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2485/3393 [20:39<07:37,  1.98batch/s, Batch Loss=0.2284, Avg Loss=0.0781, Time Left=8.32 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2485/3393 [20:39<07:37,  1.98batch/s, Batch Loss=0.0633, Avg Loss=0.0781, Time Left=8.31 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2486/3393 [20:39<07:44,  1.95batch/s, Batch Loss=0.0633, Avg Loss=0.0781, Time Left=8.31 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2486/3393 [20:40<07:44,  1.95batch/s, Batch Loss=0.2495, Avg Loss=0.0781, Time Left=8.30 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2487/3393 [20:40<07:36,  1.98batch/s, Batch Loss=0.2495, Avg Loss=0.0781, Time Left=8.30 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2487/3393 [20:40<07:36,  1.98batch/s, Batch Loss=0.0086, Avg Loss=0.0781, Time Left=8.29 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2488/3393 [20:40<07:44,  1.95batch/s, Batch Loss=0.0086, Avg Loss=0.0781, Time Left=8.29 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2488/3393 [20:41<07:44,  1.95batch/s, Batch Loss=0.0532, Avg Loss=0.0781, Time Left=8.28 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2489/3393 [20:41<07:36,  1.98batch/s, Batch Loss=0.0532, Avg Loss=0.0781, Time Left=8.28 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2489/3393 [20:41<07:36,  1.98batch/s, Batch Loss=0.0936, Avg Loss=0.0781, Time Left=8.27 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2490/3393 [20:41<07:42,  1.95batch/s, Batch Loss=0.0936, Avg Loss=0.0781, Time Left=8.27 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2490/3393 [20:42<07:42,  1.95batch/s, Batch Loss=0.1163, Avg Loss=0.0781, Time Left=8.27 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2491/3393 [20:42<07:35,  1.98batch/s, Batch Loss=0.1163, Avg Loss=0.0781, Time Left=8.27 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2491/3393 [20:42<07:35,  1.98batch/s, Batch Loss=0.0788, Avg Loss=0.0781, Time Left=8.26 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2492/3393 [20:42<07:37,  1.97batch/s, Batch Loss=0.0788, Avg Loss=0.0781, Time Left=8.26 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2492/3393 [20:43<07:37,  1.97batch/s, Batch Loss=0.1075, Avg Loss=0.0781, Time Left=8.25 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2493/3393 [20:43<07:26,  2.01batch/s, Batch Loss=0.1075, Avg Loss=0.0781, Time Left=8.25 \u001b[A\n",
      "Epoch 3/3 - Training:  73%|▋| 2493/3393 [20:43<07:26,  2.01batch/s, Batch Loss=0.0339, Avg Loss=0.0781, Time Left=8.24 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2494/3393 [20:43<07:27,  2.01batch/s, Batch Loss=0.0339, Avg Loss=0.0781, Time Left=8.24 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2494/3393 [20:44<07:27,  2.01batch/s, Batch Loss=0.1018, Avg Loss=0.0781, Time Left=8.23 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2495/3393 [20:44<07:27,  2.01batch/s, Batch Loss=0.1018, Avg Loss=0.0781, Time Left=8.23 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2495/3393 [20:44<07:27,  2.01batch/s, Batch Loss=0.0687, Avg Loss=0.0781, Time Left=8.22 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2496/3393 [20:44<07:27,  2.01batch/s, Batch Loss=0.0687, Avg Loss=0.0781, Time Left=8.22 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2496/3393 [20:45<07:27,  2.01batch/s, Batch Loss=0.0405, Avg Loss=0.0781, Time Left=8.21 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2497/3393 [20:45<07:31,  1.99batch/s, Batch Loss=0.0405, Avg Loss=0.0781, Time Left=8.21 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2497/3393 [20:45<07:31,  1.99batch/s, Batch Loss=0.0324, Avg Loss=0.0781, Time Left=8.21 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2498/3393 [20:45<07:31,  1.98batch/s, Batch Loss=0.0324, Avg Loss=0.0781, Time Left=8.21 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2498/3393 [20:46<07:31,  1.98batch/s, Batch Loss=0.0997, Avg Loss=0.0781, Time Left=8.20 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2499/3393 [20:46<07:32,  1.97batch/s, Batch Loss=0.0997, Avg Loss=0.0781, Time Left=8.20 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2499/3393 [20:46<07:32,  1.97batch/s, Batch Loss=0.0769, Avg Loss=0.0781, Time Left=8.19 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2500/3393 [20:46<07:31,  1.98batch/s, Batch Loss=0.0769, Avg Loss=0.0781, Time Left=8.19 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2500/3393 [20:47<07:31,  1.98batch/s, Batch Loss=0.0201, Avg Loss=0.0781, Time Left=8.18 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2501/3393 [20:47<07:33,  1.97batch/s, Batch Loss=0.0201, Avg Loss=0.0781, Time Left=8.18 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2501/3393 [20:48<07:33,  1.97batch/s, Batch Loss=0.0422, Avg Loss=0.0780, Time Left=8.17 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2502/3393 [20:48<07:35,  1.96batch/s, Batch Loss=0.0422, Avg Loss=0.0780, Time Left=8.17 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2502/3393 [20:48<07:35,  1.96batch/s, Batch Loss=0.0182, Avg Loss=0.0780, Time Left=8.16 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2503/3393 [20:48<07:41,  1.93batch/s, Batch Loss=0.0182, Avg Loss=0.0780, Time Left=8.16 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2503/3393 [20:49<07:41,  1.93batch/s, Batch Loss=0.0709, Avg Loss=0.0780, Time Left=8.15 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2504/3393 [20:49<07:40,  1.93batch/s, Batch Loss=0.0709, Avg Loss=0.0780, Time Left=8.15 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2504/3393 [20:49<07:40,  1.93batch/s, Batch Loss=0.0047, Avg Loss=0.0780, Time Left=8.15 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2505/3393 [20:49<07:35,  1.95batch/s, Batch Loss=0.0047, Avg Loss=0.0780, Time Left=8.15 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2505/3393 [20:50<07:35,  1.95batch/s, Batch Loss=0.0322, Avg Loss=0.0780, Time Left=8.14 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  74%|▋| 2506/3393 [20:50<07:31,  1.96batch/s, Batch Loss=0.0322, Avg Loss=0.0780, Time Left=8.14 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2506/3393 [20:50<07:31,  1.96batch/s, Batch Loss=0.0108, Avg Loss=0.0779, Time Left=8.13 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2507/3393 [20:50<07:37,  1.94batch/s, Batch Loss=0.0108, Avg Loss=0.0779, Time Left=8.13 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2507/3393 [20:51<07:37,  1.94batch/s, Batch Loss=0.1302, Avg Loss=0.0780, Time Left=8.12 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2508/3393 [20:51<07:33,  1.95batch/s, Batch Loss=0.1302, Avg Loss=0.0780, Time Left=8.12 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2508/3393 [20:51<07:33,  1.95batch/s, Batch Loss=0.0086, Avg Loss=0.0779, Time Left=8.11 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2509/3393 [20:51<07:37,  1.93batch/s, Batch Loss=0.0086, Avg Loss=0.0779, Time Left=8.11 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2509/3393 [20:52<07:37,  1.93batch/s, Batch Loss=0.1679, Avg Loss=0.0780, Time Left=8.10 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2510/3393 [20:52<07:28,  1.97batch/s, Batch Loss=0.1679, Avg Loss=0.0780, Time Left=8.10 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2510/3393 [20:52<07:28,  1.97batch/s, Batch Loss=0.2113, Avg Loss=0.0780, Time Left=8.09 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2511/3393 [20:52<07:38,  1.92batch/s, Batch Loss=0.2113, Avg Loss=0.0780, Time Left=8.09 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2511/3393 [20:53<07:38,  1.92batch/s, Batch Loss=0.0117, Avg Loss=0.0780, Time Left=8.09 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2512/3393 [20:53<07:29,  1.96batch/s, Batch Loss=0.0117, Avg Loss=0.0780, Time Left=8.09 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2512/3393 [20:53<07:29,  1.96batch/s, Batch Loss=0.3029, Avg Loss=0.0781, Time Left=8.08 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2513/3393 [20:53<07:34,  1.94batch/s, Batch Loss=0.3029, Avg Loss=0.0781, Time Left=8.08 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2513/3393 [20:54<07:34,  1.94batch/s, Batch Loss=0.1093, Avg Loss=0.0781, Time Left=8.07 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2514/3393 [20:54<07:25,  1.97batch/s, Batch Loss=0.1093, Avg Loss=0.0781, Time Left=8.07 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2514/3393 [20:54<07:25,  1.97batch/s, Batch Loss=0.0067, Avg Loss=0.0781, Time Left=8.06 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2515/3393 [20:54<07:27,  1.96batch/s, Batch Loss=0.0067, Avg Loss=0.0781, Time Left=8.06 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2515/3393 [20:55<07:27,  1.96batch/s, Batch Loss=0.0389, Avg Loss=0.0781, Time Left=8.05 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2516/3393 [20:55<07:24,  1.97batch/s, Batch Loss=0.0389, Avg Loss=0.0781, Time Left=8.05 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2516/3393 [20:55<07:24,  1.97batch/s, Batch Loss=0.0129, Avg Loss=0.0780, Time Left=8.04 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2517/3393 [20:55<07:27,  1.96batch/s, Batch Loss=0.0129, Avg Loss=0.0780, Time Left=8.04 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2517/3393 [20:56<07:27,  1.96batch/s, Batch Loss=0.1535, Avg Loss=0.0781, Time Left=8.03 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2518/3393 [20:56<07:23,  1.97batch/s, Batch Loss=0.1535, Avg Loss=0.0781, Time Left=8.03 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2518/3393 [20:56<07:23,  1.97batch/s, Batch Loss=0.0077, Avg Loss=0.0780, Time Left=8.03 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2519/3393 [20:56<07:21,  1.98batch/s, Batch Loss=0.0077, Avg Loss=0.0780, Time Left=8.03 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2519/3393 [20:57<07:21,  1.98batch/s, Batch Loss=0.0275, Avg Loss=0.0780, Time Left=8.02 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2520/3393 [20:57<07:15,  2.00batch/s, Batch Loss=0.0275, Avg Loss=0.0780, Time Left=8.02 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2520/3393 [20:57<07:15,  2.00batch/s, Batch Loss=0.0012, Avg Loss=0.0780, Time Left=8.01 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2521/3393 [20:57<07:11,  2.02batch/s, Batch Loss=0.0012, Avg Loss=0.0780, Time Left=8.01 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2521/3393 [20:58<07:11,  2.02batch/s, Batch Loss=0.0177, Avg Loss=0.0780, Time Left=8.00 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2522/3393 [20:58<07:12,  2.01batch/s, Batch Loss=0.0177, Avg Loss=0.0780, Time Left=8.00 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2522/3393 [20:58<07:12,  2.01batch/s, Batch Loss=0.0695, Avg Loss=0.0780, Time Left=7.99 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2523/3393 [20:58<07:12,  2.01batch/s, Batch Loss=0.0695, Avg Loss=0.0780, Time Left=7.99 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2523/3393 [20:59<07:12,  2.01batch/s, Batch Loss=0.0811, Avg Loss=0.0780, Time Left=7.98 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2524/3393 [20:59<07:12,  2.01batch/s, Batch Loss=0.0811, Avg Loss=0.0780, Time Left=7.98 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2524/3393 [20:59<07:12,  2.01batch/s, Batch Loss=0.0589, Avg Loss=0.0779, Time Left=7.97 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2525/3393 [20:59<07:00,  2.06batch/s, Batch Loss=0.0589, Avg Loss=0.0779, Time Left=7.97 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2525/3393 [21:00<07:00,  2.06batch/s, Batch Loss=0.1869, Avg Loss=0.0780, Time Left=7.97 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2526/3393 [21:00<07:07,  2.03batch/s, Batch Loss=0.1869, Avg Loss=0.0780, Time Left=7.97 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2526/3393 [21:00<07:07,  2.03batch/s, Batch Loss=0.1338, Avg Loss=0.0780, Time Left=7.96 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2527/3393 [21:00<07:00,  2.06batch/s, Batch Loss=0.1338, Avg Loss=0.0780, Time Left=7.96 \u001b[A\n",
      "Epoch 3/3 - Training:  74%|▋| 2527/3393 [21:01<07:00,  2.06batch/s, Batch Loss=0.0033, Avg Loss=0.0780, Time Left=7.95 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2528/3393 [21:01<07:03,  2.04batch/s, Batch Loss=0.0033, Avg Loss=0.0780, Time Left=7.95 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2528/3393 [21:01<07:03,  2.04batch/s, Batch Loss=0.2710, Avg Loss=0.0781, Time Left=7.94 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2529/3393 [21:01<07:14,  1.99batch/s, Batch Loss=0.2710, Avg Loss=0.0781, Time Left=7.94 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2529/3393 [21:02<07:14,  1.99batch/s, Batch Loss=0.0961, Avg Loss=0.0781, Time Left=7.93 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2530/3393 [21:02<07:09,  2.01batch/s, Batch Loss=0.0961, Avg Loss=0.0781, Time Left=7.93 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2530/3393 [21:02<07:09,  2.01batch/s, Batch Loss=0.0665, Avg Loss=0.0781, Time Left=7.92 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2531/3393 [21:02<07:18,  1.97batch/s, Batch Loss=0.0665, Avg Loss=0.0781, Time Left=7.92 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2531/3393 [21:03<07:18,  1.97batch/s, Batch Loss=0.0398, Avg Loss=0.0780, Time Left=7.91 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2532/3393 [21:03<07:19,  1.96batch/s, Batch Loss=0.0398, Avg Loss=0.0780, Time Left=7.91 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2532/3393 [21:03<07:19,  1.96batch/s, Batch Loss=0.0752, Avg Loss=0.0780, Time Left=7.91 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2533/3393 [21:03<07:20,  1.95batch/s, Batch Loss=0.0752, Avg Loss=0.0780, Time Left=7.91 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2533/3393 [21:04<07:20,  1.95batch/s, Batch Loss=0.0179, Avg Loss=0.0780, Time Left=7.90 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2534/3393 [21:04<07:17,  1.96batch/s, Batch Loss=0.0179, Avg Loss=0.0780, Time Left=7.90 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2534/3393 [21:04<07:17,  1.96batch/s, Batch Loss=0.0117, Avg Loss=0.0780, Time Left=7.89 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2535/3393 [21:04<07:18,  1.96batch/s, Batch Loss=0.0117, Avg Loss=0.0780, Time Left=7.89 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2535/3393 [21:05<07:18,  1.96batch/s, Batch Loss=0.0035, Avg Loss=0.0780, Time Left=7.88 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2536/3393 [21:05<07:19,  1.95batch/s, Batch Loss=0.0035, Avg Loss=0.0780, Time Left=7.88 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2536/3393 [21:05<07:19,  1.95batch/s, Batch Loss=0.0476, Avg Loss=0.0780, Time Left=7.87 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2537/3393 [21:05<07:28,  1.91batch/s, Batch Loss=0.0476, Avg Loss=0.0780, Time Left=7.87 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2537/3393 [21:06<07:28,  1.91batch/s, Batch Loss=0.0522, Avg Loss=0.0779, Time Left=7.86 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2538/3393 [21:06<07:22,  1.93batch/s, Batch Loss=0.0522, Avg Loss=0.0779, Time Left=7.86 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2538/3393 [21:06<07:22,  1.93batch/s, Batch Loss=0.0282, Avg Loss=0.0779, Time Left=7.85 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  75%|▋| 2539/3393 [21:06<07:21,  1.93batch/s, Batch Loss=0.0282, Avg Loss=0.0779, Time Left=7.85 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2539/3393 [21:07<07:21,  1.93batch/s, Batch Loss=0.1111, Avg Loss=0.0779, Time Left=7.85 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2540/3393 [21:07<07:13,  1.97batch/s, Batch Loss=0.1111, Avg Loss=0.0779, Time Left=7.85 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2540/3393 [21:07<07:13,  1.97batch/s, Batch Loss=0.0386, Avg Loss=0.0779, Time Left=7.84 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2541/3393 [21:07<07:17,  1.95batch/s, Batch Loss=0.0386, Avg Loss=0.0779, Time Left=7.84 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2541/3393 [21:08<07:17,  1.95batch/s, Batch Loss=0.0348, Avg Loss=0.0779, Time Left=7.83 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2542/3393 [21:08<07:15,  1.96batch/s, Batch Loss=0.0348, Avg Loss=0.0779, Time Left=7.83 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2542/3393 [21:08<07:15,  1.96batch/s, Batch Loss=0.0216, Avg Loss=0.0779, Time Left=7.82 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2543/3393 [21:08<07:07,  1.99batch/s, Batch Loss=0.0216, Avg Loss=0.0779, Time Left=7.82 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2543/3393 [21:09<07:07,  1.99batch/s, Batch Loss=0.0085, Avg Loss=0.0779, Time Left=7.81 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2544/3393 [21:09<07:02,  2.01batch/s, Batch Loss=0.0085, Avg Loss=0.0779, Time Left=7.81 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▋| 2544/3393 [21:09<07:02,  2.01batch/s, Batch Loss=0.0855, Avg Loss=0.0779, Time Left=7.80 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2545/3393 [21:09<07:02,  2.01batch/s, Batch Loss=0.0855, Avg Loss=0.0779, Time Left=7.80 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2545/3393 [21:10<07:02,  2.01batch/s, Batch Loss=0.0045, Avg Loss=0.0778, Time Left=7.79 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2546/3393 [21:10<06:54,  2.04batch/s, Batch Loss=0.0045, Avg Loss=0.0778, Time Left=7.79 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2546/3393 [21:10<06:54,  2.04batch/s, Batch Loss=0.0314, Avg Loss=0.0778, Time Left=7.79 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2547/3393 [21:10<06:48,  2.07batch/s, Batch Loss=0.0314, Avg Loss=0.0778, Time Left=7.79 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2547/3393 [21:11<06:48,  2.07batch/s, Batch Loss=0.0356, Avg Loss=0.0778, Time Left=7.78 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2548/3393 [21:11<06:47,  2.07batch/s, Batch Loss=0.0356, Avg Loss=0.0778, Time Left=7.78 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2548/3393 [21:11<06:47,  2.07batch/s, Batch Loss=0.0118, Avg Loss=0.0778, Time Left=7.77 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2549/3393 [21:11<06:48,  2.07batch/s, Batch Loss=0.0118, Avg Loss=0.0778, Time Left=7.77 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2549/3393 [21:12<06:48,  2.07batch/s, Batch Loss=0.0104, Avg Loss=0.0777, Time Left=7.76 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2550/3393 [21:12<06:51,  2.05batch/s, Batch Loss=0.0104, Avg Loss=0.0777, Time Left=7.76 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2550/3393 [21:12<06:51,  2.05batch/s, Batch Loss=0.0069, Avg Loss=0.0777, Time Left=7.75 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2551/3393 [21:12<07:02,  1.99batch/s, Batch Loss=0.0069, Avg Loss=0.0777, Time Left=7.75 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2551/3393 [21:13<07:02,  1.99batch/s, Batch Loss=0.0044, Avg Loss=0.0777, Time Left=7.74 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2552/3393 [21:13<07:01,  1.99batch/s, Batch Loss=0.0044, Avg Loss=0.0777, Time Left=7.74 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2552/3393 [21:13<07:01,  1.99batch/s, Batch Loss=0.0952, Avg Loss=0.0777, Time Left=7.73 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2553/3393 [21:13<07:04,  1.98batch/s, Batch Loss=0.0952, Avg Loss=0.0777, Time Left=7.73 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2553/3393 [21:14<07:04,  1.98batch/s, Batch Loss=0.0976, Avg Loss=0.0777, Time Left=7.72 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2554/3393 [21:14<07:03,  1.98batch/s, Batch Loss=0.0976, Avg Loss=0.0777, Time Left=7.72 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2554/3393 [21:14<07:03,  1.98batch/s, Batch Loss=0.0355, Avg Loss=0.0777, Time Left=7.72 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2555/3393 [21:14<07:01,  1.99batch/s, Batch Loss=0.0355, Avg Loss=0.0777, Time Left=7.72 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2555/3393 [21:15<07:01,  1.99batch/s, Batch Loss=0.1218, Avg Loss=0.0777, Time Left=7.71 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2556/3393 [21:15<07:00,  1.99batch/s, Batch Loss=0.1218, Avg Loss=0.0777, Time Left=7.71 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2556/3393 [21:15<07:00,  1.99batch/s, Batch Loss=0.0033, Avg Loss=0.0777, Time Left=7.70 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2557/3393 [21:15<07:07,  1.96batch/s, Batch Loss=0.0033, Avg Loss=0.0777, Time Left=7.70 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2557/3393 [21:16<07:07,  1.96batch/s, Batch Loss=0.0259, Avg Loss=0.0776, Time Left=7.69 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2558/3393 [21:16<07:04,  1.97batch/s, Batch Loss=0.0259, Avg Loss=0.0776, Time Left=7.69 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2558/3393 [21:16<07:04,  1.97batch/s, Batch Loss=0.0760, Avg Loss=0.0776, Time Left=7.68 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2559/3393 [21:16<07:05,  1.96batch/s, Batch Loss=0.0760, Avg Loss=0.0776, Time Left=7.68 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2559/3393 [21:17<07:05,  1.96batch/s, Batch Loss=0.0376, Avg Loss=0.0776, Time Left=7.67 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2560/3393 [21:17<07:07,  1.95batch/s, Batch Loss=0.0376, Avg Loss=0.0776, Time Left=7.67 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2560/3393 [21:17<07:07,  1.95batch/s, Batch Loss=0.0083, Avg Loss=0.0776, Time Left=7.67 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2561/3393 [21:17<07:07,  1.95batch/s, Batch Loss=0.0083, Avg Loss=0.0776, Time Left=7.67 \u001b[A\n",
      "Epoch 3/3 - Training:  75%|▊| 2561/3393 [21:18<07:07,  1.95batch/s, Batch Loss=0.0010, Avg Loss=0.0776, Time Left=7.66 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2562/3393 [21:18<07:02,  1.97batch/s, Batch Loss=0.0010, Avg Loss=0.0776, Time Left=7.66 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2562/3393 [21:18<07:02,  1.97batch/s, Batch Loss=0.0612, Avg Loss=0.0776, Time Left=7.65 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2563/3393 [21:18<07:05,  1.95batch/s, Batch Loss=0.0612, Avg Loss=0.0776, Time Left=7.65 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2563/3393 [21:19<07:05,  1.95batch/s, Batch Loss=0.0087, Avg Loss=0.0775, Time Left=7.64 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2564/3393 [21:19<07:02,  1.96batch/s, Batch Loss=0.0087, Avg Loss=0.0775, Time Left=7.64 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2564/3393 [21:19<07:02,  1.96batch/s, Batch Loss=0.0067, Avg Loss=0.0775, Time Left=7.63 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2565/3393 [21:19<07:11,  1.92batch/s, Batch Loss=0.0067, Avg Loss=0.0775, Time Left=7.63 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2565/3393 [21:20<07:11,  1.92batch/s, Batch Loss=0.0111, Avg Loss=0.0775, Time Left=7.62 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2566/3393 [21:20<06:58,  1.98batch/s, Batch Loss=0.0111, Avg Loss=0.0775, Time Left=7.62 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2566/3393 [21:20<06:58,  1.98batch/s, Batch Loss=0.0459, Avg Loss=0.0775, Time Left=7.61 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2567/3393 [21:20<07:04,  1.95batch/s, Batch Loss=0.0459, Avg Loss=0.0775, Time Left=7.61 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2567/3393 [21:21<07:04,  1.95batch/s, Batch Loss=0.0015, Avg Loss=0.0774, Time Left=7.61 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2568/3393 [21:21<06:57,  1.97batch/s, Batch Loss=0.0015, Avg Loss=0.0774, Time Left=7.61 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2568/3393 [21:21<06:57,  1.97batch/s, Batch Loss=0.0021, Avg Loss=0.0774, Time Left=7.60 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2569/3393 [21:21<07:02,  1.95batch/s, Batch Loss=0.0021, Avg Loss=0.0774, Time Left=7.60 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2569/3393 [21:22<07:02,  1.95batch/s, Batch Loss=0.0034, Avg Loss=0.0774, Time Left=7.59 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2570/3393 [21:22<06:55,  1.98batch/s, Batch Loss=0.0034, Avg Loss=0.0774, Time Left=7.59 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2570/3393 [21:22<06:55,  1.98batch/s, Batch Loss=0.0022, Avg Loss=0.0773, Time Left=7.58 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2571/3393 [21:22<06:53,  1.99batch/s, Batch Loss=0.0022, Avg Loss=0.0773, Time Left=7.58 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2571/3393 [21:23<06:53,  1.99batch/s, Batch Loss=0.0680, Avg Loss=0.0773, Time Left=7.57 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  76%|▊| 2572/3393 [21:23<06:49,  2.00batch/s, Batch Loss=0.0680, Avg Loss=0.0773, Time Left=7.57 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2572/3393 [21:23<06:49,  2.00batch/s, Batch Loss=0.2671, Avg Loss=0.0774, Time Left=7.56 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2573/3393 [21:23<06:50,  2.00batch/s, Batch Loss=0.2671, Avg Loss=0.0774, Time Left=7.56 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2573/3393 [21:24<06:50,  2.00batch/s, Batch Loss=0.0189, Avg Loss=0.0774, Time Left=7.55 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2574/3393 [21:24<06:43,  2.03batch/s, Batch Loss=0.0189, Avg Loss=0.0774, Time Left=7.55 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2574/3393 [21:24<06:43,  2.03batch/s, Batch Loss=0.0047, Avg Loss=0.0774, Time Left=7.54 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2575/3393 [21:24<06:48,  2.00batch/s, Batch Loss=0.0047, Avg Loss=0.0774, Time Left=7.54 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2575/3393 [21:25<06:48,  2.00batch/s, Batch Loss=0.0062, Avg Loss=0.0773, Time Left=7.54 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2576/3393 [21:25<06:52,  1.98batch/s, Batch Loss=0.0062, Avg Loss=0.0773, Time Left=7.54 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2576/3393 [21:25<06:52,  1.98batch/s, Batch Loss=0.0581, Avg Loss=0.0773, Time Left=7.53 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2577/3393 [21:25<06:50,  1.99batch/s, Batch Loss=0.0581, Avg Loss=0.0773, Time Left=7.53 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2577/3393 [21:26<06:50,  1.99batch/s, Batch Loss=0.0713, Avg Loss=0.0773, Time Left=7.52 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2578/3393 [21:26<06:53,  1.97batch/s, Batch Loss=0.0713, Avg Loss=0.0773, Time Left=7.52 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2578/3393 [21:26<06:53,  1.97batch/s, Batch Loss=0.0012, Avg Loss=0.0773, Time Left=7.51 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2579/3393 [21:26<06:51,  1.98batch/s, Batch Loss=0.0012, Avg Loss=0.0773, Time Left=7.51 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2579/3393 [21:27<06:51,  1.98batch/s, Batch Loss=0.0239, Avg Loss=0.0773, Time Left=7.50 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2580/3393 [21:27<06:57,  1.95batch/s, Batch Loss=0.0239, Avg Loss=0.0773, Time Left=7.50 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2580/3393 [21:27<06:57,  1.95batch/s, Batch Loss=0.0029, Avg Loss=0.0772, Time Left=7.49 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2581/3393 [21:27<06:54,  1.96batch/s, Batch Loss=0.0029, Avg Loss=0.0772, Time Left=7.49 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2581/3393 [21:28<06:54,  1.96batch/s, Batch Loss=0.1198, Avg Loss=0.0773, Time Left=7.49 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2582/3393 [21:28<06:55,  1.95batch/s, Batch Loss=0.1198, Avg Loss=0.0773, Time Left=7.49 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2582/3393 [21:28<06:55,  1.95batch/s, Batch Loss=0.3759, Avg Loss=0.0774, Time Left=7.48 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2583/3393 [21:28<06:52,  1.96batch/s, Batch Loss=0.3759, Avg Loss=0.0774, Time Left=7.48 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2583/3393 [21:29<06:52,  1.96batch/s, Batch Loss=0.0022, Avg Loss=0.0774, Time Left=7.47 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2584/3393 [21:29<06:56,  1.94batch/s, Batch Loss=0.0022, Avg Loss=0.0774, Time Left=7.47 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2584/3393 [21:30<06:56,  1.94batch/s, Batch Loss=0.2682, Avg Loss=0.0774, Time Left=7.46 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2585/3393 [21:30<06:58,  1.93batch/s, Batch Loss=0.2682, Avg Loss=0.0774, Time Left=7.46 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2585/3393 [21:30<06:58,  1.93batch/s, Batch Loss=0.0957, Avg Loss=0.0774, Time Left=7.45 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2586/3393 [21:30<06:56,  1.94batch/s, Batch Loss=0.0957, Avg Loss=0.0774, Time Left=7.45 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2586/3393 [21:31<06:56,  1.94batch/s, Batch Loss=0.0010, Avg Loss=0.0774, Time Left=7.44 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2587/3393 [21:31<06:52,  1.95batch/s, Batch Loss=0.0010, Avg Loss=0.0774, Time Left=7.44 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2587/3393 [21:31<06:52,  1.95batch/s, Batch Loss=0.2254, Avg Loss=0.0775, Time Left=7.43 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2588/3393 [21:31<06:56,  1.93batch/s, Batch Loss=0.2254, Avg Loss=0.0775, Time Left=7.43 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2588/3393 [21:32<06:56,  1.93batch/s, Batch Loss=0.0061, Avg Loss=0.0774, Time Left=7.43 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2589/3393 [21:32<06:48,  1.97batch/s, Batch Loss=0.0061, Avg Loss=0.0774, Time Left=7.43 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2589/3393 [21:32<06:48,  1.97batch/s, Batch Loss=0.0689, Avg Loss=0.0774, Time Left=7.42 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2590/3393 [21:32<06:50,  1.96batch/s, Batch Loss=0.0689, Avg Loss=0.0774, Time Left=7.42 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2590/3393 [21:33<06:50,  1.96batch/s, Batch Loss=0.0545, Avg Loss=0.0774, Time Left=7.41 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2591/3393 [21:33<06:47,  1.97batch/s, Batch Loss=0.0545, Avg Loss=0.0774, Time Left=7.41 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2591/3393 [21:33<06:47,  1.97batch/s, Batch Loss=0.0880, Avg Loss=0.0774, Time Left=7.40 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2592/3393 [21:33<06:44,  1.98batch/s, Batch Loss=0.0880, Avg Loss=0.0774, Time Left=7.40 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2592/3393 [21:34<06:44,  1.98batch/s, Batch Loss=0.0008, Avg Loss=0.0774, Time Left=7.39 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2593/3393 [21:34<06:42,  1.99batch/s, Batch Loss=0.0008, Avg Loss=0.0774, Time Left=7.39 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2593/3393 [21:34<06:42,  1.99batch/s, Batch Loss=0.2398, Avg Loss=0.0775, Time Left=7.38 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2594/3393 [21:34<06:53,  1.93batch/s, Batch Loss=0.2398, Avg Loss=0.0775, Time Left=7.38 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2594/3393 [21:35<06:53,  1.93batch/s, Batch Loss=0.1677, Avg Loss=0.0775, Time Left=7.37 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2595/3393 [21:35<06:45,  1.97batch/s, Batch Loss=0.1677, Avg Loss=0.0775, Time Left=7.37 \u001b[A\n",
      "Epoch 3/3 - Training:  76%|▊| 2595/3393 [21:35<06:45,  1.97batch/s, Batch Loss=0.0292, Avg Loss=0.0775, Time Left=7.37 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2596/3393 [21:35<06:47,  1.96batch/s, Batch Loss=0.0292, Avg Loss=0.0775, Time Left=7.37 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2596/3393 [21:36<06:47,  1.96batch/s, Batch Loss=0.1122, Avg Loss=0.0775, Time Left=7.36 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2597/3393 [21:36<06:44,  1.97batch/s, Batch Loss=0.1122, Avg Loss=0.0775, Time Left=7.36 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2597/3393 [21:36<06:44,  1.97batch/s, Batch Loss=0.0232, Avg Loss=0.0775, Time Left=7.35 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2598/3393 [21:36<06:49,  1.94batch/s, Batch Loss=0.0232, Avg Loss=0.0775, Time Left=7.35 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2598/3393 [21:37<06:49,  1.94batch/s, Batch Loss=0.0092, Avg Loss=0.0774, Time Left=7.34 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2599/3393 [21:37<06:45,  1.96batch/s, Batch Loss=0.0092, Avg Loss=0.0774, Time Left=7.34 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2599/3393 [21:37<06:45,  1.96batch/s, Batch Loss=0.0225, Avg Loss=0.0774, Time Left=7.33 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2600/3393 [21:37<06:46,  1.95batch/s, Batch Loss=0.0225, Avg Loss=0.0774, Time Left=7.33 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2600/3393 [21:38<06:46,  1.95batch/s, Batch Loss=0.0046, Avg Loss=0.0774, Time Left=7.32 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2601/3393 [21:38<06:47,  1.95batch/s, Batch Loss=0.0046, Avg Loss=0.0774, Time Left=7.32 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2601/3393 [21:38<06:47,  1.95batch/s, Batch Loss=0.0379, Avg Loss=0.0774, Time Left=7.31 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2602/3393 [21:38<06:43,  1.96batch/s, Batch Loss=0.0379, Avg Loss=0.0774, Time Left=7.31 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2602/3393 [21:39<06:43,  1.96batch/s, Batch Loss=0.1228, Avg Loss=0.0774, Time Left=7.31 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2603/3393 [21:39<06:44,  1.95batch/s, Batch Loss=0.1228, Avg Loss=0.0774, Time Left=7.31 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2603/3393 [21:39<06:44,  1.95batch/s, Batch Loss=0.0326, Avg Loss=0.0774, Time Left=7.30 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2604/3393 [21:39<06:44,  1.95batch/s, Batch Loss=0.0326, Avg Loss=0.0774, Time Left=7.30 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2604/3393 [21:40<06:44,  1.95batch/s, Batch Loss=0.0410, Avg Loss=0.0774, Time Left=7.29 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  77%|▊| 2605/3393 [21:40<06:41,  1.96batch/s, Batch Loss=0.0410, Avg Loss=0.0774, Time Left=7.29 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2605/3393 [21:40<06:41,  1.96batch/s, Batch Loss=0.0453, Avg Loss=0.0773, Time Left=7.28 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2606/3393 [21:40<06:42,  1.95batch/s, Batch Loss=0.0453, Avg Loss=0.0773, Time Left=7.28 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2606/3393 [21:41<06:42,  1.95batch/s, Batch Loss=0.0735, Avg Loss=0.0773, Time Left=7.27 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2607/3393 [21:41<06:39,  1.97batch/s, Batch Loss=0.0735, Avg Loss=0.0773, Time Left=7.27 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2607/3393 [21:41<06:39,  1.97batch/s, Batch Loss=0.0402, Avg Loss=0.0773, Time Left=7.26 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2608/3393 [21:41<06:44,  1.94batch/s, Batch Loss=0.0402, Avg Loss=0.0773, Time Left=7.26 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2608/3393 [21:42<06:44,  1.94batch/s, Batch Loss=0.0910, Avg Loss=0.0773, Time Left=7.25 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2609/3393 [21:42<06:33,  1.99batch/s, Batch Loss=0.0910, Avg Loss=0.0773, Time Left=7.25 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2609/3393 [21:42<06:33,  1.99batch/s, Batch Loss=0.0018, Avg Loss=0.0773, Time Left=7.25 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2610/3393 [21:42<06:32,  2.00batch/s, Batch Loss=0.0018, Avg Loss=0.0773, Time Left=7.25 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2610/3393 [21:43<06:32,  2.00batch/s, Batch Loss=0.0502, Avg Loss=0.0773, Time Left=7.24 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2611/3393 [21:43<06:31,  2.00batch/s, Batch Loss=0.0502, Avg Loss=0.0773, Time Left=7.24 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2611/3393 [21:43<06:31,  2.00batch/s, Batch Loss=0.0818, Avg Loss=0.0773, Time Left=7.23 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2612/3393 [21:43<06:34,  1.98batch/s, Batch Loss=0.0818, Avg Loss=0.0773, Time Left=7.23 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2612/3393 [21:44<06:34,  1.98batch/s, Batch Loss=0.0102, Avg Loss=0.0773, Time Left=7.22 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2613/3393 [21:44<06:29,  2.00batch/s, Batch Loss=0.0102, Avg Loss=0.0773, Time Left=7.22 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2613/3393 [21:44<06:29,  2.00batch/s, Batch Loss=0.1411, Avg Loss=0.0773, Time Left=7.21 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2614/3393 [21:44<06:21,  2.04batch/s, Batch Loss=0.1411, Avg Loss=0.0773, Time Left=7.21 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2614/3393 [21:45<06:21,  2.04batch/s, Batch Loss=0.0939, Avg Loss=0.0773, Time Left=7.20 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2615/3393 [21:45<06:19,  2.05batch/s, Batch Loss=0.0939, Avg Loss=0.0773, Time Left=7.20 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2615/3393 [21:45<06:19,  2.05batch/s, Batch Loss=0.0404, Avg Loss=0.0773, Time Left=7.19 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2616/3393 [21:45<06:23,  2.03batch/s, Batch Loss=0.0404, Avg Loss=0.0773, Time Left=7.19 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2616/3393 [21:46<06:23,  2.03batch/s, Batch Loss=0.0275, Avg Loss=0.0773, Time Left=7.19 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2617/3393 [21:46<06:23,  2.03batch/s, Batch Loss=0.0275, Avg Loss=0.0773, Time Left=7.19 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2617/3393 [21:46<06:23,  2.03batch/s, Batch Loss=0.5649, Avg Loss=0.0775, Time Left=7.18 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2618/3393 [21:46<06:27,  2.00batch/s, Batch Loss=0.5649, Avg Loss=0.0775, Time Left=7.18 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2618/3393 [21:47<06:27,  2.00batch/s, Batch Loss=0.1262, Avg Loss=0.0775, Time Left=7.17 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2619/3393 [21:47<06:28,  1.99batch/s, Batch Loss=0.1262, Avg Loss=0.0775, Time Left=7.17 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2619/3393 [21:47<06:28,  1.99batch/s, Batch Loss=0.0494, Avg Loss=0.0775, Time Left=7.16 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2620/3393 [21:47<06:34,  1.96batch/s, Batch Loss=0.0494, Avg Loss=0.0775, Time Left=7.16 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2620/3393 [21:48<06:34,  1.96batch/s, Batch Loss=0.0105, Avg Loss=0.0774, Time Left=7.15 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2621/3393 [21:48<06:28,  1.99batch/s, Batch Loss=0.0105, Avg Loss=0.0774, Time Left=7.15 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2621/3393 [21:48<06:28,  1.99batch/s, Batch Loss=0.0087, Avg Loss=0.0774, Time Left=7.14 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2622/3393 [21:48<06:34,  1.96batch/s, Batch Loss=0.0087, Avg Loss=0.0774, Time Left=7.14 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2622/3393 [21:49<06:34,  1.96batch/s, Batch Loss=0.0300, Avg Loss=0.0774, Time Left=7.13 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2623/3393 [21:49<06:31,  1.97batch/s, Batch Loss=0.0300, Avg Loss=0.0774, Time Left=7.13 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2623/3393 [21:49<06:31,  1.97batch/s, Batch Loss=0.0059, Avg Loss=0.0774, Time Left=7.13 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2624/3393 [21:49<06:25,  2.00batch/s, Batch Loss=0.0059, Avg Loss=0.0774, Time Left=7.13 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2624/3393 [21:50<06:25,  2.00batch/s, Batch Loss=0.0070, Avg Loss=0.0773, Time Left=7.12 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2625/3393 [21:50<06:24,  2.00batch/s, Batch Loss=0.0070, Avg Loss=0.0773, Time Left=7.12 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2625/3393 [21:50<06:24,  2.00batch/s, Batch Loss=0.0152, Avg Loss=0.0773, Time Left=7.11 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2626/3393 [21:50<06:31,  1.96batch/s, Batch Loss=0.0152, Avg Loss=0.0773, Time Left=7.11 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2626/3393 [21:51<06:31,  1.96batch/s, Batch Loss=0.0048, Avg Loss=0.0773, Time Left=7.10 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2627/3393 [21:51<06:29,  1.97batch/s, Batch Loss=0.0048, Avg Loss=0.0773, Time Left=7.10 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2627/3393 [21:51<06:29,  1.97batch/s, Batch Loss=0.2948, Avg Loss=0.0774, Time Left=7.09 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2628/3393 [21:51<06:33,  1.94batch/s, Batch Loss=0.2948, Avg Loss=0.0774, Time Left=7.09 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2628/3393 [21:52<06:33,  1.94batch/s, Batch Loss=0.0103, Avg Loss=0.0773, Time Left=7.08 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2629/3393 [21:52<06:26,  1.98batch/s, Batch Loss=0.0103, Avg Loss=0.0773, Time Left=7.08 \u001b[A\n",
      "Epoch 3/3 - Training:  77%|▊| 2629/3393 [21:52<06:26,  1.98batch/s, Batch Loss=0.0053, Avg Loss=0.0773, Time Left=7.07 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2630/3393 [21:52<06:32,  1.95batch/s, Batch Loss=0.0053, Avg Loss=0.0773, Time Left=7.07 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2630/3393 [21:53<06:32,  1.95batch/s, Batch Loss=0.0575, Avg Loss=0.0773, Time Left=7.07 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2631/3393 [21:53<06:25,  1.98batch/s, Batch Loss=0.0575, Avg Loss=0.0773, Time Left=7.07 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2631/3393 [21:53<06:25,  1.98batch/s, Batch Loss=0.0029, Avg Loss=0.0773, Time Left=7.06 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2632/3393 [21:53<06:30,  1.95batch/s, Batch Loss=0.0029, Avg Loss=0.0773, Time Left=7.06 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2632/3393 [21:54<06:30,  1.95batch/s, Batch Loss=0.1179, Avg Loss=0.0773, Time Left=7.05 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2633/3393 [21:54<06:27,  1.96batch/s, Batch Loss=0.1179, Avg Loss=0.0773, Time Left=7.05 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2633/3393 [21:54<06:27,  1.96batch/s, Batch Loss=0.0389, Avg Loss=0.0773, Time Left=7.04 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2634/3393 [21:54<06:35,  1.92batch/s, Batch Loss=0.0389, Avg Loss=0.0773, Time Left=7.04 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2634/3393 [21:55<06:35,  1.92batch/s, Batch Loss=0.0040, Avg Loss=0.0773, Time Left=7.03 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2635/3393 [21:55<06:27,  1.96batch/s, Batch Loss=0.0040, Avg Loss=0.0773, Time Left=7.03 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2635/3393 [21:55<06:27,  1.96batch/s, Batch Loss=0.0437, Avg Loss=0.0772, Time Left=7.02 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2636/3393 [21:55<06:31,  1.94batch/s, Batch Loss=0.0437, Avg Loss=0.0772, Time Left=7.02 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2636/3393 [21:56<06:31,  1.94batch/s, Batch Loss=0.0156, Avg Loss=0.0772, Time Left=7.01 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2637/3393 [21:56<06:20,  1.99batch/s, Batch Loss=0.0156, Avg Loss=0.0772, Time Left=7.01 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2637/3393 [21:56<06:20,  1.99batch/s, Batch Loss=0.0013, Avg Loss=0.0772, Time Left=7.01 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  78%|▊| 2638/3393 [21:56<06:29,  1.94batch/s, Batch Loss=0.0013, Avg Loss=0.0772, Time Left=7.01 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2638/3393 [21:57<06:29,  1.94batch/s, Batch Loss=0.0019, Avg Loss=0.0772, Time Left=7.00 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2639/3393 [21:57<06:18,  1.99batch/s, Batch Loss=0.0019, Avg Loss=0.0772, Time Left=7.00 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2639/3393 [21:57<06:18,  1.99batch/s, Batch Loss=0.0150, Avg Loss=0.0771, Time Left=6.99 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2640/3393 [21:57<06:17,  1.99batch/s, Batch Loss=0.0150, Avg Loss=0.0771, Time Left=6.99 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2640/3393 [21:58<06:17,  1.99batch/s, Batch Loss=0.0280, Avg Loss=0.0771, Time Left=6.98 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2641/3393 [21:58<06:06,  2.05batch/s, Batch Loss=0.0280, Avg Loss=0.0771, Time Left=6.98 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2641/3393 [21:58<06:06,  2.05batch/s, Batch Loss=0.0053, Avg Loss=0.0771, Time Left=6.97 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2642/3393 [21:58<06:04,  2.06batch/s, Batch Loss=0.0053, Avg Loss=0.0771, Time Left=6.97 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2642/3393 [21:59<06:04,  2.06batch/s, Batch Loss=0.0275, Avg Loss=0.0771, Time Left=6.96 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2643/3393 [21:59<06:03,  2.06batch/s, Batch Loss=0.0275, Avg Loss=0.0771, Time Left=6.96 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2643/3393 [21:59<06:03,  2.06batch/s, Batch Loss=0.2419, Avg Loss=0.0771, Time Left=6.95 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2644/3393 [21:59<06:03,  2.06batch/s, Batch Loss=0.2419, Avg Loss=0.0771, Time Left=6.95 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2644/3393 [22:00<06:03,  2.06batch/s, Batch Loss=0.0026, Avg Loss=0.0771, Time Left=6.95 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2645/3393 [22:00<06:02,  2.06batch/s, Batch Loss=0.0026, Avg Loss=0.0771, Time Left=6.95 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2645/3393 [22:00<06:02,  2.06batch/s, Batch Loss=0.0133, Avg Loss=0.0771, Time Left=6.94 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2646/3393 [22:00<06:15,  1.99batch/s, Batch Loss=0.0133, Avg Loss=0.0771, Time Left=6.94 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2646/3393 [22:01<06:15,  1.99batch/s, Batch Loss=0.2816, Avg Loss=0.0772, Time Left=6.93 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2647/3393 [22:01<06:11,  2.01batch/s, Batch Loss=0.2816, Avg Loss=0.0772, Time Left=6.93 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2647/3393 [22:01<06:11,  2.01batch/s, Batch Loss=0.0069, Avg Loss=0.0771, Time Left=6.92 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2648/3393 [22:01<06:15,  1.99batch/s, Batch Loss=0.0069, Avg Loss=0.0771, Time Left=6.92 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2648/3393 [22:02<06:15,  1.99batch/s, Batch Loss=0.0628, Avg Loss=0.0771, Time Left=6.91 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2649/3393 [22:02<06:10,  2.01batch/s, Batch Loss=0.0628, Avg Loss=0.0771, Time Left=6.91 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2649/3393 [22:02<06:10,  2.01batch/s, Batch Loss=0.0985, Avg Loss=0.0771, Time Left=6.90 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2650/3393 [22:02<06:13,  1.99batch/s, Batch Loss=0.0985, Avg Loss=0.0771, Time Left=6.90 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2650/3393 [22:03<06:13,  1.99batch/s, Batch Loss=0.1360, Avg Loss=0.0772, Time Left=6.89 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2651/3393 [22:03<06:05,  2.03batch/s, Batch Loss=0.1360, Avg Loss=0.0772, Time Left=6.89 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2651/3393 [22:03<06:05,  2.03batch/s, Batch Loss=0.1071, Avg Loss=0.0772, Time Left=6.89 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2652/3393 [22:03<06:22,  1.94batch/s, Batch Loss=0.1071, Avg Loss=0.0772, Time Left=6.89 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2652/3393 [22:04<06:22,  1.94batch/s, Batch Loss=0.0412, Avg Loss=0.0772, Time Left=6.88 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2653/3393 [22:04<06:21,  1.94batch/s, Batch Loss=0.0412, Avg Loss=0.0772, Time Left=6.88 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2653/3393 [22:04<06:21,  1.94batch/s, Batch Loss=0.0335, Avg Loss=0.0771, Time Left=6.87 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2654/3393 [22:04<06:24,  1.92batch/s, Batch Loss=0.0335, Avg Loss=0.0771, Time Left=6.87 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2654/3393 [22:05<06:24,  1.92batch/s, Batch Loss=0.0605, Avg Loss=0.0771, Time Left=6.86 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2655/3393 [22:05<06:12,  1.98batch/s, Batch Loss=0.0605, Avg Loss=0.0771, Time Left=6.86 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2655/3393 [22:05<06:12,  1.98batch/s, Batch Loss=0.0519, Avg Loss=0.0771, Time Left=6.85 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2656/3393 [22:05<06:22,  1.93batch/s, Batch Loss=0.0519, Avg Loss=0.0771, Time Left=6.85 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2656/3393 [22:06<06:22,  1.93batch/s, Batch Loss=0.0014, Avg Loss=0.0771, Time Left=6.84 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2657/3393 [22:06<06:17,  1.95batch/s, Batch Loss=0.0014, Avg Loss=0.0771, Time Left=6.84 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2657/3393 [22:06<06:17,  1.95batch/s, Batch Loss=0.0063, Avg Loss=0.0771, Time Left=6.83 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2658/3393 [22:06<06:17,  1.95batch/s, Batch Loss=0.0063, Avg Loss=0.0771, Time Left=6.83 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2658/3393 [22:07<06:17,  1.95batch/s, Batch Loss=0.0013, Avg Loss=0.0770, Time Left=6.83 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2659/3393 [22:07<06:16,  1.95batch/s, Batch Loss=0.0013, Avg Loss=0.0770, Time Left=6.83 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2659/3393 [22:07<06:16,  1.95batch/s, Batch Loss=0.0166, Avg Loss=0.0770, Time Left=6.82 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2660/3393 [22:08<06:21,  1.92batch/s, Batch Loss=0.0166, Avg Loss=0.0770, Time Left=6.82 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2660/3393 [22:08<06:21,  1.92batch/s, Batch Loss=0.2117, Avg Loss=0.0771, Time Left=6.81 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2661/3393 [22:08<06:13,  1.96batch/s, Batch Loss=0.2117, Avg Loss=0.0771, Time Left=6.81 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2661/3393 [22:09<06:13,  1.96batch/s, Batch Loss=0.0288, Avg Loss=0.0770, Time Left=6.80 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2662/3393 [22:09<06:14,  1.95batch/s, Batch Loss=0.0288, Avg Loss=0.0770, Time Left=6.80 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2662/3393 [22:09<06:14,  1.95batch/s, Batch Loss=0.0262, Avg Loss=0.0770, Time Left=6.79 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2663/3393 [22:09<06:07,  1.98batch/s, Batch Loss=0.0262, Avg Loss=0.0770, Time Left=6.79 \u001b[A\n",
      "Epoch 3/3 - Training:  78%|▊| 2663/3393 [22:10<06:07,  1.98batch/s, Batch Loss=0.0363, Avg Loss=0.0770, Time Left=6.78 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2664/3393 [22:10<06:13,  1.95batch/s, Batch Loss=0.0363, Avg Loss=0.0770, Time Left=6.78 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2664/3393 [22:10<06:13,  1.95batch/s, Batch Loss=0.1796, Avg Loss=0.0771, Time Left=6.77 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2665/3393 [22:10<06:10,  1.96batch/s, Batch Loss=0.1796, Avg Loss=0.0771, Time Left=6.77 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2665/3393 [22:11<06:10,  1.96batch/s, Batch Loss=0.0149, Avg Loss=0.0770, Time Left=6.77 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2666/3393 [22:11<06:11,  1.96batch/s, Batch Loss=0.0149, Avg Loss=0.0770, Time Left=6.77 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2666/3393 [22:11<06:11,  1.96batch/s, Batch Loss=0.0470, Avg Loss=0.0770, Time Left=6.76 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2667/3393 [22:11<06:02,  2.00batch/s, Batch Loss=0.0470, Avg Loss=0.0770, Time Left=6.76 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2667/3393 [22:12<06:02,  2.00batch/s, Batch Loss=0.0336, Avg Loss=0.0770, Time Left=6.75 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2668/3393 [22:12<06:01,  2.01batch/s, Batch Loss=0.0336, Avg Loss=0.0770, Time Left=6.75 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2668/3393 [22:12<06:01,  2.01batch/s, Batch Loss=0.0437, Avg Loss=0.0770, Time Left=6.74 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2669/3393 [22:12<05:57,  2.02batch/s, Batch Loss=0.0437, Avg Loss=0.0770, Time Left=6.74 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2669/3393 [22:13<05:57,  2.02batch/s, Batch Loss=0.1500, Avg Loss=0.0770, Time Left=6.73 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2670/3393 [22:13<06:01,  2.00batch/s, Batch Loss=0.1500, Avg Loss=0.0770, Time Left=6.73 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2670/3393 [22:13<06:01,  2.00batch/s, Batch Loss=0.0491, Avg Loss=0.0770, Time Left=6.72 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  79%|▊| 2671/3393 [22:13<06:06,  1.97batch/s, Batch Loss=0.0491, Avg Loss=0.0770, Time Left=6.72 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2671/3393 [22:14<06:06,  1.97batch/s, Batch Loss=0.0082, Avg Loss=0.0770, Time Left=6.71 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2672/3393 [22:14<05:59,  2.00batch/s, Batch Loss=0.0082, Avg Loss=0.0770, Time Left=6.71 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2672/3393 [22:14<05:59,  2.00batch/s, Batch Loss=0.0659, Avg Loss=0.0770, Time Left=6.71 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2673/3393 [22:14<06:06,  1.97batch/s, Batch Loss=0.0659, Avg Loss=0.0770, Time Left=6.71 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2673/3393 [22:15<06:06,  1.97batch/s, Batch Loss=0.0302, Avg Loss=0.0770, Time Left=6.70 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2674/3393 [22:15<06:00,  1.99batch/s, Batch Loss=0.0302, Avg Loss=0.0770, Time Left=6.70 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2674/3393 [22:15<06:00,  1.99batch/s, Batch Loss=0.1458, Avg Loss=0.0770, Time Left=6.69 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2675/3393 [22:15<06:06,  1.96batch/s, Batch Loss=0.1458, Avg Loss=0.0770, Time Left=6.69 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2675/3393 [22:16<06:06,  1.96batch/s, Batch Loss=0.0683, Avg Loss=0.0770, Time Left=6.68 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2676/3393 [22:16<06:00,  1.99batch/s, Batch Loss=0.0683, Avg Loss=0.0770, Time Left=6.68 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2676/3393 [22:16<06:00,  1.99batch/s, Batch Loss=0.0440, Avg Loss=0.0770, Time Left=6.67 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2677/3393 [22:16<06:06,  1.95batch/s, Batch Loss=0.0440, Avg Loss=0.0770, Time Left=6.67 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2677/3393 [22:17<06:06,  1.95batch/s, Batch Loss=0.1159, Avg Loss=0.0770, Time Left=6.66 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2678/3393 [22:17<06:00,  1.98batch/s, Batch Loss=0.1159, Avg Loss=0.0770, Time Left=6.66 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2678/3393 [22:17<06:00,  1.98batch/s, Batch Loss=0.2226, Avg Loss=0.0770, Time Left=6.65 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2679/3393 [22:17<05:55,  2.01batch/s, Batch Loss=0.2226, Avg Loss=0.0770, Time Left=6.65 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2679/3393 [22:18<05:55,  2.01batch/s, Batch Loss=0.0158, Avg Loss=0.0770, Time Left=6.65 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2680/3393 [22:18<05:55,  2.01batch/s, Batch Loss=0.0158, Avg Loss=0.0770, Time Left=6.65 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2680/3393 [22:18<05:55,  2.01batch/s, Batch Loss=0.0864, Avg Loss=0.0770, Time Left=6.64 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2681/3393 [22:18<05:48,  2.04batch/s, Batch Loss=0.0864, Avg Loss=0.0770, Time Left=6.64 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2681/3393 [22:19<05:48,  2.04batch/s, Batch Loss=0.0042, Avg Loss=0.0770, Time Left=6.63 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2682/3393 [22:19<05:56,  1.99batch/s, Batch Loss=0.0042, Avg Loss=0.0770, Time Left=6.63 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2682/3393 [22:19<05:56,  1.99batch/s, Batch Loss=0.0010, Avg Loss=0.0770, Time Left=6.62 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2683/3393 [22:19<05:53,  2.01batch/s, Batch Loss=0.0010, Avg Loss=0.0770, Time Left=6.62 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2683/3393 [22:20<05:53,  2.01batch/s, Batch Loss=0.0338, Avg Loss=0.0769, Time Left=6.61 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2684/3393 [22:20<05:59,  1.97batch/s, Batch Loss=0.0338, Avg Loss=0.0769, Time Left=6.61 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2684/3393 [22:20<05:59,  1.97batch/s, Batch Loss=0.0161, Avg Loss=0.0769, Time Left=6.60 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2685/3393 [22:20<05:54,  2.00batch/s, Batch Loss=0.0161, Avg Loss=0.0769, Time Left=6.60 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2685/3393 [22:21<05:54,  2.00batch/s, Batch Loss=0.0955, Avg Loss=0.0769, Time Left=6.59 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2686/3393 [22:21<06:00,  1.96batch/s, Batch Loss=0.0955, Avg Loss=0.0769, Time Left=6.59 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2686/3393 [22:21<06:00,  1.96batch/s, Batch Loss=0.0094, Avg Loss=0.0769, Time Left=6.59 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2687/3393 [22:21<05:58,  1.97batch/s, Batch Loss=0.0094, Avg Loss=0.0769, Time Left=6.59 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2687/3393 [22:22<05:58,  1.97batch/s, Batch Loss=0.0648, Avg Loss=0.0769, Time Left=6.58 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2688/3393 [22:22<06:06,  1.92batch/s, Batch Loss=0.0648, Avg Loss=0.0769, Time Left=6.58 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2688/3393 [22:22<06:06,  1.92batch/s, Batch Loss=0.0143, Avg Loss=0.0769, Time Left=6.57 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2689/3393 [22:22<05:58,  1.96batch/s, Batch Loss=0.0143, Avg Loss=0.0769, Time Left=6.57 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2689/3393 [22:23<05:58,  1.96batch/s, Batch Loss=0.0011, Avg Loss=0.0768, Time Left=6.56 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2690/3393 [22:23<05:59,  1.95batch/s, Batch Loss=0.0011, Avg Loss=0.0768, Time Left=6.56 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2690/3393 [22:23<05:59,  1.95batch/s, Batch Loss=0.0060, Avg Loss=0.0768, Time Left=6.55 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2691/3393 [22:23<05:53,  1.98batch/s, Batch Loss=0.0060, Avg Loss=0.0768, Time Left=6.55 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2691/3393 [22:24<05:53,  1.98batch/s, Batch Loss=0.0052, Avg Loss=0.0768, Time Left=6.54 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2692/3393 [22:24<05:59,  1.95batch/s, Batch Loss=0.0052, Avg Loss=0.0768, Time Left=6.54 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2692/3393 [22:24<05:59,  1.95batch/s, Batch Loss=0.1050, Avg Loss=0.0768, Time Left=6.53 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2693/3393 [22:24<05:56,  1.96batch/s, Batch Loss=0.1050, Avg Loss=0.0768, Time Left=6.53 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2693/3393 [22:25<05:56,  1.96batch/s, Batch Loss=0.0119, Avg Loss=0.0768, Time Left=6.53 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2694/3393 [22:25<06:00,  1.94batch/s, Batch Loss=0.0119, Avg Loss=0.0768, Time Left=6.53 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2694/3393 [22:25<06:00,  1.94batch/s, Batch Loss=0.1434, Avg Loss=0.0768, Time Left=6.52 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2695/3393 [22:25<05:57,  1.95batch/s, Batch Loss=0.1434, Avg Loss=0.0768, Time Left=6.52 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2695/3393 [22:26<05:57,  1.95batch/s, Batch Loss=0.0218, Avg Loss=0.0768, Time Left=6.51 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2696/3393 [22:26<06:00,  1.93batch/s, Batch Loss=0.0218, Avg Loss=0.0768, Time Left=6.51 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2696/3393 [22:26<06:00,  1.93batch/s, Batch Loss=0.0210, Avg Loss=0.0768, Time Left=6.50 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2697/3393 [22:26<05:50,  1.99batch/s, Batch Loss=0.0210, Avg Loss=0.0768, Time Left=6.50 \u001b[A\n",
      "Epoch 3/3 - Training:  79%|▊| 2697/3393 [22:27<05:50,  1.99batch/s, Batch Loss=0.0393, Avg Loss=0.0767, Time Left=6.49 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2698/3393 [22:27<05:58,  1.94batch/s, Batch Loss=0.0393, Avg Loss=0.0767, Time Left=6.49 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2698/3393 [22:27<05:58,  1.94batch/s, Batch Loss=0.0107, Avg Loss=0.0767, Time Left=6.48 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2699/3393 [22:27<05:52,  1.97batch/s, Batch Loss=0.0107, Avg Loss=0.0767, Time Left=6.48 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2699/3393 [22:28<05:52,  1.97batch/s, Batch Loss=0.0139, Avg Loss=0.0767, Time Left=6.48 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2700/3393 [22:28<05:56,  1.94batch/s, Batch Loss=0.0139, Avg Loss=0.0767, Time Left=6.48 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2700/3393 [22:28<05:56,  1.94batch/s, Batch Loss=0.0965, Avg Loss=0.0767, Time Left=6.47 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2701/3393 [22:28<05:56,  1.94batch/s, Batch Loss=0.0965, Avg Loss=0.0767, Time Left=6.47 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2701/3393 [22:29<05:56,  1.94batch/s, Batch Loss=0.0159, Avg Loss=0.0767, Time Left=6.46 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2702/3393 [22:29<06:00,  1.92batch/s, Batch Loss=0.0159, Avg Loss=0.0767, Time Left=6.46 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2702/3393 [22:29<06:00,  1.92batch/s, Batch Loss=0.1878, Avg Loss=0.0767, Time Left=6.45 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2703/3393 [22:29<05:55,  1.94batch/s, Batch Loss=0.1878, Avg Loss=0.0767, Time Left=6.45 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2703/3393 [22:30<05:55,  1.94batch/s, Batch Loss=0.0379, Avg Loss=0.0767, Time Left=6.44 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  80%|▊| 2704/3393 [22:30<05:58,  1.92batch/s, Batch Loss=0.0379, Avg Loss=0.0767, Time Left=6.44 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2704/3393 [22:30<05:58,  1.92batch/s, Batch Loss=0.8930, Avg Loss=0.0770, Time Left=6.43 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2705/3393 [22:30<05:54,  1.94batch/s, Batch Loss=0.8930, Avg Loss=0.0770, Time Left=6.43 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2705/3393 [22:31<05:54,  1.94batch/s, Batch Loss=0.0182, Avg Loss=0.0770, Time Left=6.42 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2706/3393 [22:31<05:51,  1.96batch/s, Batch Loss=0.0182, Avg Loss=0.0770, Time Left=6.42 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2706/3393 [22:31<05:51,  1.96batch/s, Batch Loss=0.0056, Avg Loss=0.0770, Time Left=6.42 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2707/3393 [22:31<05:47,  1.97batch/s, Batch Loss=0.0056, Avg Loss=0.0770, Time Left=6.42 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2707/3393 [22:32<05:47,  1.97batch/s, Batch Loss=0.0033, Avg Loss=0.0769, Time Left=6.41 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2708/3393 [22:32<05:46,  1.98batch/s, Batch Loss=0.0033, Avg Loss=0.0769, Time Left=6.41 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2708/3393 [22:32<05:46,  1.98batch/s, Batch Loss=0.0104, Avg Loss=0.0769, Time Left=6.40 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2709/3393 [22:32<05:47,  1.97batch/s, Batch Loss=0.0104, Avg Loss=0.0769, Time Left=6.40 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2709/3393 [22:33<05:47,  1.97batch/s, Batch Loss=0.0193, Avg Loss=0.0769, Time Left=6.39 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2710/3393 [22:33<05:49,  1.96batch/s, Batch Loss=0.0193, Avg Loss=0.0769, Time Left=6.39 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2710/3393 [22:33<05:49,  1.96batch/s, Batch Loss=0.0041, Avg Loss=0.0769, Time Left=6.38 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2711/3393 [22:33<05:46,  1.97batch/s, Batch Loss=0.0041, Avg Loss=0.0769, Time Left=6.38 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2711/3393 [22:34<05:46,  1.97batch/s, Batch Loss=0.0049, Avg Loss=0.0768, Time Left=6.37 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2712/3393 [22:34<05:49,  1.95batch/s, Batch Loss=0.0049, Avg Loss=0.0768, Time Left=6.37 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2712/3393 [22:34<05:49,  1.95batch/s, Batch Loss=0.0366, Avg Loss=0.0768, Time Left=6.36 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2713/3393 [22:34<05:47,  1.95batch/s, Batch Loss=0.0366, Avg Loss=0.0768, Time Left=6.36 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2713/3393 [22:35<05:47,  1.95batch/s, Batch Loss=0.0381, Avg Loss=0.0768, Time Left=6.36 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2714/3393 [22:35<05:48,  1.95batch/s, Batch Loss=0.0381, Avg Loss=0.0768, Time Left=6.36 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2714/3393 [22:35<05:48,  1.95batch/s, Batch Loss=0.0109, Avg Loss=0.0768, Time Left=6.35 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2715/3393 [22:35<05:48,  1.95batch/s, Batch Loss=0.0109, Avg Loss=0.0768, Time Left=6.35 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2715/3393 [22:36<05:48,  1.95batch/s, Batch Loss=0.0162, Avg Loss=0.0768, Time Left=6.34 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2716/3393 [22:36<05:45,  1.96batch/s, Batch Loss=0.0162, Avg Loss=0.0768, Time Left=6.34 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2716/3393 [22:36<05:45,  1.96batch/s, Batch Loss=0.0084, Avg Loss=0.0767, Time Left=6.33 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2717/3393 [22:36<05:42,  1.97batch/s, Batch Loss=0.0084, Avg Loss=0.0767, Time Left=6.33 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2717/3393 [22:37<05:42,  1.97batch/s, Batch Loss=0.0164, Avg Loss=0.0767, Time Left=6.32 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2718/3393 [22:37<05:37,  2.00batch/s, Batch Loss=0.0164, Avg Loss=0.0767, Time Left=6.32 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2718/3393 [22:37<05:37,  2.00batch/s, Batch Loss=0.1377, Avg Loss=0.0767, Time Left=6.31 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2719/3393 [22:37<05:37,  2.00batch/s, Batch Loss=0.1377, Avg Loss=0.0767, Time Left=6.31 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2719/3393 [22:38<05:37,  2.00batch/s, Batch Loss=0.0293, Avg Loss=0.0767, Time Left=6.30 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2720/3393 [22:38<05:34,  2.01batch/s, Batch Loss=0.0293, Avg Loss=0.0767, Time Left=6.30 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2720/3393 [22:38<05:34,  2.01batch/s, Batch Loss=0.0299, Avg Loss=0.0767, Time Left=6.30 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2721/3393 [22:38<05:36,  2.00batch/s, Batch Loss=0.0299, Avg Loss=0.0767, Time Left=6.30 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2721/3393 [22:39<05:36,  2.00batch/s, Batch Loss=0.0089, Avg Loss=0.0767, Time Left=6.29 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2722/3393 [22:39<05:32,  2.02batch/s, Batch Loss=0.0089, Avg Loss=0.0767, Time Left=6.29 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2722/3393 [22:39<05:32,  2.02batch/s, Batch Loss=0.0086, Avg Loss=0.0766, Time Left=6.28 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2723/3393 [22:39<05:39,  1.97batch/s, Batch Loss=0.0086, Avg Loss=0.0766, Time Left=6.28 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2723/3393 [22:40<05:39,  1.97batch/s, Batch Loss=0.0095, Avg Loss=0.0766, Time Left=6.27 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2724/3393 [22:40<05:38,  1.98batch/s, Batch Loss=0.0095, Avg Loss=0.0766, Time Left=6.27 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2724/3393 [22:40<05:38,  1.98batch/s, Batch Loss=0.0217, Avg Loss=0.0766, Time Left=6.26 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2725/3393 [22:40<05:36,  1.99batch/s, Batch Loss=0.0217, Avg Loss=0.0766, Time Left=6.26 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2725/3393 [22:41<05:36,  1.99batch/s, Batch Loss=0.0199, Avg Loss=0.0766, Time Left=6.25 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2726/3393 [22:41<05:35,  1.99batch/s, Batch Loss=0.0199, Avg Loss=0.0766, Time Left=6.25 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2726/3393 [22:41<05:35,  1.99batch/s, Batch Loss=0.0195, Avg Loss=0.0766, Time Left=6.24 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2727/3393 [22:41<05:37,  1.97batch/s, Batch Loss=0.0195, Avg Loss=0.0766, Time Left=6.24 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2727/3393 [22:42<05:37,  1.97batch/s, Batch Loss=0.0155, Avg Loss=0.0765, Time Left=6.24 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2728/3393 [22:42<05:29,  2.02batch/s, Batch Loss=0.0155, Avg Loss=0.0765, Time Left=6.24 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2728/3393 [22:42<05:29,  2.02batch/s, Batch Loss=0.0733, Avg Loss=0.0765, Time Left=6.23 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2729/3393 [22:42<05:29,  2.01batch/s, Batch Loss=0.0733, Avg Loss=0.0765, Time Left=6.23 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2729/3393 [22:43<05:29,  2.01batch/s, Batch Loss=0.0852, Avg Loss=0.0765, Time Left=6.22 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2730/3393 [22:43<05:30,  2.01batch/s, Batch Loss=0.0852, Avg Loss=0.0765, Time Left=6.22 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2730/3393 [22:43<05:30,  2.01batch/s, Batch Loss=0.0426, Avg Loss=0.0765, Time Left=6.21 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2731/3393 [22:43<05:30,  2.00batch/s, Batch Loss=0.0426, Avg Loss=0.0765, Time Left=6.21 \u001b[A\n",
      "Epoch 3/3 - Training:  80%|▊| 2731/3393 [22:44<05:30,  2.00batch/s, Batch Loss=0.1240, Avg Loss=0.0765, Time Left=6.20 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2732/3393 [22:44<05:33,  1.98batch/s, Batch Loss=0.1240, Avg Loss=0.0765, Time Left=6.20 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2732/3393 [22:44<05:33,  1.98batch/s, Batch Loss=0.0226, Avg Loss=0.0765, Time Left=6.19 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2733/3393 [22:44<05:34,  1.97batch/s, Batch Loss=0.0226, Avg Loss=0.0765, Time Left=6.19 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2733/3393 [22:45<05:34,  1.97batch/s, Batch Loss=0.0760, Avg Loss=0.0765, Time Left=6.18 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2734/3393 [22:45<05:39,  1.94batch/s, Batch Loss=0.0760, Avg Loss=0.0765, Time Left=6.18 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2734/3393 [22:45<05:39,  1.94batch/s, Batch Loss=0.1531, Avg Loss=0.0766, Time Left=6.18 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2735/3393 [22:45<05:35,  1.96batch/s, Batch Loss=0.1531, Avg Loss=0.0766, Time Left=6.18 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2735/3393 [22:46<05:35,  1.96batch/s, Batch Loss=0.2160, Avg Loss=0.0766, Time Left=6.17 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2736/3393 [22:46<05:33,  1.97batch/s, Batch Loss=0.2160, Avg Loss=0.0766, Time Left=6.17 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2736/3393 [22:47<05:33,  1.97batch/s, Batch Loss=0.3736, Avg Loss=0.0767, Time Left=6.16 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  81%|▊| 2737/3393 [22:47<05:35,  1.96batch/s, Batch Loss=0.3736, Avg Loss=0.0767, Time Left=6.16 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2737/3393 [22:47<05:35,  1.96batch/s, Batch Loss=0.0025, Avg Loss=0.0767, Time Left=6.15 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2738/3393 [22:47<05:35,  1.95batch/s, Batch Loss=0.0025, Avg Loss=0.0767, Time Left=6.15 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2738/3393 [22:48<05:35,  1.95batch/s, Batch Loss=0.3519, Avg Loss=0.0768, Time Left=6.14 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2739/3393 [22:48<05:33,  1.96batch/s, Batch Loss=0.3519, Avg Loss=0.0768, Time Left=6.14 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2739/3393 [22:48<05:33,  1.96batch/s, Batch Loss=0.0057, Avg Loss=0.0768, Time Left=6.13 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2740/3393 [22:48<05:33,  1.96batch/s, Batch Loss=0.0057, Avg Loss=0.0768, Time Left=6.13 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2740/3393 [22:49<05:33,  1.96batch/s, Batch Loss=0.0602, Avg Loss=0.0768, Time Left=6.12 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2741/3393 [22:49<05:31,  1.97batch/s, Batch Loss=0.0602, Avg Loss=0.0768, Time Left=6.12 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2741/3393 [22:49<05:31,  1.97batch/s, Batch Loss=0.0236, Avg Loss=0.0767, Time Left=6.12 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2742/3393 [22:49<05:32,  1.96batch/s, Batch Loss=0.0236, Avg Loss=0.0767, Time Left=6.12 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2742/3393 [22:50<05:32,  1.96batch/s, Batch Loss=0.0179, Avg Loss=0.0767, Time Left=6.11 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2743/3393 [22:50<05:30,  1.97batch/s, Batch Loss=0.0179, Avg Loss=0.0767, Time Left=6.11 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2743/3393 [22:50<05:30,  1.97batch/s, Batch Loss=0.0076, Avg Loss=0.0767, Time Left=6.10 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2744/3393 [22:50<05:34,  1.94batch/s, Batch Loss=0.0076, Avg Loss=0.0767, Time Left=6.10 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2744/3393 [22:51<05:34,  1.94batch/s, Batch Loss=0.1309, Avg Loss=0.0767, Time Left=6.09 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2745/3393 [22:51<05:28,  1.97batch/s, Batch Loss=0.1309, Avg Loss=0.0767, Time Left=6.09 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2745/3393 [22:51<05:28,  1.97batch/s, Batch Loss=0.0112, Avg Loss=0.0767, Time Left=6.08 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2746/3393 [22:51<05:35,  1.93batch/s, Batch Loss=0.0112, Avg Loss=0.0767, Time Left=6.08 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2746/3393 [22:52<05:35,  1.93batch/s, Batch Loss=0.0229, Avg Loss=0.0767, Time Left=6.07 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2747/3393 [22:52<05:31,  1.95batch/s, Batch Loss=0.0229, Avg Loss=0.0767, Time Left=6.07 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2747/3393 [22:52<05:31,  1.95batch/s, Batch Loss=0.0645, Avg Loss=0.0767, Time Left=6.06 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2748/3393 [22:52<05:28,  1.96batch/s, Batch Loss=0.0645, Avg Loss=0.0767, Time Left=6.06 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2748/3393 [22:53<05:28,  1.96batch/s, Batch Loss=0.0052, Avg Loss=0.0766, Time Left=6.06 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2749/3393 [22:53<05:35,  1.92batch/s, Batch Loss=0.0052, Avg Loss=0.0766, Time Left=6.06 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2749/3393 [22:53<05:35,  1.92batch/s, Batch Loss=0.2074, Avg Loss=0.0767, Time Left=6.05 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2750/3393 [22:53<05:32,  1.93batch/s, Batch Loss=0.2074, Avg Loss=0.0767, Time Left=6.05 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2750/3393 [22:54<05:32,  1.93batch/s, Batch Loss=0.0084, Avg Loss=0.0767, Time Left=6.04 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2751/3393 [22:54<05:31,  1.94batch/s, Batch Loss=0.0084, Avg Loss=0.0767, Time Left=6.04 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2751/3393 [22:54<05:31,  1.94batch/s, Batch Loss=0.0121, Avg Loss=0.0766, Time Left=6.03 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2752/3393 [22:54<05:24,  1.97batch/s, Batch Loss=0.0121, Avg Loss=0.0766, Time Left=6.03 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2752/3393 [22:55<05:24,  1.97batch/s, Batch Loss=0.0783, Avg Loss=0.0766, Time Left=6.02 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2753/3393 [22:55<05:25,  1.96batch/s, Batch Loss=0.0783, Avg Loss=0.0766, Time Left=6.02 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2753/3393 [22:55<05:25,  1.96batch/s, Batch Loss=0.0732, Avg Loss=0.0766, Time Left=6.01 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2754/3393 [22:55<05:23,  1.97batch/s, Batch Loss=0.0732, Avg Loss=0.0766, Time Left=6.01 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2754/3393 [22:56<05:23,  1.97batch/s, Batch Loss=0.1478, Avg Loss=0.0767, Time Left=6.01 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2755/3393 [22:56<05:26,  1.95batch/s, Batch Loss=0.1478, Avg Loss=0.0767, Time Left=6.01 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2755/3393 [22:56<05:26,  1.95batch/s, Batch Loss=0.0501, Avg Loss=0.0767, Time Left=6.00 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2756/3393 [22:56<05:22,  1.97batch/s, Batch Loss=0.0501, Avg Loss=0.0767, Time Left=6.00 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2756/3393 [22:57<05:22,  1.97batch/s, Batch Loss=0.1071, Avg Loss=0.0767, Time Left=5.99 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2757/3393 [22:57<05:20,  1.98batch/s, Batch Loss=0.1071, Avg Loss=0.0767, Time Left=5.99 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2757/3393 [22:57<05:20,  1.98batch/s, Batch Loss=0.0059, Avg Loss=0.0766, Time Left=5.98 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2758/3393 [22:57<05:22,  1.97batch/s, Batch Loss=0.0059, Avg Loss=0.0766, Time Left=5.98 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2758/3393 [22:58<05:22,  1.97batch/s, Batch Loss=0.0333, Avg Loss=0.0766, Time Left=5.97 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2759/3393 [22:58<05:20,  1.98batch/s, Batch Loss=0.0333, Avg Loss=0.0766, Time Left=5.97 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2759/3393 [22:58<05:20,  1.98batch/s, Batch Loss=0.0980, Avg Loss=0.0766, Time Left=5.96 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2760/3393 [22:58<05:25,  1.95batch/s, Batch Loss=0.0980, Avg Loss=0.0766, Time Left=5.96 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2760/3393 [22:59<05:25,  1.95batch/s, Batch Loss=0.0190, Avg Loss=0.0766, Time Left=5.95 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2761/3393 [22:59<05:19,  1.98batch/s, Batch Loss=0.0190, Avg Loss=0.0766, Time Left=5.95 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2761/3393 [22:59<05:19,  1.98batch/s, Batch Loss=0.4704, Avg Loss=0.0768, Time Left=5.95 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2762/3393 [22:59<05:23,  1.95batch/s, Batch Loss=0.4704, Avg Loss=0.0768, Time Left=5.95 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2762/3393 [23:00<05:23,  1.95batch/s, Batch Loss=0.0187, Avg Loss=0.0767, Time Left=5.94 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2763/3393 [23:00<05:21,  1.96batch/s, Batch Loss=0.0187, Avg Loss=0.0767, Time Left=5.94 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2763/3393 [23:00<05:21,  1.96batch/s, Batch Loss=0.0444, Avg Loss=0.0767, Time Left=5.93 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2764/3393 [23:00<05:22,  1.95batch/s, Batch Loss=0.0444, Avg Loss=0.0767, Time Left=5.93 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2764/3393 [23:01<05:22,  1.95batch/s, Batch Loss=0.0624, Avg Loss=0.0767, Time Left=5.92 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2765/3393 [23:01<05:22,  1.95batch/s, Batch Loss=0.0624, Avg Loss=0.0767, Time Left=5.92 \u001b[A\n",
      "Epoch 3/3 - Training:  81%|▊| 2765/3393 [23:01<05:22,  1.95batch/s, Batch Loss=0.0047, Avg Loss=0.0767, Time Left=5.91 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2766/3393 [23:01<05:22,  1.95batch/s, Batch Loss=0.0047, Avg Loss=0.0767, Time Left=5.91 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2766/3393 [23:02<05:22,  1.95batch/s, Batch Loss=0.0769, Avg Loss=0.0767, Time Left=5.90 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2767/3393 [23:02<05:16,  1.98batch/s, Batch Loss=0.0769, Avg Loss=0.0767, Time Left=5.90 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2767/3393 [23:02<05:16,  1.98batch/s, Batch Loss=0.0160, Avg Loss=0.0767, Time Left=5.89 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2768/3393 [23:02<05:23,  1.93batch/s, Batch Loss=0.0160, Avg Loss=0.0767, Time Left=5.89 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2768/3393 [23:03<05:23,  1.93batch/s, Batch Loss=0.1386, Avg Loss=0.0767, Time Left=5.89 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2769/3393 [23:03<05:14,  1.98batch/s, Batch Loss=0.1386, Avg Loss=0.0767, Time Left=5.89 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2769/3393 [23:03<05:14,  1.98batch/s, Batch Loss=0.2103, Avg Loss=0.0767, Time Left=5.88 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  82%|▊| 2770/3393 [23:03<05:22,  1.93batch/s, Batch Loss=0.2103, Avg Loss=0.0767, Time Left=5.88 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2770/3393 [23:04<05:22,  1.93batch/s, Batch Loss=0.0008, Avg Loss=0.0767, Time Left=5.87 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2771/3393 [23:04<05:15,  1.97batch/s, Batch Loss=0.0008, Avg Loss=0.0767, Time Left=5.87 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2771/3393 [23:04<05:15,  1.97batch/s, Batch Loss=0.0778, Avg Loss=0.0767, Time Left=5.86 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2772/3393 [23:04<05:19,  1.94batch/s, Batch Loss=0.0778, Avg Loss=0.0767, Time Left=5.86 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2772/3393 [23:05<05:19,  1.94batch/s, Batch Loss=0.0180, Avg Loss=0.0767, Time Left=5.85 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2773/3393 [23:05<05:19,  1.94batch/s, Batch Loss=0.0180, Avg Loss=0.0767, Time Left=5.85 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2773/3393 [23:05<05:19,  1.94batch/s, Batch Loss=0.0388, Avg Loss=0.0767, Time Left=5.84 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2774/3393 [23:05<05:16,  1.96batch/s, Batch Loss=0.0388, Avg Loss=0.0767, Time Left=5.84 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2774/3393 [23:06<05:16,  1.96batch/s, Batch Loss=0.0774, Avg Loss=0.0767, Time Left=5.83 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2775/3393 [23:06<05:11,  1.99batch/s, Batch Loss=0.0774, Avg Loss=0.0767, Time Left=5.83 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2775/3393 [23:06<05:11,  1.99batch/s, Batch Loss=0.0014, Avg Loss=0.0766, Time Left=5.83 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2776/3393 [23:06<05:12,  1.97batch/s, Batch Loss=0.0014, Avg Loss=0.0766, Time Left=5.83 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2776/3393 [23:07<05:12,  1.97batch/s, Batch Loss=0.0880, Avg Loss=0.0766, Time Left=5.82 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2777/3393 [23:07<05:08,  2.00batch/s, Batch Loss=0.0880, Avg Loss=0.0766, Time Left=5.82 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2777/3393 [23:07<05:08,  2.00batch/s, Batch Loss=0.0417, Avg Loss=0.0766, Time Left=5.81 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2778/3393 [23:07<05:04,  2.02batch/s, Batch Loss=0.0417, Avg Loss=0.0766, Time Left=5.81 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2778/3393 [23:08<05:04,  2.02batch/s, Batch Loss=0.0107, Avg Loss=0.0766, Time Left=5.80 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2779/3393 [23:08<05:05,  2.01batch/s, Batch Loss=0.0107, Avg Loss=0.0766, Time Left=5.80 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2779/3393 [23:08<05:05,  2.01batch/s, Batch Loss=0.0025, Avg Loss=0.0766, Time Left=5.79 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2780/3393 [23:08<05:02,  2.03batch/s, Batch Loss=0.0025, Avg Loss=0.0766, Time Left=5.79 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2780/3393 [23:09<05:02,  2.03batch/s, Batch Loss=0.0327, Avg Loss=0.0766, Time Left=5.78 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2781/3393 [23:09<05:08,  1.98batch/s, Batch Loss=0.0327, Avg Loss=0.0766, Time Left=5.78 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2781/3393 [23:09<05:08,  1.98batch/s, Batch Loss=0.0009, Avg Loss=0.0765, Time Left=5.77 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2782/3393 [23:09<05:07,  1.99batch/s, Batch Loss=0.0009, Avg Loss=0.0765, Time Left=5.77 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2782/3393 [23:10<05:07,  1.99batch/s, Batch Loss=0.2012, Avg Loss=0.0766, Time Left=5.77 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2783/3393 [23:10<05:12,  1.95batch/s, Batch Loss=0.2012, Avg Loss=0.0766, Time Left=5.77 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2783/3393 [23:10<05:12,  1.95batch/s, Batch Loss=0.0034, Avg Loss=0.0766, Time Left=5.76 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2784/3393 [23:10<05:08,  1.98batch/s, Batch Loss=0.0034, Avg Loss=0.0766, Time Left=5.76 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2784/3393 [23:11<05:08,  1.98batch/s, Batch Loss=0.0796, Avg Loss=0.0766, Time Left=5.75 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2785/3393 [23:11<05:11,  1.95batch/s, Batch Loss=0.0796, Avg Loss=0.0766, Time Left=5.75 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2785/3393 [23:11<05:11,  1.95batch/s, Batch Loss=0.0009, Avg Loss=0.0765, Time Left=5.74 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2786/3393 [23:11<05:08,  1.96batch/s, Batch Loss=0.0009, Avg Loss=0.0765, Time Left=5.74 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2786/3393 [23:12<05:08,  1.96batch/s, Batch Loss=0.2087, Avg Loss=0.0766, Time Left=5.73 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2787/3393 [23:12<05:11,  1.95batch/s, Batch Loss=0.2087, Avg Loss=0.0766, Time Left=5.73 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2787/3393 [23:12<05:11,  1.95batch/s, Batch Loss=0.0989, Avg Loss=0.0766, Time Left=5.72 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2788/3393 [23:12<05:07,  1.97batch/s, Batch Loss=0.0989, Avg Loss=0.0766, Time Left=5.72 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2788/3393 [23:13<05:07,  1.97batch/s, Batch Loss=0.0078, Avg Loss=0.0766, Time Left=5.71 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2789/3393 [23:13<05:10,  1.94batch/s, Batch Loss=0.0078, Avg Loss=0.0766, Time Left=5.71 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2789/3393 [23:14<05:10,  1.94batch/s, Batch Loss=0.0317, Avg Loss=0.0765, Time Left=5.71 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2790/3393 [23:14<05:08,  1.96batch/s, Batch Loss=0.0317, Avg Loss=0.0765, Time Left=5.71 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2790/3393 [23:14<05:08,  1.96batch/s, Batch Loss=0.0013, Avg Loss=0.0765, Time Left=5.70 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2791/3393 [23:14<05:11,  1.93batch/s, Batch Loss=0.0013, Avg Loss=0.0765, Time Left=5.70 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2791/3393 [23:15<05:11,  1.93batch/s, Batch Loss=0.0089, Avg Loss=0.0765, Time Left=5.69 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2792/3393 [23:15<05:08,  1.95batch/s, Batch Loss=0.0089, Avg Loss=0.0765, Time Left=5.69 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2792/3393 [23:15<05:08,  1.95batch/s, Batch Loss=0.4236, Avg Loss=0.0766, Time Left=5.68 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2793/3393 [23:15<05:08,  1.95batch/s, Batch Loss=0.4236, Avg Loss=0.0766, Time Left=5.68 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2793/3393 [23:16<05:08,  1.95batch/s, Batch Loss=0.0028, Avg Loss=0.0766, Time Left=5.67 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2794/3393 [23:16<05:05,  1.96batch/s, Batch Loss=0.0028, Avg Loss=0.0766, Time Left=5.67 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2794/3393 [23:16<05:05,  1.96batch/s, Batch Loss=0.0059, Avg Loss=0.0766, Time Left=5.66 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2795/3393 [23:16<05:09,  1.94batch/s, Batch Loss=0.0059, Avg Loss=0.0766, Time Left=5.66 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2795/3393 [23:17<05:09,  1.94batch/s, Batch Loss=0.0916, Avg Loss=0.0766, Time Left=5.66 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2796/3393 [23:17<05:11,  1.92batch/s, Batch Loss=0.0916, Avg Loss=0.0766, Time Left=5.66 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2796/3393 [23:17<05:11,  1.92batch/s, Batch Loss=0.2722, Avg Loss=0.0766, Time Left=5.65 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2797/3393 [23:17<05:04,  1.96batch/s, Batch Loss=0.2722, Avg Loss=0.0766, Time Left=5.65 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2797/3393 [23:18<05:04,  1.96batch/s, Batch Loss=0.0402, Avg Loss=0.0766, Time Left=5.64 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2798/3393 [23:18<04:59,  1.99batch/s, Batch Loss=0.0402, Avg Loss=0.0766, Time Left=5.64 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2798/3393 [23:18<04:59,  1.99batch/s, Batch Loss=0.1367, Avg Loss=0.0767, Time Left=5.63 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2799/3393 [23:18<05:04,  1.95batch/s, Batch Loss=0.1367, Avg Loss=0.0767, Time Left=5.63 \u001b[A\n",
      "Epoch 3/3 - Training:  82%|▊| 2799/3393 [23:19<05:04,  1.95batch/s, Batch Loss=0.1233, Avg Loss=0.0767, Time Left=5.62 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2800/3393 [23:19<05:01,  1.97batch/s, Batch Loss=0.1233, Avg Loss=0.0767, Time Left=5.62 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2800/3393 [23:19<05:01,  1.97batch/s, Batch Loss=0.0028, Avg Loss=0.0766, Time Left=5.61 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2801/3393 [23:19<05:08,  1.92batch/s, Batch Loss=0.0028, Avg Loss=0.0766, Time Left=5.61 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2801/3393 [23:20<05:08,  1.92batch/s, Batch Loss=0.0210, Avg Loss=0.0766, Time Left=5.60 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2802/3393 [23:20<05:04,  1.94batch/s, Batch Loss=0.0210, Avg Loss=0.0766, Time Left=5.60 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2802/3393 [23:20<05:04,  1.94batch/s, Batch Loss=0.2619, Avg Loss=0.0767, Time Left=5.60 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  83%|▊| 2803/3393 [23:20<05:03,  1.94batch/s, Batch Loss=0.2619, Avg Loss=0.0767, Time Left=5.60 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2803/3393 [23:21<05:03,  1.94batch/s, Batch Loss=0.0248, Avg Loss=0.0767, Time Left=5.59 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2804/3393 [23:21<05:01,  1.96batch/s, Batch Loss=0.0248, Avg Loss=0.0767, Time Left=5.59 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2804/3393 [23:21<05:01,  1.96batch/s, Batch Loss=0.1369, Avg Loss=0.0767, Time Left=5.58 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2805/3393 [23:21<05:04,  1.93batch/s, Batch Loss=0.1369, Avg Loss=0.0767, Time Left=5.58 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2805/3393 [23:22<05:04,  1.93batch/s, Batch Loss=0.0564, Avg Loss=0.0767, Time Left=5.57 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2806/3393 [23:22<05:01,  1.95batch/s, Batch Loss=0.0564, Avg Loss=0.0767, Time Left=5.57 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2806/3393 [23:22<05:01,  1.95batch/s, Batch Loss=0.0174, Avg Loss=0.0767, Time Left=5.56 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2807/3393 [23:22<05:00,  1.95batch/s, Batch Loss=0.0174, Avg Loss=0.0767, Time Left=5.56 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2807/3393 [23:23<05:00,  1.95batch/s, Batch Loss=0.0355, Avg Loss=0.0767, Time Left=5.55 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2808/3393 [23:23<04:58,  1.96batch/s, Batch Loss=0.0355, Avg Loss=0.0767, Time Left=5.55 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2808/3393 [23:23<04:58,  1.96batch/s, Batch Loss=0.0137, Avg Loss=0.0766, Time Left=5.54 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2809/3393 [23:23<04:58,  1.95batch/s, Batch Loss=0.0137, Avg Loss=0.0766, Time Left=5.54 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2809/3393 [23:24<04:58,  1.95batch/s, Batch Loss=0.0118, Avg Loss=0.0766, Time Left=5.54 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2810/3393 [23:24<04:56,  1.96batch/s, Batch Loss=0.0118, Avg Loss=0.0766, Time Left=5.54 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2810/3393 [23:24<04:56,  1.96batch/s, Batch Loss=0.0404, Avg Loss=0.0766, Time Left=5.53 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2811/3393 [23:24<04:57,  1.96batch/s, Batch Loss=0.0404, Avg Loss=0.0766, Time Left=5.53 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2811/3393 [23:25<04:57,  1.96batch/s, Batch Loss=0.0258, Avg Loss=0.0766, Time Left=5.52 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2812/3393 [23:25<04:52,  1.99batch/s, Batch Loss=0.0258, Avg Loss=0.0766, Time Left=5.52 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2812/3393 [23:25<04:52,  1.99batch/s, Batch Loss=0.1235, Avg Loss=0.0766, Time Left=5.51 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2813/3393 [23:25<04:54,  1.97batch/s, Batch Loss=0.1235, Avg Loss=0.0766, Time Left=5.51 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2813/3393 [23:26<04:54,  1.97batch/s, Batch Loss=0.0327, Avg Loss=0.0766, Time Left=5.50 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2814/3393 [23:26<04:52,  1.98batch/s, Batch Loss=0.0327, Avg Loss=0.0766, Time Left=5.50 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2814/3393 [23:26<04:52,  1.98batch/s, Batch Loss=0.0549, Avg Loss=0.0766, Time Left=5.49 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2815/3393 [23:26<04:56,  1.95batch/s, Batch Loss=0.0549, Avg Loss=0.0766, Time Left=5.49 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2815/3393 [23:27<04:56,  1.95batch/s, Batch Loss=0.0080, Avg Loss=0.0765, Time Left=5.48 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2816/3393 [23:27<04:51,  1.98batch/s, Batch Loss=0.0080, Avg Loss=0.0765, Time Left=5.48 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2816/3393 [23:27<04:51,  1.98batch/s, Batch Loss=0.1606, Avg Loss=0.0766, Time Left=5.48 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2817/3393 [23:27<04:52,  1.97batch/s, Batch Loss=0.1606, Avg Loss=0.0766, Time Left=5.48 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2817/3393 [23:28<04:52,  1.97batch/s, Batch Loss=0.0471, Avg Loss=0.0766, Time Left=5.47 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2818/3393 [23:28<04:45,  2.01batch/s, Batch Loss=0.0471, Avg Loss=0.0766, Time Left=5.47 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2818/3393 [23:28<04:45,  2.01batch/s, Batch Loss=0.0246, Avg Loss=0.0765, Time Left=5.46 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2819/3393 [23:28<04:45,  2.01batch/s, Batch Loss=0.0246, Avg Loss=0.0765, Time Left=5.46 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2819/3393 [23:29<04:45,  2.01batch/s, Batch Loss=0.1370, Avg Loss=0.0766, Time Left=5.45 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2820/3393 [23:29<04:45,  2.01batch/s, Batch Loss=0.1370, Avg Loss=0.0766, Time Left=5.45 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2820/3393 [23:29<04:45,  2.01batch/s, Batch Loss=0.0039, Avg Loss=0.0765, Time Left=5.44 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2821/3393 [23:29<04:45,  2.00batch/s, Batch Loss=0.0039, Avg Loss=0.0765, Time Left=5.44 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2821/3393 [23:30<04:45,  2.00batch/s, Batch Loss=0.0063, Avg Loss=0.0765, Time Left=5.43 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2822/3393 [23:30<04:45,  2.00batch/s, Batch Loss=0.0063, Avg Loss=0.0765, Time Left=5.43 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2822/3393 [23:30<04:45,  2.00batch/s, Batch Loss=0.0553, Avg Loss=0.0765, Time Left=5.42 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2823/3393 [23:30<04:48,  1.98batch/s, Batch Loss=0.0553, Avg Loss=0.0765, Time Left=5.42 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2823/3393 [23:31<04:48,  1.98batch/s, Batch Loss=0.0085, Avg Loss=0.0765, Time Left=5.42 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2824/3393 [23:31<04:51,  1.95batch/s, Batch Loss=0.0085, Avg Loss=0.0765, Time Left=5.42 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2824/3393 [23:31<04:51,  1.95batch/s, Batch Loss=0.0013, Avg Loss=0.0765, Time Left=5.41 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2825/3393 [23:31<04:46,  1.98batch/s, Batch Loss=0.0013, Avg Loss=0.0765, Time Left=5.41 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2825/3393 [23:32<04:46,  1.98batch/s, Batch Loss=0.0018, Avg Loss=0.0764, Time Left=5.40 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2826/3393 [23:32<04:47,  1.97batch/s, Batch Loss=0.0018, Avg Loss=0.0764, Time Left=5.40 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2826/3393 [23:32<04:47,  1.97batch/s, Batch Loss=0.0638, Avg Loss=0.0764, Time Left=5.39 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2827/3393 [23:32<04:49,  1.96batch/s, Batch Loss=0.0638, Avg Loss=0.0764, Time Left=5.39 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2827/3393 [23:33<04:49,  1.96batch/s, Batch Loss=0.0385, Avg Loss=0.0764, Time Left=5.38 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2828/3393 [23:33<04:50,  1.94batch/s, Batch Loss=0.0385, Avg Loss=0.0764, Time Left=5.38 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2828/3393 [23:33<04:50,  1.94batch/s, Batch Loss=0.0145, Avg Loss=0.0764, Time Left=5.37 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2829/3393 [23:33<04:46,  1.97batch/s, Batch Loss=0.0145, Avg Loss=0.0764, Time Left=5.37 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2829/3393 [23:34<04:46,  1.97batch/s, Batch Loss=0.0627, Avg Loss=0.0764, Time Left=5.36 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2830/3393 [23:34<04:44,  1.98batch/s, Batch Loss=0.0627, Avg Loss=0.0764, Time Left=5.36 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2830/3393 [23:34<04:44,  1.98batch/s, Batch Loss=0.2004, Avg Loss=0.0764, Time Left=5.36 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2831/3393 [23:34<04:46,  1.96batch/s, Batch Loss=0.2004, Avg Loss=0.0764, Time Left=5.36 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2831/3393 [23:35<04:46,  1.96batch/s, Batch Loss=0.0330, Avg Loss=0.0764, Time Left=5.35 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2832/3393 [23:35<04:46,  1.96batch/s, Batch Loss=0.0330, Avg Loss=0.0764, Time Left=5.35 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2832/3393 [23:35<04:46,  1.96batch/s, Batch Loss=0.1162, Avg Loss=0.0764, Time Left=5.34 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2833/3393 [23:35<04:44,  1.97batch/s, Batch Loss=0.1162, Avg Loss=0.0764, Time Left=5.34 \u001b[A\n",
      "Epoch 3/3 - Training:  83%|▊| 2833/3393 [23:36<04:44,  1.97batch/s, Batch Loss=0.2014, Avg Loss=0.0765, Time Left=5.33 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2834/3393 [23:36<04:45,  1.96batch/s, Batch Loss=0.2014, Avg Loss=0.0765, Time Left=5.33 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2834/3393 [23:36<04:45,  1.96batch/s, Batch Loss=0.0011, Avg Loss=0.0764, Time Left=5.32 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2835/3393 [23:36<04:45,  1.95batch/s, Batch Loss=0.0011, Avg Loss=0.0764, Time Left=5.32 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2835/3393 [23:37<04:45,  1.95batch/s, Batch Loss=0.1356, Avg Loss=0.0765, Time Left=5.31 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  84%|▊| 2836/3393 [23:37<04:46,  1.95batch/s, Batch Loss=0.1356, Avg Loss=0.0765, Time Left=5.31 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2836/3393 [23:37<04:46,  1.95batch/s, Batch Loss=0.0387, Avg Loss=0.0765, Time Left=5.30 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2837/3393 [23:37<04:43,  1.96batch/s, Batch Loss=0.0387, Avg Loss=0.0765, Time Left=5.30 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2837/3393 [23:38<04:43,  1.96batch/s, Batch Loss=0.0388, Avg Loss=0.0764, Time Left=5.30 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2838/3393 [23:38<04:46,  1.94batch/s, Batch Loss=0.0388, Avg Loss=0.0764, Time Left=5.30 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2838/3393 [23:38<04:46,  1.94batch/s, Batch Loss=0.0008, Avg Loss=0.0764, Time Left=5.29 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2839/3393 [23:38<04:41,  1.97batch/s, Batch Loss=0.0008, Avg Loss=0.0764, Time Left=5.29 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2839/3393 [23:39<04:41,  1.97batch/s, Batch Loss=0.0390, Avg Loss=0.0764, Time Left=5.28 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2840/3393 [23:39<04:44,  1.94batch/s, Batch Loss=0.0390, Avg Loss=0.0764, Time Left=5.28 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2840/3393 [23:40<04:44,  1.94batch/s, Batch Loss=0.0354, Avg Loss=0.0764, Time Left=5.27 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2841/3393 [23:40<04:39,  1.98batch/s, Batch Loss=0.0354, Avg Loss=0.0764, Time Left=5.27 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2841/3393 [23:40<04:39,  1.98batch/s, Batch Loss=0.0946, Avg Loss=0.0764, Time Left=5.26 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2842/3393 [23:40<04:43,  1.95batch/s, Batch Loss=0.0946, Avg Loss=0.0764, Time Left=5.26 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2842/3393 [23:41<04:43,  1.95batch/s, Batch Loss=0.0052, Avg Loss=0.0764, Time Left=5.25 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2843/3393 [23:41<04:43,  1.94batch/s, Batch Loss=0.0052, Avg Loss=0.0764, Time Left=5.25 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2843/3393 [23:41<04:43,  1.94batch/s, Batch Loss=0.0407, Avg Loss=0.0763, Time Left=5.25 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2844/3393 [23:41<04:42,  1.94batch/s, Batch Loss=0.0407, Avg Loss=0.0763, Time Left=5.25 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2844/3393 [23:42<04:42,  1.94batch/s, Batch Loss=0.0433, Avg Loss=0.0763, Time Left=5.24 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2845/3393 [23:42<04:37,  1.98batch/s, Batch Loss=0.0433, Avg Loss=0.0763, Time Left=5.24 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2845/3393 [23:42<04:37,  1.98batch/s, Batch Loss=0.0179, Avg Loss=0.0763, Time Left=5.23 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2846/3393 [23:42<04:38,  1.96batch/s, Batch Loss=0.0179, Avg Loss=0.0763, Time Left=5.23 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2846/3393 [23:43<04:38,  1.96batch/s, Batch Loss=0.3513, Avg Loss=0.0764, Time Left=5.22 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2847/3393 [23:43<04:39,  1.95batch/s, Batch Loss=0.3513, Avg Loss=0.0764, Time Left=5.22 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2847/3393 [23:43<04:39,  1.95batch/s, Batch Loss=0.0052, Avg Loss=0.0764, Time Left=5.21 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2848/3393 [23:43<04:39,  1.95batch/s, Batch Loss=0.0052, Avg Loss=0.0764, Time Left=5.21 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2848/3393 [23:44<04:39,  1.95batch/s, Batch Loss=0.2686, Avg Loss=0.0765, Time Left=5.20 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2849/3393 [23:44<04:40,  1.94batch/s, Batch Loss=0.2686, Avg Loss=0.0765, Time Left=5.20 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2849/3393 [23:44<04:40,  1.94batch/s, Batch Loss=0.0421, Avg Loss=0.0764, Time Left=5.19 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2850/3393 [23:44<04:42,  1.92batch/s, Batch Loss=0.0421, Avg Loss=0.0764, Time Left=5.19 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2850/3393 [23:45<04:42,  1.92batch/s, Batch Loss=0.0582, Avg Loss=0.0764, Time Left=5.19 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2851/3393 [23:45<04:38,  1.94batch/s, Batch Loss=0.0582, Avg Loss=0.0764, Time Left=5.19 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2851/3393 [23:45<04:38,  1.94batch/s, Batch Loss=0.0314, Avg Loss=0.0764, Time Left=5.18 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2852/3393 [23:45<04:33,  1.98batch/s, Batch Loss=0.0314, Avg Loss=0.0764, Time Left=5.18 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2852/3393 [23:46<04:33,  1.98batch/s, Batch Loss=0.2612, Avg Loss=0.0765, Time Left=5.17 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2853/3393 [23:46<04:34,  1.97batch/s, Batch Loss=0.2612, Avg Loss=0.0765, Time Left=5.17 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2853/3393 [23:46<04:34,  1.97batch/s, Batch Loss=0.0092, Avg Loss=0.0765, Time Left=5.16 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2854/3393 [23:46<04:35,  1.96batch/s, Batch Loss=0.0092, Avg Loss=0.0765, Time Left=5.16 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2854/3393 [23:47<04:35,  1.96batch/s, Batch Loss=0.0962, Avg Loss=0.0765, Time Left=5.15 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2855/3393 [23:47<04:36,  1.95batch/s, Batch Loss=0.0962, Avg Loss=0.0765, Time Left=5.15 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2855/3393 [23:47<04:36,  1.95batch/s, Batch Loss=0.1961, Avg Loss=0.0765, Time Left=5.14 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2856/3393 [23:47<04:35,  1.95batch/s, Batch Loss=0.1961, Avg Loss=0.0765, Time Left=5.14 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2856/3393 [23:48<04:35,  1.95batch/s, Batch Loss=0.0147, Avg Loss=0.0765, Time Left=5.13 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2857/3393 [23:48<04:38,  1.92batch/s, Batch Loss=0.0147, Avg Loss=0.0765, Time Left=5.13 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2857/3393 [23:48<04:38,  1.92batch/s, Batch Loss=0.0491, Avg Loss=0.0765, Time Left=5.13 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2858/3393 [23:48<04:37,  1.93batch/s, Batch Loss=0.0491, Avg Loss=0.0765, Time Left=5.13 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2858/3393 [23:49<04:37,  1.93batch/s, Batch Loss=0.0823, Avg Loss=0.0765, Time Left=5.12 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2859/3393 [23:49<04:34,  1.95batch/s, Batch Loss=0.0823, Avg Loss=0.0765, Time Left=5.12 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2859/3393 [23:49<04:34,  1.95batch/s, Batch Loss=0.0950, Avg Loss=0.0765, Time Left=5.11 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2860/3393 [23:49<04:36,  1.93batch/s, Batch Loss=0.0950, Avg Loss=0.0765, Time Left=5.11 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2860/3393 [23:50<04:36,  1.93batch/s, Batch Loss=0.0568, Avg Loss=0.0765, Time Left=5.10 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2861/3393 [23:50<04:33,  1.95batch/s, Batch Loss=0.0568, Avg Loss=0.0765, Time Left=5.10 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2861/3393 [23:50<04:33,  1.95batch/s, Batch Loss=0.0839, Avg Loss=0.0765, Time Left=5.09 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2862/3393 [23:50<04:35,  1.93batch/s, Batch Loss=0.0839, Avg Loss=0.0765, Time Left=5.09 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2862/3393 [23:51<04:35,  1.93batch/s, Batch Loss=0.0372, Avg Loss=0.0765, Time Left=5.08 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2863/3393 [23:51<04:27,  1.98batch/s, Batch Loss=0.0372, Avg Loss=0.0765, Time Left=5.08 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2863/3393 [23:51<04:27,  1.98batch/s, Batch Loss=0.0157, Avg Loss=0.0765, Time Left=5.07 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2864/3393 [23:51<04:31,  1.95batch/s, Batch Loss=0.0157, Avg Loss=0.0765, Time Left=5.07 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2864/3393 [23:52<04:31,  1.95batch/s, Batch Loss=0.0276, Avg Loss=0.0764, Time Left=5.07 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2865/3393 [23:52<04:23,  2.00batch/s, Batch Loss=0.0276, Avg Loss=0.0764, Time Left=5.07 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2865/3393 [23:52<04:23,  2.00batch/s, Batch Loss=0.1185, Avg Loss=0.0765, Time Left=5.06 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2866/3393 [23:52<04:29,  1.96batch/s, Batch Loss=0.1185, Avg Loss=0.0765, Time Left=5.06 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2866/3393 [23:53<04:29,  1.96batch/s, Batch Loss=0.0564, Avg Loss=0.0764, Time Left=5.05 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2867/3393 [23:53<04:26,  1.97batch/s, Batch Loss=0.0564, Avg Loss=0.0764, Time Left=5.05 \u001b[A\n",
      "Epoch 3/3 - Training:  84%|▊| 2867/3393 [23:53<04:26,  1.97batch/s, Batch Loss=0.0525, Avg Loss=0.0764, Time Left=5.04 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2868/3393 [23:53<04:30,  1.94batch/s, Batch Loss=0.0525, Avg Loss=0.0764, Time Left=5.04 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2868/3393 [23:54<04:30,  1.94batch/s, Batch Loss=0.1157, Avg Loss=0.0764, Time Left=5.03 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  85%|▊| 2869/3393 [23:54<04:25,  1.98batch/s, Batch Loss=0.1157, Avg Loss=0.0764, Time Left=5.03 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2869/3393 [23:54<04:25,  1.98batch/s, Batch Loss=0.0965, Avg Loss=0.0765, Time Left=5.02 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2870/3393 [23:54<04:28,  1.95batch/s, Batch Loss=0.0965, Avg Loss=0.0765, Time Left=5.02 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2870/3393 [23:55<04:28,  1.95batch/s, Batch Loss=0.0211, Avg Loss=0.0764, Time Left=5.01 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2871/3393 [23:55<04:23,  1.98batch/s, Batch Loss=0.0211, Avg Loss=0.0764, Time Left=5.01 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2871/3393 [23:55<04:23,  1.98batch/s, Batch Loss=0.0083, Avg Loss=0.0764, Time Left=5.01 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2872/3393 [23:55<04:27,  1.95batch/s, Batch Loss=0.0083, Avg Loss=0.0764, Time Left=5.01 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2872/3393 [23:56<04:27,  1.95batch/s, Batch Loss=0.0236, Avg Loss=0.0764, Time Left=5.00 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2873/3393 [23:56<04:25,  1.96batch/s, Batch Loss=0.0236, Avg Loss=0.0764, Time Left=5.00 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2873/3393 [23:56<04:25,  1.96batch/s, Batch Loss=0.0432, Avg Loss=0.0764, Time Left=4.99 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2874/3393 [23:56<04:27,  1.94batch/s, Batch Loss=0.0432, Avg Loss=0.0764, Time Left=4.99 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2874/3393 [23:57<04:27,  1.94batch/s, Batch Loss=0.0138, Avg Loss=0.0764, Time Left=4.98 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2875/3393 [23:57<04:25,  1.95batch/s, Batch Loss=0.0138, Avg Loss=0.0764, Time Left=4.98 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2875/3393 [23:57<04:25,  1.95batch/s, Batch Loss=0.0702, Avg Loss=0.0764, Time Left=4.97 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2876/3393 [23:57<04:25,  1.95batch/s, Batch Loss=0.0702, Avg Loss=0.0764, Time Left=4.97 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2876/3393 [23:58<04:25,  1.95batch/s, Batch Loss=0.0033, Avg Loss=0.0763, Time Left=4.96 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2877/3393 [23:58<04:23,  1.96batch/s, Batch Loss=0.0033, Avg Loss=0.0763, Time Left=4.96 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2877/3393 [23:58<04:23,  1.96batch/s, Batch Loss=0.0492, Avg Loss=0.0763, Time Left=4.96 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2878/3393 [23:58<04:23,  1.95batch/s, Batch Loss=0.0492, Avg Loss=0.0763, Time Left=4.96 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2878/3393 [23:59<04:23,  1.95batch/s, Batch Loss=0.1947, Avg Loss=0.0764, Time Left=4.95 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2879/3393 [23:59<04:25,  1.94batch/s, Batch Loss=0.1947, Avg Loss=0.0764, Time Left=4.95 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2879/3393 [24:00<04:25,  1.94batch/s, Batch Loss=0.0241, Avg Loss=0.0763, Time Left=4.94 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2880/3393 [24:00<04:23,  1.95batch/s, Batch Loss=0.0241, Avg Loss=0.0763, Time Left=4.94 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2880/3393 [24:00<04:23,  1.95batch/s, Batch Loss=0.0389, Avg Loss=0.0763, Time Left=4.93 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2881/3393 [24:00<04:18,  1.98batch/s, Batch Loss=0.0389, Avg Loss=0.0763, Time Left=4.93 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2881/3393 [24:01<04:18,  1.98batch/s, Batch Loss=0.0040, Avg Loss=0.0763, Time Left=4.92 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2882/3393 [24:01<04:22,  1.95batch/s, Batch Loss=0.0040, Avg Loss=0.0763, Time Left=4.92 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2882/3393 [24:01<04:22,  1.95batch/s, Batch Loss=0.0023, Avg Loss=0.0763, Time Left=4.91 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2883/3393 [24:01<04:17,  1.98batch/s, Batch Loss=0.0023, Avg Loss=0.0763, Time Left=4.91 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2883/3393 [24:02<04:17,  1.98batch/s, Batch Loss=0.0095, Avg Loss=0.0763, Time Left=4.90 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2884/3393 [24:02<04:18,  1.97batch/s, Batch Loss=0.0095, Avg Loss=0.0763, Time Left=4.90 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2884/3393 [24:02<04:18,  1.97batch/s, Batch Loss=0.0168, Avg Loss=0.0762, Time Left=4.90 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2885/3393 [24:02<04:14,  1.99batch/s, Batch Loss=0.0168, Avg Loss=0.0762, Time Left=4.90 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2885/3393 [24:02<04:14,  1.99batch/s, Batch Loss=0.1080, Avg Loss=0.0762, Time Left=4.89 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2886/3393 [24:03<04:13,  2.00batch/s, Batch Loss=0.1080, Avg Loss=0.0762, Time Left=4.89 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2886/3393 [24:03<04:13,  2.00batch/s, Batch Loss=0.0047, Avg Loss=0.0762, Time Left=4.88 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2887/3393 [24:03<04:15,  1.98batch/s, Batch Loss=0.0047, Avg Loss=0.0762, Time Left=4.88 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2887/3393 [24:04<04:15,  1.98batch/s, Batch Loss=0.0733, Avg Loss=0.0762, Time Left=4.87 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2888/3393 [24:04<04:25,  1.90batch/s, Batch Loss=0.0733, Avg Loss=0.0762, Time Left=4.87 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2888/3393 [24:04<04:25,  1.90batch/s, Batch Loss=0.0486, Avg Loss=0.0762, Time Left=4.86 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2889/3393 [24:04<04:20,  1.93batch/s, Batch Loss=0.0486, Avg Loss=0.0762, Time Left=4.86 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2889/3393 [24:05<04:20,  1.93batch/s, Batch Loss=0.0015, Avg Loss=0.0762, Time Left=4.85 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2890/3393 [24:05<04:22,  1.92batch/s, Batch Loss=0.0015, Avg Loss=0.0762, Time Left=4.85 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2890/3393 [24:05<04:22,  1.92batch/s, Batch Loss=0.0059, Avg Loss=0.0762, Time Left=4.84 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2891/3393 [24:05<04:20,  1.93batch/s, Batch Loss=0.0059, Avg Loss=0.0762, Time Left=4.84 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2891/3393 [24:06<04:20,  1.93batch/s, Batch Loss=0.2789, Avg Loss=0.0762, Time Left=4.84 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2892/3393 [24:06<04:18,  1.94batch/s, Batch Loss=0.2789, Avg Loss=0.0762, Time Left=4.84 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2892/3393 [24:06<04:18,  1.94batch/s, Batch Loss=0.0722, Avg Loss=0.0762, Time Left=4.83 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2893/3393 [24:06<04:12,  1.98batch/s, Batch Loss=0.0722, Avg Loss=0.0762, Time Left=4.83 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2893/3393 [24:07<04:12,  1.98batch/s, Batch Loss=0.2142, Avg Loss=0.0763, Time Left=4.82 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2894/3393 [24:07<04:18,  1.93batch/s, Batch Loss=0.2142, Avg Loss=0.0763, Time Left=4.82 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2894/3393 [24:07<04:18,  1.93batch/s, Batch Loss=0.0029, Avg Loss=0.0763, Time Left=4.81 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2895/3393 [24:07<04:15,  1.95batch/s, Batch Loss=0.0029, Avg Loss=0.0763, Time Left=4.81 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2895/3393 [24:08<04:15,  1.95batch/s, Batch Loss=0.2053, Avg Loss=0.0763, Time Left=4.80 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2896/3393 [24:08<04:17,  1.93batch/s, Batch Loss=0.2053, Avg Loss=0.0763, Time Left=4.80 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2896/3393 [24:08<04:17,  1.93batch/s, Batch Loss=0.0022, Avg Loss=0.0763, Time Left=4.79 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2897/3393 [24:08<04:12,  1.96batch/s, Batch Loss=0.0022, Avg Loss=0.0763, Time Left=4.79 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2897/3393 [24:09<04:12,  1.96batch/s, Batch Loss=0.0363, Avg Loss=0.0763, Time Left=4.78 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2898/3393 [24:09<04:17,  1.92batch/s, Batch Loss=0.0363, Avg Loss=0.0763, Time Left=4.78 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2898/3393 [24:09<04:17,  1.92batch/s, Batch Loss=0.0275, Avg Loss=0.0762, Time Left=4.78 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2899/3393 [24:09<04:14,  1.94batch/s, Batch Loss=0.0275, Avg Loss=0.0762, Time Left=4.78 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2899/3393 [24:10<04:14,  1.94batch/s, Batch Loss=0.1181, Avg Loss=0.0763, Time Left=4.77 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2900/3393 [24:10<04:14,  1.94batch/s, Batch Loss=0.1181, Avg Loss=0.0763, Time Left=4.77 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2900/3393 [24:10<04:14,  1.94batch/s, Batch Loss=0.4475, Avg Loss=0.0764, Time Left=4.76 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2901/3393 [24:10<04:11,  1.96batch/s, Batch Loss=0.4475, Avg Loss=0.0764, Time Left=4.76 \u001b[A\n",
      "Epoch 3/3 - Training:  85%|▊| 2901/3393 [24:11<04:11,  1.96batch/s, Batch Loss=0.0272, Avg Loss=0.0764, Time Left=4.75 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  86%|▊| 2902/3393 [24:11<04:11,  1.95batch/s, Batch Loss=0.0272, Avg Loss=0.0764, Time Left=4.75 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2902/3393 [24:11<04:11,  1.95batch/s, Batch Loss=0.0331, Avg Loss=0.0764, Time Left=4.74 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2903/3393 [24:11<04:09,  1.96batch/s, Batch Loss=0.0331, Avg Loss=0.0764, Time Left=4.74 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2903/3393 [24:12<04:09,  1.96batch/s, Batch Loss=0.0505, Avg Loss=0.0763, Time Left=4.73 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2904/3393 [24:12<04:05,  1.99batch/s, Batch Loss=0.0505, Avg Loss=0.0763, Time Left=4.73 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2904/3393 [24:12<04:05,  1.99batch/s, Batch Loss=0.1490, Avg Loss=0.0764, Time Left=4.72 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2905/3393 [24:12<04:00,  2.03batch/s, Batch Loss=0.1490, Avg Loss=0.0764, Time Left=4.72 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2905/3393 [24:13<04:00,  2.03batch/s, Batch Loss=0.0171, Avg Loss=0.0763, Time Left=4.72 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2906/3393 [24:13<04:00,  2.03batch/s, Batch Loss=0.0171, Avg Loss=0.0763, Time Left=4.72 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2906/3393 [24:13<04:00,  2.03batch/s, Batch Loss=0.3042, Avg Loss=0.0764, Time Left=4.71 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2907/3393 [24:13<04:00,  2.02batch/s, Batch Loss=0.3042, Avg Loss=0.0764, Time Left=4.71 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2907/3393 [24:14<04:00,  2.02batch/s, Batch Loss=0.0410, Avg Loss=0.0764, Time Left=4.70 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2908/3393 [24:14<03:58,  2.03batch/s, Batch Loss=0.0410, Avg Loss=0.0764, Time Left=4.70 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2908/3393 [24:14<03:58,  2.03batch/s, Batch Loss=0.0394, Avg Loss=0.0764, Time Left=4.69 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2909/3393 [24:14<04:03,  1.98batch/s, Batch Loss=0.0394, Avg Loss=0.0764, Time Left=4.69 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2909/3393 [24:15<04:03,  1.98batch/s, Batch Loss=0.2586, Avg Loss=0.0765, Time Left=4.68 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2910/3393 [24:15<04:00,  2.01batch/s, Batch Loss=0.2586, Avg Loss=0.0765, Time Left=4.68 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2910/3393 [24:15<04:00,  2.01batch/s, Batch Loss=0.0279, Avg Loss=0.0764, Time Left=4.67 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2911/3393 [24:15<04:05,  1.97batch/s, Batch Loss=0.0279, Avg Loss=0.0764, Time Left=4.67 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2911/3393 [24:16<04:05,  1.97batch/s, Batch Loss=0.1387, Avg Loss=0.0765, Time Left=4.66 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2912/3393 [24:16<04:03,  1.97batch/s, Batch Loss=0.1387, Avg Loss=0.0765, Time Left=4.66 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2912/3393 [24:16<04:03,  1.97batch/s, Batch Loss=0.2234, Avg Loss=0.0765, Time Left=4.66 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2913/3393 [24:16<04:04,  1.96batch/s, Batch Loss=0.2234, Avg Loss=0.0765, Time Left=4.66 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2913/3393 [24:17<04:04,  1.96batch/s, Batch Loss=0.0315, Avg Loss=0.0765, Time Left=4.65 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2914/3393 [24:17<04:04,  1.96batch/s, Batch Loss=0.0315, Avg Loss=0.0765, Time Left=4.65 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2914/3393 [24:17<04:04,  1.96batch/s, Batch Loss=0.0127, Avg Loss=0.0765, Time Left=4.64 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2915/3393 [24:17<04:05,  1.95batch/s, Batch Loss=0.0127, Avg Loss=0.0765, Time Left=4.64 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2915/3393 [24:18<04:05,  1.95batch/s, Batch Loss=0.1033, Avg Loss=0.0765, Time Left=4.63 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2916/3393 [24:18<04:00,  1.98batch/s, Batch Loss=0.1033, Avg Loss=0.0765, Time Left=4.63 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2916/3393 [24:18<04:00,  1.98batch/s, Batch Loss=0.0208, Avg Loss=0.0765, Time Left=4.62 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2917/3393 [24:18<04:04,  1.95batch/s, Batch Loss=0.0208, Avg Loss=0.0765, Time Left=4.62 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2917/3393 [24:19<04:04,  1.95batch/s, Batch Loss=0.0838, Avg Loss=0.0765, Time Left=4.61 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2918/3393 [24:19<04:02,  1.96batch/s, Batch Loss=0.0838, Avg Loss=0.0765, Time Left=4.61 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2918/3393 [24:19<04:02,  1.96batch/s, Batch Loss=0.1788, Avg Loss=0.0765, Time Left=4.61 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2919/3393 [24:19<04:04,  1.94batch/s, Batch Loss=0.1788, Avg Loss=0.0765, Time Left=4.61 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2919/3393 [24:20<04:04,  1.94batch/s, Batch Loss=0.0182, Avg Loss=0.0765, Time Left=4.60 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2920/3393 [24:20<04:00,  1.97batch/s, Batch Loss=0.0182, Avg Loss=0.0765, Time Left=4.60 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2920/3393 [24:20<04:00,  1.97batch/s, Batch Loss=0.0088, Avg Loss=0.0765, Time Left=4.59 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2921/3393 [24:20<04:02,  1.94batch/s, Batch Loss=0.0088, Avg Loss=0.0765, Time Left=4.59 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2921/3393 [24:21<04:02,  1.94batch/s, Batch Loss=0.2096, Avg Loss=0.0765, Time Left=4.58 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2922/3393 [24:21<03:58,  1.98batch/s, Batch Loss=0.2096, Avg Loss=0.0765, Time Left=4.58 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2922/3393 [24:21<03:58,  1.98batch/s, Batch Loss=0.1859, Avg Loss=0.0766, Time Left=4.57 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2923/3393 [24:21<03:56,  1.99batch/s, Batch Loss=0.1859, Avg Loss=0.0766, Time Left=4.57 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2923/3393 [24:22<03:56,  1.99batch/s, Batch Loss=0.0974, Avg Loss=0.0766, Time Left=4.56 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2924/3393 [24:22<03:53,  2.01batch/s, Batch Loss=0.0974, Avg Loss=0.0766, Time Left=4.56 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2924/3393 [24:22<03:53,  2.01batch/s, Batch Loss=0.1328, Avg Loss=0.0766, Time Left=4.55 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2925/3393 [24:22<03:51,  2.03batch/s, Batch Loss=0.1328, Avg Loss=0.0766, Time Left=4.55 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2925/3393 [24:23<03:51,  2.03batch/s, Batch Loss=0.1064, Avg Loss=0.0766, Time Left=4.55 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2926/3393 [24:23<03:53,  2.00batch/s, Batch Loss=0.1064, Avg Loss=0.0766, Time Left=4.55 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2926/3393 [24:23<03:53,  2.00batch/s, Batch Loss=0.1541, Avg Loss=0.0766, Time Left=4.54 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2927/3393 [24:23<03:55,  1.98batch/s, Batch Loss=0.1541, Avg Loss=0.0766, Time Left=4.54 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2927/3393 [24:24<03:55,  1.98batch/s, Batch Loss=0.2332, Avg Loss=0.0767, Time Left=4.53 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2928/3393 [24:24<03:54,  1.98batch/s, Batch Loss=0.2332, Avg Loss=0.0767, Time Left=4.53 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2928/3393 [24:24<03:54,  1.98batch/s, Batch Loss=0.0805, Avg Loss=0.0767, Time Left=4.52 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2929/3393 [24:24<03:53,  1.99batch/s, Batch Loss=0.0805, Avg Loss=0.0767, Time Left=4.52 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2929/3393 [24:25<03:53,  1.99batch/s, Batch Loss=0.0961, Avg Loss=0.0767, Time Left=4.51 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2930/3393 [24:25<03:54,  1.97batch/s, Batch Loss=0.0961, Avg Loss=0.0767, Time Left=4.51 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2930/3393 [24:25<03:54,  1.97batch/s, Batch Loss=0.1012, Avg Loss=0.0767, Time Left=4.50 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2931/3393 [24:25<03:55,  1.96batch/s, Batch Loss=0.1012, Avg Loss=0.0767, Time Left=4.50 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2931/3393 [24:26<03:55,  1.96batch/s, Batch Loss=0.1075, Avg Loss=0.0767, Time Left=4.49 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2932/3393 [24:26<03:54,  1.97batch/s, Batch Loss=0.1075, Avg Loss=0.0767, Time Left=4.49 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2932/3393 [24:26<03:54,  1.97batch/s, Batch Loss=0.0728, Avg Loss=0.0767, Time Left=4.49 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2933/3393 [24:26<03:52,  1.98batch/s, Batch Loss=0.0728, Avg Loss=0.0767, Time Left=4.49 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2933/3393 [24:27<03:52,  1.98batch/s, Batch Loss=0.0162, Avg Loss=0.0767, Time Left=4.48 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2934/3393 [24:27<03:55,  1.95batch/s, Batch Loss=0.0162, Avg Loss=0.0767, Time Left=4.48 \u001b[A\n",
      "Epoch 3/3 - Training:  86%|▊| 2934/3393 [24:27<03:55,  1.95batch/s, Batch Loss=0.0426, Avg Loss=0.0767, Time Left=4.47 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  87%|▊| 2935/3393 [24:27<03:51,  1.98batch/s, Batch Loss=0.0426, Avg Loss=0.0767, Time Left=4.47 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2935/3393 [24:28<03:51,  1.98batch/s, Batch Loss=0.0290, Avg Loss=0.0766, Time Left=4.46 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2936/3393 [24:28<03:54,  1.95batch/s, Batch Loss=0.0290, Avg Loss=0.0766, Time Left=4.46 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2936/3393 [24:28<03:54,  1.95batch/s, Batch Loss=0.0489, Avg Loss=0.0766, Time Left=4.45 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2937/3393 [24:28<03:52,  1.96batch/s, Batch Loss=0.0489, Avg Loss=0.0766, Time Left=4.45 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2937/3393 [24:29<03:52,  1.96batch/s, Batch Loss=0.0176, Avg Loss=0.0766, Time Left=4.44 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2938/3393 [24:29<03:54,  1.94batch/s, Batch Loss=0.0176, Avg Loss=0.0766, Time Left=4.44 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2938/3393 [24:29<03:54,  1.94batch/s, Batch Loss=0.0350, Avg Loss=0.0766, Time Left=4.43 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2939/3393 [24:29<03:50,  1.97batch/s, Batch Loss=0.0350, Avg Loss=0.0766, Time Left=4.43 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2939/3393 [24:30<03:50,  1.97batch/s, Batch Loss=0.0585, Avg Loss=0.0766, Time Left=4.43 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2940/3393 [24:30<03:53,  1.94batch/s, Batch Loss=0.0585, Avg Loss=0.0766, Time Left=4.43 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2940/3393 [24:30<03:53,  1.94batch/s, Batch Loss=0.0118, Avg Loss=0.0766, Time Left=4.42 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2941/3393 [24:30<03:48,  1.98batch/s, Batch Loss=0.0118, Avg Loss=0.0766, Time Left=4.42 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2941/3393 [24:31<03:48,  1.98batch/s, Batch Loss=0.0361, Avg Loss=0.0766, Time Left=4.41 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2942/3393 [24:31<03:53,  1.93batch/s, Batch Loss=0.0361, Avg Loss=0.0766, Time Left=4.41 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2942/3393 [24:32<03:53,  1.93batch/s, Batch Loss=0.0146, Avg Loss=0.0765, Time Left=4.40 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2943/3393 [24:32<03:48,  1.97batch/s, Batch Loss=0.0146, Avg Loss=0.0765, Time Left=4.40 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2943/3393 [24:32<03:48,  1.97batch/s, Batch Loss=0.0662, Avg Loss=0.0765, Time Left=4.39 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2944/3393 [24:32<03:49,  1.96batch/s, Batch Loss=0.0662, Avg Loss=0.0765, Time Left=4.39 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2944/3393 [24:33<03:49,  1.96batch/s, Batch Loss=0.0617, Avg Loss=0.0765, Time Left=4.38 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2945/3393 [24:33<03:47,  1.97batch/s, Batch Loss=0.0617, Avg Loss=0.0765, Time Left=4.38 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2945/3393 [24:33<03:47,  1.97batch/s, Batch Loss=0.2154, Avg Loss=0.0766, Time Left=4.37 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2946/3393 [24:33<03:48,  1.96batch/s, Batch Loss=0.2154, Avg Loss=0.0766, Time Left=4.37 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2946/3393 [24:34<03:48,  1.96batch/s, Batch Loss=0.0032, Avg Loss=0.0766, Time Left=4.37 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2947/3393 [24:34<03:44,  1.99batch/s, Batch Loss=0.0032, Avg Loss=0.0766, Time Left=4.37 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2947/3393 [24:34<03:44,  1.99batch/s, Batch Loss=0.0188, Avg Loss=0.0765, Time Left=4.36 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2948/3393 [24:34<03:43,  1.99batch/s, Batch Loss=0.0188, Avg Loss=0.0765, Time Left=4.36 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2948/3393 [24:35<03:43,  1.99batch/s, Batch Loss=0.0469, Avg Loss=0.0765, Time Left=4.35 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2949/3393 [24:35<03:40,  2.01batch/s, Batch Loss=0.0469, Avg Loss=0.0765, Time Left=4.35 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2949/3393 [24:35<03:40,  2.01batch/s, Batch Loss=0.0096, Avg Loss=0.0765, Time Left=4.34 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2950/3393 [24:35<03:36,  2.05batch/s, Batch Loss=0.0096, Avg Loss=0.0765, Time Left=4.34 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2950/3393 [24:36<03:36,  2.05batch/s, Batch Loss=0.0020, Avg Loss=0.0765, Time Left=4.33 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2951/3393 [24:36<03:39,  2.02batch/s, Batch Loss=0.0020, Avg Loss=0.0765, Time Left=4.33 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2951/3393 [24:36<03:39,  2.02batch/s, Batch Loss=0.0011, Avg Loss=0.0764, Time Left=4.32 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2952/3393 [24:36<03:35,  2.05batch/s, Batch Loss=0.0011, Avg Loss=0.0764, Time Left=4.32 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2952/3393 [24:36<03:35,  2.05batch/s, Batch Loss=0.0189, Avg Loss=0.0764, Time Left=4.31 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2953/3393 [24:36<03:34,  2.06batch/s, Batch Loss=0.0189, Avg Loss=0.0764, Time Left=4.31 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2953/3393 [24:37<03:34,  2.06batch/s, Batch Loss=0.1770, Avg Loss=0.0765, Time Left=4.31 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2954/3393 [24:37<03:39,  2.00batch/s, Batch Loss=0.1770, Avg Loss=0.0765, Time Left=4.31 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2954/3393 [24:38<03:39,  2.00batch/s, Batch Loss=0.3233, Avg Loss=0.0765, Time Left=4.30 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2955/3393 [24:38<03:41,  1.98batch/s, Batch Loss=0.3233, Avg Loss=0.0765, Time Left=4.30 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2955/3393 [24:38<03:41,  1.98batch/s, Batch Loss=0.0837, Avg Loss=0.0766, Time Left=4.29 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2956/3393 [24:38<03:40,  1.99batch/s, Batch Loss=0.0837, Avg Loss=0.0766, Time Left=4.29 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2956/3393 [24:39<03:40,  1.99batch/s, Batch Loss=0.0098, Avg Loss=0.0765, Time Left=4.28 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2957/3393 [24:39<03:39,  1.99batch/s, Batch Loss=0.0098, Avg Loss=0.0765, Time Left=4.28 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2957/3393 [24:39<03:39,  1.99batch/s, Batch Loss=0.0627, Avg Loss=0.0765, Time Left=4.27 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2958/3393 [24:39<03:38,  1.99batch/s, Batch Loss=0.0627, Avg Loss=0.0765, Time Left=4.27 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2958/3393 [24:40<03:38,  1.99batch/s, Batch Loss=0.0390, Avg Loss=0.0765, Time Left=4.26 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2959/3393 [24:40<03:35,  2.01batch/s, Batch Loss=0.0390, Avg Loss=0.0765, Time Left=4.26 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2959/3393 [24:40<03:35,  2.01batch/s, Batch Loss=0.0255, Avg Loss=0.0765, Time Left=4.25 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2960/3393 [24:40<03:35,  2.01batch/s, Batch Loss=0.0255, Avg Loss=0.0765, Time Left=4.25 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2960/3393 [24:40<03:35,  2.01batch/s, Batch Loss=0.4650, Avg Loss=0.0766, Time Left=4.25 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2961/3393 [24:40<03:33,  2.03batch/s, Batch Loss=0.4650, Avg Loss=0.0766, Time Left=4.25 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2961/3393 [24:41<03:33,  2.03batch/s, Batch Loss=0.0072, Avg Loss=0.0766, Time Left=4.24 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2962/3393 [24:41<03:33,  2.02batch/s, Batch Loss=0.0072, Avg Loss=0.0766, Time Left=4.24 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2962/3393 [24:42<03:33,  2.02batch/s, Batch Loss=0.1961, Avg Loss=0.0766, Time Left=4.23 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2963/3393 [24:42<03:37,  1.97batch/s, Batch Loss=0.1961, Avg Loss=0.0766, Time Left=4.23 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2963/3393 [24:42<03:37,  1.97batch/s, Batch Loss=0.0199, Avg Loss=0.0766, Time Left=4.22 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2964/3393 [24:42<03:36,  1.98batch/s, Batch Loss=0.0199, Avg Loss=0.0766, Time Left=4.22 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2964/3393 [24:43<03:36,  1.98batch/s, Batch Loss=0.0568, Avg Loss=0.0766, Time Left=4.21 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2965/3393 [24:43<03:35,  1.98batch/s, Batch Loss=0.0568, Avg Loss=0.0766, Time Left=4.21 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2965/3393 [24:43<03:35,  1.98batch/s, Batch Loss=0.1810, Avg Loss=0.0767, Time Left=4.20 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2966/3393 [24:43<03:34,  1.99batch/s, Batch Loss=0.1810, Avg Loss=0.0767, Time Left=4.20 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2966/3393 [24:44<03:34,  1.99batch/s, Batch Loss=0.0026, Avg Loss=0.0766, Time Left=4.19 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2967/3393 [24:44<03:39,  1.94batch/s, Batch Loss=0.0026, Avg Loss=0.0766, Time Left=4.19 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2967/3393 [24:44<03:39,  1.94batch/s, Batch Loss=0.0713, Avg Loss=0.0766, Time Left=4.19 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  87%|▊| 2968/3393 [24:44<03:37,  1.95batch/s, Batch Loss=0.0713, Avg Loss=0.0766, Time Left=4.19 \u001b[A\n",
      "Epoch 3/3 - Training:  87%|▊| 2968/3393 [24:45<03:37,  1.95batch/s, Batch Loss=0.0786, Avg Loss=0.0766, Time Left=4.18 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2969/3393 [24:45<03:33,  1.98batch/s, Batch Loss=0.0786, Avg Loss=0.0766, Time Left=4.18 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2969/3393 [24:45<03:33,  1.98batch/s, Batch Loss=0.0260, Avg Loss=0.0766, Time Left=4.17 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2970/3393 [24:45<03:32,  1.99batch/s, Batch Loss=0.0260, Avg Loss=0.0766, Time Left=4.17 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2970/3393 [24:46<03:32,  1.99batch/s, Batch Loss=0.0076, Avg Loss=0.0766, Time Left=4.16 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2971/3393 [24:46<03:33,  1.98batch/s, Batch Loss=0.0076, Avg Loss=0.0766, Time Left=4.16 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2971/3393 [24:46<03:33,  1.98batch/s, Batch Loss=0.0021, Avg Loss=0.0766, Time Left=4.15 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2972/3393 [24:46<03:32,  1.98batch/s, Batch Loss=0.0021, Avg Loss=0.0766, Time Left=4.15 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2972/3393 [24:47<03:32,  1.98batch/s, Batch Loss=0.0061, Avg Loss=0.0765, Time Left=4.14 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2973/3393 [24:47<03:35,  1.95batch/s, Batch Loss=0.0061, Avg Loss=0.0765, Time Left=4.14 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2973/3393 [24:47<03:35,  1.95batch/s, Batch Loss=0.0029, Avg Loss=0.0765, Time Left=4.14 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2974/3393 [24:47<03:31,  1.98batch/s, Batch Loss=0.0029, Avg Loss=0.0765, Time Left=4.14 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2974/3393 [24:48<03:31,  1.98batch/s, Batch Loss=0.0012, Avg Loss=0.0765, Time Left=4.13 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2975/3393 [24:48<03:34,  1.95batch/s, Batch Loss=0.0012, Avg Loss=0.0765, Time Left=4.13 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2975/3393 [24:48<03:34,  1.95batch/s, Batch Loss=0.0804, Avg Loss=0.0765, Time Left=4.12 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2976/3393 [24:48<03:30,  1.98batch/s, Batch Loss=0.0804, Avg Loss=0.0765, Time Left=4.12 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2976/3393 [24:49<03:30,  1.98batch/s, Batch Loss=0.0672, Avg Loss=0.0765, Time Left=4.11 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2977/3393 [24:49<03:35,  1.93batch/s, Batch Loss=0.0672, Avg Loss=0.0765, Time Left=4.11 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2977/3393 [24:49<03:35,  1.93batch/s, Batch Loss=0.0009, Avg Loss=0.0765, Time Left=4.10 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2978/3393 [24:49<03:30,  1.97batch/s, Batch Loss=0.0009, Avg Loss=0.0765, Time Left=4.10 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2978/3393 [24:50<03:30,  1.97batch/s, Batch Loss=0.0119, Avg Loss=0.0764, Time Left=4.09 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2979/3393 [24:50<03:31,  1.96batch/s, Batch Loss=0.0119, Avg Loss=0.0764, Time Left=4.09 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2979/3393 [24:50<03:31,  1.96batch/s, Batch Loss=0.0012, Avg Loss=0.0764, Time Left=4.08 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2980/3393 [24:50<03:25,  2.01batch/s, Batch Loss=0.0012, Avg Loss=0.0764, Time Left=4.08 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2980/3393 [24:51<03:25,  2.01batch/s, Batch Loss=0.0105, Avg Loss=0.0764, Time Left=4.08 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2981/3393 [24:51<03:25,  2.01batch/s, Batch Loss=0.0105, Avg Loss=0.0764, Time Left=4.08 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2981/3393 [24:51<03:25,  2.01batch/s, Batch Loss=0.4427, Avg Loss=0.0765, Time Left=4.07 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2982/3393 [24:51<03:25,  2.00batch/s, Batch Loss=0.4427, Avg Loss=0.0765, Time Left=4.07 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2982/3393 [24:52<03:25,  2.00batch/s, Batch Loss=0.0784, Avg Loss=0.0765, Time Left=4.06 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2983/3393 [24:52<03:24,  2.00batch/s, Batch Loss=0.0784, Avg Loss=0.0765, Time Left=4.06 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2983/3393 [24:52<03:24,  2.00batch/s, Batch Loss=0.1593, Avg Loss=0.0765, Time Left=4.05 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2984/3393 [24:52<03:28,  1.96batch/s, Batch Loss=0.1593, Avg Loss=0.0765, Time Left=4.05 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2984/3393 [24:53<03:28,  1.96batch/s, Batch Loss=0.1899, Avg Loss=0.0766, Time Left=4.04 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2985/3393 [24:53<03:26,  1.97batch/s, Batch Loss=0.1899, Avg Loss=0.0766, Time Left=4.04 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2985/3393 [24:53<03:26,  1.97batch/s, Batch Loss=0.0025, Avg Loss=0.0766, Time Left=4.03 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2986/3393 [24:53<03:25,  1.98batch/s, Batch Loss=0.0025, Avg Loss=0.0766, Time Left=4.03 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2986/3393 [24:54<03:25,  1.98batch/s, Batch Loss=0.0332, Avg Loss=0.0765, Time Left=4.02 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2987/3393 [24:54<03:26,  1.97batch/s, Batch Loss=0.0332, Avg Loss=0.0765, Time Left=4.02 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2987/3393 [24:54<03:26,  1.97batch/s, Batch Loss=0.0196, Avg Loss=0.0765, Time Left=4.02 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2988/3393 [24:54<03:23,  2.00batch/s, Batch Loss=0.0196, Avg Loss=0.0765, Time Left=4.02 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2988/3393 [24:55<03:23,  2.00batch/s, Batch Loss=0.0248, Avg Loss=0.0765, Time Left=4.01 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2989/3393 [24:55<03:24,  1.98batch/s, Batch Loss=0.0248, Avg Loss=0.0765, Time Left=4.01 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2989/3393 [24:55<03:24,  1.98batch/s, Batch Loss=0.0794, Avg Loss=0.0765, Time Left=4.00 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2990/3393 [24:55<03:25,  1.96batch/s, Batch Loss=0.0794, Avg Loss=0.0765, Time Left=4.00 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2990/3393 [24:56<03:25,  1.96batch/s, Batch Loss=0.1778, Avg Loss=0.0765, Time Left=3.99 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2991/3393 [24:56<03:23,  1.97batch/s, Batch Loss=0.1778, Avg Loss=0.0765, Time Left=3.99 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2991/3393 [24:56<03:23,  1.97batch/s, Batch Loss=0.0332, Avg Loss=0.0765, Time Left=3.98 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2992/3393 [24:56<03:24,  1.96batch/s, Batch Loss=0.0332, Avg Loss=0.0765, Time Left=3.98 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2992/3393 [24:57<03:24,  1.96batch/s, Batch Loss=0.1050, Avg Loss=0.0765, Time Left=3.97 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2993/3393 [24:57<03:22,  1.97batch/s, Batch Loss=0.1050, Avg Loss=0.0765, Time Left=3.97 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2993/3393 [24:57<03:22,  1.97batch/s, Batch Loss=0.1124, Avg Loss=0.0765, Time Left=3.96 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2994/3393 [24:57<03:25,  1.94batch/s, Batch Loss=0.1124, Avg Loss=0.0765, Time Left=3.96 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2994/3393 [24:58<03:25,  1.94batch/s, Batch Loss=0.0424, Avg Loss=0.0765, Time Left=3.96 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2995/3393 [24:58<03:21,  1.98batch/s, Batch Loss=0.0424, Avg Loss=0.0765, Time Left=3.96 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2995/3393 [24:58<03:21,  1.98batch/s, Batch Loss=0.1461, Avg Loss=0.0766, Time Left=3.95 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2996/3393 [24:58<03:23,  1.95batch/s, Batch Loss=0.1461, Avg Loss=0.0766, Time Left=3.95 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2996/3393 [24:59<03:23,  1.95batch/s, Batch Loss=0.1687, Avg Loss=0.0766, Time Left=3.94 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2997/3393 [24:59<03:22,  1.96batch/s, Batch Loss=0.1687, Avg Loss=0.0766, Time Left=3.94 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2997/3393 [24:59<03:22,  1.96batch/s, Batch Loss=0.0728, Avg Loss=0.0766, Time Left=3.93 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2998/3393 [24:59<03:24,  1.94batch/s, Batch Loss=0.0728, Avg Loss=0.0766, Time Left=3.93 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2998/3393 [25:00<03:24,  1.94batch/s, Batch Loss=0.0303, Avg Loss=0.0766, Time Left=3.92 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2999/3393 [25:00<03:17,  1.99batch/s, Batch Loss=0.0303, Avg Loss=0.0766, Time Left=3.92 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 2999/3393 [25:00<03:17,  1.99batch/s, Batch Loss=0.0945, Avg Loss=0.0766, Time Left=3.91 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 3000/3393 [25:00<03:17,  1.99batch/s, Batch Loss=0.0945, Avg Loss=0.0766, Time Left=3.91 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 3000/3393 [25:01<03:17,  1.99batch/s, Batch Loss=0.0054, Avg Loss=0.0766, Time Left=3.90 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  88%|▉| 3001/3393 [25:01<03:14,  2.01batch/s, Batch Loss=0.0054, Avg Loss=0.0766, Time Left=3.90 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 3001/3393 [25:01<03:14,  2.01batch/s, Batch Loss=0.0151, Avg Loss=0.0765, Time Left=3.90 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 3002/3393 [25:01<03:12,  2.03batch/s, Batch Loss=0.0151, Avg Loss=0.0765, Time Left=3.90 \u001b[A\n",
      "Epoch 3/3 - Training:  88%|▉| 3002/3393 [25:02<03:12,  2.03batch/s, Batch Loss=0.0152, Avg Loss=0.0765, Time Left=3.89 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3003/3393 [25:02<03:14,  2.00batch/s, Batch Loss=0.0152, Avg Loss=0.0765, Time Left=3.89 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3003/3393 [25:02<03:14,  2.00batch/s, Batch Loss=0.0010, Avg Loss=0.0765, Time Left=3.88 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3004/3393 [25:02<03:14,  2.00batch/s, Batch Loss=0.0010, Avg Loss=0.0765, Time Left=3.88 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3004/3393 [25:03<03:14,  2.00batch/s, Batch Loss=0.0215, Avg Loss=0.0765, Time Left=3.87 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3005/3393 [25:03<03:15,  1.98batch/s, Batch Loss=0.0215, Avg Loss=0.0765, Time Left=3.87 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3005/3393 [25:03<03:15,  1.98batch/s, Batch Loss=0.0963, Avg Loss=0.0765, Time Left=3.86 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3006/3393 [25:03<03:14,  1.99batch/s, Batch Loss=0.0963, Avg Loss=0.0765, Time Left=3.86 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3006/3393 [25:04<03:14,  1.99batch/s, Batch Loss=0.0346, Avg Loss=0.0765, Time Left=3.85 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3007/3393 [25:04<03:17,  1.95batch/s, Batch Loss=0.0346, Avg Loss=0.0765, Time Left=3.85 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3007/3393 [25:04<03:17,  1.95batch/s, Batch Loss=0.0258, Avg Loss=0.0764, Time Left=3.84 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3008/3393 [25:04<03:15,  1.97batch/s, Batch Loss=0.0258, Avg Loss=0.0764, Time Left=3.84 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3008/3393 [25:05<03:15,  1.97batch/s, Batch Loss=0.0386, Avg Loss=0.0764, Time Left=3.84 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3009/3393 [25:05<03:16,  1.96batch/s, Batch Loss=0.0386, Avg Loss=0.0764, Time Left=3.84 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3009/3393 [25:05<03:16,  1.96batch/s, Batch Loss=0.0179, Avg Loss=0.0764, Time Left=3.83 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3010/3393 [25:05<03:14,  1.97batch/s, Batch Loss=0.0179, Avg Loss=0.0764, Time Left=3.83 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3010/3393 [25:06<03:14,  1.97batch/s, Batch Loss=0.0590, Avg Loss=0.0764, Time Left=3.82 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3011/3393 [25:06<03:16,  1.94batch/s, Batch Loss=0.0590, Avg Loss=0.0764, Time Left=3.82 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3011/3393 [25:06<03:16,  1.94batch/s, Batch Loss=0.0313, Avg Loss=0.0764, Time Left=3.81 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3012/3393 [25:06<03:14,  1.96batch/s, Batch Loss=0.0313, Avg Loss=0.0764, Time Left=3.81 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3012/3393 [25:07<03:14,  1.96batch/s, Batch Loss=0.0877, Avg Loss=0.0764, Time Left=3.80 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3013/3393 [25:07<03:16,  1.94batch/s, Batch Loss=0.0877, Avg Loss=0.0764, Time Left=3.80 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3013/3393 [25:07<03:16,  1.94batch/s, Batch Loss=0.1119, Avg Loss=0.0764, Time Left=3.79 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3014/3393 [25:07<03:14,  1.95batch/s, Batch Loss=0.1119, Avg Loss=0.0764, Time Left=3.79 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3014/3393 [25:08<03:14,  1.95batch/s, Batch Loss=0.0707, Avg Loss=0.0764, Time Left=3.79 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3015/3393 [25:08<03:14,  1.95batch/s, Batch Loss=0.0707, Avg Loss=0.0764, Time Left=3.79 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3015/3393 [25:08<03:14,  1.95batch/s, Batch Loss=0.0516, Avg Loss=0.0764, Time Left=3.78 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3016/3393 [25:08<03:12,  1.96batch/s, Batch Loss=0.0516, Avg Loss=0.0764, Time Left=3.78 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3016/3393 [25:09<03:12,  1.96batch/s, Batch Loss=0.0198, Avg Loss=0.0764, Time Left=3.77 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3017/3393 [25:09<03:14,  1.93batch/s, Batch Loss=0.0198, Avg Loss=0.0764, Time Left=3.77 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3017/3393 [25:09<03:14,  1.93batch/s, Batch Loss=0.0166, Avg Loss=0.0764, Time Left=3.76 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3018/3393 [25:09<03:12,  1.95batch/s, Batch Loss=0.0166, Avg Loss=0.0764, Time Left=3.76 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3018/3393 [25:10<03:12,  1.95batch/s, Batch Loss=0.0142, Avg Loss=0.0763, Time Left=3.75 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3019/3393 [25:10<03:13,  1.93batch/s, Batch Loss=0.0142, Avg Loss=0.0763, Time Left=3.75 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3019/3393 [25:10<03:13,  1.93batch/s, Batch Loss=0.0456, Avg Loss=0.0763, Time Left=3.74 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3020/3393 [25:10<03:09,  1.97batch/s, Batch Loss=0.0456, Avg Loss=0.0763, Time Left=3.74 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3020/3393 [25:11<03:09,  1.97batch/s, Batch Loss=0.0133, Avg Loss=0.0763, Time Left=3.73 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3021/3393 [25:11<03:11,  1.94batch/s, Batch Loss=0.0133, Avg Loss=0.0763, Time Left=3.73 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3021/3393 [25:11<03:11,  1.94batch/s, Batch Loss=0.0681, Avg Loss=0.0763, Time Left=3.73 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3022/3393 [25:11<03:09,  1.96batch/s, Batch Loss=0.0681, Avg Loss=0.0763, Time Left=3.73 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3022/3393 [25:12<03:09,  1.96batch/s, Batch Loss=0.0018, Avg Loss=0.0763, Time Left=3.72 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3023/3393 [25:12<03:07,  1.97batch/s, Batch Loss=0.0018, Avg Loss=0.0763, Time Left=3.72 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3023/3393 [25:12<03:07,  1.97batch/s, Batch Loss=0.0023, Avg Loss=0.0762, Time Left=3.71 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3024/3393 [25:12<03:06,  1.98batch/s, Batch Loss=0.0023, Avg Loss=0.0762, Time Left=3.71 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3024/3393 [25:13<03:06,  1.98batch/s, Batch Loss=0.0989, Avg Loss=0.0763, Time Left=3.70 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3025/3393 [25:13<03:05,  1.98batch/s, Batch Loss=0.0989, Avg Loss=0.0763, Time Left=3.70 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3025/3393 [25:13<03:05,  1.98batch/s, Batch Loss=0.0223, Avg Loss=0.0762, Time Left=3.69 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3026/3393 [25:13<03:04,  1.99batch/s, Batch Loss=0.0223, Avg Loss=0.0762, Time Left=3.69 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3026/3393 [25:14<03:04,  1.99batch/s, Batch Loss=0.0007, Avg Loss=0.0762, Time Left=3.68 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3027/3393 [25:14<03:08,  1.94batch/s, Batch Loss=0.0007, Avg Loss=0.0762, Time Left=3.68 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3027/3393 [25:15<03:08,  1.94batch/s, Batch Loss=0.1405, Avg Loss=0.0762, Time Left=3.67 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3028/3393 [25:15<03:06,  1.95batch/s, Batch Loss=0.1405, Avg Loss=0.0762, Time Left=3.67 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3028/3393 [25:15<03:06,  1.95batch/s, Batch Loss=0.1179, Avg Loss=0.0762, Time Left=3.67 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3029/3393 [25:15<03:06,  1.95batch/s, Batch Loss=0.1179, Avg Loss=0.0762, Time Left=3.67 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3029/3393 [25:16<03:06,  1.95batch/s, Batch Loss=0.1571, Avg Loss=0.0763, Time Left=3.66 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3030/3393 [25:16<03:03,  1.98batch/s, Batch Loss=0.1571, Avg Loss=0.0763, Time Left=3.66 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3030/3393 [25:16<03:03,  1.98batch/s, Batch Loss=0.0032, Avg Loss=0.0763, Time Left=3.65 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3031/3393 [25:16<03:07,  1.93batch/s, Batch Loss=0.0032, Avg Loss=0.0763, Time Left=3.65 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3031/3393 [25:17<03:07,  1.93batch/s, Batch Loss=0.1392, Avg Loss=0.0763, Time Left=3.64 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3032/3393 [25:17<03:03,  1.97batch/s, Batch Loss=0.1392, Avg Loss=0.0763, Time Left=3.64 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3032/3393 [25:17<03:03,  1.97batch/s, Batch Loss=0.0262, Avg Loss=0.0763, Time Left=3.63 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3033/3393 [25:17<03:05,  1.94batch/s, Batch Loss=0.0262, Avg Loss=0.0763, Time Left=3.63 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3033/3393 [25:18<03:05,  1.94batch/s, Batch Loss=0.2092, Avg Loss=0.0763, Time Left=3.62 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  89%|▉| 3034/3393 [25:18<03:04,  1.94batch/s, Batch Loss=0.2092, Avg Loss=0.0763, Time Left=3.62 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3034/3393 [25:18<03:04,  1.94batch/s, Batch Loss=0.0361, Avg Loss=0.0763, Time Left=3.61 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3035/3393 [25:18<03:03,  1.95batch/s, Batch Loss=0.0361, Avg Loss=0.0763, Time Left=3.61 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3035/3393 [25:19<03:03,  1.95batch/s, Batch Loss=0.1069, Avg Loss=0.0763, Time Left=3.61 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3036/3393 [25:19<02:58,  2.00batch/s, Batch Loss=0.1069, Avg Loss=0.0763, Time Left=3.61 \u001b[A\n",
      "Epoch 3/3 - Training:  89%|▉| 3036/3393 [25:19<02:58,  2.00batch/s, Batch Loss=0.0385, Avg Loss=0.0763, Time Left=3.60 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3037/3393 [25:19<02:57,  2.00batch/s, Batch Loss=0.0385, Avg Loss=0.0763, Time Left=3.60 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3037/3393 [25:20<02:57,  2.00batch/s, Batch Loss=0.0139, Avg Loss=0.0763, Time Left=3.59 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3038/3393 [25:20<02:57,  2.00batch/s, Batch Loss=0.0139, Avg Loss=0.0763, Time Left=3.59 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3038/3393 [25:20<02:57,  2.00batch/s, Batch Loss=0.2126, Avg Loss=0.0763, Time Left=3.58 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3039/3393 [25:20<02:56,  2.01batch/s, Batch Loss=0.2126, Avg Loss=0.0763, Time Left=3.58 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3039/3393 [25:21<02:56,  2.01batch/s, Batch Loss=0.0008, Avg Loss=0.0763, Time Left=3.57 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3040/3393 [25:21<02:59,  1.96batch/s, Batch Loss=0.0008, Avg Loss=0.0763, Time Left=3.57 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3040/3393 [25:21<02:59,  1.96batch/s, Batch Loss=0.0307, Avg Loss=0.0763, Time Left=3.56 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3041/3393 [25:21<02:56,  1.99batch/s, Batch Loss=0.0307, Avg Loss=0.0763, Time Left=3.56 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3041/3393 [25:22<02:56,  1.99batch/s, Batch Loss=0.0036, Avg Loss=0.0762, Time Left=3.55 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3042/3393 [25:22<02:59,  1.96batch/s, Batch Loss=0.0036, Avg Loss=0.0762, Time Left=3.55 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3042/3393 [25:22<02:59,  1.96batch/s, Batch Loss=0.0053, Avg Loss=0.0762, Time Left=3.55 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3043/3393 [25:22<02:56,  1.99batch/s, Batch Loss=0.0053, Avg Loss=0.0762, Time Left=3.55 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3043/3393 [25:23<02:56,  1.99batch/s, Batch Loss=0.0163, Avg Loss=0.0762, Time Left=3.54 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3044/3393 [25:23<02:58,  1.95batch/s, Batch Loss=0.0163, Avg Loss=0.0762, Time Left=3.54 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3044/3393 [25:23<02:58,  1.95batch/s, Batch Loss=0.0007, Avg Loss=0.0762, Time Left=3.53 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3045/3393 [25:23<02:55,  1.98batch/s, Batch Loss=0.0007, Avg Loss=0.0762, Time Left=3.53 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3045/3393 [25:24<02:55,  1.98batch/s, Batch Loss=0.1612, Avg Loss=0.0762, Time Left=3.52 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3046/3393 [25:24<02:59,  1.94batch/s, Batch Loss=0.1612, Avg Loss=0.0762, Time Left=3.52 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3046/3393 [25:24<02:59,  1.94batch/s, Batch Loss=0.0091, Avg Loss=0.0762, Time Left=3.51 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3047/3393 [25:24<02:57,  1.95batch/s, Batch Loss=0.0091, Avg Loss=0.0762, Time Left=3.51 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3047/3393 [25:25<02:57,  1.95batch/s, Batch Loss=0.0068, Avg Loss=0.0762, Time Left=3.50 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3048/3393 [25:25<02:58,  1.93batch/s, Batch Loss=0.0068, Avg Loss=0.0762, Time Left=3.50 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3048/3393 [25:25<02:58,  1.93batch/s, Batch Loss=0.0608, Avg Loss=0.0762, Time Left=3.50 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3049/3393 [25:25<02:55,  1.97batch/s, Batch Loss=0.0608, Avg Loss=0.0762, Time Left=3.50 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3049/3393 [25:26<02:55,  1.97batch/s, Batch Loss=0.1205, Avg Loss=0.0762, Time Left=3.49 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3050/3393 [25:26<02:53,  1.97batch/s, Batch Loss=0.1205, Avg Loss=0.0762, Time Left=3.49 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3050/3393 [25:26<02:53,  1.97batch/s, Batch Loss=0.0136, Avg Loss=0.0761, Time Left=3.48 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3051/3393 [25:26<02:51,  2.00batch/s, Batch Loss=0.0136, Avg Loss=0.0761, Time Left=3.48 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3051/3393 [25:27<02:51,  2.00batch/s, Batch Loss=0.0307, Avg Loss=0.0761, Time Left=3.47 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3052/3393 [25:27<02:55,  1.95batch/s, Batch Loss=0.0307, Avg Loss=0.0761, Time Left=3.47 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3052/3393 [25:27<02:55,  1.95batch/s, Batch Loss=0.0482, Avg Loss=0.0761, Time Left=3.46 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3053/3393 [25:27<02:53,  1.96batch/s, Batch Loss=0.0482, Avg Loss=0.0761, Time Left=3.46 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3053/3393 [25:28<02:53,  1.96batch/s, Batch Loss=0.0514, Avg Loss=0.0761, Time Left=3.45 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3054/3393 [25:28<02:52,  1.97batch/s, Batch Loss=0.0514, Avg Loss=0.0761, Time Left=3.45 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3054/3393 [25:28<02:52,  1.97batch/s, Batch Loss=0.1435, Avg Loss=0.0761, Time Left=3.44 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3055/3393 [25:28<02:50,  1.98batch/s, Batch Loss=0.1435, Avg Loss=0.0761, Time Left=3.44 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3055/3393 [25:29<02:50,  1.98batch/s, Batch Loss=0.0799, Avg Loss=0.0761, Time Left=3.44 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3056/3393 [25:29<02:53,  1.95batch/s, Batch Loss=0.0799, Avg Loss=0.0761, Time Left=3.44 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3056/3393 [25:29<02:53,  1.95batch/s, Batch Loss=0.0132, Avg Loss=0.0761, Time Left=3.43 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3057/3393 [25:29<02:51,  1.96batch/s, Batch Loss=0.0132, Avg Loss=0.0761, Time Left=3.43 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3057/3393 [25:30<02:51,  1.96batch/s, Batch Loss=0.1402, Avg Loss=0.0761, Time Left=3.42 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3058/3393 [25:30<02:52,  1.94batch/s, Batch Loss=0.1402, Avg Loss=0.0761, Time Left=3.42 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3058/3393 [25:30<02:52,  1.94batch/s, Batch Loss=0.0499, Avg Loss=0.0761, Time Left=3.41 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3059/3393 [25:30<02:47,  1.99batch/s, Batch Loss=0.0499, Avg Loss=0.0761, Time Left=3.41 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3059/3393 [25:31<02:47,  1.99batch/s, Batch Loss=0.1837, Avg Loss=0.0762, Time Left=3.40 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3060/3393 [25:31<02:48,  1.98batch/s, Batch Loss=0.1837, Avg Loss=0.0762, Time Left=3.40 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3060/3393 [25:31<02:48,  1.98batch/s, Batch Loss=0.0529, Avg Loss=0.0762, Time Left=3.39 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3061/3393 [25:31<02:45,  2.00batch/s, Batch Loss=0.0529, Avg Loss=0.0762, Time Left=3.39 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3061/3393 [25:32<02:45,  2.00batch/s, Batch Loss=0.0064, Avg Loss=0.0761, Time Left=3.38 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3062/3393 [25:32<02:45,  2.00batch/s, Batch Loss=0.0064, Avg Loss=0.0761, Time Left=3.38 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3062/3393 [25:32<02:45,  2.00batch/s, Batch Loss=0.0683, Avg Loss=0.0761, Time Left=3.38 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3063/3393 [25:32<02:45,  2.00batch/s, Batch Loss=0.0683, Avg Loss=0.0761, Time Left=3.38 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3063/3393 [25:33<02:45,  2.00batch/s, Batch Loss=0.0813, Avg Loss=0.0761, Time Left=3.37 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3064/3393 [25:33<02:46,  1.98batch/s, Batch Loss=0.0813, Avg Loss=0.0761, Time Left=3.37 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3064/3393 [25:33<02:46,  1.98batch/s, Batch Loss=0.0075, Avg Loss=0.0761, Time Left=3.36 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3065/3393 [25:33<02:43,  2.00batch/s, Batch Loss=0.0075, Avg Loss=0.0761, Time Left=3.36 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3065/3393 [25:34<02:43,  2.00batch/s, Batch Loss=0.0294, Avg Loss=0.0761, Time Left=3.35 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3066/3393 [25:34<02:41,  2.02batch/s, Batch Loss=0.0294, Avg Loss=0.0761, Time Left=3.35 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3066/3393 [25:34<02:41,  2.02batch/s, Batch Loss=0.0787, Avg Loss=0.0761, Time Left=3.34 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  90%|▉| 3067/3393 [25:34<02:46,  1.96batch/s, Batch Loss=0.0787, Avg Loss=0.0761, Time Left=3.34 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3067/3393 [25:35<02:46,  1.96batch/s, Batch Loss=0.0103, Avg Loss=0.0761, Time Left=3.33 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3068/3393 [25:35<02:45,  1.97batch/s, Batch Loss=0.0103, Avg Loss=0.0761, Time Left=3.33 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3068/3393 [25:35<02:45,  1.97batch/s, Batch Loss=0.0006, Avg Loss=0.0760, Time Left=3.32 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3069/3393 [25:35<02:46,  1.94batch/s, Batch Loss=0.0006, Avg Loss=0.0760, Time Left=3.32 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3069/3393 [25:36<02:46,  1.94batch/s, Batch Loss=0.0621, Avg Loss=0.0760, Time Left=3.32 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3070/3393 [25:36<02:45,  1.95batch/s, Batch Loss=0.0621, Avg Loss=0.0760, Time Left=3.32 \u001b[A\n",
      "Epoch 3/3 - Training:  90%|▉| 3070/3393 [25:36<02:45,  1.95batch/s, Batch Loss=0.0527, Avg Loss=0.0760, Time Left=3.31 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3071/3393 [25:36<02:48,  1.92batch/s, Batch Loss=0.0527, Avg Loss=0.0760, Time Left=3.31 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3071/3393 [25:37<02:48,  1.92batch/s, Batch Loss=0.0016, Avg Loss=0.0760, Time Left=3.30 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3072/3393 [25:37<02:42,  1.97batch/s, Batch Loss=0.0016, Avg Loss=0.0760, Time Left=3.30 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3072/3393 [25:37<02:42,  1.97batch/s, Batch Loss=0.0064, Avg Loss=0.0760, Time Left=3.29 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3073/3393 [25:37<02:45,  1.93batch/s, Batch Loss=0.0064, Avg Loss=0.0760, Time Left=3.29 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3073/3393 [25:38<02:45,  1.93batch/s, Batch Loss=0.0023, Avg Loss=0.0760, Time Left=3.28 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3074/3393 [25:38<02:44,  1.94batch/s, Batch Loss=0.0023, Avg Loss=0.0760, Time Left=3.28 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3074/3393 [25:38<02:44,  1.94batch/s, Batch Loss=0.0848, Avg Loss=0.0760, Time Left=3.27 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3075/3393 [25:38<02:42,  1.96batch/s, Batch Loss=0.0848, Avg Loss=0.0760, Time Left=3.27 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3075/3393 [25:39<02:42,  1.96batch/s, Batch Loss=0.0075, Avg Loss=0.0759, Time Left=3.26 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3076/3393 [25:39<02:41,  1.97batch/s, Batch Loss=0.0075, Avg Loss=0.0759, Time Left=3.26 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3076/3393 [25:39<02:41,  1.97batch/s, Batch Loss=0.0023, Avg Loss=0.0759, Time Left=3.26 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3077/3393 [25:39<02:40,  1.96batch/s, Batch Loss=0.0023, Avg Loss=0.0759, Time Left=3.26 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3077/3393 [25:40<02:40,  1.96batch/s, Batch Loss=0.0011, Avg Loss=0.0759, Time Left=3.25 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3078/3393 [25:40<02:38,  1.99batch/s, Batch Loss=0.0011, Avg Loss=0.0759, Time Left=3.25 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3078/3393 [25:40<02:38,  1.99batch/s, Batch Loss=0.0757, Avg Loss=0.0759, Time Left=3.24 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3079/3393 [25:40<02:40,  1.96batch/s, Batch Loss=0.0757, Avg Loss=0.0759, Time Left=3.24 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3079/3393 [25:41<02:40,  1.96batch/s, Batch Loss=0.0036, Avg Loss=0.0759, Time Left=3.23 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3080/3393 [25:41<02:39,  1.97batch/s, Batch Loss=0.0036, Avg Loss=0.0759, Time Left=3.23 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3080/3393 [25:42<02:39,  1.97batch/s, Batch Loss=0.2056, Avg Loss=0.0759, Time Left=3.22 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3081/3393 [25:42<02:40,  1.94batch/s, Batch Loss=0.2056, Avg Loss=0.0759, Time Left=3.22 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3081/3393 [25:42<02:40,  1.94batch/s, Batch Loss=0.0838, Avg Loss=0.0759, Time Left=3.21 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3082/3393 [25:42<02:39,  1.95batch/s, Batch Loss=0.0838, Avg Loss=0.0759, Time Left=3.21 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3082/3393 [25:43<02:39,  1.95batch/s, Batch Loss=0.0658, Avg Loss=0.0759, Time Left=3.21 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3083/3393 [25:43<02:38,  1.95batch/s, Batch Loss=0.0658, Avg Loss=0.0759, Time Left=3.21 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3083/3393 [25:43<02:38,  1.95batch/s, Batch Loss=0.1724, Avg Loss=0.0759, Time Left=3.20 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3084/3393 [25:43<02:37,  1.96batch/s, Batch Loss=0.1724, Avg Loss=0.0759, Time Left=3.20 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3084/3393 [25:44<02:37,  1.96batch/s, Batch Loss=0.0371, Avg Loss=0.0759, Time Left=3.19 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3085/3393 [25:44<02:40,  1.92batch/s, Batch Loss=0.0371, Avg Loss=0.0759, Time Left=3.19 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3085/3393 [25:44<02:40,  1.92batch/s, Batch Loss=0.0460, Avg Loss=0.0759, Time Left=3.18 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3086/3393 [25:44<02:36,  1.96batch/s, Batch Loss=0.0460, Avg Loss=0.0759, Time Left=3.18 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3086/3393 [25:45<02:36,  1.96batch/s, Batch Loss=0.0333, Avg Loss=0.0759, Time Left=3.17 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3087/3393 [25:45<02:36,  1.95batch/s, Batch Loss=0.0333, Avg Loss=0.0759, Time Left=3.17 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3087/3393 [25:45<02:36,  1.95batch/s, Batch Loss=0.0055, Avg Loss=0.0759, Time Left=3.16 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3088/3393 [25:45<02:35,  1.97batch/s, Batch Loss=0.0055, Avg Loss=0.0759, Time Left=3.16 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3088/3393 [25:46<02:35,  1.97batch/s, Batch Loss=0.0102, Avg Loss=0.0759, Time Left=3.15 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3089/3393 [25:46<02:38,  1.92batch/s, Batch Loss=0.0102, Avg Loss=0.0759, Time Left=3.15 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3089/3393 [25:46<02:38,  1.92batch/s, Batch Loss=0.0008, Avg Loss=0.0758, Time Left=3.15 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3090/3393 [25:46<02:36,  1.94batch/s, Batch Loss=0.0008, Avg Loss=0.0758, Time Left=3.15 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3090/3393 [25:47<02:36,  1.94batch/s, Batch Loss=0.0205, Avg Loss=0.0758, Time Left=3.14 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3091/3393 [25:47<02:34,  1.96batch/s, Batch Loss=0.0205, Avg Loss=0.0758, Time Left=3.14 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3091/3393 [25:47<02:34,  1.96batch/s, Batch Loss=0.0024, Avg Loss=0.0758, Time Left=3.13 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3092/3393 [25:47<02:32,  1.97batch/s, Batch Loss=0.0024, Avg Loss=0.0758, Time Left=3.13 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3092/3393 [25:48<02:32,  1.97batch/s, Batch Loss=0.0067, Avg Loss=0.0758, Time Left=3.12 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3093/3393 [25:48<02:35,  1.93batch/s, Batch Loss=0.0067, Avg Loss=0.0758, Time Left=3.12 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3093/3393 [25:48<02:35,  1.93batch/s, Batch Loss=0.0061, Avg Loss=0.0758, Time Left=3.11 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3094/3393 [25:48<02:32,  1.96batch/s, Batch Loss=0.0061, Avg Loss=0.0758, Time Left=3.11 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3094/3393 [25:49<02:32,  1.96batch/s, Batch Loss=0.0187, Avg Loss=0.0757, Time Left=3.10 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3095/3393 [25:49<02:31,  1.97batch/s, Batch Loss=0.0187, Avg Loss=0.0757, Time Left=3.10 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3095/3393 [25:49<02:31,  1.97batch/s, Batch Loss=0.0140, Avg Loss=0.0757, Time Left=3.09 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3096/3393 [25:49<02:28,  2.00batch/s, Batch Loss=0.0140, Avg Loss=0.0757, Time Left=3.09 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3096/3393 [25:50<02:28,  2.00batch/s, Batch Loss=0.0357, Avg Loss=0.0757, Time Left=3.09 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3097/3393 [25:50<02:30,  1.96batch/s, Batch Loss=0.0357, Avg Loss=0.0757, Time Left=3.09 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3097/3393 [25:50<02:30,  1.96batch/s, Batch Loss=0.0730, Avg Loss=0.0757, Time Left=3.08 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3098/3393 [25:50<02:29,  1.97batch/s, Batch Loss=0.0730, Avg Loss=0.0757, Time Left=3.08 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3098/3393 [25:51<02:29,  1.97batch/s, Batch Loss=0.0393, Avg Loss=0.0757, Time Left=3.07 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3099/3393 [25:51<02:31,  1.94batch/s, Batch Loss=0.0393, Avg Loss=0.0757, Time Left=3.07 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3099/3393 [25:51<02:31,  1.94batch/s, Batch Loss=0.0117, Avg Loss=0.0757, Time Left=3.06 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  91%|▉| 3100/3393 [25:51<02:29,  1.96batch/s, Batch Loss=0.0117, Avg Loss=0.0757, Time Left=3.06 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3100/3393 [25:52<02:29,  1.96batch/s, Batch Loss=0.1772, Avg Loss=0.0757, Time Left=3.05 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3101/3393 [25:52<02:31,  1.93batch/s, Batch Loss=0.1772, Avg Loss=0.0757, Time Left=3.05 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3101/3393 [25:52<02:31,  1.93batch/s, Batch Loss=0.0689, Avg Loss=0.0757, Time Left=3.04 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3102/3393 [25:52<02:29,  1.95batch/s, Batch Loss=0.0689, Avg Loss=0.0757, Time Left=3.04 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3102/3393 [25:53<02:29,  1.95batch/s, Batch Loss=0.0006, Avg Loss=0.0757, Time Left=3.03 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3103/3393 [25:53<02:30,  1.93batch/s, Batch Loss=0.0006, Avg Loss=0.0757, Time Left=3.03 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3103/3393 [25:53<02:30,  1.93batch/s, Batch Loss=0.1222, Avg Loss=0.0757, Time Left=3.03 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3104/3393 [25:53<02:27,  1.97batch/s, Batch Loss=0.1222, Avg Loss=0.0757, Time Left=3.03 \u001b[A\n",
      "Epoch 3/3 - Training:  91%|▉| 3104/3393 [25:54<02:27,  1.97batch/s, Batch Loss=0.0263, Avg Loss=0.0757, Time Left=3.02 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3105/3393 [25:54<02:25,  1.98batch/s, Batch Loss=0.0263, Avg Loss=0.0757, Time Left=3.02 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3105/3393 [25:54<02:25,  1.98batch/s, Batch Loss=0.0034, Avg Loss=0.0756, Time Left=3.01 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3106/3393 [25:54<02:21,  2.02batch/s, Batch Loss=0.0034, Avg Loss=0.0756, Time Left=3.01 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3106/3393 [25:55<02:21,  2.02batch/s, Batch Loss=0.0010, Avg Loss=0.0756, Time Left=3.00 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3107/3393 [25:55<02:21,  2.02batch/s, Batch Loss=0.0010, Avg Loss=0.0756, Time Left=3.00 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3107/3393 [25:55<02:21,  2.02batch/s, Batch Loss=0.0521, Avg Loss=0.0756, Time Left=2.99 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3108/3393 [25:55<02:20,  2.03batch/s, Batch Loss=0.0521, Avg Loss=0.0756, Time Left=2.99 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3108/3393 [25:56<02:20,  2.03batch/s, Batch Loss=0.0006, Avg Loss=0.0756, Time Left=2.98 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3109/3393 [25:56<02:21,  2.00batch/s, Batch Loss=0.0006, Avg Loss=0.0756, Time Left=2.98 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3109/3393 [25:56<02:21,  2.00batch/s, Batch Loss=0.0071, Avg Loss=0.0756, Time Left=2.97 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3110/3393 [25:56<02:22,  1.98batch/s, Batch Loss=0.0071, Avg Loss=0.0756, Time Left=2.97 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3110/3393 [25:57<02:22,  1.98batch/s, Batch Loss=0.0157, Avg Loss=0.0755, Time Left=2.97 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3111/3393 [25:57<02:21,  1.99batch/s, Batch Loss=0.0157, Avg Loss=0.0755, Time Left=2.97 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3111/3393 [25:57<02:21,  1.99batch/s, Batch Loss=0.1203, Avg Loss=0.0756, Time Left=2.96 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3112/3393 [25:57<02:22,  1.97batch/s, Batch Loss=0.1203, Avg Loss=0.0756, Time Left=2.96 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3112/3393 [25:58<02:22,  1.97batch/s, Batch Loss=0.0160, Avg Loss=0.0755, Time Left=2.95 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3113/3393 [25:58<02:21,  1.98batch/s, Batch Loss=0.0160, Avg Loss=0.0755, Time Left=2.95 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3113/3393 [25:58<02:21,  1.98batch/s, Batch Loss=0.1392, Avg Loss=0.0756, Time Left=2.94 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3114/3393 [25:58<02:21,  1.97batch/s, Batch Loss=0.1392, Avg Loss=0.0756, Time Left=2.94 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3114/3393 [25:59<02:21,  1.97batch/s, Batch Loss=0.0006, Avg Loss=0.0755, Time Left=2.93 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3115/3393 [25:59<02:22,  1.96batch/s, Batch Loss=0.0006, Avg Loss=0.0755, Time Left=2.93 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3115/3393 [25:59<02:22,  1.96batch/s, Batch Loss=0.1379, Avg Loss=0.0756, Time Left=2.92 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3116/3393 [25:59<02:24,  1.91batch/s, Batch Loss=0.1379, Avg Loss=0.0756, Time Left=2.92 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3116/3393 [26:00<02:24,  1.91batch/s, Batch Loss=0.0110, Avg Loss=0.0755, Time Left=2.92 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3117/3393 [26:00<02:21,  1.95batch/s, Batch Loss=0.0110, Avg Loss=0.0755, Time Left=2.92 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3117/3393 [26:00<02:21,  1.95batch/s, Batch Loss=0.0136, Avg Loss=0.0755, Time Left=2.91 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3118/3393 [26:00<02:23,  1.92batch/s, Batch Loss=0.0136, Avg Loss=0.0755, Time Left=2.91 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3118/3393 [26:01<02:23,  1.92batch/s, Batch Loss=0.0524, Avg Loss=0.0755, Time Left=2.90 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3119/3393 [26:01<02:20,  1.95batch/s, Batch Loss=0.0524, Avg Loss=0.0755, Time Left=2.90 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3119/3393 [26:01<02:20,  1.95batch/s, Batch Loss=0.0591, Avg Loss=0.0755, Time Left=2.89 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3120/3393 [26:01<02:21,  1.93batch/s, Batch Loss=0.0591, Avg Loss=0.0755, Time Left=2.89 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3120/3393 [26:02<02:21,  1.93batch/s, Batch Loss=0.0659, Avg Loss=0.0755, Time Left=2.88 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3121/3393 [26:02<02:18,  1.97batch/s, Batch Loss=0.0659, Avg Loss=0.0755, Time Left=2.88 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3121/3393 [26:02<02:18,  1.97batch/s, Batch Loss=0.1066, Avg Loss=0.0755, Time Left=2.87 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3122/3393 [26:02<02:19,  1.94batch/s, Batch Loss=0.1066, Avg Loss=0.0755, Time Left=2.87 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3122/3393 [26:03<02:19,  1.94batch/s, Batch Loss=0.0346, Avg Loss=0.0755, Time Left=2.86 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3123/3393 [26:03<02:18,  1.96batch/s, Batch Loss=0.0346, Avg Loss=0.0755, Time Left=2.86 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3123/3393 [26:03<02:18,  1.96batch/s, Batch Loss=0.0251, Avg Loss=0.0755, Time Left=2.86 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3124/3393 [26:03<02:21,  1.90batch/s, Batch Loss=0.0251, Avg Loss=0.0755, Time Left=2.86 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3124/3393 [26:04<02:21,  1.90batch/s, Batch Loss=0.0342, Avg Loss=0.0755, Time Left=2.85 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3125/3393 [26:04<02:26,  1.83batch/s, Batch Loss=0.0342, Avg Loss=0.0755, Time Left=2.85 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3125/3393 [26:05<02:26,  1.83batch/s, Batch Loss=0.0006, Avg Loss=0.0754, Time Left=2.84 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3126/3393 [26:05<02:25,  1.83batch/s, Batch Loss=0.0006, Avg Loss=0.0754, Time Left=2.84 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3126/3393 [26:05<02:25,  1.83batch/s, Batch Loss=0.0006, Avg Loss=0.0754, Time Left=2.83 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3127/3393 [26:05<02:21,  1.87batch/s, Batch Loss=0.0006, Avg Loss=0.0754, Time Left=2.83 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3127/3393 [26:06<02:21,  1.87batch/s, Batch Loss=0.0129, Avg Loss=0.0754, Time Left=2.82 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3128/3393 [26:06<02:22,  1.86batch/s, Batch Loss=0.0129, Avg Loss=0.0754, Time Left=2.82 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3128/3393 [26:06<02:22,  1.86batch/s, Batch Loss=0.0102, Avg Loss=0.0754, Time Left=2.81 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3129/3393 [26:06<02:19,  1.90batch/s, Batch Loss=0.0102, Avg Loss=0.0754, Time Left=2.81 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3129/3393 [26:07<02:19,  1.90batch/s, Batch Loss=0.0006, Avg Loss=0.0754, Time Left=2.80 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3130/3393 [26:07<02:20,  1.88batch/s, Batch Loss=0.0006, Avg Loss=0.0754, Time Left=2.80 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3130/3393 [26:07<02:20,  1.88batch/s, Batch Loss=0.0006, Avg Loss=0.0753, Time Left=2.80 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3131/3393 [26:07<02:16,  1.93batch/s, Batch Loss=0.0006, Avg Loss=0.0753, Time Left=2.80 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3131/3393 [26:08<02:16,  1.93batch/s, Batch Loss=0.1394, Avg Loss=0.0753, Time Left=2.79 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3132/3393 [26:08<02:15,  1.93batch/s, Batch Loss=0.1394, Avg Loss=0.0753, Time Left=2.79 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3132/3393 [26:08<02:15,  1.93batch/s, Batch Loss=0.1509, Avg Loss=0.0754, Time Left=2.78 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  92%|▉| 3133/3393 [26:08<02:13,  1.95batch/s, Batch Loss=0.1509, Avg Loss=0.0754, Time Left=2.78 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3133/3393 [26:09<02:13,  1.95batch/s, Batch Loss=0.0454, Avg Loss=0.0754, Time Left=2.77 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3134/3393 [26:09<02:14,  1.93batch/s, Batch Loss=0.0454, Avg Loss=0.0754, Time Left=2.77 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3134/3393 [26:09<02:14,  1.93batch/s, Batch Loss=0.0006, Avg Loss=0.0753, Time Left=2.76 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3135/3393 [26:09<02:12,  1.95batch/s, Batch Loss=0.0006, Avg Loss=0.0753, Time Left=2.76 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3135/3393 [26:10<02:12,  1.95batch/s, Batch Loss=0.0118, Avg Loss=0.0753, Time Left=2.75 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3136/3393 [26:10<02:12,  1.94batch/s, Batch Loss=0.0118, Avg Loss=0.0753, Time Left=2.75 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3136/3393 [26:10<02:12,  1.94batch/s, Batch Loss=0.0859, Avg Loss=0.0753, Time Left=2.74 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3137/3393 [26:10<02:10,  1.96batch/s, Batch Loss=0.0859, Avg Loss=0.0753, Time Left=2.74 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3137/3393 [26:11<02:10,  1.96batch/s, Batch Loss=0.0044, Avg Loss=0.0753, Time Left=2.74 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3138/3393 [26:11<02:11,  1.93batch/s, Batch Loss=0.0044, Avg Loss=0.0753, Time Left=2.74 \u001b[A\n",
      "Epoch 3/3 - Training:  92%|▉| 3138/3393 [26:11<02:11,  1.93batch/s, Batch Loss=0.0642, Avg Loss=0.0753, Time Left=2.73 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3139/3393 [26:11<02:08,  1.97batch/s, Batch Loss=0.0642, Avg Loss=0.0753, Time Left=2.73 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3139/3393 [26:12<02:08,  1.97batch/s, Batch Loss=0.0005, Avg Loss=0.0753, Time Left=2.72 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3140/3393 [26:12<02:07,  1.98batch/s, Batch Loss=0.0005, Avg Loss=0.0753, Time Left=2.72 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3140/3393 [26:12<02:07,  1.98batch/s, Batch Loss=0.0007, Avg Loss=0.0752, Time Left=2.71 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3141/3393 [26:12<02:05,  2.00batch/s, Batch Loss=0.0007, Avg Loss=0.0752, Time Left=2.71 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3141/3393 [26:13<02:05,  2.00batch/s, Batch Loss=0.0009, Avg Loss=0.0752, Time Left=2.70 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3142/3393 [26:13<02:04,  2.02batch/s, Batch Loss=0.0009, Avg Loss=0.0752, Time Left=2.70 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3142/3393 [26:13<02:04,  2.02batch/s, Batch Loss=0.0797, Avg Loss=0.0752, Time Left=2.69 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3143/3393 [26:13<02:02,  2.04batch/s, Batch Loss=0.0797, Avg Loss=0.0752, Time Left=2.69 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3143/3393 [26:14<02:02,  2.04batch/s, Batch Loss=0.0059, Avg Loss=0.0752, Time Left=2.69 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3144/3393 [26:14<02:02,  2.03batch/s, Batch Loss=0.0059, Avg Loss=0.0752, Time Left=2.69 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3144/3393 [26:14<02:02,  2.03batch/s, Batch Loss=0.0006, Avg Loss=0.0752, Time Left=2.68 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3145/3393 [26:14<02:02,  2.02batch/s, Batch Loss=0.0006, Avg Loss=0.0752, Time Left=2.68 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3145/3393 [26:15<02:02,  2.02batch/s, Batch Loss=0.0292, Avg Loss=0.0752, Time Left=2.67 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3146/3393 [26:15<02:01,  2.03batch/s, Batch Loss=0.0292, Avg Loss=0.0752, Time Left=2.67 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3146/3393 [26:15<02:01,  2.03batch/s, Batch Loss=0.0427, Avg Loss=0.0752, Time Left=2.66 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3147/3393 [26:15<02:02,  2.00batch/s, Batch Loss=0.0427, Avg Loss=0.0752, Time Left=2.66 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3147/3393 [26:16<02:02,  2.00batch/s, Batch Loss=0.1181, Avg Loss=0.0752, Time Left=2.65 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3148/3393 [26:16<02:01,  2.02batch/s, Batch Loss=0.1181, Avg Loss=0.0752, Time Left=2.65 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3148/3393 [26:16<02:01,  2.02batch/s, Batch Loss=0.1860, Avg Loss=0.0752, Time Left=2.64 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3149/3393 [26:16<02:01,  2.01batch/s, Batch Loss=0.1860, Avg Loss=0.0752, Time Left=2.64 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3149/3393 [26:17<02:01,  2.01batch/s, Batch Loss=0.0009, Avg Loss=0.0752, Time Left=2.63 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3150/3393 [26:17<01:59,  2.03batch/s, Batch Loss=0.0009, Avg Loss=0.0752, Time Left=2.63 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3150/3393 [26:17<01:59,  2.03batch/s, Batch Loss=0.0600, Avg Loss=0.0752, Time Left=2.63 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3151/3393 [26:17<02:00,  2.02batch/s, Batch Loss=0.0600, Avg Loss=0.0752, Time Left=2.63 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3151/3393 [26:18<02:00,  2.02batch/s, Batch Loss=0.0253, Avg Loss=0.0752, Time Left=2.62 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3152/3393 [26:18<02:00,  2.00batch/s, Batch Loss=0.0253, Avg Loss=0.0752, Time Left=2.62 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3152/3393 [26:18<02:00,  2.00batch/s, Batch Loss=0.0243, Avg Loss=0.0751, Time Left=2.61 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3153/3393 [26:18<02:00,  2.00batch/s, Batch Loss=0.0243, Avg Loss=0.0751, Time Left=2.61 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3153/3393 [26:19<02:00,  2.00batch/s, Batch Loss=0.0026, Avg Loss=0.0751, Time Left=2.60 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3154/3393 [26:19<02:00,  1.98batch/s, Batch Loss=0.0026, Avg Loss=0.0751, Time Left=2.60 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3154/3393 [26:19<02:00,  1.98batch/s, Batch Loss=0.0007, Avg Loss=0.0751, Time Left=2.59 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3155/3393 [26:19<01:57,  2.02batch/s, Batch Loss=0.0007, Avg Loss=0.0751, Time Left=2.59 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3155/3393 [26:20<01:57,  2.02batch/s, Batch Loss=0.0065, Avg Loss=0.0751, Time Left=2.58 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3156/3393 [26:20<01:57,  2.02batch/s, Batch Loss=0.0065, Avg Loss=0.0751, Time Left=2.58 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3156/3393 [26:20<01:57,  2.02batch/s, Batch Loss=0.0526, Avg Loss=0.0751, Time Left=2.57 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3157/3393 [26:20<01:56,  2.03batch/s, Batch Loss=0.0526, Avg Loss=0.0751, Time Left=2.57 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3157/3393 [26:21<01:56,  2.03batch/s, Batch Loss=0.0191, Avg Loss=0.0750, Time Left=2.57 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3158/3393 [26:21<01:55,  2.04batch/s, Batch Loss=0.0191, Avg Loss=0.0750, Time Left=2.57 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3158/3393 [26:21<01:55,  2.04batch/s, Batch Loss=0.0360, Avg Loss=0.0750, Time Left=2.56 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3159/3393 [26:21<01:57,  1.99batch/s, Batch Loss=0.0360, Avg Loss=0.0750, Time Left=2.56 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3159/3393 [26:22<01:57,  1.99batch/s, Batch Loss=0.4347, Avg Loss=0.0751, Time Left=2.55 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3160/3393 [26:22<01:58,  1.97batch/s, Batch Loss=0.4347, Avg Loss=0.0751, Time Left=2.55 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3160/3393 [26:22<01:58,  1.97batch/s, Batch Loss=0.2385, Avg Loss=0.0752, Time Left=2.54 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3161/3393 [26:22<01:58,  1.96batch/s, Batch Loss=0.2385, Avg Loss=0.0752, Time Left=2.54 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3161/3393 [26:23<01:58,  1.96batch/s, Batch Loss=0.2164, Avg Loss=0.0752, Time Left=2.53 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3162/3393 [26:23<01:57,  1.97batch/s, Batch Loss=0.2164, Avg Loss=0.0752, Time Left=2.53 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3162/3393 [26:23<01:57,  1.97batch/s, Batch Loss=0.0113, Avg Loss=0.0752, Time Left=2.52 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3163/3393 [26:23<01:58,  1.94batch/s, Batch Loss=0.0113, Avg Loss=0.0752, Time Left=2.52 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3163/3393 [26:24<01:58,  1.94batch/s, Batch Loss=0.0343, Avg Loss=0.0752, Time Left=2.51 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3164/3393 [26:24<01:56,  1.96batch/s, Batch Loss=0.0343, Avg Loss=0.0752, Time Left=2.51 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3164/3393 [26:24<01:56,  1.96batch/s, Batch Loss=0.0212, Avg Loss=0.0752, Time Left=2.51 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3165/3393 [26:24<01:57,  1.93batch/s, Batch Loss=0.0212, Avg Loss=0.0752, Time Left=2.51 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3165/3393 [26:25<01:57,  1.93batch/s, Batch Loss=0.1115, Avg Loss=0.0752, Time Left=2.50 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  93%|▉| 3166/3393 [26:25<01:56,  1.95batch/s, Batch Loss=0.1115, Avg Loss=0.0752, Time Left=2.50 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3166/3393 [26:25<01:56,  1.95batch/s, Batch Loss=0.0088, Avg Loss=0.0752, Time Left=2.49 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3167/3393 [26:25<01:57,  1.93batch/s, Batch Loss=0.0088, Avg Loss=0.0752, Time Left=2.49 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3167/3393 [26:26<01:57,  1.93batch/s, Batch Loss=0.0410, Avg Loss=0.0752, Time Left=2.48 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3168/3393 [26:26<01:54,  1.96batch/s, Batch Loss=0.0410, Avg Loss=0.0752, Time Left=2.48 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3168/3393 [26:26<01:54,  1.96batch/s, Batch Loss=0.0128, Avg Loss=0.0752, Time Left=2.47 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3169/3393 [26:26<01:52,  2.00batch/s, Batch Loss=0.0128, Avg Loss=0.0752, Time Left=2.47 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3169/3393 [26:27<01:52,  2.00batch/s, Batch Loss=0.0009, Avg Loss=0.0751, Time Left=2.46 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3170/3393 [26:27<01:50,  2.02batch/s, Batch Loss=0.0009, Avg Loss=0.0751, Time Left=2.46 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3170/3393 [26:27<01:50,  2.02batch/s, Batch Loss=0.0129, Avg Loss=0.0751, Time Left=2.45 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3171/3393 [26:27<01:49,  2.03batch/s, Batch Loss=0.0129, Avg Loss=0.0751, Time Left=2.45 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3171/3393 [26:28<01:49,  2.03batch/s, Batch Loss=0.0707, Avg Loss=0.0751, Time Left=2.45 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3172/3393 [26:28<01:48,  2.04batch/s, Batch Loss=0.0707, Avg Loss=0.0751, Time Left=2.45 \u001b[A\n",
      "Epoch 3/3 - Training:  93%|▉| 3172/3393 [26:28<01:48,  2.04batch/s, Batch Loss=0.0096, Avg Loss=0.0751, Time Left=2.44 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3173/3393 [26:28<01:47,  2.05batch/s, Batch Loss=0.0096, Avg Loss=0.0751, Time Left=2.44 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3173/3393 [26:29<01:47,  2.05batch/s, Batch Loss=0.0121, Avg Loss=0.0751, Time Left=2.43 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3174/3393 [26:29<01:49,  2.00batch/s, Batch Loss=0.0121, Avg Loss=0.0751, Time Left=2.43 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3174/3393 [26:29<01:49,  2.00batch/s, Batch Loss=0.0559, Avg Loss=0.0751, Time Left=2.42 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3175/3393 [26:29<01:49,  1.99batch/s, Batch Loss=0.0559, Avg Loss=0.0751, Time Left=2.42 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3175/3393 [26:30<01:49,  1.99batch/s, Batch Loss=0.0099, Avg Loss=0.0750, Time Left=2.41 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3176/3393 [26:30<01:50,  1.96batch/s, Batch Loss=0.0099, Avg Loss=0.0750, Time Left=2.41 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3176/3393 [26:30<01:50,  1.96batch/s, Batch Loss=0.0874, Avg Loss=0.0750, Time Left=2.40 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3177/3393 [26:30<01:48,  1.99batch/s, Batch Loss=0.0874, Avg Loss=0.0750, Time Left=2.40 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3177/3393 [26:31<01:48,  1.99batch/s, Batch Loss=0.0640, Avg Loss=0.0750, Time Left=2.39 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3178/3393 [26:31<01:49,  1.95batch/s, Batch Loss=0.0640, Avg Loss=0.0750, Time Left=2.39 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3178/3393 [26:31<01:49,  1.95batch/s, Batch Loss=0.0289, Avg Loss=0.0750, Time Left=2.39 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3179/3393 [26:31<01:48,  1.97batch/s, Batch Loss=0.0289, Avg Loss=0.0750, Time Left=2.39 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3179/3393 [26:32<01:48,  1.97batch/s, Batch Loss=0.0134, Avg Loss=0.0750, Time Left=2.38 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3180/3393 [26:32<01:48,  1.96batch/s, Batch Loss=0.0134, Avg Loss=0.0750, Time Left=2.38 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3180/3393 [26:32<01:48,  1.96batch/s, Batch Loss=0.0928, Avg Loss=0.0750, Time Left=2.37 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3181/3393 [26:32<01:48,  1.95batch/s, Batch Loss=0.0928, Avg Loss=0.0750, Time Left=2.37 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3181/3393 [26:33<01:48,  1.95batch/s, Batch Loss=0.1813, Avg Loss=0.0750, Time Left=2.36 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3182/3393 [26:33<01:47,  1.96batch/s, Batch Loss=0.1813, Avg Loss=0.0750, Time Left=2.36 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3182/3393 [26:33<01:47,  1.96batch/s, Batch Loss=0.1531, Avg Loss=0.0751, Time Left=2.35 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3183/3393 [26:33<01:46,  1.97batch/s, Batch Loss=0.1531, Avg Loss=0.0751, Time Left=2.35 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3183/3393 [26:34<01:46,  1.97batch/s, Batch Loss=0.0008, Avg Loss=0.0750, Time Left=2.34 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3184/3393 [26:34<01:47,  1.94batch/s, Batch Loss=0.0008, Avg Loss=0.0750, Time Left=2.34 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3184/3393 [26:34<01:47,  1.94batch/s, Batch Loss=0.0009, Avg Loss=0.0750, Time Left=2.34 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3185/3393 [26:34<01:47,  1.94batch/s, Batch Loss=0.0009, Avg Loss=0.0750, Time Left=2.34 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3185/3393 [26:35<01:47,  1.94batch/s, Batch Loss=0.0013, Avg Loss=0.0750, Time Left=2.33 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3186/3393 [26:35<01:46,  1.94batch/s, Batch Loss=0.0013, Avg Loss=0.0750, Time Left=2.33 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3186/3393 [26:35<01:46,  1.94batch/s, Batch Loss=0.0222, Avg Loss=0.0750, Time Left=2.32 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3187/3393 [26:35<01:45,  1.95batch/s, Batch Loss=0.0222, Avg Loss=0.0750, Time Left=2.32 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3187/3393 [26:36<01:45,  1.95batch/s, Batch Loss=0.0146, Avg Loss=0.0750, Time Left=2.31 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3188/3393 [26:36<01:45,  1.95batch/s, Batch Loss=0.0146, Avg Loss=0.0750, Time Left=2.31 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3188/3393 [26:36<01:45,  1.95batch/s, Batch Loss=0.0160, Avg Loss=0.0749, Time Left=2.30 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3189/3393 [26:36<01:43,  1.97batch/s, Batch Loss=0.0160, Avg Loss=0.0749, Time Left=2.30 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3189/3393 [26:37<01:43,  1.97batch/s, Batch Loss=0.1650, Avg Loss=0.0750, Time Left=2.29 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3190/3393 [26:37<01:43,  1.95batch/s, Batch Loss=0.1650, Avg Loss=0.0750, Time Left=2.29 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3190/3393 [26:37<01:43,  1.95batch/s, Batch Loss=0.0230, Avg Loss=0.0750, Time Left=2.28 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3191/3393 [26:37<01:42,  1.97batch/s, Batch Loss=0.0230, Avg Loss=0.0750, Time Left=2.28 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3191/3393 [26:38<01:42,  1.97batch/s, Batch Loss=0.0201, Avg Loss=0.0749, Time Left=2.28 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3192/3393 [26:38<01:43,  1.94batch/s, Batch Loss=0.0201, Avg Loss=0.0749, Time Left=2.28 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3192/3393 [26:39<01:43,  1.94batch/s, Batch Loss=0.1485, Avg Loss=0.0750, Time Left=2.27 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3193/3393 [26:39<01:43,  1.94batch/s, Batch Loss=0.1485, Avg Loss=0.0750, Time Left=2.27 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3193/3393 [26:39<01:43,  1.94batch/s, Batch Loss=0.0031, Avg Loss=0.0749, Time Left=2.26 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3194/3393 [26:39<01:42,  1.94batch/s, Batch Loss=0.0031, Avg Loss=0.0749, Time Left=2.26 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3194/3393 [26:40<01:42,  1.94batch/s, Batch Loss=0.1494, Avg Loss=0.0750, Time Left=2.25 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3195/3393 [26:40<01:41,  1.95batch/s, Batch Loss=0.1494, Avg Loss=0.0750, Time Left=2.25 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3195/3393 [26:40<01:41,  1.95batch/s, Batch Loss=0.1360, Avg Loss=0.0750, Time Left=2.24 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3196/3393 [26:40<01:42,  1.93batch/s, Batch Loss=0.1360, Avg Loss=0.0750, Time Left=2.24 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3196/3393 [26:41<01:42,  1.93batch/s, Batch Loss=0.0192, Avg Loss=0.0750, Time Left=2.23 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3197/3393 [26:41<01:40,  1.95batch/s, Batch Loss=0.0192, Avg Loss=0.0750, Time Left=2.23 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3197/3393 [26:41<01:40,  1.95batch/s, Batch Loss=0.0581, Avg Loss=0.0750, Time Left=2.22 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3198/3393 [26:41<01:41,  1.93batch/s, Batch Loss=0.0581, Avg Loss=0.0750, Time Left=2.22 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3198/3393 [26:42<01:41,  1.93batch/s, Batch Loss=0.1050, Avg Loss=0.0750, Time Left=2.22 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  94%|▉| 3199/3393 [26:42<01:39,  1.95batch/s, Batch Loss=0.1050, Avg Loss=0.0750, Time Left=2.22 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3199/3393 [26:42<01:39,  1.95batch/s, Batch Loss=0.0415, Avg Loss=0.0750, Time Left=2.21 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3200/3393 [26:42<01:40,  1.93batch/s, Batch Loss=0.0415, Avg Loss=0.0750, Time Left=2.21 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3200/3393 [26:43<01:40,  1.93batch/s, Batch Loss=0.0287, Avg Loss=0.0749, Time Left=2.20 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3201/3393 [26:43<01:38,  1.94batch/s, Batch Loss=0.0287, Avg Loss=0.0749, Time Left=2.20 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3201/3393 [26:43<01:38,  1.94batch/s, Batch Loss=0.1172, Avg Loss=0.0750, Time Left=2.19 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3202/3393 [26:43<01:36,  1.98batch/s, Batch Loss=0.1172, Avg Loss=0.0750, Time Left=2.19 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3202/3393 [26:44<01:36,  1.98batch/s, Batch Loss=0.0084, Avg Loss=0.0749, Time Left=2.18 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3203/3393 [26:44<01:35,  1.99batch/s, Batch Loss=0.0084, Avg Loss=0.0749, Time Left=2.18 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3203/3393 [26:44<01:35,  1.99batch/s, Batch Loss=0.0024, Avg Loss=0.0749, Time Left=2.17 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3204/3393 [26:44<01:35,  1.97batch/s, Batch Loss=0.0024, Avg Loss=0.0749, Time Left=2.17 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3204/3393 [26:45<01:35,  1.97batch/s, Batch Loss=0.0144, Avg Loss=0.0749, Time Left=2.16 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3205/3393 [26:45<01:35,  1.96batch/s, Batch Loss=0.0144, Avg Loss=0.0749, Time Left=2.16 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3205/3393 [26:45<01:35,  1.96batch/s, Batch Loss=0.0485, Avg Loss=0.0749, Time Left=2.16 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3206/3393 [26:45<01:35,  1.97batch/s, Batch Loss=0.0485, Avg Loss=0.0749, Time Left=2.16 \u001b[A\n",
      "Epoch 3/3 - Training:  94%|▉| 3206/3393 [26:46<01:35,  1.97batch/s, Batch Loss=0.4183, Avg Loss=0.0750, Time Left=2.15 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3207/3393 [26:46<01:32,  2.01batch/s, Batch Loss=0.4183, Avg Loss=0.0750, Time Left=2.15 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3207/3393 [26:46<01:32,  2.01batch/s, Batch Loss=0.0131, Avg Loss=0.0750, Time Left=2.14 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3208/3393 [26:46<01:31,  2.01batch/s, Batch Loss=0.0131, Avg Loss=0.0750, Time Left=2.14 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3208/3393 [26:47<01:31,  2.01batch/s, Batch Loss=0.0157, Avg Loss=0.0750, Time Left=2.13 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3209/3393 [26:47<01:31,  2.01batch/s, Batch Loss=0.0157, Avg Loss=0.0750, Time Left=2.13 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3209/3393 [26:47<01:31,  2.01batch/s, Batch Loss=0.0188, Avg Loss=0.0749, Time Left=2.12 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3210/3393 [26:47<01:31,  2.00batch/s, Batch Loss=0.0188, Avg Loss=0.0749, Time Left=2.12 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3210/3393 [26:48<01:31,  2.00batch/s, Batch Loss=0.1166, Avg Loss=0.0750, Time Left=2.11 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3211/3393 [26:48<01:32,  1.97batch/s, Batch Loss=0.1166, Avg Loss=0.0750, Time Left=2.11 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3211/3393 [26:48<01:32,  1.97batch/s, Batch Loss=0.0631, Avg Loss=0.0749, Time Left=2.10 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3212/3393 [26:48<01:30,  1.99batch/s, Batch Loss=0.0631, Avg Loss=0.0749, Time Left=2.10 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3212/3393 [26:49<01:30,  1.99batch/s, Batch Loss=0.0418, Avg Loss=0.0749, Time Left=2.10 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3213/3393 [26:49<01:31,  1.96batch/s, Batch Loss=0.0418, Avg Loss=0.0749, Time Left=2.10 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3213/3393 [26:49<01:31,  1.96batch/s, Batch Loss=0.0617, Avg Loss=0.0749, Time Left=2.09 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3214/3393 [26:49<01:30,  1.97batch/s, Batch Loss=0.0617, Avg Loss=0.0749, Time Left=2.09 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3214/3393 [26:50<01:30,  1.97batch/s, Batch Loss=0.0177, Avg Loss=0.0749, Time Left=2.08 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3215/3393 [26:50<01:31,  1.94batch/s, Batch Loss=0.0177, Avg Loss=0.0749, Time Left=2.08 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3215/3393 [26:50<01:31,  1.94batch/s, Batch Loss=0.0513, Avg Loss=0.0749, Time Left=2.07 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3216/3393 [26:50<01:29,  1.97batch/s, Batch Loss=0.0513, Avg Loss=0.0749, Time Left=2.07 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3216/3393 [26:51<01:29,  1.97batch/s, Batch Loss=0.0277, Avg Loss=0.0749, Time Left=2.06 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3217/3393 [26:51<01:30,  1.95batch/s, Batch Loss=0.0277, Avg Loss=0.0749, Time Left=2.06 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3217/3393 [26:51<01:30,  1.95batch/s, Batch Loss=0.2070, Avg Loss=0.0749, Time Left=2.05 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3218/3393 [26:51<01:29,  1.96batch/s, Batch Loss=0.2070, Avg Loss=0.0749, Time Left=2.05 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3218/3393 [26:52<01:29,  1.96batch/s, Batch Loss=0.0084, Avg Loss=0.0749, Time Left=2.05 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3219/3393 [26:52<01:28,  1.97batch/s, Batch Loss=0.0084, Avg Loss=0.0749, Time Left=2.05 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3219/3393 [26:52<01:28,  1.97batch/s, Batch Loss=0.0063, Avg Loss=0.0749, Time Left=2.04 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3220/3393 [26:52<01:28,  1.96batch/s, Batch Loss=0.0063, Avg Loss=0.0749, Time Left=2.04 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3220/3393 [26:53<01:28,  1.96batch/s, Batch Loss=0.0035, Avg Loss=0.0749, Time Left=2.03 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3221/3393 [26:53<01:28,  1.93batch/s, Batch Loss=0.0035, Avg Loss=0.0749, Time Left=2.03 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3221/3393 [26:53<01:28,  1.93batch/s, Batch Loss=0.0101, Avg Loss=0.0748, Time Left=2.02 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3222/3393 [26:53<01:27,  1.95batch/s, Batch Loss=0.0101, Avg Loss=0.0748, Time Left=2.02 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3222/3393 [26:54<01:27,  1.95batch/s, Batch Loss=0.1263, Avg Loss=0.0749, Time Left=2.01 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3223/3393 [26:54<01:27,  1.95batch/s, Batch Loss=0.1263, Avg Loss=0.0749, Time Left=2.01 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3223/3393 [26:54<01:27,  1.95batch/s, Batch Loss=0.0996, Avg Loss=0.0749, Time Left=2.00 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3224/3393 [26:54<01:26,  1.96batch/s, Batch Loss=0.0996, Avg Loss=0.0749, Time Left=2.00 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3224/3393 [26:55<01:26,  1.96batch/s, Batch Loss=0.0128, Avg Loss=0.0749, Time Left=1.99 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3225/3393 [26:55<01:26,  1.94batch/s, Batch Loss=0.0128, Avg Loss=0.0749, Time Left=1.99 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3225/3393 [26:55<01:26,  1.94batch/s, Batch Loss=0.2324, Avg Loss=0.0749, Time Left=1.99 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3226/3393 [26:55<01:24,  1.97batch/s, Batch Loss=0.2324, Avg Loss=0.0749, Time Left=1.99 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3226/3393 [26:56<01:24,  1.97batch/s, Batch Loss=0.1497, Avg Loss=0.0749, Time Left=1.98 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3227/3393 [26:56<01:25,  1.94batch/s, Batch Loss=0.1497, Avg Loss=0.0749, Time Left=1.98 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3227/3393 [26:56<01:25,  1.94batch/s, Batch Loss=0.0109, Avg Loss=0.0749, Time Left=1.97 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3228/3393 [26:56<01:24,  1.96batch/s, Batch Loss=0.0109, Avg Loss=0.0749, Time Left=1.97 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3228/3393 [26:57<01:24,  1.96batch/s, Batch Loss=0.0281, Avg Loss=0.0749, Time Left=1.96 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3229/3393 [26:57<01:24,  1.95batch/s, Batch Loss=0.0281, Avg Loss=0.0749, Time Left=1.96 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3229/3393 [26:57<01:24,  1.95batch/s, Batch Loss=0.0023, Avg Loss=0.0749, Time Left=1.95 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3230/3393 [26:57<01:22,  1.96batch/s, Batch Loss=0.0023, Avg Loss=0.0749, Time Left=1.95 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3230/3393 [26:58<01:22,  1.96batch/s, Batch Loss=0.1373, Avg Loss=0.0749, Time Left=1.94 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3231/3393 [26:58<01:22,  1.97batch/s, Batch Loss=0.1373, Avg Loss=0.0749, Time Left=1.94 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3231/3393 [26:58<01:22,  1.97batch/s, Batch Loss=0.1485, Avg Loss=0.0749, Time Left=1.93 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  95%|▉| 3232/3393 [26:58<01:22,  1.96batch/s, Batch Loss=0.1485, Avg Loss=0.0749, Time Left=1.93 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3232/3393 [26:59<01:22,  1.96batch/s, Batch Loss=0.0022, Avg Loss=0.0749, Time Left=1.93 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3233/3393 [26:59<01:22,  1.94batch/s, Batch Loss=0.0022, Avg Loss=0.0749, Time Left=1.93 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3233/3393 [26:59<01:22,  1.94batch/s, Batch Loss=0.0039, Avg Loss=0.0749, Time Left=1.92 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3234/3393 [26:59<01:21,  1.95batch/s, Batch Loss=0.0039, Avg Loss=0.0749, Time Left=1.92 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3234/3393 [27:00<01:21,  1.95batch/s, Batch Loss=0.2066, Avg Loss=0.0749, Time Left=1.91 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3235/3393 [27:00<01:20,  1.97batch/s, Batch Loss=0.2066, Avg Loss=0.0749, Time Left=1.91 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3235/3393 [27:00<01:20,  1.97batch/s, Batch Loss=0.0112, Avg Loss=0.0749, Time Left=1.90 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3236/3393 [27:00<01:18,  2.00batch/s, Batch Loss=0.0112, Avg Loss=0.0749, Time Left=1.90 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3236/3393 [27:01<01:18,  2.00batch/s, Batch Loss=0.0106, Avg Loss=0.0749, Time Left=1.89 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3237/3393 [27:01<01:18,  2.00batch/s, Batch Loss=0.0106, Avg Loss=0.0749, Time Left=1.89 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3237/3393 [27:01<01:18,  2.00batch/s, Batch Loss=0.0030, Avg Loss=0.0748, Time Left=1.88 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3238/3393 [27:01<01:17,  1.99batch/s, Batch Loss=0.0030, Avg Loss=0.0748, Time Left=1.88 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3238/3393 [27:02<01:17,  1.99batch/s, Batch Loss=0.0282, Avg Loss=0.0748, Time Left=1.87 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3239/3393 [27:02<01:17,  1.98batch/s, Batch Loss=0.0282, Avg Loss=0.0748, Time Left=1.87 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3239/3393 [27:02<01:17,  1.98batch/s, Batch Loss=0.0040, Avg Loss=0.0748, Time Left=1.87 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3240/3393 [27:02<01:17,  1.98batch/s, Batch Loss=0.0040, Avg Loss=0.0748, Time Left=1.87 \u001b[A\n",
      "Epoch 3/3 - Training:  95%|▉| 3240/3393 [27:03<01:17,  1.98batch/s, Batch Loss=0.0210, Avg Loss=0.0748, Time Left=1.86 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3241/3393 [27:03<01:15,  2.01batch/s, Batch Loss=0.0210, Avg Loss=0.0748, Time Left=1.86 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3241/3393 [27:03<01:15,  2.01batch/s, Batch Loss=0.0190, Avg Loss=0.0748, Time Left=1.85 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3242/3393 [27:03<01:15,  2.01batch/s, Batch Loss=0.0190, Avg Loss=0.0748, Time Left=1.85 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3242/3393 [27:04<01:15,  2.01batch/s, Batch Loss=0.0586, Avg Loss=0.0748, Time Left=1.84 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3243/3393 [27:04<01:14,  2.02batch/s, Batch Loss=0.0586, Avg Loss=0.0748, Time Left=1.84 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3243/3393 [27:04<01:14,  2.02batch/s, Batch Loss=0.2012, Avg Loss=0.0748, Time Left=1.83 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3244/3393 [27:04<01:14,  2.00batch/s, Batch Loss=0.2012, Avg Loss=0.0748, Time Left=1.83 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3244/3393 [27:05<01:14,  2.00batch/s, Batch Loss=0.0270, Avg Loss=0.0748, Time Left=1.82 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3245/3393 [27:05<01:14,  1.98batch/s, Batch Loss=0.0270, Avg Loss=0.0748, Time Left=1.82 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3245/3393 [27:05<01:14,  1.98batch/s, Batch Loss=0.0184, Avg Loss=0.0748, Time Left=1.82 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3246/3393 [27:05<01:14,  1.97batch/s, Batch Loss=0.0184, Avg Loss=0.0748, Time Left=1.82 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3246/3393 [27:06<01:14,  1.97batch/s, Batch Loss=0.0023, Avg Loss=0.0748, Time Left=1.81 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3247/3393 [27:06<01:13,  1.99batch/s, Batch Loss=0.0023, Avg Loss=0.0748, Time Left=1.81 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3247/3393 [27:06<01:13,  1.99batch/s, Batch Loss=0.0506, Avg Loss=0.0747, Time Left=1.80 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3248/3393 [27:06<01:14,  1.96batch/s, Batch Loss=0.0506, Avg Loss=0.0747, Time Left=1.80 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3248/3393 [27:07<01:14,  1.96batch/s, Batch Loss=0.0130, Avg Loss=0.0747, Time Left=1.79 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3249/3393 [27:07<01:12,  1.98batch/s, Batch Loss=0.0130, Avg Loss=0.0747, Time Left=1.79 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3249/3393 [27:07<01:12,  1.98batch/s, Batch Loss=0.1492, Avg Loss=0.0747, Time Left=1.78 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3250/3393 [27:07<01:12,  1.97batch/s, Batch Loss=0.1492, Avg Loss=0.0747, Time Left=1.78 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3250/3393 [27:08<01:12,  1.97batch/s, Batch Loss=0.0033, Avg Loss=0.0747, Time Left=1.77 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3251/3393 [27:08<01:11,  2.00batch/s, Batch Loss=0.0033, Avg Loss=0.0747, Time Left=1.77 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3251/3393 [27:08<01:11,  2.00batch/s, Batch Loss=0.0892, Avg Loss=0.0747, Time Left=1.76 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3252/3393 [27:08<01:11,  1.98batch/s, Batch Loss=0.0892, Avg Loss=0.0747, Time Left=1.76 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3252/3393 [27:09<01:11,  1.98batch/s, Batch Loss=0.0046, Avg Loss=0.0747, Time Left=1.76 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3253/3393 [27:09<01:10,  1.99batch/s, Batch Loss=0.0046, Avg Loss=0.0747, Time Left=1.76 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3253/3393 [27:09<01:10,  1.99batch/s, Batch Loss=0.0421, Avg Loss=0.0747, Time Left=1.75 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3254/3393 [27:09<01:09,  1.99batch/s, Batch Loss=0.0421, Avg Loss=0.0747, Time Left=1.75 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3254/3393 [27:10<01:09,  1.99batch/s, Batch Loss=0.0012, Avg Loss=0.0747, Time Left=1.74 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3255/3393 [27:10<01:08,  2.01batch/s, Batch Loss=0.0012, Avg Loss=0.0747, Time Left=1.74 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3255/3393 [27:10<01:08,  2.01batch/s, Batch Loss=0.0176, Avg Loss=0.0747, Time Left=1.73 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3256/3393 [27:10<01:08,  2.00batch/s, Batch Loss=0.0176, Avg Loss=0.0747, Time Left=1.73 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3256/3393 [27:11<01:08,  2.00batch/s, Batch Loss=0.0087, Avg Loss=0.0746, Time Left=1.72 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3257/3393 [27:11<01:08,  1.99batch/s, Batch Loss=0.0087, Avg Loss=0.0746, Time Left=1.72 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3257/3393 [27:11<01:08,  1.99batch/s, Batch Loss=0.0011, Avg Loss=0.0746, Time Left=1.71 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3258/3393 [27:11<01:07,  2.01batch/s, Batch Loss=0.0011, Avg Loss=0.0746, Time Left=1.71 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3258/3393 [27:12<01:07,  2.01batch/s, Batch Loss=0.0011, Avg Loss=0.0746, Time Left=1.70 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3259/3393 [27:12<01:08,  1.95batch/s, Batch Loss=0.0011, Avg Loss=0.0746, Time Left=1.70 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3259/3393 [27:13<01:08,  1.95batch/s, Batch Loss=0.0441, Avg Loss=0.0746, Time Left=1.70 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3260/3393 [27:13<01:08,  1.94batch/s, Batch Loss=0.0441, Avg Loss=0.0746, Time Left=1.70 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3260/3393 [27:13<01:08,  1.94batch/s, Batch Loss=0.1985, Avg Loss=0.0746, Time Left=1.69 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3261/3393 [27:13<01:08,  1.94batch/s, Batch Loss=0.1985, Avg Loss=0.0746, Time Left=1.69 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3261/3393 [27:14<01:08,  1.94batch/s, Batch Loss=0.0078, Avg Loss=0.0746, Time Left=1.68 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3262/3393 [27:14<01:06,  1.96batch/s, Batch Loss=0.0078, Avg Loss=0.0746, Time Left=1.68 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3262/3393 [27:14<01:06,  1.96batch/s, Batch Loss=0.0062, Avg Loss=0.0746, Time Left=1.67 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3263/3393 [27:14<01:07,  1.93batch/s, Batch Loss=0.0062, Avg Loss=0.0746, Time Left=1.67 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3263/3393 [27:15<01:07,  1.93batch/s, Batch Loss=0.0006, Avg Loss=0.0746, Time Left=1.66 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3264/3393 [27:15<01:06,  1.95batch/s, Batch Loss=0.0006, Avg Loss=0.0746, Time Left=1.66 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3264/3393 [27:15<01:06,  1.95batch/s, Batch Loss=0.0010, Avg Loss=0.0745, Time Left=1.65 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  96%|▉| 3265/3393 [27:15<01:05,  1.97batch/s, Batch Loss=0.0010, Avg Loss=0.0745, Time Left=1.65 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3265/3393 [27:16<01:05,  1.97batch/s, Batch Loss=0.0734, Avg Loss=0.0745, Time Left=1.64 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3266/3393 [27:16<01:04,  1.97batch/s, Batch Loss=0.0734, Avg Loss=0.0745, Time Left=1.64 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3266/3393 [27:16<01:04,  1.97batch/s, Batch Loss=0.0035, Avg Loss=0.0745, Time Left=1.64 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3267/3393 [27:16<01:04,  1.96batch/s, Batch Loss=0.0035, Avg Loss=0.0745, Time Left=1.64 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3267/3393 [27:17<01:04,  1.96batch/s, Batch Loss=0.0008, Avg Loss=0.0745, Time Left=1.63 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3268/3393 [27:17<01:03,  1.97batch/s, Batch Loss=0.0008, Avg Loss=0.0745, Time Left=1.63 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3268/3393 [27:17<01:03,  1.97batch/s, Batch Loss=0.0158, Avg Loss=0.0745, Time Left=1.62 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3269/3393 [27:17<01:03,  1.96batch/s, Batch Loss=0.0158, Avg Loss=0.0745, Time Left=1.62 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3269/3393 [27:18<01:03,  1.96batch/s, Batch Loss=0.0041, Avg Loss=0.0744, Time Left=1.61 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3270/3393 [27:18<01:02,  1.97batch/s, Batch Loss=0.0041, Avg Loss=0.0744, Time Left=1.61 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3270/3393 [27:18<01:02,  1.97batch/s, Batch Loss=0.0009, Avg Loss=0.0744, Time Left=1.60 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3271/3393 [27:18<01:02,  1.94batch/s, Batch Loss=0.0009, Avg Loss=0.0744, Time Left=1.60 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3271/3393 [27:19<01:02,  1.94batch/s, Batch Loss=0.0398, Avg Loss=0.0744, Time Left=1.59 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3272/3393 [27:19<01:01,  1.96batch/s, Batch Loss=0.0398, Avg Loss=0.0744, Time Left=1.59 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3272/3393 [27:19<01:01,  1.96batch/s, Batch Loss=0.0973, Avg Loss=0.0744, Time Left=1.58 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3273/3393 [27:19<01:02,  1.93batch/s, Batch Loss=0.0973, Avg Loss=0.0744, Time Left=1.58 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3273/3393 [27:20<01:02,  1.93batch/s, Batch Loss=0.0346, Avg Loss=0.0744, Time Left=1.58 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3274/3393 [27:20<01:01,  1.93batch/s, Batch Loss=0.0346, Avg Loss=0.0744, Time Left=1.58 \u001b[A\n",
      "Epoch 3/3 - Training:  96%|▉| 3274/3393 [27:20<01:01,  1.93batch/s, Batch Loss=0.0011, Avg Loss=0.0744, Time Left=1.57 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3275/3393 [27:20<01:00,  1.95batch/s, Batch Loss=0.0011, Avg Loss=0.0744, Time Left=1.57 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3275/3393 [27:21<01:00,  1.95batch/s, Batch Loss=0.0087, Avg Loss=0.0744, Time Left=1.56 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3276/3393 [27:21<01:00,  1.95batch/s, Batch Loss=0.0087, Avg Loss=0.0744, Time Left=1.56 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3276/3393 [27:21<01:00,  1.95batch/s, Batch Loss=0.0125, Avg Loss=0.0743, Time Left=1.55 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3277/3393 [27:21<00:59,  1.96batch/s, Batch Loss=0.0125, Avg Loss=0.0743, Time Left=1.55 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3277/3393 [27:22<00:59,  1.96batch/s, Batch Loss=0.0008, Avg Loss=0.0743, Time Left=1.54 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3278/3393 [27:22<00:58,  1.95batch/s, Batch Loss=0.0008, Avg Loss=0.0743, Time Left=1.54 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3278/3393 [27:22<00:58,  1.95batch/s, Batch Loss=0.0131, Avg Loss=0.0743, Time Left=1.53 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3279/3393 [27:22<00:59,  1.93batch/s, Batch Loss=0.0131, Avg Loss=0.0743, Time Left=1.53 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3279/3393 [27:23<00:59,  1.93batch/s, Batch Loss=0.0031, Avg Loss=0.0743, Time Left=1.53 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3280/3393 [27:23<00:57,  1.97batch/s, Batch Loss=0.0031, Avg Loss=0.0743, Time Left=1.53 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3280/3393 [27:23<00:57,  1.97batch/s, Batch Loss=0.0006, Avg Loss=0.0743, Time Left=1.52 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3281/3393 [27:23<00:56,  1.98batch/s, Batch Loss=0.0006, Avg Loss=0.0743, Time Left=1.52 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3281/3393 [27:24<00:56,  1.98batch/s, Batch Loss=0.0011, Avg Loss=0.0742, Time Left=1.51 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3282/3393 [27:24<00:56,  1.98batch/s, Batch Loss=0.0011, Avg Loss=0.0742, Time Left=1.51 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3282/3393 [27:24<00:56,  1.98batch/s, Batch Loss=0.0078, Avg Loss=0.0742, Time Left=1.50 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3283/3393 [27:24<00:56,  1.95batch/s, Batch Loss=0.0078, Avg Loss=0.0742, Time Left=1.50 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3283/3393 [27:25<00:56,  1.95batch/s, Batch Loss=0.0017, Avg Loss=0.0742, Time Left=1.49 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3284/3393 [27:25<00:55,  1.96batch/s, Batch Loss=0.0017, Avg Loss=0.0742, Time Left=1.49 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3284/3393 [27:25<00:55,  1.96batch/s, Batch Loss=0.0235, Avg Loss=0.0742, Time Left=1.48 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3285/3393 [27:25<00:55,  1.94batch/s, Batch Loss=0.0235, Avg Loss=0.0742, Time Left=1.48 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3285/3393 [27:26<00:55,  1.94batch/s, Batch Loss=0.0058, Avg Loss=0.0742, Time Left=1.47 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3286/3393 [27:26<00:54,  1.95batch/s, Batch Loss=0.0058, Avg Loss=0.0742, Time Left=1.47 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3286/3393 [27:26<00:54,  1.95batch/s, Batch Loss=0.0167, Avg Loss=0.0741, Time Left=1.47 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3287/3393 [27:26<00:54,  1.93batch/s, Batch Loss=0.0167, Avg Loss=0.0741, Time Left=1.47 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3287/3393 [27:27<00:54,  1.93batch/s, Batch Loss=0.3519, Avg Loss=0.0742, Time Left=1.46 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3288/3393 [27:27<00:53,  1.95batch/s, Batch Loss=0.3519, Avg Loss=0.0742, Time Left=1.46 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3288/3393 [27:27<00:53,  1.95batch/s, Batch Loss=0.0006, Avg Loss=0.0742, Time Left=1.45 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3289/3393 [27:27<00:54,  1.91batch/s, Batch Loss=0.0006, Avg Loss=0.0742, Time Left=1.45 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3289/3393 [27:28<00:54,  1.91batch/s, Batch Loss=0.1811, Avg Loss=0.0742, Time Left=1.44 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3290/3393 [27:28<00:52,  1.95batch/s, Batch Loss=0.1811, Avg Loss=0.0742, Time Left=1.44 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3290/3393 [27:28<00:52,  1.95batch/s, Batch Loss=0.0008, Avg Loss=0.0742, Time Left=1.43 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3291/3393 [27:28<00:53,  1.91batch/s, Batch Loss=0.0008, Avg Loss=0.0742, Time Left=1.43 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3291/3393 [27:29<00:53,  1.91batch/s, Batch Loss=0.0017, Avg Loss=0.0742, Time Left=1.42 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3292/3393 [27:29<00:52,  1.93batch/s, Batch Loss=0.0017, Avg Loss=0.0742, Time Left=1.42 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3292/3393 [27:29<00:52,  1.93batch/s, Batch Loss=0.0137, Avg Loss=0.0742, Time Left=1.41 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3293/3393 [27:29<00:50,  1.97batch/s, Batch Loss=0.0137, Avg Loss=0.0742, Time Left=1.41 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3293/3393 [27:30<00:50,  1.97batch/s, Batch Loss=0.0064, Avg Loss=0.0741, Time Left=1.41 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3294/3393 [27:30<00:49,  1.98batch/s, Batch Loss=0.0064, Avg Loss=0.0741, Time Left=1.41 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3294/3393 [27:30<00:49,  1.98batch/s, Batch Loss=0.0495, Avg Loss=0.0741, Time Left=1.40 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3295/3393 [27:30<00:49,  1.97batch/s, Batch Loss=0.0495, Avg Loss=0.0741, Time Left=1.40 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3295/3393 [27:31<00:49,  1.97batch/s, Batch Loss=0.0013, Avg Loss=0.0741, Time Left=1.39 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3296/3393 [27:31<00:49,  1.97batch/s, Batch Loss=0.0013, Avg Loss=0.0741, Time Left=1.39 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3296/3393 [27:31<00:49,  1.97batch/s, Batch Loss=0.0255, Avg Loss=0.0741, Time Left=1.38 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3297/3393 [27:31<00:48,  1.98batch/s, Batch Loss=0.0255, Avg Loss=0.0741, Time Left=1.38 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3297/3393 [27:32<00:48,  1.98batch/s, Batch Loss=0.0877, Avg Loss=0.0741, Time Left=1.37 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  97%|▉| 3298/3393 [27:32<00:46,  2.03batch/s, Batch Loss=0.0877, Avg Loss=0.0741, Time Left=1.37 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3298/3393 [27:32<00:46,  2.03batch/s, Batch Loss=0.1382, Avg Loss=0.0741, Time Left=1.36 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3299/3393 [27:32<00:46,  2.00batch/s, Batch Loss=0.1382, Avg Loss=0.0741, Time Left=1.36 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3299/3393 [27:33<00:46,  2.00batch/s, Batch Loss=0.0507, Avg Loss=0.0741, Time Left=1.35 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3300/3393 [27:33<00:46,  2.00batch/s, Batch Loss=0.0507, Avg Loss=0.0741, Time Left=1.35 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3300/3393 [27:33<00:46,  2.00batch/s, Batch Loss=0.1346, Avg Loss=0.0741, Time Left=1.35 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3301/3393 [27:33<00:45,  2.00batch/s, Batch Loss=0.1346, Avg Loss=0.0741, Time Left=1.35 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3301/3393 [27:34<00:45,  2.00batch/s, Batch Loss=0.0026, Avg Loss=0.0741, Time Left=1.34 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3302/3393 [27:34<00:45,  2.00batch/s, Batch Loss=0.0026, Avg Loss=0.0741, Time Left=1.34 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3302/3393 [27:34<00:45,  2.00batch/s, Batch Loss=0.1783, Avg Loss=0.0741, Time Left=1.33 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3303/3393 [27:34<00:45,  2.00batch/s, Batch Loss=0.1783, Avg Loss=0.0741, Time Left=1.33 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3303/3393 [27:35<00:45,  2.00batch/s, Batch Loss=0.0062, Avg Loss=0.0741, Time Left=1.32 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3304/3393 [27:35<00:44,  2.00batch/s, Batch Loss=0.0062, Avg Loss=0.0741, Time Left=1.32 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3304/3393 [27:35<00:44,  2.00batch/s, Batch Loss=0.0158, Avg Loss=0.0741, Time Left=1.31 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3305/3393 [27:35<00:43,  2.02batch/s, Batch Loss=0.0158, Avg Loss=0.0741, Time Left=1.31 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3305/3393 [27:36<00:43,  2.02batch/s, Batch Loss=0.1535, Avg Loss=0.0741, Time Left=1.30 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3306/3393 [27:36<00:43,  2.01batch/s, Batch Loss=0.1535, Avg Loss=0.0741, Time Left=1.30 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3306/3393 [27:36<00:43,  2.01batch/s, Batch Loss=0.0124, Avg Loss=0.0741, Time Left=1.30 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3307/3393 [27:36<00:42,  2.01batch/s, Batch Loss=0.0124, Avg Loss=0.0741, Time Left=1.30 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3307/3393 [27:37<00:42,  2.01batch/s, Batch Loss=0.0864, Avg Loss=0.0741, Time Left=1.29 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3308/3393 [27:37<00:41,  2.02batch/s, Batch Loss=0.0864, Avg Loss=0.0741, Time Left=1.29 \u001b[A\n",
      "Epoch 3/3 - Training:  97%|▉| 3308/3393 [27:37<00:41,  2.02batch/s, Batch Loss=0.0423, Avg Loss=0.0741, Time Left=1.28 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3309/3393 [27:37<00:42,  1.96batch/s, Batch Loss=0.0423, Avg Loss=0.0741, Time Left=1.28 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3309/3393 [27:38<00:42,  1.96batch/s, Batch Loss=0.0211, Avg Loss=0.0741, Time Left=1.27 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3310/3393 [27:38<00:42,  1.97batch/s, Batch Loss=0.0211, Avg Loss=0.0741, Time Left=1.27 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3310/3393 [27:38<00:42,  1.97batch/s, Batch Loss=0.0885, Avg Loss=0.0741, Time Left=1.26 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3311/3393 [27:38<00:41,  2.00batch/s, Batch Loss=0.0885, Avg Loss=0.0741, Time Left=1.26 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3311/3393 [27:39<00:41,  2.00batch/s, Batch Loss=0.0339, Avg Loss=0.0741, Time Left=1.25 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3312/3393 [27:39<00:40,  2.00batch/s, Batch Loss=0.0339, Avg Loss=0.0741, Time Left=1.25 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3312/3393 [27:39<00:40,  2.00batch/s, Batch Loss=0.0010, Avg Loss=0.0741, Time Left=1.24 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3313/3393 [27:39<00:40,  1.98batch/s, Batch Loss=0.0010, Avg Loss=0.0741, Time Left=1.24 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3313/3393 [27:40<00:40,  1.98batch/s, Batch Loss=0.0836, Avg Loss=0.0741, Time Left=1.24 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3314/3393 [27:40<00:39,  1.98batch/s, Batch Loss=0.0836, Avg Loss=0.0741, Time Left=1.24 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3314/3393 [27:40<00:39,  1.98batch/s, Batch Loss=0.0056, Avg Loss=0.0740, Time Left=1.23 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3315/3393 [27:40<00:39,  1.97batch/s, Batch Loss=0.0056, Avg Loss=0.0740, Time Left=1.23 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3315/3393 [27:41<00:39,  1.97batch/s, Batch Loss=0.1165, Avg Loss=0.0741, Time Left=1.22 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3316/3393 [27:41<00:38,  1.98batch/s, Batch Loss=0.1165, Avg Loss=0.0741, Time Left=1.22 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3316/3393 [27:41<00:38,  1.98batch/s, Batch Loss=0.0283, Avg Loss=0.0740, Time Left=1.21 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3317/3393 [27:42<00:38,  1.95batch/s, Batch Loss=0.0283, Avg Loss=0.0740, Time Left=1.21 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3317/3393 [27:42<00:38,  1.95batch/s, Batch Loss=0.0728, Avg Loss=0.0740, Time Left=1.20 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3318/3393 [27:42<00:37,  1.98batch/s, Batch Loss=0.0728, Avg Loss=0.0740, Time Left=1.20 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3318/3393 [27:43<00:37,  1.98batch/s, Batch Loss=0.1630, Avg Loss=0.0741, Time Left=1.19 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3319/3393 [27:43<00:37,  1.95batch/s, Batch Loss=0.1630, Avg Loss=0.0741, Time Left=1.19 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3319/3393 [27:43<00:37,  1.95batch/s, Batch Loss=0.0010, Avg Loss=0.0740, Time Left=1.18 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3320/3393 [27:43<00:37,  1.95batch/s, Batch Loss=0.0010, Avg Loss=0.0740, Time Left=1.18 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3320/3393 [27:44<00:37,  1.95batch/s, Batch Loss=0.1557, Avg Loss=0.0741, Time Left=1.18 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3321/3393 [27:44<00:37,  1.93batch/s, Batch Loss=0.1557, Avg Loss=0.0741, Time Left=1.18 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3321/3393 [27:44<00:37,  1.93batch/s, Batch Loss=0.0061, Avg Loss=0.0741, Time Left=1.17 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3322/3393 [27:44<00:36,  1.96batch/s, Batch Loss=0.0061, Avg Loss=0.0741, Time Left=1.17 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3322/3393 [27:45<00:36,  1.96batch/s, Batch Loss=0.0525, Avg Loss=0.0740, Time Left=1.16 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3323/3393 [27:45<00:36,  1.91batch/s, Batch Loss=0.0525, Avg Loss=0.0740, Time Left=1.16 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3323/3393 [27:45<00:36,  1.91batch/s, Batch Loss=0.0290, Avg Loss=0.0740, Time Left=1.15 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3324/3393 [27:45<00:35,  1.96batch/s, Batch Loss=0.0290, Avg Loss=0.0740, Time Left=1.15 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3324/3393 [27:46<00:35,  1.96batch/s, Batch Loss=0.0053, Avg Loss=0.0740, Time Left=1.14 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3325/3393 [27:46<00:35,  1.94batch/s, Batch Loss=0.0053, Avg Loss=0.0740, Time Left=1.14 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3325/3393 [27:46<00:35,  1.94batch/s, Batch Loss=0.1113, Avg Loss=0.0740, Time Left=1.13 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3326/3393 [27:46<00:34,  1.95batch/s, Batch Loss=0.1113, Avg Loss=0.0740, Time Left=1.13 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3326/3393 [27:47<00:34,  1.95batch/s, Batch Loss=0.1450, Avg Loss=0.0740, Time Left=1.12 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3327/3393 [27:47<00:33,  1.95batch/s, Batch Loss=0.1450, Avg Loss=0.0740, Time Left=1.12 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3327/3393 [27:47<00:33,  1.95batch/s, Batch Loss=0.0030, Avg Loss=0.0740, Time Left=1.12 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3328/3393 [27:47<00:33,  1.96batch/s, Batch Loss=0.0030, Avg Loss=0.0740, Time Left=1.12 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3328/3393 [27:48<00:33,  1.96batch/s, Batch Loss=0.0123, Avg Loss=0.0740, Time Left=1.11 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3329/3393 [27:48<00:32,  1.95batch/s, Batch Loss=0.0123, Avg Loss=0.0740, Time Left=1.11 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3329/3393 [27:48<00:32,  1.95batch/s, Batch Loss=0.0087, Avg Loss=0.0740, Time Left=1.10 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3330/3393 [27:48<00:32,  1.97batch/s, Batch Loss=0.0087, Avg Loss=0.0740, Time Left=1.10 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3330/3393 [27:49<00:32,  1.97batch/s, Batch Loss=0.1012, Avg Loss=0.0740, Time Left=1.09 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  98%|▉| 3331/3393 [27:49<00:31,  1.98batch/s, Batch Loss=0.1012, Avg Loss=0.0740, Time Left=1.09 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3331/3393 [27:49<00:31,  1.98batch/s, Batch Loss=0.0082, Avg Loss=0.0740, Time Left=1.08 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3332/3393 [27:49<00:30,  2.00batch/s, Batch Loss=0.0082, Avg Loss=0.0740, Time Left=1.08 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3332/3393 [27:50<00:30,  2.00batch/s, Batch Loss=0.0333, Avg Loss=0.0740, Time Left=1.07 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3333/3393 [27:50<00:29,  2.02batch/s, Batch Loss=0.0333, Avg Loss=0.0740, Time Left=1.07 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3333/3393 [27:50<00:29,  2.02batch/s, Batch Loss=0.0175, Avg Loss=0.0739, Time Left=1.07 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3334/3393 [27:50<00:29,  2.03batch/s, Batch Loss=0.0175, Avg Loss=0.0739, Time Left=1.07 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3334/3393 [27:51<00:29,  2.03batch/s, Batch Loss=0.1302, Avg Loss=0.0740, Time Left=1.06 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3335/3393 [27:51<00:28,  2.00batch/s, Batch Loss=0.1302, Avg Loss=0.0740, Time Left=1.06 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3335/3393 [27:51<00:28,  2.00batch/s, Batch Loss=0.0018, Avg Loss=0.0739, Time Left=1.05 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3336/3393 [27:51<00:28,  1.99batch/s, Batch Loss=0.0018, Avg Loss=0.0739, Time Left=1.05 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3336/3393 [27:52<00:28,  1.99batch/s, Batch Loss=0.1358, Avg Loss=0.0740, Time Left=1.04 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3337/3393 [27:52<00:28,  1.99batch/s, Batch Loss=0.1358, Avg Loss=0.0740, Time Left=1.04 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3337/3393 [27:52<00:28,  1.99batch/s, Batch Loss=0.0920, Avg Loss=0.0740, Time Left=1.03 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3338/3393 [27:52<00:28,  1.95batch/s, Batch Loss=0.0920, Avg Loss=0.0740, Time Left=1.03 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3338/3393 [27:53<00:28,  1.95batch/s, Batch Loss=0.0756, Avg Loss=0.0740, Time Left=1.02 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3339/3393 [27:53<00:28,  1.92batch/s, Batch Loss=0.0756, Avg Loss=0.0740, Time Left=1.02 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3339/3393 [27:53<00:28,  1.92batch/s, Batch Loss=0.1313, Avg Loss=0.0740, Time Left=1.01 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3340/3393 [27:53<00:26,  1.97batch/s, Batch Loss=0.1313, Avg Loss=0.0740, Time Left=1.01 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3340/3393 [27:54<00:26,  1.97batch/s, Batch Loss=0.0196, Avg Loss=0.0740, Time Left=1.01 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3341/3393 [27:54<00:26,  1.96batch/s, Batch Loss=0.0196, Avg Loss=0.0740, Time Left=1.01 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3341/3393 [27:54<00:26,  1.96batch/s, Batch Loss=0.0947, Avg Loss=0.0740, Time Left=1.00 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3342/3393 [27:54<00:25,  1.99batch/s, Batch Loss=0.0947, Avg Loss=0.0740, Time Left=1.00 \u001b[A\n",
      "Epoch 3/3 - Training:  98%|▉| 3342/3393 [27:55<00:25,  1.99batch/s, Batch Loss=0.1265, Avg Loss=0.0740, Time Left=0.99 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3343/3393 [27:55<00:25,  1.99batch/s, Batch Loss=0.1265, Avg Loss=0.0740, Time Left=0.99 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3343/3393 [27:55<00:25,  1.99batch/s, Batch Loss=0.0332, Avg Loss=0.0740, Time Left=0.98 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3344/3393 [27:55<00:24,  1.99batch/s, Batch Loss=0.0332, Avg Loss=0.0740, Time Left=0.98 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3344/3393 [27:56<00:24,  1.99batch/s, Batch Loss=0.1303, Avg Loss=0.0740, Time Left=0.97 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3345/3393 [27:56<00:23,  2.01batch/s, Batch Loss=0.1303, Avg Loss=0.0740, Time Left=0.97 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3345/3393 [27:56<00:23,  2.01batch/s, Batch Loss=0.0008, Avg Loss=0.0740, Time Left=0.96 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3346/3393 [27:56<00:24,  1.96batch/s, Batch Loss=0.0008, Avg Loss=0.0740, Time Left=0.96 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3346/3393 [27:57<00:24,  1.96batch/s, Batch Loss=0.0368, Avg Loss=0.0740, Time Left=0.95 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3347/3393 [27:57<00:22,  2.00batch/s, Batch Loss=0.0368, Avg Loss=0.0740, Time Left=0.95 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3347/3393 [27:57<00:22,  2.00batch/s, Batch Loss=0.0019, Avg Loss=0.0739, Time Left=0.95 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3348/3393 [27:57<00:22,  1.98batch/s, Batch Loss=0.0019, Avg Loss=0.0739, Time Left=0.95 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3348/3393 [27:58<00:22,  1.98batch/s, Batch Loss=0.2731, Avg Loss=0.0740, Time Left=0.94 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3349/3393 [27:58<00:22,  1.99batch/s, Batch Loss=0.2731, Avg Loss=0.0740, Time Left=0.94 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3349/3393 [27:58<00:22,  1.99batch/s, Batch Loss=0.0090, Avg Loss=0.0740, Time Left=0.93 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3350/3393 [27:58<00:21,  2.01batch/s, Batch Loss=0.0090, Avg Loss=0.0740, Time Left=0.93 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3350/3393 [27:59<00:21,  2.01batch/s, Batch Loss=0.2484, Avg Loss=0.0740, Time Left=0.92 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3351/3393 [27:59<00:21,  1.99batch/s, Batch Loss=0.2484, Avg Loss=0.0740, Time Left=0.92 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3351/3393 [27:59<00:21,  1.99batch/s, Batch Loss=0.1787, Avg Loss=0.0741, Time Left=0.91 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3352/3393 [27:59<00:20,  1.99batch/s, Batch Loss=0.1787, Avg Loss=0.0741, Time Left=0.91 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3352/3393 [28:00<00:20,  1.99batch/s, Batch Loss=0.0025, Avg Loss=0.0740, Time Left=0.90 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3353/3393 [28:00<00:19,  2.01batch/s, Batch Loss=0.0025, Avg Loss=0.0740, Time Left=0.90 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3353/3393 [28:00<00:19,  2.01batch/s, Batch Loss=0.0233, Avg Loss=0.0740, Time Left=0.89 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3354/3393 [28:00<00:19,  2.03batch/s, Batch Loss=0.0233, Avg Loss=0.0740, Time Left=0.89 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3354/3393 [28:01<00:19,  2.03batch/s, Batch Loss=0.0695, Avg Loss=0.0740, Time Left=0.89 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3355/3393 [28:01<00:19,  1.98batch/s, Batch Loss=0.0695, Avg Loss=0.0740, Time Left=0.89 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3355/3393 [28:01<00:19,  1.98batch/s, Batch Loss=0.0192, Avg Loss=0.0740, Time Left=0.88 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3356/3393 [28:01<00:18,  2.02batch/s, Batch Loss=0.0192, Avg Loss=0.0740, Time Left=0.88 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3356/3393 [28:02<00:18,  2.02batch/s, Batch Loss=0.3262, Avg Loss=0.0741, Time Left=0.87 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3357/3393 [28:02<00:17,  2.02batch/s, Batch Loss=0.3262, Avg Loss=0.0741, Time Left=0.87 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3357/3393 [28:02<00:17,  2.02batch/s, Batch Loss=0.0539, Avg Loss=0.0741, Time Left=0.86 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3358/3393 [28:02<00:17,  1.99batch/s, Batch Loss=0.0539, Avg Loss=0.0741, Time Left=0.86 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3358/3393 [28:03<00:17,  1.99batch/s, Batch Loss=0.1429, Avg Loss=0.0741, Time Left=0.85 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3359/3393 [28:03<00:17,  1.97batch/s, Batch Loss=0.1429, Avg Loss=0.0741, Time Left=0.85 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3359/3393 [28:03<00:17,  1.97batch/s, Batch Loss=0.0690, Avg Loss=0.0741, Time Left=0.84 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3360/3393 [28:03<00:17,  1.91batch/s, Batch Loss=0.0690, Avg Loss=0.0741, Time Left=0.84 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3360/3393 [28:04<00:17,  1.91batch/s, Batch Loss=0.0430, Avg Loss=0.0741, Time Left=0.83 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3361/3393 [28:04<00:16,  1.89batch/s, Batch Loss=0.0430, Avg Loss=0.0741, Time Left=0.83 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3361/3393 [28:04<00:16,  1.89batch/s, Batch Loss=0.0652, Avg Loss=0.0741, Time Left=0.83 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3362/3393 [28:04<00:16,  1.92batch/s, Batch Loss=0.0652, Avg Loss=0.0741, Time Left=0.83 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3362/3393 [28:05<00:16,  1.92batch/s, Batch Loss=0.0157, Avg Loss=0.0741, Time Left=0.82 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3363/3393 [28:05<00:15,  1.93batch/s, Batch Loss=0.0157, Avg Loss=0.0741, Time Left=0.82 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3363/3393 [28:05<00:15,  1.93batch/s, Batch Loss=0.2126, Avg Loss=0.0741, Time Left=0.81 \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training:  99%|▉| 3364/3393 [28:05<00:15,  1.87batch/s, Batch Loss=0.2126, Avg Loss=0.0741, Time Left=0.81 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3364/3393 [28:06<00:15,  1.87batch/s, Batch Loss=0.0703, Avg Loss=0.0741, Time Left=0.80 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3365/3393 [28:06<00:14,  1.95batch/s, Batch Loss=0.0703, Avg Loss=0.0741, Time Left=0.80 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3365/3393 [28:06<00:14,  1.95batch/s, Batch Loss=0.1216, Avg Loss=0.0741, Time Left=0.79 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3366/3393 [28:06<00:13,  1.98batch/s, Batch Loss=0.1216, Avg Loss=0.0741, Time Left=0.79 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3366/3393 [28:07<00:13,  1.98batch/s, Batch Loss=0.0323, Avg Loss=0.0741, Time Left=0.78 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3367/3393 [28:07<00:13,  1.97batch/s, Batch Loss=0.0323, Avg Loss=0.0741, Time Left=0.78 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3367/3393 [28:07<00:13,  1.97batch/s, Batch Loss=0.0838, Avg Loss=0.0741, Time Left=0.78 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3368/3393 [28:07<00:12,  1.99batch/s, Batch Loss=0.0838, Avg Loss=0.0741, Time Left=0.78 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3368/3393 [28:08<00:12,  1.99batch/s, Batch Loss=0.1481, Avg Loss=0.0741, Time Left=0.77 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3369/3393 [28:08<00:11,  2.04batch/s, Batch Loss=0.1481, Avg Loss=0.0741, Time Left=0.77 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3369/3393 [28:08<00:11,  2.04batch/s, Batch Loss=0.0330, Avg Loss=0.0741, Time Left=0.76 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3370/3393 [28:08<00:11,  1.99batch/s, Batch Loss=0.0330, Avg Loss=0.0741, Time Left=0.76 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3370/3393 [28:09<00:11,  1.99batch/s, Batch Loss=0.0508, Avg Loss=0.0741, Time Left=0.75 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3371/3393 [28:09<00:10,  2.01batch/s, Batch Loss=0.0508, Avg Loss=0.0741, Time Left=0.75 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3371/3393 [28:09<00:10,  2.01batch/s, Batch Loss=0.1022, Avg Loss=0.0741, Time Left=0.74 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3372/3393 [28:09<00:10,  2.01batch/s, Batch Loss=0.1022, Avg Loss=0.0741, Time Left=0.74 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3372/3393 [28:10<00:10,  2.01batch/s, Batch Loss=0.0113, Avg Loss=0.0741, Time Left=0.73 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3373/3393 [28:10<00:09,  2.00batch/s, Batch Loss=0.0113, Avg Loss=0.0741, Time Left=0.73 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3373/3393 [28:10<00:09,  2.00batch/s, Batch Loss=0.0645, Avg Loss=0.0741, Time Left=0.72 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3374/3393 [28:10<00:09,  1.97batch/s, Batch Loss=0.0645, Avg Loss=0.0741, Time Left=0.72 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3374/3393 [28:11<00:09,  1.97batch/s, Batch Loss=0.0976, Avg Loss=0.0741, Time Left=0.72 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3375/3393 [28:11<00:08,  2.01batch/s, Batch Loss=0.0976, Avg Loss=0.0741, Time Left=0.72 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3375/3393 [28:11<00:08,  2.01batch/s, Batch Loss=0.3177, Avg Loss=0.0742, Time Left=0.71 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3376/3393 [28:11<00:08,  2.03batch/s, Batch Loss=0.3177, Avg Loss=0.0742, Time Left=0.71 \u001b[A\n",
      "Epoch 3/3 - Training:  99%|▉| 3376/3393 [28:12<00:08,  2.03batch/s, Batch Loss=0.1507, Avg Loss=0.0742, Time Left=0.70 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3377/3393 [28:12<00:07,  2.02batch/s, Batch Loss=0.1507, Avg Loss=0.0742, Time Left=0.70 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3377/3393 [28:12<00:07,  2.02batch/s, Batch Loss=0.1454, Avg Loss=0.0742, Time Left=0.69 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3378/3393 [28:12<00:07,  2.01batch/s, Batch Loss=0.1454, Avg Loss=0.0742, Time Left=0.69 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3378/3393 [28:13<00:07,  2.01batch/s, Batch Loss=0.0106, Avg Loss=0.0742, Time Left=0.68 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3379/3393 [28:13<00:07,  1.98batch/s, Batch Loss=0.0106, Avg Loss=0.0742, Time Left=0.68 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3379/3393 [28:13<00:07,  1.98batch/s, Batch Loss=0.0151, Avg Loss=0.0742, Time Left=0.67 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3380/3393 [28:13<00:06,  1.98batch/s, Batch Loss=0.0151, Avg Loss=0.0742, Time Left=0.67 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3380/3393 [28:14<00:06,  1.98batch/s, Batch Loss=0.0227, Avg Loss=0.0742, Time Left=0.66 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3381/3393 [28:14<00:06,  1.98batch/s, Batch Loss=0.0227, Avg Loss=0.0742, Time Left=0.66 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3381/3393 [28:14<00:06,  1.98batch/s, Batch Loss=0.1171, Avg Loss=0.0742, Time Left=0.66 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3382/3393 [28:14<00:05,  1.98batch/s, Batch Loss=0.1171, Avg Loss=0.0742, Time Left=0.66 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3382/3393 [28:15<00:05,  1.98batch/s, Batch Loss=0.2635, Avg Loss=0.0742, Time Left=0.65 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3383/3393 [28:15<00:05,  1.97batch/s, Batch Loss=0.2635, Avg Loss=0.0742, Time Left=0.65 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3383/3393 [28:15<00:05,  1.97batch/s, Batch Loss=0.0145, Avg Loss=0.0742, Time Left=0.64 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3384/3393 [28:15<00:04,  1.98batch/s, Batch Loss=0.0145, Avg Loss=0.0742, Time Left=0.64 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3384/3393 [28:16<00:04,  1.98batch/s, Batch Loss=0.0598, Avg Loss=0.0742, Time Left=0.63 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3385/3393 [28:16<00:04,  1.95batch/s, Batch Loss=0.0598, Avg Loss=0.0742, Time Left=0.63 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3385/3393 [28:16<00:04,  1.95batch/s, Batch Loss=0.0251, Avg Loss=0.0742, Time Left=0.62 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3386/3393 [28:16<00:03,  1.96batch/s, Batch Loss=0.0251, Avg Loss=0.0742, Time Left=0.62 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3386/3393 [28:17<00:03,  1.96batch/s, Batch Loss=0.0391, Avg Loss=0.0742, Time Left=0.61 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3387/3393 [28:17<00:03,  1.95batch/s, Batch Loss=0.0391, Avg Loss=0.0742, Time Left=0.61 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3387/3393 [28:17<00:03,  1.95batch/s, Batch Loss=0.0166, Avg Loss=0.0742, Time Left=0.60 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3388/3393 [28:17<00:02,  1.98batch/s, Batch Loss=0.0166, Avg Loss=0.0742, Time Left=0.60 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3388/3393 [28:18<00:02,  1.98batch/s, Batch Loss=0.0104, Avg Loss=0.0742, Time Left=0.60 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3389/3393 [28:18<00:02,  1.95batch/s, Batch Loss=0.0104, Avg Loss=0.0742, Time Left=0.60 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3389/3393 [28:18<00:02,  1.95batch/s, Batch Loss=0.0243, Avg Loss=0.0741, Time Left=0.59 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3390/3393 [28:18<00:01,  1.95batch/s, Batch Loss=0.0243, Avg Loss=0.0741, Time Left=0.59 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3390/3393 [28:19<00:01,  1.95batch/s, Batch Loss=0.0274, Avg Loss=0.0741, Time Left=0.58 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3391/3393 [28:19<00:01,  1.96batch/s, Batch Loss=0.0274, Avg Loss=0.0741, Time Left=0.58 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3391/3393 [28:19<00:01,  1.96batch/s, Batch Loss=0.0371, Avg Loss=0.0741, Time Left=0.57 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3392/3393 [28:19<00:00,  1.95batch/s, Batch Loss=0.0371, Avg Loss=0.0741, Time Left=0.57 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|▉| 3392/3393 [28:20<00:00,  1.95batch/s, Batch Loss=0.0008, Avg Loss=0.0741, Time Left=0.56 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|█| 3393/3393 [28:20<00:00,  1.95batch/s, Batch Loss=0.0008, Avg Loss=0.0741, Time Left=0.56 \u001b[A\n",
      "Epoch 3/3 - Training: 100%|█| 3393/3393 [28:20<00:00,  1.95batch/s, Batch Loss=0.0245, Avg Loss=0.0741, Time Left=0.55 \u001b[A\n",
      "Epoch 3/3 - Training: 3394batch [28:21,  1.95batch/s, Batch Loss=0.0245, Avg Loss=0.0741, Time Left=0.55 min]          \u001b[A\n",
      "Epoch 3/3 - Training: 3394batch [28:21,  1.95batch/s, Batch Loss=0.0371, Avg Loss=0.0741, Time Left=0.55 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3395batch [28:21,  1.96batch/s, Batch Loss=0.0371, Avg Loss=0.0741, Time Left=0.55 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3395batch [28:21,  1.96batch/s, Batch Loss=0.1184, Avg Loss=0.0741, Time Left=0.54 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3396batch [28:21,  2.01batch/s, Batch Loss=0.1184, Avg Loss=0.0741, Time Left=0.54 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3396batch [28:22,  2.01batch/s, Batch Loss=0.0691, Avg Loss=0.0741, Time Left=0.53 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3397batch [28:22,  2.00batch/s, Batch Loss=0.0691, Avg Loss=0.0741, Time Left=0.53 min]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training: 3397batch [28:22,  2.00batch/s, Batch Loss=0.1044, Avg Loss=0.0741, Time Left=0.52 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3398batch [28:22,  2.00batch/s, Batch Loss=0.1044, Avg Loss=0.0741, Time Left=0.52 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3398batch [28:23,  2.00batch/s, Batch Loss=0.1128, Avg Loss=0.0741, Time Left=0.51 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3399batch [28:23,  1.98batch/s, Batch Loss=0.1128, Avg Loss=0.0741, Time Left=0.51 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3399batch [28:24,  1.98batch/s, Batch Loss=0.0020, Avg Loss=0.0741, Time Left=0.50 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3400batch [28:24,  1.97batch/s, Batch Loss=0.0020, Avg Loss=0.0741, Time Left=0.50 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3400batch [28:24,  1.97batch/s, Batch Loss=0.2448, Avg Loss=0.0741, Time Left=0.49 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3401batch [28:24,  2.00batch/s, Batch Loss=0.2448, Avg Loss=0.0741, Time Left=0.49 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3401batch [28:24,  2.00batch/s, Batch Loss=0.0574, Avg Loss=0.0741, Time Left=0.49 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3402batch [28:24,  2.00batch/s, Batch Loss=0.0574, Avg Loss=0.0741, Time Left=0.49 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3402batch [28:25,  2.00batch/s, Batch Loss=0.0025, Avg Loss=0.0741, Time Left=0.48 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3403batch [28:25,  2.00batch/s, Batch Loss=0.0025, Avg Loss=0.0741, Time Left=0.48 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3403batch [28:25,  2.00batch/s, Batch Loss=0.0947, Avg Loss=0.0741, Time Left=0.47 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3404batch [28:25,  2.00batch/s, Batch Loss=0.0947, Avg Loss=0.0741, Time Left=0.47 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3404batch [28:26,  2.00batch/s, Batch Loss=0.0886, Avg Loss=0.0741, Time Left=0.46 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3405batch [28:26,  2.02batch/s, Batch Loss=0.0886, Avg Loss=0.0741, Time Left=0.46 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3405batch [28:26,  2.02batch/s, Batch Loss=0.1015, Avg Loss=0.0741, Time Left=0.45 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3406batch [28:26,  2.01batch/s, Batch Loss=0.1015, Avg Loss=0.0741, Time Left=0.45 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3406batch [28:27,  2.01batch/s, Batch Loss=0.0113, Avg Loss=0.0741, Time Left=0.44 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3407batch [28:27,  2.03batch/s, Batch Loss=0.0113, Avg Loss=0.0741, Time Left=0.44 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3407batch [28:27,  2.03batch/s, Batch Loss=0.1177, Avg Loss=0.0741, Time Left=0.43 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3408batch [28:27,  2.00batch/s, Batch Loss=0.1177, Avg Loss=0.0741, Time Left=0.43 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3408batch [28:28,  2.00batch/s, Batch Loss=0.0133, Avg Loss=0.0741, Time Left=0.43 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3409batch [28:28,  2.00batch/s, Batch Loss=0.0133, Avg Loss=0.0741, Time Left=0.43 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3409batch [28:28,  2.00batch/s, Batch Loss=0.0199, Avg Loss=0.0741, Time Left=0.42 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3410batch [28:28,  2.02batch/s, Batch Loss=0.0199, Avg Loss=0.0741, Time Left=0.42 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3410batch [28:29,  2.02batch/s, Batch Loss=0.0666, Avg Loss=0.0741, Time Left=0.41 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3411batch [28:29,  1.98batch/s, Batch Loss=0.0666, Avg Loss=0.0741, Time Left=0.41 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3411batch [28:30,  1.98batch/s, Batch Loss=0.0548, Avg Loss=0.0741, Time Left=0.40 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3412batch [28:30,  1.98batch/s, Batch Loss=0.0548, Avg Loss=0.0741, Time Left=0.40 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3412batch [28:30,  1.98batch/s, Batch Loss=0.0549, Avg Loss=0.0741, Time Left=0.39 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3413batch [28:30,  1.97batch/s, Batch Loss=0.0549, Avg Loss=0.0741, Time Left=0.39 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3413batch [28:31,  1.97batch/s, Batch Loss=0.1072, Avg Loss=0.0741, Time Left=0.38 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3414batch [28:31,  1.99batch/s, Batch Loss=0.1072, Avg Loss=0.0741, Time Left=0.38 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3414batch [28:31,  1.99batch/s, Batch Loss=0.0949, Avg Loss=0.0741, Time Left=0.37 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3415batch [28:31,  2.00batch/s, Batch Loss=0.0949, Avg Loss=0.0741, Time Left=0.37 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3415batch [28:32,  2.00batch/s, Batch Loss=0.0510, Avg Loss=0.0741, Time Left=0.37 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3416batch [28:32,  2.00batch/s, Batch Loss=0.0510, Avg Loss=0.0741, Time Left=0.37 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3416batch [28:32,  2.00batch/s, Batch Loss=0.0028, Avg Loss=0.0741, Time Left=0.36 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3417batch [28:32,  2.00batch/s, Batch Loss=0.0028, Avg Loss=0.0741, Time Left=0.36 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3417batch [28:33,  2.00batch/s, Batch Loss=0.0089, Avg Loss=0.0740, Time Left=0.35 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3418batch [28:33,  2.00batch/s, Batch Loss=0.0089, Avg Loss=0.0740, Time Left=0.35 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3418batch [28:33,  2.00batch/s, Batch Loss=0.0154, Avg Loss=0.0740, Time Left=0.34 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3419batch [28:33,  2.00batch/s, Batch Loss=0.0154, Avg Loss=0.0740, Time Left=0.34 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3419batch [28:34,  2.00batch/s, Batch Loss=0.0087, Avg Loss=0.0740, Time Left=0.33 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3420batch [28:34,  1.98batch/s, Batch Loss=0.0087, Avg Loss=0.0740, Time Left=0.33 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3420batch [28:34,  1.98batch/s, Batch Loss=0.0016, Avg Loss=0.0740, Time Left=0.32 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3421batch [28:34,  2.02batch/s, Batch Loss=0.0016, Avg Loss=0.0740, Time Left=0.32 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3421batch [28:35,  2.02batch/s, Batch Loss=0.0607, Avg Loss=0.0740, Time Left=0.32 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3422batch [28:35,  1.96batch/s, Batch Loss=0.0607, Avg Loss=0.0740, Time Left=0.32 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3422batch [28:35,  1.96batch/s, Batch Loss=0.0693, Avg Loss=0.0740, Time Left=0.31 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3423batch [28:35,  1.97batch/s, Batch Loss=0.0693, Avg Loss=0.0740, Time Left=0.31 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3423batch [28:36,  1.97batch/s, Batch Loss=0.0149, Avg Loss=0.0740, Time Left=0.30 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3424batch [28:36,  1.92batch/s, Batch Loss=0.0149, Avg Loss=0.0740, Time Left=0.30 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3424batch [28:36,  1.92batch/s, Batch Loss=0.0123, Avg Loss=0.0739, Time Left=0.29 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3425batch [28:36,  1.96batch/s, Batch Loss=0.0123, Avg Loss=0.0739, Time Left=0.29 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3425batch [28:37,  1.96batch/s, Batch Loss=0.0039, Avg Loss=0.0739, Time Left=0.28 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3426batch [28:37,  1.92batch/s, Batch Loss=0.0039, Avg Loss=0.0739, Time Left=0.28 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3426batch [28:37,  1.92batch/s, Batch Loss=0.0945, Avg Loss=0.0739, Time Left=0.27 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3427batch [28:37,  1.96batch/s, Batch Loss=0.0945, Avg Loss=0.0739, Time Left=0.27 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3427batch [28:38,  1.96batch/s, Batch Loss=0.0781, Avg Loss=0.0739, Time Left=0.26 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3428batch [28:38,  1.97batch/s, Batch Loss=0.0781, Avg Loss=0.0739, Time Left=0.26 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3428batch [28:38,  1.97batch/s, Batch Loss=0.0232, Avg Loss=0.0739, Time Left=0.26 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3429batch [28:38,  1.97batch/s, Batch Loss=0.0232, Avg Loss=0.0739, Time Left=0.26 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3429batch [28:39,  1.97batch/s, Batch Loss=0.0210, Avg Loss=0.0739, Time Left=0.25 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3430batch [28:39,  1.93batch/s, Batch Loss=0.0210, Avg Loss=0.0739, Time Left=0.25 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3430batch [28:39,  1.93batch/s, Batch Loss=0.0011, Avg Loss=0.0739, Time Left=0.24 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3431batch [28:39,  1.93batch/s, Batch Loss=0.0011, Avg Loss=0.0739, Time Left=0.24 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3431batch [28:40,  1.93batch/s, Batch Loss=0.0032, Avg Loss=0.0739, Time Left=0.23 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3432batch [28:40,  1.95batch/s, Batch Loss=0.0032, Avg Loss=0.0739, Time Left=0.23 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3432batch [28:40,  1.95batch/s, Batch Loss=0.1463, Avg Loss=0.0739, Time Left=0.22 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3433batch [28:40,  1.96batch/s, Batch Loss=0.1463, Avg Loss=0.0739, Time Left=0.22 min]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training: 3433batch [28:41,  1.96batch/s, Batch Loss=0.0038, Avg Loss=0.0739, Time Left=0.21 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3434batch [28:41,  1.97batch/s, Batch Loss=0.0038, Avg Loss=0.0739, Time Left=0.21 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3434batch [28:41,  1.97batch/s, Batch Loss=0.0010, Avg Loss=0.0738, Time Left=0.20 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3435batch [28:41,  2.00batch/s, Batch Loss=0.0010, Avg Loss=0.0738, Time Left=0.20 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3435batch [28:42,  2.00batch/s, Batch Loss=0.0066, Avg Loss=0.0738, Time Left=0.20 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3436batch [28:42,  1.94batch/s, Batch Loss=0.0066, Avg Loss=0.0738, Time Left=0.20 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3436batch [28:42,  1.94batch/s, Batch Loss=0.0284, Avg Loss=0.0738, Time Left=0.19 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3437batch [28:42,  1.98batch/s, Batch Loss=0.0284, Avg Loss=0.0738, Time Left=0.19 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3437batch [28:43,  1.98batch/s, Batch Loss=0.0005, Avg Loss=0.0738, Time Left=0.18 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3438batch [28:43,  1.95batch/s, Batch Loss=0.0005, Avg Loss=0.0738, Time Left=0.18 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3438batch [28:43,  1.95batch/s, Batch Loss=0.1281, Avg Loss=0.0738, Time Left=0.17 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3439batch [28:43,  2.00batch/s, Batch Loss=0.1281, Avg Loss=0.0738, Time Left=0.17 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3439batch [28:44,  2.00batch/s, Batch Loss=0.1860, Avg Loss=0.0738, Time Left=0.16 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3440batch [28:44,  1.94batch/s, Batch Loss=0.1860, Avg Loss=0.0738, Time Left=0.16 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3440batch [28:44,  1.94batch/s, Batch Loss=0.0005, Avg Loss=0.0738, Time Left=0.15 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3441batch [28:44,  1.96batch/s, Batch Loss=0.0005, Avg Loss=0.0738, Time Left=0.15 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3441batch [28:45,  1.96batch/s, Batch Loss=0.0706, Avg Loss=0.0738, Time Left=0.14 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3442batch [28:45,  1.93batch/s, Batch Loss=0.0706, Avg Loss=0.0738, Time Left=0.14 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3442batch [28:45,  1.93batch/s, Batch Loss=0.0415, Avg Loss=0.0738, Time Left=0.14 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3443batch [28:45,  1.95batch/s, Batch Loss=0.0415, Avg Loss=0.0738, Time Left=0.14 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3443batch [28:46,  1.95batch/s, Batch Loss=0.1180, Avg Loss=0.0738, Time Left=0.13 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3444batch [28:46,  1.93batch/s, Batch Loss=0.1180, Avg Loss=0.0738, Time Left=0.13 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3444batch [28:46,  1.93batch/s, Batch Loss=0.0008, Avg Loss=0.0738, Time Left=0.12 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3445batch [28:46,  1.95batch/s, Batch Loss=0.0008, Avg Loss=0.0738, Time Left=0.12 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3445batch [28:47,  1.95batch/s, Batch Loss=0.0004, Avg Loss=0.0738, Time Left=0.11 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3446batch [28:47,  1.98batch/s, Batch Loss=0.0004, Avg Loss=0.0738, Time Left=0.11 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3446batch [28:47,  1.98batch/s, Batch Loss=0.0226, Avg Loss=0.0737, Time Left=0.10 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3447batch [28:47,  2.00batch/s, Batch Loss=0.0226, Avg Loss=0.0737, Time Left=0.10 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3447batch [28:48,  2.00batch/s, Batch Loss=0.2877, Avg Loss=0.0738, Time Left=0.09 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3448batch [28:48,  2.00batch/s, Batch Loss=0.2877, Avg Loss=0.0738, Time Left=0.09 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3448batch [28:48,  2.00batch/s, Batch Loss=0.1259, Avg Loss=0.0738, Time Left=0.09 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3449batch [28:48,  2.04batch/s, Batch Loss=0.1259, Avg Loss=0.0738, Time Left=0.09 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3449batch [28:49,  2.04batch/s, Batch Loss=0.0271, Avg Loss=0.0738, Time Left=0.08 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3450batch [28:49,  2.07batch/s, Batch Loss=0.0271, Avg Loss=0.0738, Time Left=0.08 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3450batch [28:49,  2.07batch/s, Batch Loss=0.0327, Avg Loss=0.0738, Time Left=0.07 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3451batch [28:49,  2.05batch/s, Batch Loss=0.0327, Avg Loss=0.0738, Time Left=0.07 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3451batch [28:50,  2.05batch/s, Batch Loss=0.0060, Avg Loss=0.0738, Time Left=0.06 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3452batch [28:50,  2.06batch/s, Batch Loss=0.0060, Avg Loss=0.0738, Time Left=0.06 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3452batch [28:50,  2.06batch/s, Batch Loss=0.2673, Avg Loss=0.0738, Time Left=0.05 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3453batch [28:50,  2.06batch/s, Batch Loss=0.2673, Avg Loss=0.0738, Time Left=0.05 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3453batch [28:51,  2.06batch/s, Batch Loss=0.0021, Avg Loss=0.0738, Time Left=0.04 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3454batch [28:51,  2.00batch/s, Batch Loss=0.0021, Avg Loss=0.0738, Time Left=0.04 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3454batch [28:51,  2.00batch/s, Batch Loss=0.2334, Avg Loss=0.0739, Time Left=0.03 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3455batch [28:51,  2.00batch/s, Batch Loss=0.2334, Avg Loss=0.0739, Time Left=0.03 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3455batch [28:52,  2.00batch/s, Batch Loss=0.0018, Avg Loss=0.0738, Time Left=0.03 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3456batch [28:52,  1.98batch/s, Batch Loss=0.0018, Avg Loss=0.0738, Time Left=0.03 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3456batch [28:52,  1.98batch/s, Batch Loss=0.0442, Avg Loss=0.0738, Time Left=0.02 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3457batch [28:52,  1.98batch/s, Batch Loss=0.0442, Avg Loss=0.0738, Time Left=0.02 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3457batch [28:53,  1.98batch/s, Batch Loss=0.0638, Avg Loss=0.0738, Time Left=0.01 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3458batch [28:53,  1.97batch/s, Batch Loss=0.0638, Avg Loss=0.0738, Time Left=0.01 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3458batch [28:53,  1.97batch/s, Batch Loss=0.0252, Avg Loss=0.0738, Time Left=0.00 min]\u001b[A\n",
      "Epoch 3/3 - Training: 3459batch [28:53,  2.32batch/s, Batch Loss=0.0252, Avg Loss=0.0738, Time Left=0.00 min]\u001b[A\n",
      "                                                                                                             \u001b[A\n",
      "Epoch 3/3 - Evaluating:   0%|                                                               | 0/849 [00:00<?, ?batch/s]\u001b[A\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3 Results:\n",
      "Train Loss: 0.0738\n",
      "Validation Loss: 0.0757, Accuracy: 0.9740\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./phobert-finetuned-vietnamese\\\\tokenizer_config.json',\n",
       " './phobert-finetuned-vietnamese\\\\special_tokens_map.json',\n",
       " './phobert-finetuned-vietnamese\\\\vocab.txt',\n",
       " './phobert-finetuned-vietnamese\\\\bpe.codes',\n",
       " './phobert-finetuned-vietnamese\\\\added_tokens.json')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the PhoBERTTrainer\n",
    "trainer = PhoBERTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=16,\n",
    "    lr=2e-5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.fine_tune(epochs=3)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.phobert.save_pretrained(\"./phobert-finetuned-vietnamese\")\n",
    "tokenizer.save_pretrained(\"./phobert-finetuned-vietnamese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3fbc6efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01935733,  0.01708665,  0.03139011, ..., -0.01061608,\n",
       "         0.02952815,  0.05609883],\n",
       "       [-0.0293919 ,  0.01535334,  0.0241307 , ..., -0.00761916,\n",
       "         0.04555051,  0.0392148 ],\n",
       "       [-0.04319261,  0.01367596,  0.01487689, ..., -0.0075201 ,\n",
       "         0.03285275,  0.01059048],\n",
       "       ...,\n",
       "       [-0.03481395,  0.0213473 ,  0.0120395 , ..., -0.01107377,\n",
       "         0.05397564,  0.02819412],\n",
       "       [-0.02788912,  0.01280272,  0.02456283, ..., -0.00767387,\n",
       "         0.04115546,  0.04625097],\n",
       "       [-0.0420851 ,  0.03776307, -0.00973398, ...,  0.02489198,\n",
       "         0.0119226 ,  0.014562  ]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Average pooling function\n",
    "def average_pool(last_hidden_states: torch.Tensor,\n",
    "                 attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "# Function to get embeddings using saved tokenizer and model\n",
    "def get_embeddings2(texts, model_dir):  # `texts` is a list of sentences\n",
    "    # Load the tokenizer and model from the local directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModel.from_pretrained(model_dir)\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    batch_dict = tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_dict)\n",
    "\n",
    "    # Average pooling\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    attention_mask = batch_dict['attention_mask']\n",
    "    embeddings = average_pool(last_hidden_states, attention_mask)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Function to precompute and save embeddings for a list of food names\n",
    "def precompute_embeddings(food_list, model_dir, save_path):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModel.from_pretrained(model_dir)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize and compute embeddings in batches\n",
    "    batch_size = 64  # Adjust batch size for memory usage\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(food_list), batch_size):\n",
    "        batch_texts = food_list[i:i + batch_size]\n",
    "        batch_dict = tokenizer(batch_texts, max_length=64, padding=True, truncation=True, return_tensors='pt')\n",
    "        batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_dict)\n",
    "        \n",
    "        # Average pooling\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        attention_mask = batch_dict['attention_mask']\n",
    "        embeddings = average_pool(last_hidden_states, attention_mask)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all embeddings and save to file\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    np.save(save_path, all_embeddings)  # Save embeddings as .npy file\n",
    "    return all_embeddings\n",
    "\n",
    "df = pd.read_csv(\"./dataset.txt\", sep='|', on_bad_lines='skip', header=None)\n",
    "df = df[[0,2,4]]\n",
    "df.columns = ['description','name','labels']\n",
    "df['labels'] = df['labels'].astype(int)\n",
    "\n",
    "# Example: Precompute and save embeddings\n",
    "model_dir = './phobert-finetuned-vietnamese'\n",
    "food_list = df['name'].unique().tolist()  # Extract unique food names\n",
    "save_path = './food_embeddings.npy'\n",
    "precompute_embeddings(food_list, model_dir, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "be22f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Function to find similar food names using precomputed embeddings\n",
    "def get_similar_vietnamese_food_fast(food_name, food_list, embeddings_path, model_dir, threshold=0.6, limit=10):\n",
    "    # Load precomputed embeddings\n",
    "    food_embeddings = np.load(embeddings_path)\n",
    "    \n",
    "    # Compute embedding for the input food name\n",
    "    input_embedding = get_embeddings2([food_name], model_dir)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(input_embedding, food_embeddings)[0]\n",
    "    \n",
    "    # Pair similarities with corresponding food names\n",
    "    indexed_scores = [(i, score) for i, score in enumerate(similarities) if score > threshold]\n",
    "    \n",
    "    # Sort scores in descending order\n",
    "    sorted_scores = sorted(indexed_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the top `limit` most similar items\n",
    "    top_results = sorted_scores[:limit]\n",
    "    \n",
    "    # Retrieve the corresponding food names\n",
    "    results = [(food_list[i], score) for i, score in top_results]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "faf1cb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Food Description: nước ép cam tươi nguyên chất\n",
      "Top Similar Foods:\n",
      "+ nước ép cam tươi nguyên chất (Similarity: 1.0000)\n",
      "+ nước ép lê táo nguyên chất (Similarity: 0.9998)\n",
      "+ nước ép táo dứa nguyên chất (Similarity: 0.9997)\n",
      "+ nước ép cam nguyên chất (Similarity: 0.9997)\n",
      "+ nước chanh tươi nguyên chất (Similarity: 0.9996)\n",
      "+ nước ép lựu đỏ nguyên chất (Similarity: 0.9995)\n",
      "+ nước ép thơm nguyên chất (Similarity: 0.9995)\n",
      "+ nước ép cam nguyên chất chai (Similarity: 0.9993)\n",
      "+ nước ép cà rốt nguyên chất (Similarity: 0.9993)\n",
      "+ nước ép chai dưa lưới (Similarity: 0.9993)\n",
      "\n",
      "Input Food Description: nuoc ep cam nguyen chat\n",
      "Top Similar Foods:\n",
      "+ combo sôt măm toi vưa (Similarity: 0.9951)\n",
      "+ twister hoac fanta lon (Similarity: 0.9927)\n",
      "+ mega combo (Similarity: 0.9927)\n",
      "+ rice meal (Similarity: 0.9926)\n",
      "+ tra tăc chém gió (Similarity: 0.9922)\n",
      "+ sprite lon (Similarity: 0.9921)\n",
      "+ almond tuile (Similarity: 0.9921)\n",
      "+ chicken sandwich (Similarity: 0.9920)\n",
      "+ cold whisk matcha latte (Similarity: 0.9917)\n",
      "+ olong sen kem cheese (Similarity: 0.9916)\n",
      "\n",
      "Input Food Description: nuoc ep\n",
      "Top Similar Foods:\n",
      "+ nước ép dâu (Similarity: 0.9858)\n",
      "+ dâu tây lắc (Similarity: 0.9835)\n",
      "+ kem xoài tươi (Similarity: 0.9828)\n",
      "+ chả giò chay (Similarity: 0.9820)\n",
      "+ chân vịt rang muối (Similarity: 0.9816)\n",
      "+ nem chua vỏ giòn (Similarity: 0.9815)\n",
      "+ cơm sườn lạp xưởng (Similarity: 0.9815)\n",
      "+ chè dưỡng nhang (Similarity: 0.9804)\n",
      "+ bột chiên (Similarity: 0.9798)\n",
      "+ thạch dừa mini (Similarity: 0.9796)\n",
      "\n",
      "Input Food Description: nước cam\n",
      "Top Similar Foods:\n",
      "+ nước cam (Similarity: 1.0000)\n",
      "+ cafe muối (Similarity: 0.9860)\n",
      "+ cá viên (Similarity: 0.9847)\n",
      "+ nâu đá (Similarity: 0.9819)\n",
      "+ múi đào (Similarity: 0.9808)\n",
      "+ miến trộn (Similarity: 0.9801)\n",
      "+ trứng gà non (Similarity: 0.9784)\n",
      "+ cua viên (Similarity: 0.9776)\n",
      "+ ép cam (Similarity: 0.9775)\n",
      "+ bánh bèo (Similarity: 0.9763)\n",
      "\n",
      "Input Food Description: nuoc cam\n",
      "Top Similar Foods:\n",
      "+ thums up (Similarity: 0.9914)\n",
      "+ coca ly m (Similarity: 0.9902)\n",
      "+ trưng ôp (Similarity: 0.9882)\n",
      "+ cha hai san banh mi (Similarity: 0.9876)\n",
      "+ fanta lon (Similarity: 0.9866)\n",
      "+ nuoc sâu đa (Similarity: 0.9862)\n",
      "+ pina colada (Similarity: 0.9859)\n",
      "+ sting (Similarity: 0.9855)\n",
      "+ dr thanh (Similarity: 0.9831)\n",
      "+ set gift box (Similarity: 0.9828)\n",
      "\n",
      "Input Food Description: cơm đùi gà nướng\n",
      "Top Similar Foods:\n",
      "+ cơm thịt nướng (Similarity: 0.9917)\n",
      "+ cơm đùi gà ta (Similarity: 0.9881)\n",
      "+ cơm sườn nướng (Similarity: 0.9873)\n",
      "+ cơm đùi gà chay (Similarity: 0.9860)\n",
      "+ đĩa thịt gà nướng (Similarity: 0.9860)\n",
      "+ thịt bò xào (Similarity: 0.9839)\n",
      "+ cơm ức gà chặt (Similarity: 0.9819)\n",
      "+ cánh gà cái (Similarity: 0.9819)\n",
      "+ cơm salad ức gà (Similarity: 0.9818)\n",
      "+ cơm ức gà vừa (Similarity: 0.9809)\n",
      "\n",
      "Input Food Description: cơm dui ga\n",
      "Top Similar Foods:\n",
      "+ nui chiên hàu trứng (Similarity: 0.9964)\n",
      "+ cơm cari gà viên (Similarity: 0.9964)\n",
      "+ kim khều inox cái (Similarity: 0.9962)\n",
      "+ bánh bao gà nướng phô mai (Similarity: 0.9961)\n",
      "+ banh chưng rán dồi sụn (Similarity: 0.9960)\n",
      "+ sữa tươi sốt môn tươi (Similarity: 0.9959)\n",
      "+ matcha latte sữa (Similarity: 0.9956)\n",
      "+ bạc sửu cốt dừa (Similarity: 0.9956)\n",
      "+ olong sữa rang đậm vị (Similarity: 0.9956)\n",
      "+ tré trộn lớn da gà (Similarity: 0.9956)\n",
      "\n",
      "Input Food Description: tra sữa\n",
      "Top Similar Foods:\n",
      "+ bún nắm (Similarity: 0.9915)\n",
      "+ trà đào full (Similarity: 0.9912)\n",
      "+ gừng thêm (Similarity: 0.9909)\n",
      "+ tra dâu đo (Similarity: 0.9907)\n",
      "+ trà đác thơm (Similarity: 0.9906)\n",
      "+ trà dâu dầm (Similarity: 0.9900)\n",
      "+ trà dứa mật (Similarity: 0.9900)\n",
      "+ trà nho xanh (Similarity: 0.9895)\n",
      "+ latte nóng (Similarity: 0.9894)\n",
      "+ trà chanh đá (Similarity: 0.9893)\n",
      "\n",
      "Input Food Description: trà sữa phúc long\n",
      "Top Similar Foods:\n",
      "+ trà sữa phúc long (Similarity: 1.0000)\n",
      "+ trà sữa ô long đà lạt (Similarity: 0.9990)\n",
      "+ trà sữa đường đen (Similarity: 0.9990)\n",
      "+ trà sữa lipton (Similarity: 0.9989)\n",
      "+ trà sữa lài l (Similarity: 0.9989)\n",
      "+ trà sữa bạc hà the mát (Similarity: 0.9988)\n",
      "+ trà sữa vị vải (Similarity: 0.9988)\n",
      "+ trà sữa wah (Similarity: 0.9988)\n",
      "+ trà olong chanh vàng (Similarity: 0.9988)\n",
      "+ trà sữa nê bon (Similarity: 0.9987)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test function\n",
    "def test_local_model_fast():\n",
    "    # Define the saved model directory and embeddings path\n",
    "    model_dir = './phobert-finetuned-vietnamese'\n",
    "    embeddings_path = './food_embeddings.npy'\n",
    "    \n",
    "    # Example food descriptions\n",
    "    food_names = [\"nước ép cam tươi nguyên chất\",\n",
    "                  \"nuoc ep cam nguyen chat\",\n",
    "                  \"nuoc ep\",\n",
    "                  \"nước cam\",\n",
    "                  \"nuoc cam\",\n",
    "                  \"cơm đùi gà nướng\",\n",
    "                  \"cơm dui ga\",\n",
    "                  \"tra sữa\",\n",
    "                  \"trà sữa phúc long\"]\n",
    "\n",
    "    # Similarity threshold\n",
    "    threshold = 0.6\n",
    "\n",
    "    for food_name in food_names:\n",
    "        # Get similar food names\n",
    "        similar_foods = get_similar_vietnamese_food_fast(food_name, df['name'].unique().tolist(), embeddings_path, model_dir, threshold, limit=10)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Input Food Description: {food_name}\")\n",
    "        print(\"Top Similar Foods:\")\n",
    "        for food, score in similar_foods:\n",
    "            print(f\"+ {food} (Similarity: {score:.4f})\")\n",
    "        print()\n",
    "test_local_model_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0de600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel\n",
    "import torch\n",
    "from functools import partial\n",
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, r, alpha):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Initialize A to kaiming uniform following code: https://github.com/microsoft/LoRA/blob/main/loralib/layers.py\n",
    "        self.A = torch.nn.Parameter(torch.empty(r, in_dim))\n",
    "        # Initialize B to zeros.\n",
    "        self.B = torch.nn.Parameter(torch.empty(out_dim, r))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        torch.nn.init.zeros_(self.B)\n",
    "\n",
    "        self.scaling = self.alpha / self.r\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.scaling * (x @ self.A.transpose(0, 1) @ self.B.transpose(0, 1))\n",
    "        return x\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, r, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, r, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "class RobertaWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, base_model_name, r=16, alpha=32, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.base_model = RobertaModel.from_pretrained(base_model_name)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
    "\n",
    "        # Apply LoRA to the attention and intermediate layers\n",
    "        self.apply_lora(self.base_model, r, alpha)\n",
    "\n",
    "    def apply_lora(self, model, r, alpha):\n",
    "        assign_lora = partial(LinearWithLoRA, r=r, alpha=alpha)\n",
    "\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                # Replace the linear layer with a LoRA-enhanced version\n",
    "                setattr(\n",
    "                    module,\n",
    "                    \"forward\",\n",
    "                    assign_lora(module).forward,\n",
    "                )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # Use [CLS] token representation\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46970ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer and model from the local directory\n",
    "model_dir = \"vinai/phobert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Initialize the LoRA-enhanced model\n",
    "num_labels = 2  # Change according to your task\n",
    "lora_r = 8\n",
    "lora_alpha = lora_r * 2\n",
    "lora_model = RobertaWithLoRA(base_model_name=model_dir, r=lora_r, alpha=lora_alpha, num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5a3848e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters without LoRA: 135590402\n",
      "Trainable parameters with LoRA: 134999810\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Trainable parameters without LoRA:\", count_trainable_params(model))\n",
    "print(\"Trainable parameters with LoRA:\", count_trainable_params(lora_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a500b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\envs\\tf26\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 - Training:   0%|                                                                | 0/6785 [00:00<?, ?batch/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Train the LoRA-enhanced model\n",
    "trainer = PhoBERTTrainer(\n",
    "    model=lora_model, \n",
    "    tokenizer=tokenizer, \n",
    "    train_dataset=train_dataset, \n",
    "    val_dataset=val_dataset, \n",
    "    batch_size=8, \n",
    "    lr=5e-5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "trainer.fine_tune(epochs=3)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "lora_model.phobert.save_pretrained(\"./phobert-finetuned-lora-vietnamese\")\n",
    "tokenizer.save_pretrained(\"./phobert-finetuned-lora-vietnamese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00369a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf26",
   "language": "python",
   "name": "tf26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
